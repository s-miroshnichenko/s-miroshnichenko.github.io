[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Блог",
    "section": "",
    "text": "Конфигурация молекулы ДНК у прокариотов\n\n\n\n\n\n\n\n\nOct 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nКонфигурация молекулы ДНК у прокариотов\n\n\n\n\n\n\n\n\nOct 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nЧто такое теломера и почему она укорачивается?\n\n\n\n\n\n\n\n\nOct 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nУлучшения генома homo sapiens\n\n\n\n\n\n\n\n\nOct 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCollaborative filtering\n\n\n\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nВозвращение к курсу ML от fast.ai\n\n\n\n\n\n\n\n\nOct 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nПочему я завел этот блог\n\n\n\n\n\n\n\n\nSep 25, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Fastbook/README_ar.html",
    "href": "Fastbook/README_ar.html",
    "title": "Fastai كتاب",
    "section": "",
    "text": "English / Spanish / Korean / Chinese / Bengali / Indonesian / Italian / Portuguese / Vietnamese / Arabic"
  },
  {
    "objectID": "Fastbook/README_ar.html#colab-غوغل-كولاب",
    "href": "Fastbook/README_ar.html#colab-غوغل-كولاب",
    "title": "Fastai كتاب",
    "section": "Colab غوغل كولاب",
    "text": "Colab غوغل كولاب\nيمكنك القراءة والعمل مع دفاتر الملاحظات باستخدام غوغل كولاب كبديل عوضا عن استنساخ هذا المستودع وفتحه على جهازك\nهذا هو الأسلوب الموصى به بالنسبة للأشخاص المبتدئين في هذا المجال حيث يمكنك العمل مباشرة في متصفح الويب الخاص بك و ليست هناك حاجة لإعداد بيئة تطوير بايثون على جهازك الخاص\n:يمكنك فتح أي فصل من الكتاب في منصة غوغل كولاب بالضغط على أحد هذه الروابط ادناه\nمقدمة إلى جوبيتر| الفصل 1، المقدمة | الفصل 2، من النموذج إلى الإنتاج |الفصل 3، أخلاقيات معالحة البيانات | الفصل 4، تدريب مصنف أرقام| الفصل 5، تصنيف الصور | الفصل 6، تصنيف الصور（تابع） | الفصل 7، تدريب نموذج حديث | الفصل 8 ،نبذة مفصلة في التصفية التعاونية | الفصل 9، نبذة مفصلة في النمذجة الجدولية |الفصل 10، معالجة اللغات الطبيعية | الفصل 11، واجهة برمجة التطبيقات للمستوى المتوسط | الفصل 12، نبذة مفصلة في معالجة اللغات الطبيعية | الفصل 13، الشبكات العصبية التلافيفية | الفصل 14، ريسنيت | الفصل 15، هندسة التطبيقات، نبذة مفصلة | الفصل 16، عملية التدريب | الفصل 17، الشبكة العصبية من الأسس | الفصل 18، تفسير الشبكات العصبية | fastai الفصل 19، متعلم سريع من الصفر |\nالفصل 20، الخاتمة"
  },
  {
    "objectID": "Fastbook/README_ar.html#مساهمات",
    "href": "Fastbook/README_ar.html#مساهمات",
    "title": "Fastai كتاب",
    "section": "مساهمات",
    "text": "مساهمات\nإذا قمت بتقديم أي طلبات سحب لهذا المستودع، فإنك تقوم بتعيين حقوق الطبع والنشر لهذا العمل لجيريمي هوارد وسيلفان جوجر. (بالإضافة إلى ذلك، إذا كنت تقوم بإجراء تعديلات صغيرة على التهجئة أو النص، فيرجى تحديد اسم الملف وارفاق وصف موجز لما قمت بتعديله لتسهيل عمل المراجعين في معرفة التصحيحات التي تم إجراؤها بالفعل. شكرًا لك.)"
  },
  {
    "objectID": "Fastbook/README_ar.html#اقتباسات",
    "href": "Fastbook/README_ar.html#اقتباسات",
    "title": "Fastai كتاب",
    "section": "اقتباسات",
    "text": "اقتباسات\n:إذا أردت الاستشهاد بالكتاب يمكنك استخدام الصيفة الموضحة في الاسفل\n@book{howard2020deep,\ntitle={Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD},\nauthor={Howard, J. and Gugger, S.},\nisbn={9781492045526},\nurl={https://books.google.no/books?id=xd6LxgEACAAJ},\nyear={2020},\npublisher={O'Reilly Media, Incorporated}\n}"
  },
  {
    "objectID": "Fastbook/translations/cn/14_resnet.html",
    "href": "Fastbook/translations/cn/14_resnet.html",
    "title": "第十四章：ResNets",
    "section": "",
    "text": "在本章中，我们将在上一章介绍的 CNN 基础上构建，并向您解释 ResNet（残差网络）架构。它是由 Kaiming He 等人于 2015 年在文章“Deep Residual Learning for Image Recognition”中引入的，到目前为止是最常用的模型架构。最近在图像模型中的发展几乎总是使用残差连接的相同技巧，大多数时候，它们只是原始 ResNet 的调整。\n我们将首先展示最初设计的基本 ResNet，然后解释使其性能更好的现代调整。但首先，我们需要一个比 MNIST 数据集更难一点的问题，因为我们已经在常规 CNN 上接近 100%的准确率了。"
  },
  {
    "objectID": "Fastbook/translations/cn/14_resnet.html#跳跃连接",
    "href": "Fastbook/translations/cn/14_resnet.html#跳跃连接",
    "title": "第十四章：ResNets",
    "section": "跳跃连接",
    "text": "跳跃连接\n2015 年，ResNet 论文的作者们注意到了一件他们觉得奇怪的事情。即使使用了批量归一化，他们发现使用更多层的网络表现不如使用更少层的网络，并且模型之间没有其他差异。最有趣的是，这种差异不仅在验证集中观察到，而且在训练集中也观察到；因此这不仅仅是一个泛化问题，而是一个训练问题。正如论文所解释的：\n\n出乎意料的是，这种退化并不是由过拟合引起的，向适当深度的模型添加更多层会导致更高的训练错误，正如我们的实验[先前报告]和彻底验证的那样。\n\n这种现象在图 14-1 中的图表中有所说明，左侧是训练错误，右侧是测试错误。\n\n\n\n不同深度网络的训练\n\n\n\n图 14-1。不同深度网络的训练（由 Kaiming He 等人提供）。\n正如作者在这里提到的，他们并不是第一个注意到这个奇怪事实的人。但他们是第一个迈出非常重要的一步：\n\n让我们考虑一个更浅的架构及其更深的对应物，后者在其上添加更多层。存在一种通过构建解决更深模型的方法：添加的层是恒等映射，其他层是从学习的更浅模型中复制的。\n\n由于这是一篇学术论文，这个过程以一种不太易懂的方式描述，但概念实际上非常简单：从一个训练良好的 20 层神经网络开始，然后添加另外 36 层什么都不做的层（例如，它们可以是具有单个权重等于 1 和偏置等于 0 的线性层）。结果将是一个 56 层的网络，它与 20 层网络完全相同，证明总是存在深度网络应该至少和任何浅层网络一样好。但由于某种原因，随机梯度下降似乎无法找到它们。"
  },
  {
    "objectID": "Fastbook/translations/cn/14_resnet.html#一个最先进的-resnet",
    "href": "Fastbook/translations/cn/14_resnet.html#一个最先进的-resnet",
    "title": "第十四章：ResNets",
    "section": "一个最先进的 ResNet",
    "text": "一个最先进的 ResNet\n在“用卷积神经网络进行图像分类的技巧”中，Tong He 等人研究了 ResNet 架构的变体，这几乎没有额外的参数或计算成本。通过使用调整后的 ResNet-50 架构和 Mixup，他们在 ImageNet 上实现了 94.6%的 Top-5 准确率，而普通的 ResNet-50 没有 Mixup 只有 92.2%。这个结果比普通 ResNet 模型取得的结果更好，后者深度是它的两倍（速度也是两倍，更容易过拟合）。"
  },
  {
    "objectID": "Fastbook/translations/cn/14_resnet.html#瓶颈层",
    "href": "Fastbook/translations/cn/14_resnet.html#瓶颈层",
    "title": "第十四章：ResNets",
    "section": "瓶颈层",
    "text": "瓶颈层\n瓶颈层不是使用 3 个内核大小为 3 的卷积堆叠，而是使用三个卷积：两个 1×1（在开头和结尾）和一个 3×3，如右侧在图 14-4 中所示。\n\n\n\n常规和瓶颈 ResNet 块的比较\n\n\n\n图 14-4. 常规和瓶颈 ResNet 块的比较（由 Kaiming He 等人提供）\n为什么这很有用？1×1 卷积速度更快，因此即使这似乎是一个更复杂的设计，这个块的执行速度比我们看到的第一个 ResNet 块更快。这样一来，我们可以使用更多的滤波器：正如我们在插图中看到的，输入和输出的滤波器数量是四倍更高的（256 而不是 64）。1×1 卷积减少然后恢复通道数（因此称为瓶颈）。总体影响是我们可以在相同的时间内使用更多的滤波器。\n让我们尝试用这种瓶颈设计替换我们的ResBlock：\ndef _conv_block(ni,nf,stride):\n    return nn.Sequential(\n        ConvLayer(ni, nf//4, 1),\n        ConvLayer(nf//4, nf//4, stride=stride),\n        ConvLayer(nf//4, nf, 1, act_cls=None, norm_type=NormType.BatchZero))\n我们将使用这个来创建一个具有组大小(3,4,6,3)的 ResNet-50。现在我们需要将4传递给ResNet的expansion参数，因为我们需要从四倍少的通道开始，最终将以四倍多的通道结束。\n像这样更深的网络通常在仅训练 5 个时期时不会显示出改进，所以这次我们将将其增加到 20 个时期，以充分利用我们更大的模型。为了获得更好的结果，让我们也使用更大的图像：\ndls = get_data(URLs.IMAGENETTE_320, presize=320, resize=224)\n我们不必为更大的 224 像素图像做任何调整；由于我们的全卷积网络，它可以正常工作。这也是为什么我们能够在本书的早期进行渐进调整的原因——我们使用的模型是全卷积的，所以我们甚至能够微调使用不同尺寸训练的模型。现在我们可以训练我们的模型并查看效果：\nrn = ResNet(dls.c, [3,4,6,3], 4)\nlearn = get_learner(rn)\nlearn.fit_one_cycle(20, 3e-3)\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.613448\n1.473355\n0.514140\n00:31\n\n\n1\n1.359604\n2.050794\n0.397452\n00:31\n\n\n2\n1.253112\n4.511735\n0.387006\n00:31\n\n\n3\n1.133450\n2.575221\n0.396178\n00:31\n\n\n4\n1.054752\n1.264525\n0.613758\n00:32\n\n\n5\n0.927930\n2.670484\n0.422675\n00:32\n\n\n6\n0.838268\n1.724588\n0.528662\n00:32\n\n\n7\n0.748289\n1.180668\n0.666497\n00:31\n\n\n8\n0.688637\n1.245039\n0.650446\n00:32\n\n\n9\n0.645530\n1.053691\n0.674904\n00:31\n\n\n10\n0.593401\n1.180786\n0.676433\n00:32\n\n\n11\n0.536634\n0.879937\n0.713885\n00:32\n\n\n12\n0.479208\n0.798356\n0.741656\n00:32\n\n\n13\n0.440071\n0.600644\n0.806879\n00:32\n\n\n14\n0.402952\n0.450296\n0.858599\n00:32\n\n\n15\n0.359117\n0.486126\n0.846369\n00:32\n\n\n16\n0.313642\n0.442215\n0.861911\n00:32\n\n\n17\n0.294050\n0.485967\n0.853503\n00:32\n\n\n18\n0.270583\n0.408566\n0.875924\n00:32\n\n\n19\n0.266003\n0.411752\n0.872611\n00:33\n\n\n\n现在我们得到了一个很好的结果！尝试添加 Mixup，然后在吃午餐时将其训练一百个时期。你将拥有一个从头开始训练的非常准确的图像分类器。\n这里展示的瓶颈设计通常仅用于 ResNet-50、-101 和-152 模型。ResNet-18 和-34 模型通常使用前一节中看到的非瓶颈设计。然而，我们注意到瓶颈层通常即使对于较浅的网络也效果更好。这只是表明，论文中的细节往往会持续多年，即使它们并不是最佳设计！质疑假设和“每个人都知道的东西”总是一个好主意，因为这仍然是一个新领域，很多细节并不总是做得很好。"
  },
  {
    "objectID": "Fastbook/translations/cn/14_resnet.html#进一步研究",
    "href": "Fastbook/translations/cn/14_resnet.html#进一步研究",
    "title": "第十四章：ResNets",
    "section": "进一步研究",
    "text": "进一步研究\n\n尝试为 MNIST 创建一个带有自适应平均池化的完全卷积网络（请注意，您将需要更少的步幅为 2 的层）。与没有这种池化层的网络相比如何？\n在第十七章中，我们介绍了爱因斯坦求和符号。快进去看看它是如何工作的，然后使用torch.einsum编写一个 1×1 卷积操作的实现。将其与使用torch.conv2d进行相同操作进行比较。\n使用纯 PyTorch 或纯 Python 编写一个前 5 准确度函数。\n在 Imagenette 上训练一个模型更多的 epochs，使用和不使用标签平滑。查看 Imagenette 排行榜，看看你能达到最佳结果有多接近。阅读描述领先方法的链接页面。"
  },
  {
    "objectID": "Fastbook/translations/cn/06_multicat.html",
    "href": "Fastbook/translations/cn/06_multicat.html",
    "title": "第六章：其他计算机视觉问题",
    "section": "",
    "text": "在上一章中，你学习了一些在实践中训练模型的重要技术。选择学习率和周期数等考虑因素对于获得良好结果非常重要。\n在本章中，我们将看到另外两种计算机视觉问题：多标签分类和回归。第一种情况发生在你想要预测每个图像的多个标签（有时甚至没有标签），第二种情况发生在你的标签是一个或多个数字——数量而不是类别。\n在这个过程中，我们将更深入地研究深度学习模型中的输出激活、目标和损失函数。"
  },
  {
    "objectID": "Fastbook/translations/cn/06_multicat.html#数据",
    "href": "Fastbook/translations/cn/06_multicat.html#数据",
    "title": "第六章：其他计算机视觉问题",
    "section": "数据",
    "text": "数据\n对于我们的示例，我们将使用 PASCAL 数据集，该数据集中的每个图像可以有多种分类对象。\n我们首先按照通常的方式下载和提取数据集：\nfrom fastai.vision.all import *\npath = untar_data(URLs.PASCAL_2007)\n这个数据集与我们之前看到的不同，它不是按文件名或文件夹结构化的，而是附带一个 CSV 文件，告诉我们每个图像要使用的标签。我们可以通过将其读入 Pandas DataFrame 来检查 CSV 文件：\ndf = pd.read_csv(path/'train.csv')\ndf.head()\n\n\n\n\n文件名\n标签\n是否有效\n\n\n\n\n0\n000005.jpg\n椅子\nTrue\n\n\n1\n000007.jpg\n汽车\nTrue\n\n\n2\n000009.jpg\n马 人\nTrue\n\n\n3\n000012.jpg\n汽车\nFalse\n\n\n4\n000016.jpg\n自行车\nTrue\n\n\n\n正如你所看到的，每个图像中的类别列表显示为一个以空格分隔的字符串。\n既然我们已经看到了数据的样子，让我们准备好进行模型训练。"
  },
  {
    "objectID": "Fastbook/translations/cn/06_multicat.html#构建数据块",
    "href": "Fastbook/translations/cn/06_multicat.html#构建数据块",
    "title": "第六章：其他计算机视觉问题",
    "section": "构建数据块",
    "text": "构建数据块\n我们如何将DataFrame对象转换为DataLoaders对象？我们通常建议在可能的情况下使用数据块 API 来创建DataLoaders对象，因为它提供了灵活性和简单性的良好组合。在这里，我们将展示使用数据块 API 构建DataLoaders对象的实践步骤，以这个数据集为例。\n正如我们所看到的，PyTorch 和 fastai 有两个主要类用于表示和访问训练集或验证集：\n数据集\n返回单个项目的独立变量和依赖变量的元组的集合\n数据加载器\n提供一系列小批量的迭代器，其中每个小批量是一批独立变量和一批因变量的组合\n除此之外，fastai 还提供了两个类来将您的训练和验证集合在一起：\nDatasets\n包含一个训练Dataset和一个验证Dataset的迭代器\nDataLoaders\n包含一个训练DataLoader和一个验证DataLoader的对象\n由于DataLoader是建立在Dataset之上并为其添加附加功能（将多个项目整合成一个小批量），通常最容易的方法是首先创建和测试Datasets，然后再查看DataLoaders。\n当我们创建DataBlock时，我们逐步逐步构建，并使用笔记本检查我们的数据。这是一个很好的方式，可以确保您在编码时保持动力，并留意任何问题。易于调试，因为您知道如果出现问题，它就在您刚刚输入的代码行中！\n让我们从没有参数创建的数据块开始，这是最简单的情况：\ndblock = DataBlock()\n我们可以从中创建一个Datasets对象。唯一需要的是一个源——在这种情况下是我们的 DataFrame：\ndsets = dblock.datasets(df)\n这包含一个train和一个valid数据集，我们可以对其进行索引：\ndsets.train[0]\n(fname       008663.jpg\n labels      car person\n is_valid    False\n Name: 4346, dtype: object,\n fname       008663.jpg\n labels      car person\n is_valid    False\n Name: 4346, dtype: object)\n正如您所看到的，这只是简单地两次返回 DataFrame 的一行。这是因为默认情况下，数据块假定我们有两个东西：输入和目标。我们需要从 DataFrame 中获取适当的字段，可以通过传递get_x和get_y函数来实现：\ndblock = DataBlock(get_x = lambda r: r['fname'], get_y = lambda r: r['labels'])\ndsets = dblock.datasets(df)\ndsets.train[0]\n('005620.jpg', 'aeroplane')\n正如您所看到的，我们并没有以通常的方式定义函数，而是使用了 Python 的lambda关键字。这只是定义并引用函数的一种快捷方式。以下更冗长的方法是相同的：\ndef get_x(r): return r['fname']\ndef get_y(r): return r['labels']\ndblock = DataBlock(get_x = get_x, get_y = get_y)\ndsets = dblock.datasets(df)\ndsets.train[0]\n('002549.jpg', 'tvmonitor')\nLambda 函数非常适合快速迭代，但不兼容序列化，因此我们建议您在训练后要导出您的Learner时使用更冗长的方法（如果您只是在尝试实验，lambda 是可以的）。\n我们可以看到独立变量需要转换为完整路径，以便我们可以将其作为图像打开，而因变量需要根据空格字符（这是 Python 的split函数的默认值）进行拆分，以便它变成一个列表：\ndef get_x(r): return path/'train'/r['fname']\ndef get_y(r): return r['labels'].split(' ')\ndblock = DataBlock(get_x = get_x, get_y = get_y)\ndsets = dblock.datasets(df)\ndsets.train[0]\n(Path('/home/sgugger/.fastai/data/pascal_2007/train/008663.jpg'),\n ['car', 'person'])\n要实际打开图像并将其转换为张量，我们需要使用一组转换；块类型将为我们提供这些。我们可以使用先前使用过的相同块类型，只有一个例外：ImageBlock将再次正常工作，因为我们有一个指向有效图像的路径，但CategoryBlock不会起作用。问题在于该块返回一个单个整数，但我们需要为每个项目有多个标签。为了解决这个问题，我们使用MultiCategoryBlock。这种类型的块期望接收一个字符串列表，就像我们在这种情况下所做的那样，所以让我们来测试一下：\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   get_x = get_x, get_y = get_y)\ndsets = dblock.datasets(df)\ndsets.train[0]\n(PILImage mode=RGB size=500x375,\n TensorMultiCategory([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n &gt; 0., 0., 0., 0., 0., 0.]))\n正如您所看到的，我们的类别列表的编码方式与常规的CategoryBlock不同。在那种情况下，我们有一个整数表示哪个类别存在，基于它在我们的词汇表中的位置。然而，在这种情况下，我们有一系列 0，其中任何位置上有一个 1 表示该类别存在。例如，如果第二和第四位置上有一个 1，那意味着词汇项二和四在这个图像中存在。这被称为独热编码。我们不能简单地使用类别索引列表的原因是每个列表的长度都不同，而 PyTorch 需要张量，其中所有内容必须是相同长度。"
  },
  {
    "objectID": "Fastbook/translations/cn/06_multicat.html#二元交叉熵",
    "href": "Fastbook/translations/cn/06_multicat.html#二元交叉熵",
    "title": "第六章：其他计算机视觉问题",
    "section": "二元交叉熵",
    "text": "二元交叉熵\n现在我们将创建我们的Learner。我们在第四章中看到，Learner对象包含四个主要内容：模型、DataLoaders对象、优化器和要使用的损失函数。我们已经有了我们的DataLoaders，我们可以利用 fastai 的resnet模型（稍后我们将学习如何从头开始创建），并且我们知道如何创建一个SGD优化器。因此，让我们专注于确保我们有一个合适的损失函数。为此，让我们使用cnn_learner创建一个Learner，这样我们就可以查看它的激活：\nlearn = cnn_learner(dls, resnet18)\n我们还看到，Learner中的模型通常是从nn.Module继承的类的对象，并且我们可以使用括号调用它，它将返回模型的激活。你应该将独立变量作为一个小批量传递给它。我们可以尝试从我们的DataLoader中获取一个小批量，然后将其传递给模型：\nx,y = dls.train.one_batch()\nactivs = learn.model(x)\nactivs.shape\ntorch.Size([64, 20])\n想想为什么activs有这种形状——我们的批量大小为 64，我们需要计算 20 个类别中的每一个的概率。这是其中一个激活的样子：\nactivs[0]\ntensor([ 2.0258, -1.3543,  1.4640,  1.7754, -1.2820, -5.8053,  3.6130,  0.7193,\n &gt; -4.3683, -2.5001, -2.8373, -1.8037,  2.0122,  0.6189,  1.9729,  0.8999,\n &gt; -2.6769, -0.3829,  1.2212,  1.6073],\n       device='cuda:0', grad_fn=&lt;SelectBackward&gt;)"
  },
  {
    "objectID": "Fastbook/translations/cn/06_multicat.html#数据组装",
    "href": "Fastbook/translations/cn/06_multicat.html#数据组装",
    "title": "第六章：其他计算机视觉问题",
    "section": "数据组装",
    "text": "数据组装\n我们将在这一部分使用Biwi Kinect Head Pose 数据集。我们将像往常一样开始下载数据集：\npath = untar_data(URLs.BIWI_HEAD_POSE)\n让我们看看我们有什么！\npath.ls()\n(#50) [Path('13.obj'),Path('07.obj'),Path('06.obj'),Path('13'),Path('10'),Path('\n &gt; 02'),Path('11'),Path('01'),Path('20.obj'),Path('17')...]\n有 24 个从 01 到 24 编号的目录（它们对应不同的被摄人物），以及每个目录对应的.obj文件（我们这里不需要）。让我们看看其中一个目录的内容：\n(path/'01').ls()\n(#1000) [Path('01/frame_00281_pose.txt'),Path('01/frame_00078_pose.txt'),Path('0\n &gt; 1/frame_00349_rgb.jpg'),Path('01/frame_00304_pose.txt'),Path('01/frame_00207_\n &gt; pose.txt'),Path('01/frame_00116_rgb.jpg'),Path('01/frame_00084_rgb.jpg'),Path\n &gt; ('01/frame_00070_rgb.jpg'),Path('01/frame_00125_pose.txt'),Path('01/frame_003\n &gt; 24_rgb.jpg')...]\n在子目录中，我们有不同的帧。每个帧都带有一个图像（*_rgb.jpg）和一个姿势文件（_pose.txt*）。我们可以使用get_image_files轻松递归获取所有图像文件，然后编写一个函数，将图像文件名转换为其关联的姿势文件：\nimg_files = get_image_files(path)\ndef img2pose(x): return Path(f'{str(x)[:-7]}pose.txt')\nimg2pose(img_files[0])\nPath('13/frame_00349_pose.txt')\n让我们来看看我们的第一张图片：\nim = PILImage.create(img_files[0])\nim.shape\n(480, 640)\nim.to_thumb(160)\n\nBiwi 数据集网站用于解释与每个图像关联的姿势文本文件的格式，显示头部中心的位置。这些细节对我们来说并不重要，所以我们只会展示我们用来提取头部中心点的函数：\ncal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)\ndef get_ctr(f):\n    ctr = np.genfromtxt(img2pose(f), skip_header=3)\n    c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n    c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n    return tensor([c1,c2])\n这个函数将坐标作为两个项目的张量返回：\nget_ctr(img_files[0])\ntensor([384.6370, 259.4787])\n我们可以将此函数传递给DataBlock作为get_y，因为它负责为每个项目标记。我们将将图像调整为其输入大小的一半，以加快训练速度。\n一个重要的要点是我们不应该只使用随机分割器。在这个数据集中，同一个人出现在多个图像中，但我们希望确保我们的模型可以泛化到它尚未见过的人。数据集中的每个文件夹包含一个人的图像。因此，我们可以创建一个分割器函数，仅为一个人返回True，从而使验证集仅包含该人的图像。\n与以前的数据块示例的唯一区别是第二个块是PointBlock。这是必要的，以便 fastai 知道标签代表坐标；这样，它就知道在进行数据增强时，应该对这些坐标执行与图像相同的增强：\nbiwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    get_y=get_ctr,\n    splitter=FuncSplitter(lambda o: o.parent.name=='13'),\n    batch_tfms=[*aug_transforms(size=(240,320)),\n                Normalize.from_stats(*imagenet_stats)]\n)"
  },
  {
    "objectID": "Fastbook/translations/cn/06_multicat.html#训练模型",
    "href": "Fastbook/translations/cn/06_multicat.html#训练模型",
    "title": "第六章：其他计算机视觉问题",
    "section": "训练模型",
    "text": "训练模型\n像往常一样，我们可以使用cnn_learner来创建我们的Learner。还记得在第一章中我们如何使用y_range告诉 fastai 我们目标的范围吗？我们将在这里做同样的事情（fastai 和 PyTorch 中的坐标始终在-1 和+1 之间重新缩放）：\nlearn = cnn_learner(dls, resnet18, y_range=(-1,1))\ny_range在 fastai 中使用sigmoid_range实现，其定义如下：\ndef sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo\n如果定义了y_range，则将其设置为模型的最终层。花点时间思考一下这个函数的作用，以及为什么它强制模型在范围(lo,hi)内输出激活。\n这是它的样子：\nplot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4)\n\n我们没有指定损失函数，这意味着我们得到了 fastai 选择的默认值。让我们看看它为我们选择了什么：\ndls.loss_func\nFlattenedLoss of MSELoss()\n这是有道理的，因为当坐标被用作因变量时，大多数情况下我们可能会尽可能地预测接近某个值；这基本上就是 MSELoss（均方误差损失）所做的。如果你想使用不同的损失函数，你可以通过使用 loss_func 参数将其传递给 cnn_learner。\n还要注意，我们没有指定任何指标。这是因为均方误差已经是这个任务的一个有用指标（尽管在我们取平方根之后可能更易解释）。\n我们可以使用学习率查找器选择一个好的学习率：\nlearn.lr_find()\n\n我们将尝试一个学习率为 2e-2：\nlr = 2e-2\nlearn.fit_one_cycle(5, lr)\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.045840\n0.012957\n00:36\n\n\n1\n0.006369\n0.001853\n00:36\n\n\n2\n0.003000\n0.000496\n00:37\n\n\n3\n0.001963\n0.000360\n00:37\n\n\n4\n0.001584\n0.000116\n00:36\n\n\n\n通常情况下，当我们运行这个时，我们得到的损失大约是 0.0001，这对应于这个平均坐标预测误差：\nmath.sqrt(0.0001)\n0.01\n这听起来非常准确！但是重要的是要用 Learner.show_results 查看我们的结果。左侧是实际（真实）坐标，右侧是我们模型的预测：\nlearn.show_results(ds_idx=1, max_n=3, figsize=(6,8))\n\n令人惊讶的是，仅仅几分钟的计算，我们就创建了一个如此准确的关键点模型，而且没有任何特定领域的应用。这就是在灵活的 API 上构建并使用迁移学习的力量！特别引人注目的是，我们能够如此有效地使用迁移学习，即使在完全不同的任务之间；我们的预训练模型是用来进行图像分类的，而我们对图像回归进行了微调。"
  },
  {
    "objectID": "Fastbook/translations/cn/06_multicat.html#进一步研究",
    "href": "Fastbook/translations/cn/06_multicat.html#进一步研究",
    "title": "第六章：其他计算机视觉问题",
    "section": "进一步研究",
    "text": "进一步研究\n\n阅读关于 Pandas DataFrames 的教程，并尝试一些看起来有趣的方法。查看书籍网站上推荐的教程。\n使用多标签分类重新训练熊分类器。看看你是否可以使其有效地处理不包含任何熊的图像，包括在 Web 应用程序中显示该信息。尝试一张包含两种熊的图像。检查在单标签数据集上使用多标签分类是否会影响准确性。"
  },
  {
    "objectID": "Fastbook/translations/cn/03_ethics.html",
    "href": "Fastbook/translations/cn/03_ethics.html",
    "title": "第三章：数据伦理",
    "section": "",
    "text": "正如我们在第一章和第二章中讨论的，有时机器学习模型可能出错。它们可能有错误。它们可能被呈现出以前没有见过的数据，并以我们意料之外的方式行事。或者它们可能完全按设计工作，但被用于我们非常希望它们永远不要被用于的事情。\n因为深度学习是如此强大的工具，可以用于很多事情，所以我们特别需要考虑我们选择的后果。哲学上对伦理的研究是对对错的研究，包括我们如何定义这些术语，识别对错行为，以及理解行为和后果之间的联系。数据伦理领域已经存在很长时间，许多学者都专注于这个领域。它被用来帮助定义许多司法管辖区的政策；它被用在大大小小的公司中，考虑如何最好地确保产品开发对社会的良好结果；它被研究人员用来确保他们正在做的工作被用于好的目的，而不是坏的目的。\n因此，作为一个深度学习从业者，你很可能在某个时候会面临需要考虑数据伦理的情况。那么数据伦理是什么？它是伦理学的一个子领域，所以让我们从那里开始。"
  },
  {
    "objectID": "Fastbook/translations/cn/03_ethics.html#错误和救济用于医疗福利的错误算法",
    "href": "Fastbook/translations/cn/03_ethics.html#错误和救济用于医疗福利的错误算法",
    "title": "第三章：数据伦理",
    "section": "错误和救济：用于医疗福利的错误算法",
    "text": "错误和救济：用于医疗福利的错误算法\n《The Verge》调查了在美国半数以上州使用的软件，以确定人们接受多少医疗保健，并在文章《当算法削减您的医疗保健时会发生什么》中记录了其发现。在阿肯色州实施算法后，数百人（许多患有严重残疾的人）的医疗保健被大幅削减。\n例如，Tammy Dobbs 是一名患有脑瘫的女性，需要助手帮助她起床、上厕所、拿食物等，她的帮助时间突然减少了 20 小时每周。她无法得到任何解释为什么她的医疗保健被削减。最终，一场法庭案件揭示了算法的软件实施中存在错误，对患有糖尿病或脑瘫的人造成了负面影响。然而，Dobbs 和许多其他依赖这些医疗福利的人生活在恐惧中，担心他们的福利可能再次突然而莫名其妙地被削减。"
  },
  {
    "objectID": "Fastbook/translations/cn/03_ethics.html#反馈循环youtube-的推荐系统",
    "href": "Fastbook/translations/cn/03_ethics.html#反馈循环youtube-的推荐系统",
    "title": "第三章：数据伦理",
    "section": "反馈循环：YouTube 的推荐系统",
    "text": "反馈循环：YouTube 的推荐系统\n当您的模型控制您获得的下一轮数据时，反馈循环可能会发生。返回的数据很快就会被软件本身破坏。\n例如，YouTube 有 19 亿用户，他们每天观看超过 10 亿小时的 YouTube 视频。其推荐算法（由谷歌构建）旨在优化观看时间，负责约 70%的观看内容。但出现了问题：它导致了失控的反馈循环，导致《纽约时报》在 2019 年 2 月发表了标题为《YouTube 引发了阴谋论繁荣。能够控制吗？》的文章。表面上，推荐系统正在预测人们会喜欢什么内容，但它们也在很大程度上决定了人们甚至看到什么内容。"
  },
  {
    "objectID": "Fastbook/translations/cn/03_ethics.html#偏见拉塔尼亚斯威尼被捕",
    "href": "Fastbook/translations/cn/03_ethics.html#偏见拉塔尼亚斯威尼被捕",
    "title": "第三章：数据伦理",
    "section": "偏见：拉塔尼亚·斯威尼“被捕”",
    "text": "偏见：拉塔尼亚·斯威尼“被捕”\n拉塔尼亚·斯威尼博士是哈佛大学的教授，也是该大学数据隐私实验室的主任。在论文《在线广告投放中的歧视》中，她描述了她发现谷歌搜索她的名字会出现“拉塔尼亚·斯威尼，被捕了？”的广告，尽管她是唯一已知的拉塔尼亚·斯威尼，从未被捕。然而，当她搜索其他名字，如“Kirsten Lindquist”时，她得到了更中立的广告，尽管 Kirsten Lindquist 已经被捕了三次。\n\n\n\n谷歌搜索显示关于拉塔尼亚·斯威尼（不存在的）被捕记录的广告\n\n\n\n图 3-1。谷歌搜索显示关于拉塔尼亚·斯威尼（不存在的）被捕记录的广告\n作为一名计算机科学家，她系统地研究了这个问题，并查看了 2000 多个名字。她发现了一个明显的模式：历史上黑人的名字会收到暗示这个人有犯罪记录的广告，而传统上的白人名字则会有更中立的广告。\n这是偏见的一个例子。它可能对人们的生活产生重大影响，例如，如果一个求职者被谷歌搜索，可能会出现他们有犯罪记录的情况，而实际上并非如此。"
  },
  {
    "objectID": "Fastbook/translations/cn/03_ethics.html#这为什么重要",
    "href": "Fastbook/translations/cn/03_ethics.html#这为什么重要",
    "title": "第三章：数据伦理",
    "section": "这为什么重要？",
    "text": "这为什么重要？\n考虑这些问题的一个非常自然的反应是：“那又怎样？这和我有什么关系？我是一名数据科学家，不是政治家。我不是公司的高级执行官之一，他们决定我们要做什么。我只是尽力构建我能构建的最具预测性的模型。”\n这些是非常合理的问题。但我们将试图说服您，答案是每个训练模型的人都绝对需要考虑他们的模型将如何被使用，并考虑如何最好地确保它们被尽可能积极地使用。有一些你可以做的事情。如果你不这样做，事情可能会变得相当糟糕。\n当技术人员以任何代价专注于技术时，发生的一个特别可怕的例子是 IBM 与纳粹德国的故事。2001 年，一名瑞士法官裁定认为“推断 IBM 的技术援助促进了纳粹在犯下反人类罪行时的任务，这些行为还涉及 IBM 机器进行的会计和分类，并在集中营中使用。”\n你看，IBM 向纳粹提供了数据制表产品，以追踪大规模灭绝犹太人和其他群体。这是公司高层的决定，向希特勒及其领导团队推销。公司总裁托马斯·沃森亲自批准了 1939 年发布特殊的 IBM 字母排序机，以帮助组织波兰犹太人的驱逐。在图 3-2 中，阿道夫·希特勒（最左）与 IBM 首席执行官汤姆·沃森（左二）会面，希特勒在 1937 年授予沃森特别的“对帝国的服务”奖章。\n\n\n\nIBM 首席执行官汤姆·沃森与阿道夫·希特勒会面的图片\n\n\n\n图 3-2. IBM 首席执行官汤姆·沃森与阿道夫·希特勒会面\n但这并不是个案 - 该组织的涉入是广泛的。IBM 及其子公司在集中营现场提供定期培训和维护：打印卡片，配置机器，并在它们经常出现故障时进行维修。IBM 在其打孔卡系统上设置了每个人被杀害的方式，他们被分配到的组别以及跟踪他们通过庞大的大屠杀系统所需的后勤信息的分类。IBM 在集中营中对犹太人的代码是 8：约有 600 万人被杀害。对于罗姆人的代码是 12（纳粹将他们标记为“不合群者”，在“吉普赛营”中有超过 30 万人被杀害）。一般处决被编码为 4，毒气室中的死亡被编码为 6。\n\n\n\nIBM 在集中营中使用的打孔卡的图片\n\n\n\n\n图 3-3. IBM 在集中营中使用的打孔卡\n当然，参与其中的项目经理、工程师和技术人员只是过着普通的生活。照顾家人，周日去教堂，尽力做好自己的工作。服从命令。市场营销人员只是尽力实现他们的业务发展目标。正如《IBM 与大屠杀》（Dialog Press）的作者埃德温·布莱克所观察到的：“对于盲目的技术官僚来说，手段比目的更重要。犹太人民的毁灭变得更不重要，因为 IBM 技术成就的振奋性只会因在面包排长队的时候赚取的奇幻利润而更加突出。”\n退一步思考一下：如果你发现自己是一个最终伤害社会的系统的一部分，你会有什么感受？你会愿意了解吗？你如何帮助确保这种情况不会发生？我们在这里描述了最极端的情况，但今天观察到与人工智能和机器学习相关的许多负面社会后果，其中一些我们将在本章中描述。\n这也不仅仅是道德负担。有时，技术人员会直接为他们的行为付出代价。例如，作为大众汽车丑闻的结果而被监禁的第一个人并不是监督该项目的经理，也不是公司的执行主管。而是其中一名工程师詹姆斯·梁，他只是听从命令。\n当然，情况并非全是坏的 - 如果你参与的项目最终对一个人产生了巨大的积极影响，这会让你感到非常棒！\n好的，希望我们已经说服您应该关心这个问题。但是您应该怎么做呢？作为数据科学家，我们自然倾向于通过优化某些指标来改进我们的模型。但是优化这个指标可能不会导致更好的结果。即使它确实有助于创造更好的结果，几乎肯定不会是唯一重要的事情。考虑一下从研究人员或从业者开发模型或算法到使用这项工作做出决策之间发生的步骤流程。如果我们希望获得我们想要的结果，整个流程必须被视为一个整体。\n通常，从一端到另一端有一条非常长的链。如果您是一名研究人员，甚至可能不知道您的研究是否会被用于任何事情，或者如果您参与数据收集，那就更早了。但是没有人比您更适合告知所有参与这一链的人您的工作的能力、约束和细节。虽然没有“灵丹妙药”可以确保您的工作被正确使用，但通过参与这个过程，并提出正确的问题，您至少可以确保正确的问题正在被考虑。\n有时，对于被要求做一项工作的正确回应就是说“不”。然而，我们经常听到的回应是：“如果我不做，别人会做。”但请考虑：如果您被选中做这项工作，那么您是他们找到的最合适的人——所以如果您不做，最合适的人就不会参与该项目。如果他们询问的前五个人也都说不，那就更好了！"
  },
  {
    "objectID": "Fastbook/translations/cn/03_ethics.html#追索和问责制",
    "href": "Fastbook/translations/cn/03_ethics.html#追索和问责制",
    "title": "第三章：数据伦理",
    "section": "追索和问责制",
    "text": "追索和问责制\n在一个复杂的系统中，很容易没有任何一个人感到对结果负责。虽然这是可以理解的，但这并不会带来好的结果。在早期的阿肯色州医疗保健系统的例子中，一个错误导致患有脑瘫的人失去了所需护理的访问权限，算法的创建者责怪政府官员，政府官员责怪那些实施软件的人。纽约大学教授丹娜·博伊德描述了这种现象：“官僚主义经常被用来转移或逃避责任……今天的算法系统正在扩展官僚主义。”\n追索如此必要的另一个原因是数据经常包含错误。审计和纠错机制至关重要。加利福尼亚执法官员维护的一个涉嫌帮派成员的数据库发现充满了错误，包括 42 名不到 1 岁的婴儿被添加到数据库中（其中 28 名被标记为“承认是帮派成员”）。在这种情况下，没有流程来纠正错误或在添加后删除人员。另一个例子是美国信用报告系统：2012 年联邦贸易委员会（FTC）对信用报告进行的大规模研究发现，26%的消费者的档案中至少有一个错误，5%的错误可能是灾难性的。\n然而，纠正这类错误的过程非常缓慢和不透明。当公共广播记者鲍比·艾伦发现自己被错误列为有枪支罪时，他花了“十几个电话，一个县法院书记的手工操作和六周的时间来解决问题。而且这还是在我作为一名记者联系了公司的传播部门之后。”\n作为机器学习从业者，我们并不总是认为理解我们的算法最终如何在实践中实施是我们的责任。但我们需要。"
  },
  {
    "objectID": "Fastbook/translations/cn/03_ethics.html#反馈循环",
    "href": "Fastbook/translations/cn/03_ethics.html#反馈循环",
    "title": "第三章：数据伦理",
    "section": "反馈循环",
    "text": "反馈循环\n我们在第一章中解释了算法如何与环境互动以创建反馈循环，做出预测以加强在现实世界中采取的行动，从而导致更加明显朝着同一方向的预测。举个例子，让我们再次考虑 YouTube 的推荐系统。几年前，谷歌团队谈到他们如何引入了强化学习（与深度学习密切相关，但你的损失函数代表了潜在长时间后行动发生的结果）来改进 YouTube 的推荐系统。他们描述了如何使用一个算法，使推荐以优化观看时间为目标。\n然而，人类往往被争议性内容所吸引。这意味着关于阴谋论之类的视频开始越来越多地被推荐给用户。此外，事实证明，对阴谋论感兴趣的人也是那些经常观看在线视频的人！因此，他们开始越来越多地被吸引到 YouTube。越来越多的阴谋论者在 YouTube 上观看视频导致算法推荐越来越多的阴谋论和其他极端内容，这导致更多的极端分子在 YouTube 上观看视频，更多的人在 YouTube 上形成极端观点，进而导致算法推荐更多的极端内容。系统失控了。\n这种现象并不局限于这种特定类型的内容。2019 年 6 月，《纽约时报》发表了一篇关于 YouTube 推荐系统的文章，标题为“在 YouTube 的数字游乐场，对恋童癖者敞开大门”。文章以这个令人不安的故事开头：\n\n当 Christiane C.的 10 岁女儿和一个朋友上传了一个在后院游泳池玩耍的视频时，她并没有在意……几天后……视频的观看次数已经达到了数千次。不久之后，观看次数已经增加到 40 万……“我再次看到视频，看到观看次数，我感到害怕，”Christiane 说。她有理由感到害怕。研究人员发现，YouTube 的自动推荐系统……开始向观看其他预备期、部分穿着少儿视频的用户展示这个视频。\n单独看，每个视频可能是完全无辜的，比如一个孩子制作的家庭影片。任何暴露的画面都是短暂的，看起来是偶然的。但是，当它们被组合在一起时，它们共享的特征变得明显。\n\nYouTube 的推荐算法开始为恋童癖者策划播放列表，挑选出偶然包含预备期、部分穿着少儿的无辜家庭视频。\n谷歌没有计划创建一个将家庭视频变成儿童色情片的系统。那么发生了什么？\n这里的问题之一是指标在推动一个财政重要系统中的核心性。当一个算法有一个要优化的指标时，正如你所看到的，它会尽其所能来优化这个数字。这往往会导致各种边缘情况，与系统互动的人类会寻找、发现并利用这些边缘情况和反馈循环以谋取利益。\n有迹象表明，这正是发生在 YouTube 的推荐系统中的情况。卫报发表了一篇题为“一位前 YouTube 内部人员是如何调查其秘密算法的”的文章，讲述了前 YouTube 工程师 Guillaume Chaslot 创建了一个网站来跟踪这些问题。Chaslot 在罗伯特·穆勒“关于 2016 年总统选举中俄罗斯干预调查”的发布后发布了图表，如图 3-5 所示。\n\n\n\n穆勒报告的报道\n\n\n\n图 3-5. 穆勒报告的报道\n俄罗斯今日电视台对穆勒报告的报道在推荐频道中是一个极端的离群值。这表明俄罗斯今日电视台，一个俄罗斯国有媒体机构，成功地操纵了 YouTube 的推荐算法。不幸的是，这种系统缺乏透明度，使我们很难揭示我们正在讨论的问题。\n本书的一位审阅者 Aurélien Géron，曾在 2013 年至 2016 年间领导 YouTube 的视频分类团队（远在这里讨论的事件之前）。他指出，涉及人类的反馈循环不仅是一个问题。也可能存在没有人类参与的反馈循环！他向我们讲述了 YouTube 的一个例子：\n\n对视频的主题进行分类的一个重要信号是视频的来源频道。例如，上传到烹饪频道的视频很可能是烹饪视频。但我们如何知道一个频道的主题是什么？嗯…部分是通过查看它包含的视频的主题！你看到循环了吗？例如，许多视频有描述，指示拍摄视频所使用的相机。因此，一些视频可能被分类为“摄影”视频。如果一个频道有这样一个错误分类的视频，它可能被分类为“摄影”频道，使得未来在该频道上的视频更有可能被错误分类为“摄影”。这甚至可能导致失控的病毒般的分类！打破这种反馈循环的一种方法是对有和没有频道信号的视频进行分类。然后在对频道进行分类时，只能使用没有频道信号获得的类别。这样，反馈循环就被打破了。\n\n有人和组织试图解决这些问题的积极例子。Meetup 的首席机器学习工程师 Evan Estola 讨论了男性对科技见面会表现出比女性更感兴趣的例子。因此，考虑性别可能会导致 Meetup 的算法向女性推荐更少的科技见面会，结果导致更少的女性了解并参加科技见面会，这可能导致算法向女性推荐更少的科技见面会，如此循环反馈。因此，Evan 和他的团队做出了道德决定，让他们的推荐算法不会创建这样的反馈循环，明确不在模型的那部分使用性别。看到一家公司不仅仅是盲目地优化指标，而是考虑其影响是令人鼓舞的。根据 Evan 的说法，“你需要决定在算法中不使用哪个特征…最优算法也许不是最适合投入生产的算法。”\n尽管 Meetup 选择避免这种结果，但 Facebook 提供了一个允许失控的反馈循环肆虐的例子。与 YouTube 类似，它倾向于通过向用户介绍更多阴谋论来激化用户。正如虚构信息传播研究员 Renee DiResta 所写的那样：\n\n一旦人们加入一个阴谋论倾向的[Facebook]群组，他们就会被算法路由到其他大量群组。加入反疫苗群组，你的建议将包括反转基因、化学尾迹观察、地平论者（是的，真的）和“自然治愈癌症”群组。推荐引擎不是将用户拉出兔子洞，而是将他们推得更深。\n\n非常重要的是要记住这种行为可能会发生，并在看到自己项目中出现第一个迹象时，要么预见到一个反馈循环，要么采取积极行动来打破它。另一件要记住的事情是偏见，正如我们在上一章中简要讨论的那样，它可能与反馈循环以非常麻烦的方式相互作用。"
  },
  {
    "objectID": "Fastbook/translations/cn/03_ethics.html#偏见",
    "href": "Fastbook/translations/cn/03_ethics.html#偏见",
    "title": "第三章：数据伦理",
    "section": "偏见",
    "text": "偏见\n在线讨论偏见往往会变得非常混乱。 “偏见”一词有很多不同的含义。统计学家经常认为，当数据伦理学家谈论偏见时，他们在谈论统计学术语“偏见”，但他们并没有。他们当然也没有在谈论出现在模型参数中的权重和偏见中的偏见！\n他们所谈论的是社会科学概念中的偏见。在“理解机器学习意外后果的框架”中，麻省理工学院的 Harini Suresh 和 John Guttag 描述了机器学习中的六种偏见类型，总结在图 3-6 中。\n\n\n\n显示机器学习中偏见可能出现的所有来源的图表\n\n\n\n图 3-6。机器学习中的偏见可能来自多个来源（由 Harini Suresh 和 John V. Guttag 提供）\n我们将讨论其中四种偏见类型，这些是我们在自己的工作中发现最有帮助的（有关其他类型的详细信息，请参阅论文）。\n\n\n历史偏见\n历史偏见源于人们的偏见，过程的偏见，以及社会的偏见。苏雷什和古塔格说：“历史偏见是数据生成过程的第一步存在的基本结构性问题，即使进行了完美的抽样和特征选择，它也可能存在。”\n例如，以下是美国历史上种族偏见的几个例子，来自芝加哥大学 Sendhil Mullainathan 的《纽约时报》文章“种族偏见，即使我们有良好意图”：\n\n当医生看到相同的档案时，他们更不可能向黑人患者推荐心脏导管化（一种有益的程序）。\n在讨价还价购买二手车时，黑人被要求支付的初始价格高出 700 美元，并获得了远低于预期的让步。\n在 Craigslist 上回应带有黑人姓名的公寓出租广告比带有白人姓名的回应要少。\n一个全白人陪审团比一个黑人被告有 16 个百分点更有可能定罪，但当陪审团有一个黑人成员时，他们以相同的比率定罪。\n\n在美国用于判决和保释决定的 COMPAS 算法是一个重要算法的例子，当ProPublica进行测试时，实际上显示出明显的种族偏见（图 3-7）。\n\n\n\n表格显示，即使重新犯罪，COMPAS 算法更有可能给白人保释\n\n\n\n图 3-7。COMPAS 算法的结果\n任何涉及人类的数据集都可能存在这种偏见：医疗数据、销售数据、住房数据、政治数据等等。由于潜在偏见是如此普遍，数据集中的偏见也非常普遍。甚至在计算机视觉中也会出现种族偏见，正如 Twitter 上一位 Google 照片用户分享的自动分类照片的例子所示，见图 3-8。\n\n\n\nGoogle 照片使用黑人用户和她朋友的照片标记为大猩猩的屏幕截图\n\n\n\n\n图 3-8。其中一个标签是非常错误的…\n是的，这正是你认为的：Google 照片将一位黑人用户的照片与她的朋友一起分类为“大猩猩”！这种算法错误引起了媒体的广泛关注。一位公司女发言人表示：“我们对此感到震惊和真诚地抱歉。自动图像标记仍然存在许多问题，我们正在研究如何防止将来发生这类错误。”\n不幸的是，当输入数据存在问题时，修复机器学习系统中的问题是困难的。谷歌的第一次尝试并没有激发信心，正如卫报的报道所建议的那样（图 3-9）。\n\n\n\n来自卫报的标题图片，当谷歌从其算法的可能标签中删除大猩猩和其他猴子时\n\n\n\n\n图 3-9。谷歌对问题的第一次回应\n这些问题当然不仅限于谷歌。麻省理工学院的研究人员研究了最受欢迎的在线计算机视觉 API，以了解它们的准确性。但他们并不只是计算一个准确性数字，而是查看了四个组的准确性，如图 3-10 所示。\n\n\n\n表格显示各种面部识别系统在较深肤色和女性上表现更差\n\n\n\n\n图 3-10。各种面部识别系统的性别和种族错误率\n例如，IBM 的系统对较深肤色的女性有 34.7%的错误率，而对较浅肤色的男性只有 0.3%的错误率——错误率高出 100 多倍！一些人对这些实验的反应是错误的，他们声称差异仅仅是因为较深的皮肤更难被计算机识别。然而，事实是，由于这一结果带来的负面宣传，所有相关公司都大幅改进了他们对较深肤色的模型，以至于一年后，它们几乎和对较浅肤色的一样好。因此，这表明开发人员未能利用包含足够多较深肤色面孔的数据集，或者未能用较深肤色的面孔测试他们的产品。\n麻省理工学院的一位研究人员 Joy Buolamwini 警告说：“我们已经进入了自信过度但准备不足的自动化时代。如果我们未能制定道德和包容性的人工智能，我们将冒着在机器中立的幌子下失去民权和性别平等所取得的成就的风险。”\n问题的一部分似乎是流行数据集的构成存在系统性不平衡，用于训练模型。Shreya Shankar 等人的论文“没有代表性就没有分类：评估发展中国家开放数据集中的地理多样性问题”的摘要中指出，“我们分析了两个大型公开可用的图像数据集，以评估地理多样性，并发现这些数据集似乎存在明显的美洲中心和欧洲中心的代表性偏见。此外，我们分析了在这些数据集上训练的分类器，以评估这些训练分布的影响，并发现在不同地区的图像上表现出强烈的相对性能差异。”图 3-11 展示了论文中的一个图表，展示了当时（以及本书撰写时仍然如此）两个最重要的图像数据集的地理构成。\n\n\n\n图表显示流行训练数据集中绝大多数图像来自美国或西欧\n\n\n\n\n图 3-11。流行训练集中的图像来源\n绝大多数图像来自美国和其他西方国家，导致在 ImageNet 上训练的模型在其他国家和文化的场景中表现更差。例如，研究发现这样的模型在识别低收入国家的家庭物品（如肥皂、香料、沙发或床）时表现更差。图 3-12 展示了 Facebook AI Research 的 Terrance DeVries 等人的论文“目标识别对每个人都有效吗？”中的一幅图像，说明了这一点。\n\n\n\n图表显示一个目标检测算法在西方产品上表现更好\n\n\n\n\n图 3-12。目标检测的实际应用\n在这个例子中，我们可以看到低收入肥皂的例子离准确还有很长的路要走，每个商业图像识别服务都预测“食物”是最可能的答案！\n接下来我们将讨论，绝大多数人工智能研究人员和开发人员都是年轻的白人男性。我们看到的大多数项目都是使用产品开发团队的朋友和家人进行用户测试。鉴于此，我们刚刚讨论的问题不应该令人惊讶。\n类似的历史偏见也存在于用作自然语言处理模型数据的文本中。这会在许多下游机器学习任务中出现。例如，据广泛报道，直到去年，Google 翻译在将土耳其中性代词“o”翻译成英语时显示了系统性偏见：当应用于通常与男性相关联的工作时，它使用“he”，而当应用于通常与女性相关联的工作时，它使用“she”（图 3-13）。\n\n\n\n显示语言模型训练中数据集中性别偏见在翻译中的体现的图表\n\n\n\n\n图 3-13。文本数据集中的性别偏见\n我们也在在线广告中看到这种偏见。例如，2019 年穆罕默德·阿里等人的一项研究发现，即使放置广告的人没有故意歧视，Facebook 也会根据种族和性别向非常不同的受众展示广告。展示了同样文本但图片分别是白人家庭或黑人家庭的房屋广告被展示给了种族不同的受众。\n\n\n\n测量偏见\n在《“机器学习是否自动化了道德风险和错误”》一文中，Sendhil Mullainathan 和 Ziad Obermeyer 研究了一个模型，试图回答这个问题：使用历史电子健康记录（EHR）数据，哪些因素最能预测中风？这是该模型的前几个预测因素：\n\n先前的中风\n心血管疾病\n意外伤害\n良性乳腺肿块\n结肠镜检查\n鼻窦炎\n\n然而，只有前两个与中风有关！根据我们迄今所学，你可能已经猜到原因。我们实际上并没有测量中风，中风是由于脑部某个区域由于血液供应中断而被剥夺氧气而发生的。我们测量的是谁有症状，去看医生，接受了适当的检查，并且被诊断出中风。实际上患中风不仅与这个完整列表相关联，还与那些会去看医生的人相关联（这受到谁能获得医疗保健、能否负担得起自付款、是否经历种族或性别歧视等影响）！如果你在发生意外伤害时可能会去看医生，那么在中风时你也可能会去看医生。\n这是测量偏见的一个例子。当我们的模型因为测量错误、以错误方式测量或不恰当地将该测量纳入模型时，就会发生这种偏见。\n\n\n聚合偏见\n聚合偏见发生在模型未以包含所有适当因素的方式聚合数据，或者模型未包含必要的交互项、非线性等情况下。这在医疗环境中尤其常见。例如，糖尿病的治疗通常基于简单的单变量统计和涉及小组异质人群的研究。结果分析通常未考虑不同种族或性别。然而，事实证明糖尿病患者在不同种族之间有不同的并发症，HbA1c 水平（用于诊断和监测糖尿病的广泛指标）在不同种族和性别之间以复杂方式不同。这可能导致人们被误诊或错误治疗，因为医疗决策基于不包含这些重要变量和交互作用的模型。\n\n\n表征偏见\nMaria De-Arteaga 等人的论文“Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting”的摘要指出，职业中存在性别不平衡（例如，女性更有可能成为护士，男性更有可能成为牧师），并表示“性别之间的真正阳性率差异与职业中现有的性别不平衡相关，这可能会加剧这些不平衡。”\n换句话说，研究人员注意到，预测职业的模型不仅反映了潜在人口中的实际性别不平衡，而且放大了它！这种表征偏差是相当常见的，特别是对于简单模型。当存在明显、容易看到的基本关系时，简单模型通常会假定这种关系始终存在。正如论文中的图 3-14 所示，对于女性比例较高的职业，模型往往会高估该职业的普遍性。\n\n\n\n显示模型预测如何过度放大现有偏见的图表\n\n\n\n图 3-14。预测职业中的模型误差与该职业中女性比例的关系\n例如，在训练数据集中，14.6%的外科医生是女性，然而在模型预测中，真正阳性中只有 11.6%是女性。因此，模型放大了训练集中存在的偏见。\n既然我们已经看到这些偏见存在，我们可以采取什么措施来减轻它们呢？\n\n\n\n解决不同类型的偏见\n不同类型的偏见需要不同的缓解方法。虽然收集更多样化的数据集可以解决表征偏见，但这对历史偏见或测量偏见无济于事。所有数据集都包含偏见。没有完全无偏的数据集。该领域的许多研究人员一直在提出一系列建议，以便更好地记录决策、背景和有关特定数据集创建方式的细节，以及为什么在什么情况下使用它，以及其局限性。这样，使用特定数据集的人不会被其偏见和局限性所困扰。\n我们经常听到这样的问题，“人类有偏见，那么算法偏见真的重要吗？”这个问题经常被提出，肯定有一些让提问者认为有道理的理由，但对我们来说似乎并不太合乎逻辑！独立于这是否合乎逻辑，重要的是要意识到算法（特别是机器学习算法！）和人类是不同的。考虑一下关于机器学习算法的这些观点：\n机器学习可以创建反馈循环\n少量偏见可能会因为反馈循环而迅速呈指数增长。\n机器学习可能会放大偏见\n人类偏见可能导致更多的机器学习偏见。\n算法和人类的使用方式不同\n在实践中，人类决策者和算法决策者并不是以插拔方式互换使用的。这些例子列在下一页的清单中。\n技术就是力量\n随之而来的是责任。\n正如阿肯色州医疗保健的例子所示，机器学习通常在实践中实施并不是因为它能带来更好的结果，而是因为它更便宜和更高效。凯西·奥尼尔在她的书《数学毁灭的武器》（Crown）中描述了一个模式，即特权人士由人处理，而穷人由算法处理。这只是算法与人类决策者使用方式的许多方式之一。其他方式包括以下内容：\n\n人们更有可能认为算法是客观或无误差的（即使他们有人类覆盖的选项）。\n算法更有可能在没有上诉程序的情况下实施。\n算法通常以规模使用。\n算法系统成本低廉。\n\n即使在没有偏见的情况下，算法（尤其是深度学习，因为它是一种如此有效和可扩展的算法）也可能导致负面社会问题，比如当用于虚假信息时。"
  },
  {
    "objectID": "Fastbook/translations/cn/03_ethics.html#虚假信息",
    "href": "Fastbook/translations/cn/03_ethics.html#虚假信息",
    "title": "第三章：数据伦理",
    "section": "虚假信息",
    "text": "虚假信息\n虚假信息的历史可以追溯到数百甚至数千年前。它不一定是让某人相信错误的事情，而是经常用来播撒不和谐和不确定性，并让人们放弃寻求真相。收到矛盾的说法可能会导致人们认为他们永远无法知道该信任谁或什么。\n有些人认为虚假信息主要是关于错误信息或假新闻，但实际上，虚假信息经常包含真相的种子，或者是脱离上下文的半真相。拉迪斯拉夫·比特曼是苏联的一名情报官员，后来叛逃到美国，并在 20 世纪 70 年代和 80 年代写了一些关于苏联宣传行动中虚假信息角色的书籍。在《克格勃和苏联虚假信息》（Pergamon）中，他写道“大多数活动都是精心设计的事实、半真相、夸大和故意谎言的混合物。”\n在美国，近年来，FBI 详细描述了与 2016 年选举中的俄罗斯有关的大规模虚假信息活动。了解在这次活动中使用的虚假信息非常有教育意义。例如，FBI 发现俄罗斯的虚假信息活动经常组织两个独立的假“草根”抗议活动，一个支持某一方面，另一个支持另一方面，并让他们同时抗议！休斯顿纪事报报道了其中一个奇怪事件（图 3-15）：\n\n一个自称为“德克萨斯之心”的团体在社交媒体上组织了一场抗议活动，他们声称这是反对“德克萨斯伊斯兰化”的。在特拉维斯街的一边，我发现大约有 10 名抗议者。在另一边，我发现大约有 50 名反对抗议者。但我找不到集会的组织者。没有“德克萨斯之心”。我觉得这很奇怪，并在文章中提到：一个团体在自己的活动中缺席是什么样的团体？现在我知道为什么了。显然，集会的组织者当时在俄罗斯的圣彼得堡。“德克萨斯之心”是特别检察官罗伯特·穆勒最近指控试图干预美国总统选举的俄罗斯人中引用的一个互联网喷子团体。\n\n\n\n\n德克萨斯之心组织的活动截图\n\n\n\n图 3-15。由德克萨斯之心组织的活动\n虚假信息通常涉及协调的不真实行为活动。例如，欺诈账户可能试图让人们认为许多人持有特定观点。虽然大多数人喜欢认为自己是独立思考的，但实际上我们进化为受到内部群体的影响，并与外部群体对立。在线讨论可能会影响我们的观点，或改变我们认为可接受观点的范围。人类是社会动物，作为社会动物，我们受周围人的影响极大。越来越多的极端化发生在在线环境中；因此影响来自虚拟空间中的在线论坛和社交网络中的人们。\n通过自动生成的文本进行虚假信息传播是一个特别重要的问题，这是由于深度学习提供的大大增强的能力。当我们深入研究创建语言模型时，我们会深入讨论这个问题第十章。\n一种提出的方法是开发某种形式的数字签名，以无缝方式实施它，并创建我们应该信任仅经过验证的内容的规范。艾伦人工智能研究所的负责人奥伦·艾齐奥尼在一篇题为“我们将如何防止基于人工智能的伪造？”的文章中写道：“人工智能正准备使高保真伪造变得廉价和自动化，可能会对民主、安全和社会造成灾难性后果。人工智能伪造的幽灵意味着我们需要采取行动，使数字签名成为验证数字内容的手段。”\n虽然我们无法讨论深度学习和算法带来的所有伦理问题，但希望这个简短的介绍可以成为您的有用起点。现在我们将继续讨论如何识别伦理问题以及如何处理它们。"
  },
  {
    "objectID": "Fastbook/translations/cn/03_ethics.html#分析你正在进行的项目",
    "href": "Fastbook/translations/cn/03_ethics.html#分析你正在进行的项目",
    "title": "第三章：数据伦理",
    "section": "分析你正在进行的项目",
    "text": "分析你正在进行的项目\n在考虑工作的伦理影响时很容易忽略重要问题。一个极大的帮助是简单地提出正确的问题。Rachel Thomas 建议在数据项目的开发过程中考虑以下问题：\n\n我们甚至应该这样做吗？\n数据中存在什么偏见？\n代码和数据可以进行审计吗？\n不同子群体的错误率是多少？\n基于简单规则的替代方案的准确性如何？\n有哪些处理申诉或错误的流程？\n构建它的团队有多少多样性？\n\n这些问题可能有助于您识别未解决的问题，以及更容易理解和控制的可能替代方案。除了提出正确的问题外，考虑实施的实践和流程也很重要。\n在这个阶段需要考虑的一件事是你正在收集和存储的数据。数据往往最终被用于不同于最初意图的目的。例如，IBM 在大屠杀之前就开始向纳粹德国出售产品，包括帮助纳粹德国进行的 1933 年人口普查，这次普查有效地识别出了比之前在德国被认可的犹太人更多。同样，美国人口普查数据被用来拘留二战期间的日裔美国人（他们是美国公民）。重要的是要认识到收集的数据和图像如何在以后被武器化。哥伦比亚大学教授蒂姆·吴写道：“你必须假设 Facebook 或 Android 保存的任何个人数据都是世界各国政府将试图获取或盗贼将试图窃取的数据。”"
  },
  {
    "objectID": "Fastbook/translations/cn/03_ethics.html#实施流程",
    "href": "Fastbook/translations/cn/03_ethics.html#实施流程",
    "title": "第三章：数据伦理",
    "section": "实施流程",
    "text": "实施流程\n马库拉中心发布了工程/设计实践的伦理工具包，其中包括在您的公司实施的具体实践，包括定期安排的扫描，以主动搜索伦理风险（类似于网络安全渗透测试），扩大伦理圈，包括各种利益相关者的观点，并考虑可怕的人（坏人如何滥用、窃取、误解、黑客、破坏或武器化您正在构建的东西？）。\n即使您没有多样化的团队，您仍然可以尝试主动包括更广泛群体的观点，考虑这些问题（由马库拉中心提供）：\n\n我们是否只是假设了谁/哪些团体和个人的利益、愿望、技能、经验和价值观，而没有实际咨询？\n谁将直接受到我们产品影响的所有利益相关者？他们的利益是如何得到保护的？我们如何知道他们的真正利益是什么——我们有没有询问过？\n哪些团体和个人将受到重大影响而间接受到影响？\n谁可能会使用这个产品，而我们没有预料到会使用它，或者出于我们最初没有打算的目的？\n\n\n伦理镜头\n马库拉中心的另一个有用资源是其技术和工程实践中的概念框架。这考虑了不同基础伦理镜头如何帮助识别具体问题，并列出以下方法和关键问题：\n权利的观点\n哪个选项最尊重所有利益相关者的权利？\n正义的观点\n哪个选项平等或成比例地对待人们？\n功利主义的观点\n哪个选项将产生最多的好处并造成最少的伤害？\n共同利益的观点\n哪个选项最好地服务于整个社区，而不仅仅是一些成员？\n美德的观点\n哪个选项会让我表现得像我想成为的那种人？\n马库拉的建议包括更深入地探讨这些观点，包括通过后果的视角来审视一个项目：\n\n谁将直接受到这个项目的影响？谁将间接受到影响？\n总体上，这些影响可能会产生更多的好处还是伤害，以及什么类型的好处和伤害？\n我们是否考虑了所有相关类型的伤害/好处（心理、政治、环境、道德、认知、情感、制度、文化）？\n未来的后代可能会受到这个项目的影响吗？\n这个项目可能会对社会中最弱势的人造成的伤害风险是否不成比例？好处是否会不成比例地给予富裕者？\n我们是否充分考虑了“双重使用”和意外的下游影响？\n\n另一种视角是义务论的视角，它侧重于对和错的基本概念：\n\n我们必须尊重他人的哪些权利和对他人的义务？\n这个项目可能会如何影响每个利益相关者的尊严和自主权？\n信任和正义的考虑对这个设计/项目有何影响？\n这个项目是否涉及与他人的冲突道德责任，或者与利益相关者的冲突权利？我们如何能够优先考虑这些？\n\n帮助提出完整和周到的答案的最佳方法之一是确保提出问题的人是多样化的。"
  },
  {
    "objectID": "Fastbook/translations/cn/03_ethics.html#多样性的力量",
    "href": "Fastbook/translations/cn/03_ethics.html#多样性的力量",
    "title": "第三章：数据伦理",
    "section": "多样性的力量",
    "text": "多样性的力量\n根据Element AI 的一项研究，目前不到 12%的人工智能研究人员是女性。在种族和年龄方面的统计数据同样令人堪忧。当团队中的每个人背景相似时，他们很可能在道德风险方面有相似的盲点。哈佛商业评论（HBR）发表了许多研究，显示了多样化团队的许多好处，包括以下内容：\n\n“多样性如何推动创新”\n“当团队的认知多样性更高时，他们解决问题更快”\n“为什么多样化的团队更聪明”\n“捍卫您的研究：什么使一个团队更聪明？更多的女性”\n\n多样性可以导致问题更早地被识别，并考虑更广泛的解决方案。例如，Tracy Chou 是 Quora 的一名早期工程师。她描述了自己的经历，描述了她在内部为添加一个功能而进行倡导，该功能可以允许封锁恶意用户和其他不良行为者。Chou 回忆道，“我渴望参与这个功能的开发，因为我在网站上感到被挑衅和虐待（性别可能是一个原因）…但如果我没有那种个人视角，Quora 团队可能不会那么早地将构建封锁按钮作为优先事项。”骚扰经常会导致边缘群体的人离开在线平台，因此这种功能对于维护 Quora 社区的健康至关重要。\n一个关键的方面要理解的是，女性离开科技行业的速度是男性的两倍以上。根据哈佛商业评论的数据，41%的从事科技行业的女性离开，而男性只有 17%。对 200 多本书籍、白皮书和文章的分析发现，她们离开的原因是“她们受到不公平对待；薪酬较低，不如男同事那样容易获得快速晋升，无法晋升。”\n研究已经证实了一些使女性在职场中更难晋升的因素。女性在绩效评估中收到更多模糊的反馈和个性批评，而男性收到与业务结果相关的可操作建议（更有用）。女性经常被排除在更具创造性和创新性的角色之外，并且没有获得有助于晋升的高能见度的“拓展”任务。一项研究发现，即使阅读相同的脚本，男性的声音被认为比女性的声音更具有说服力、基于事实和逻辑。\n统计数据显示，接受指导有助于男性晋升，但对女性没有帮助。背后的原因是，当女性接受指导时，这是关于她们应该如何改变和获得更多自我认识的建议。当男性接受指导时，这是对他们权威的公开认可。猜猜哪个对于晋升更有用？\n只要合格的女性继续退出科技行业，教更多女孩编程并不能解决困扰该领域的多样性问题。多样性倡议往往主要关注白人女性，尽管有色人种女性面临许多额外障碍。在对从事 STEM 研究的 60 名有色人种女性进行的采访中，100%的人表示曾经遭受过歧视。\n技术领域的招聘过程特别混乱。一项表明这种功能障碍的研究来自 Triplebyte，这是一家帮助将软件工程师安置到公司的公司，作为这一过程的一部分进行了标准化的技术面试。该公司拥有一个引人入胜的数据集：300 多名工程师在考试中的表现结果，以及这些工程师在各种公司的面试过程中的表现结果。Triplebyte 的研究中的第一个发现是，“每家公司寻找的程序员类型往往与公司的需求或业务无关。相反，它们反映了公司文化和创始人的背景。”\n这对于试图进入深度学习领域的人来说是一个挑战，因为大多数公司的深度学习团队今天都是由学者创立的。这些团队往往寻找“像他们一样”的人——也就是说，能够解决复杂数学问题并理解密集行话的人。他们并不总是知道如何发现那些真正擅长使用深度学习解决实际问题的人。\n这为那些愿意超越地位和门第，专注于结果的公司提供了一个巨大的机会！"
  },
  {
    "objectID": "Fastbook/translations/cn/03_ethics.html#公平问责和透明度",
    "href": "Fastbook/translations/cn/03_ethics.html#公平问责和透明度",
    "title": "第三章：数据伦理",
    "section": "公平、问责和透明度",
    "text": "公平、问责和透明度\n计算机科学家的专业协会 ACM 举办了一个名为“公平性、问责制和透明度会议”的数据伦理会议（ACM FAccT），以前使用的缩写是 FAT，现在使用不那么有争议的 FAccT。微软也有一个专注于 AI 中的公平性、问责制、透明度和伦理的团队（FATE）。在本节中，我们将使用缩写 FAccT 来指代公平性、问责制和透明度的概念。\nFAccT 是一些人用来考虑伦理问题的一种视角。一个有用的资源是 Solon Barocas 等人的免费在线书籍《公平性与机器学习：限制与机会》，该书“提供了一个将公平性视为中心问题而不是事后想法的机器学习视角”。然而，它也警告说，“它故意范围狭窄……机器学习伦理的狭窄框架可能会诱使技术人员和企业专注于技术干预，而回避有关权力和问责制的更深层次问题。我们警告不要陷入这种诱惑。”与提供 FAccT 伦理方法概述的重点不同（最好在像那样的书籍中完成），我们的重点将放在这种狭窄框架的局限性上。\n考虑伦理视角是否完整的一个好方法是尝试提出一个例子，其中视角和我们自己的伦理直觉给出不同的结果。Os Keyes 等人在他们的论文中以图形方式探讨了这一点。该论文的摘要如下：\n\n算法系统的伦理含义在人机交互和对技术设计、开发和政策感兴趣的更广泛社区中已经被广泛讨论。在本文中，我们探讨了一个著名的伦理框架——公平性、问责制和透明度——在一个旨在解决食品安全和人口老龄化等各种社会问题的算法中的应用。通过使用各种标准化的算法审计和评估形式，我们大大增加了算法对 FAT 框架的遵从，从而实现了更具伦理和善意的系统。我们讨论了这如何可以作为其他研究人员或从业者的指南，帮助他们确保在工作中的算法系统产生更好的伦理结果。\n\n在本文中，相当有争议的提议（“将老年人变成高营养浆料”）和结果（“大大增加算法对 FAT 框架的遵从，从而实现更具伦理和善意的系统”）是相互矛盾的……至少可以这么说！\n在哲学中，尤其是伦理哲学中，这是最有效的工具之一：首先，提出一个过程、定义、一组问题等，旨在解决问题。然后尝试提出一个例子，其中明显的解决方案导致一个没有人会认为可接受的提议。这可以进一步完善解决方案。\n到目前为止，我们关注的是您和您的组织可以做的事情。但有时个人或组织的行动是不够的。有时政府也需要考虑政策影响。"
  },
  {
    "objectID": "Fastbook/translations/cn/03_ethics.html#监管的有效性",
    "href": "Fastbook/translations/cn/03_ethics.html#监管的有效性",
    "title": "第三章：数据伦理",
    "section": "监管的有效性",
    "text": "监管的有效性\n要看看是什么导致公司采取具体行动，考虑 Facebook 的以下两个行为示例。2018 年，联合国调查发现 Facebook 在缅甸罗兴亚人持续种族灭绝中发挥了“决定性作用”，联合国秘书长安东尼奥·古特雷斯将罗兴亚人描述为“世界上最受歧视的人之一，如果不是最受歧视的人”。自 2013 年以来，当地活动人士一直在警告 Facebook 高管，称他们的平台被用来传播仇恨言论和煽动暴力。2015 年，他们被警告说，Facebook 可能在缅甸扮演与卢旺达种族灭绝期间广播电台扮演的相同角色（那里有一百万人被杀）。然而，到 2015 年底，Facebook 只雇用了四名会说缅甸语的承包商。正如一位知情人士所说，“这不是事后诸葛亮。这个问题的规模很大，而且已经显而易见。”扎克伯格在国会听证会上承诺雇佣“几十人”来解决缅甸的种族灭绝问题（2018 年，数年后种族灭绝已经开始，包括 2017 年 8 月之后至少摧毁了北拉钦邦至少 288 个村庄）。\n这与 Facebook 迅速在德国雇佣了 1,200 人以避免根据德国新法律反对仇恨言论面临高达 5000 万欧元的昂贵罚款形成鲜明对比。显然，在这种情况下，Facebook 更多地是对财务处罚的威胁做出反应，而不是对一个种族少数群体的系统性破坏。\n在一篇关于隐私问题的文章中，马切伊·塞格洛夫斯基与环境运动进行了类比：\n\n这一监管项目在第一世界取得了如此成功，以至于我们可能忘记了之前的生活是什么样子。今天在雅加达和德里杀死成千上万人的浓烟曾经是伦敦的象征。俄亥俄州的奎哈霍加河曾经经常起火。在一个特别可怕的意外后果的例子中，添加到汽油中的四乙基铅导致全球暴力犯罪率上升了五十年。这些伤害都不能通过告诉人们用钱包投票，或者仔细审查他们给予业务的每家公司的环境政策，或者停止使用相关技术来解决。这需要跨越司法辖区的协调和有时高度技术化的监管来解决。在一些情况下，比如禁止商用制冷剂导致臭氧层消耗，这种监管需要全球共识。我们已经到了需要在隐私法中进行类似转变的时候。"
  },
  {
    "objectID": "Fastbook/translations/cn/03_ethics.html#权利和政策",
    "href": "Fastbook/translations/cn/03_ethics.html#权利和政策",
    "title": "第三章：数据伦理",
    "section": "权利和政策",
    "text": "权利和政策\n清洁空气和清洁饮用水是几乎不可能通过个人市场决策来保护的公共物品，而是需要协调的监管行动。同样，许多技术误用的意外后果造成的伤害涉及公共物品，比如污染的信息环境或恶化的环境隐私。隐私往往被框定为个人权利，然而广泛监视会产生社会影响（即使有一些个人可以选择退出也是如此）。\n我们在科技领域看到的许多问题都是人权问题，比如一个带有偏见的算法建议黑人被告应该获得更长的监禁，特定的工作广告只显示给年轻人，或者警察使用面部识别来识别抗议者。解决人权问题的适当场所通常是法律。\n我们需要监管和法律变革，以及个人的道德行为。个人行为的改变无法解决不一致的利润激励、外部性（即企业在向更广泛社会转嫁成本和危害的同时获得巨额利润）或系统性失败。然而，法律永远不可能涵盖所有边缘案例，重要的是个人软件开发人员和数据科学家能够在实践中做出道德决策。"
  },
  {
    "objectID": "Fastbook/translations/cn/03_ethics.html#汽车历史先例",
    "href": "Fastbook/translations/cn/03_ethics.html#汽车历史先例",
    "title": "第三章：数据伦理",
    "section": "汽车：历史先例",
    "text": "汽车：历史先例\n我们面临的问题是复杂的，没有简单的解决方案。这可能令人沮丧，但我们在考虑历史上人们已经解决的其他重大挑战时找到了希望。一个例子是增加汽车安全的运动，被提及为“数据集数据表”一书中的案例研究，作者是 Timnit Gebru 等人，以及设计播客99% Invisible。早期汽车没有安全带，仪表盘上有金属旋钮，在事故中可能刺入人们的头颅，常规平板玻璃窗以危险的方式破碎，非可折叠转向柱刺穿驾驶员。然而，汽车公司甚至不愿讨论安全作为他们可以帮助解决的问题，普遍的看法是汽车就是它们的样子，是使用它们的人造成了问题。\n消费者安全活动家和倡导者经过几十年的努力，改变了国家对汽车公司可能需要通过监管来解决一些责任的讨论。可折叠转向柱发明后，由于没有财务激励，几年内并未实施。主要汽车公司通用汽车公司雇佣了私家侦探，试图挖掘消费者安全倡导者拉尔夫·纳德的黑材料。安全带、碰撞测试假人和可折叠转向柱的要求是重大胜利。直到 2011 年，汽车公司才被要求开始使用代表普通女性的碰撞测试假人，而不仅仅是代表普通男性的身体；在此之前，女性在相同冲击下的车祸中受伤的可能性比男性高 40%。这是偏见、政策和技术产生重要后果的生动例证。"
  },
  {
    "objectID": "Fastbook/translations/cn/03_ethics.html#进一步研究",
    "href": "Fastbook/translations/cn/03_ethics.html#进一步研究",
    "title": "第三章：数据伦理",
    "section": "进一步研究",
    "text": "进一步研究\n\n阅读文章“当算法削减您的医疗保健”（链接）。未来如何避免类似问题？\n研究更多关于 YouTube 推荐系统及其社会影响的信息。你认为推荐系统是否必须始终具有带有负面结果的反馈循环？谷歌可以采取什么方法来避免这种情况？政府呢？\n阅读论文“在线广告投放中的歧视”。你认为谷歌应该对 Sweeney 博士发生的事情负责吗？什么是一个合适的回应？\n跨学科团队如何帮助避免负面后果？\n阅读论文“机器学习是否自动化了道德风险和错误？” 你认为应该采取什么行动来处理这篇论文中指出的问题？\n阅读文章“我们将如何防止基于 AI 的伪造？” 你认为 Etzioni 提出的方法能行得通吗？为什么？\n完成部分“分析你正在进行的项目”。\n考虑一下你的团队是否可以更多元化。如果可以，有哪些方法可能会有所帮助？"
  },
  {
    "objectID": "Fastbook/translations/cn/16_accel_sgd.html",
    "href": "Fastbook/translations/cn/16_accel_sgd.html",
    "title": "第十六章：训练过程",
    "section": "",
    "text": "现在你知道如何为计算机视觉、自然图像处理、表格分析和协同过滤创建最先进的架构，也知道如何快速训练它们。所以我们完成了，对吧？还没有。我们仍然需要探索一下训练过程的更多内容。\n我们在第四章中解释了随机梯度下降的基础：将一个小批量数据传递给模型，用损失函数将其与目标进行比较，然后计算这个损失函数对每个权重的梯度，然后使用公式更新权重：\nnew_weight = weight - lr * weight.grad\n我们在训练循环中从头开始实现了这个，看到 PyTorch 提供了一个简单的nn.SGD类，可以为我们的每个参数进行这个计算。在本章中，我们将构建一些更快的优化器，使用一个灵活的基础。但在训练过程中，我们可能还想要改变一些东西。对于训练循环的任何调整，我们都需要一种方法来向 SGD 的基础添加一些代码。fastai 库有一个回调系统来做到这一点，我们将教你所有相关知识。\n让我们从标准的 SGD 开始建立一个基线；然后我们将介绍最常用的优化器。"
  },
  {
    "objectID": "Fastbook/translations/cn/16_accel_sgd.html#创建回调",
    "href": "Fastbook/translations/cn/16_accel_sgd.html#创建回调",
    "title": "第十六章：训练过程",
    "section": "创建回调",
    "text": "创建回调\n当您想要编写自己的回调时，可用事件的完整列表如下：\nbegin_fit\n在做任何事情之前调用；适用于初始设置。\nbegin_epoch\n在每个周期开始时调用；对于需要在每个周期重置的任何行为都很有用。\nbegin_train\n在周期的训练部分开始时调用。\nbegin_batch\n在每个批次开始时调用，就在绘制该批次之后。可以用于对批次进行任何必要的设置（如超参数调度）或在输入/目标进入模型之前对其进行更改（例如，通过应用 Mixup）。\nafter_pred\n在计算模型对批次的输出后调用。可以用于在将其馈送到损失函数之前更改该输出。\nafter_loss\n在计算损失之后但在反向传播之前调用。可以用于向损失添加惩罚（例如在 RNN 训练中的 AR 或 TAR）。\nafter_backward\n在反向传播之后调用，但在参数更新之前调用。可以在更新之前对梯度进行更改（例如通过梯度裁剪）。\nafter_step\n在步骤之后和梯度归零之前调用。\nafter_batch\n在批次结束时调用，以在下一个批次之前执行任何必要的清理。\nafter_train\n在时代的训练阶段结束时调用。\nbegin_validate\n在时代的验证阶段开始时调用；用于特定于验证所需的任何设置。\nafter_validate\n在时代的验证部分结束时调用。\nafter_epoch\n在时代结束时调用，进行下一个时代之前的任何清理。\nafter_fit\n在训练结束时调用，进行最终清理。\n此列表的元素可作为特殊变量event的属性使用，因此您只需在笔记本中键入event.并按 Tab 键即可查看所有选项的列表\n让我们看一个例子。您是否还记得在第十二章中我们需要确保在每个时代的训练和验证开始时调用我们的特殊reset方法？我们使用 fastai 提供的ModelResetter回调来为我们执行此操作。但它究竟是如何工作的呢？这是该类的完整源代码：\nclass ModelResetter(Callback):\n    def begin_train(self):    self.model.reset()\n    def begin_validate(self): self.model.reset()\n是的，实际上就是这样！它只是在完成时代的训练或验证后，调用一个名为reset的方法。\n回调通常像这样“简短而甜美”。实际上，让我们再看一个。这是添加 RNN 正则化（AR 和 TAR）的 fastai 回调的源代码：\nclass RNNRegularizer(Callback):\n    def __init__(self, alpha=0., beta=0.): self.alpha,self.beta = alpha,beta\n\n    def after_pred(self):\n        self.raw_out,self.out = self.pred[1],self.pred[2]\n        self.learn.pred = self.pred[0]\n\n    def after_loss(self):\n        if not self.training: return\n        if self.alpha != 0.:\n            self.learn.loss += self.alpha * self.out[-1].float().pow(2).mean()\n        if self.beta != 0.:\n            h = self.raw_out[-1]\n            if len(h)&gt;1:\n                self.learn.loss += self.beta * (h[:,1:] - h[:,:-1]\n                                               ).float().pow(2).mean()"
  },
  {
    "objectID": "Fastbook/translations/cn/16_accel_sgd.html#回调排序和异常",
    "href": "Fastbook/translations/cn/16_accel_sgd.html#回调排序和异常",
    "title": "第十六章：训练过程",
    "section": "回调排序和异常",
    "text": "回调排序和异常\n有时回调需要能够告诉 fastai 跳过一个批次或一个纪元，或者完全停止训练。例如，考虑TerminateOnNaNCallback。这个方便的回调将在损失变为无穷大或NaN（不是一个数字）时自动停止训练。以下是此回调的 fastai 源代码：\nclass TerminateOnNaNCallback(Callback):\n    run_before=Recorder\n    def after_batch(self):\n        if torch.isinf(self.loss) or torch.isnan(self.loss):\n            raise CancelFitException\nraise CancelFitException这一行告诉训练循环在这一点中断训练。训练循环捕获此异常并不再运行任何进一步的训练或验证。可用的回调控制流异常如下：\nCancelFitException\n跳过本批次的其余部分并转到after_batch。\nCancelEpochException\n跳过本纪元的训练部分的其余部分并转到after_train。\nCancelTrainException\n跳过本纪元的验证部分的其余部分并转到after_validate。\nCancelValidException\n跳过本纪元的其余部分并转到after_epoch。\nCancelBatchException\n训练中断并转到after_fit。\n您可以检测是否发生了其中一个异常，并添加代码，以在以下事件之后立即执行：\nafter_cancel_batch\n在继续到after_batch之前立即到达CancelBatchException后\nafter_cancel_train\n在继续到after_epoch之前立即到达CancelTrainException后\nafter_cancel_valid\n在继续到after_epoch之前立即到达CancelValidException后\nafter_cancel_epoch\n在继续到after_epoch之前立即到达CancelEpochException后\nafter_cancel_fit\n在继续到after_fit之前立即到达CancelFitException后\n有时需要按特定顺序调用回调。例如，在TerminateOnNaNCallback的情况下，很重要的是Recorder在此回调之后运行其after_batch，以避免注册NaN损失。您可以在回调中指定run_before（此回调必须在之前运行…）或run_after（此回调必须在之后运行…）以确保您需要的顺序。"
  },
  {
    "objectID": "Fastbook/translations/cn/16_accel_sgd.html#进一步研究",
    "href": "Fastbook/translations/cn/16_accel_sgd.html#进一步研究",
    "title": "第十六章：训练过程",
    "section": "进一步研究",
    "text": "进一步研究\n\n查阅“修正的 Adam”论文，使用通用优化器框架实现它，并尝试一下。搜索其他最近在实践中表现良好的优化器，并选择一个实现。\n查看文档中的混合精度回调。尝试理解每个事件和代码行的作用。\n从头开始实现自己版本的学习率查找器。与 fastai 的版本进行比较。\n查看 fastai 附带的回调的源代码。看看能否找到一个与你要做的类似的回调，以获得一些灵感。"
  },
  {
    "objectID": "Fastbook/translations/cn/02_production.html",
    "href": "Fastbook/translations/cn/02_production.html",
    "title": "第二章：从模型到生产",
    "section": "",
    "text": "我们在第一章中看到的六行代码只是在实践中使用深度学习过程的一小部分。在本章中，我们将使用一个计算机视觉示例来查看创建深度学习应用的端到端过程。更具体地说，我们将构建一个熊分类器！在这个过程中，我们将讨论深度学习的能力和限制，探讨如何创建数据集，在实践中使用深度学习时可能遇到的问题等等。许多关键点同样适用于其他深度学习问题，例如第一章中的问题。如果您解决的问题在关键方面类似于我们的示例问题，我们期望您可以快速获得极好的结果，而只需很少的代码。\n让我们从如何构建您的问题开始。"
  },
  {
    "objectID": "Fastbook/translations/cn/02_production.html#开始您的项目",
    "href": "Fastbook/translations/cn/02_production.html#开始您的项目",
    "title": "第二章：从模型到生产",
    "section": "开始您的项目",
    "text": "开始您的项目\n那么您应该从哪里开始深度学习之旅呢？最重要的是确保您有一个要处理的项目-只有通过处理自己的项目，您才能获得构建和使用模型的真实经验。在选择项目时，最重要的考虑因素是数据的可用性。\n无论您是为了自己的学习还是为了在组织中的实际应用而进行项目，您都希望能够快速开始。我们看到许多学生、研究人员和行业从业者在试图找到他们完美的数据集时浪费了几个月甚至几年的时间。目标不是找到“完美”的数据集或项目，而只是开始并从那里迭代。如果您采取这种方法，您将在完美主义者仍处于规划阶段时进行第三次迭代学习和改进！\n我们还建议您在项目中端到端迭代；不要花几个月来微调您的模型，或打磨完美的 GUI，或标记完美的数据集……相反，尽可能在合理的时间内完成每一步，一直到最后。例如，如果您的最终目标是一个在手机上运行的应用程序，那么每次迭代后您都应该拥有这个。但也许在早期迭代中您会采取捷径；例如，在远程服务器上进行所有处理，并使用简单的响应式 Web 应用程序。通过完成项目的端到端，您将看到最棘手的部分在哪里，以及哪些部分对最终结果产生最大影响。\n当您阅读本书时，我们建议您完成许多小实验，通过运行和调整我们提供的笔记本，同时逐渐开发自己的项目。这样，您将获得所有我们解释的工具和技术的经验，同时我们讨论它们。"
  },
  {
    "objectID": "Fastbook/translations/cn/02_production.html#深度学习的现状",
    "href": "Fastbook/translations/cn/02_production.html#深度学习的现状",
    "title": "第二章：从模型到生产",
    "section": "深度学习的现状",
    "text": "深度学习的现状\n让我们首先考虑深度学习是否能够解决您要解决的问题。本节概述了 2020 年初深度学习的现状。然而，事情发展得非常快，当您阅读本文时，其中一些限制可能已经不存在。我们将尽力保持本书网站的最新信息；此外，搜索“AI 现在能做什么”可能会提供当前信息。\n\n计算机视觉\n深度学习尚未用于分析图像的许多领域，但在已经尝试过的领域中，几乎普遍表明计算机可以至少与人类一样好地识别图像中的物品，甚至是经过专门训练的人，如放射科医生。这被称为物体识别。深度学习还擅长识别图像中物体的位置，并可以突出它们的位置并命名每个找到的物体。这被称为物体检测（在我们在第一章中看到的变体中，每个像素根据其所属的对象类型进行分类—这被称为分割）。\n深度学习算法通常不擅长识别结构或风格与用于训练模型的图像明显不同的图像。例如，如果训练数据中没有黑白图像，模型可能在黑白图像上表现不佳。同样，如果训练数据不包含手绘图像，模型可能在手绘图像上表现不佳。没有一般方法可以检查训练集中缺少哪些类型的图像，但我们将在本章中展示一些方法，以尝试识别当模型在生产中使用时数据中出现意外图像类型的情况（这被称为检查域外数据）。\n物体检测系统面临的一个主要挑战是图像标记可能会很慢且昂贵。目前有很多工作正在进行中，旨在开发工具以尝试使这种标记更快速、更容易，并且需要更少的手工标签来训练准确的物体检测模型。一个特别有帮助的方法是合成生成输入图像的变化，例如通过旋转它们或改变它们的亮度和对比度；这被称为数据增强，并且对文本和其他类型的模型也很有效。我们将在本章中详细讨论这一点。\n另一个要考虑的问题是，尽管您的问题可能看起来不像是一个计算机视觉问题，但通过一点想象力可能可以将其转变为一个。例如，如果您要分类的是声音，您可以尝试将声音转换为其声学波形的图像，然后在这些图像上训练模型。\n\n\n自然语言处理\n计算机擅长基于类别对短文档和长文档进行分类，例如垃圾邮件或非垃圾邮件、情感（例如，评论是积极的还是消极的）、作者、来源网站等。我们不知道在这个领域是否有任何严格的工作来比较计算机和人类，但从经验上看，我们认为深度学习的性能在这些任务上与人类的性能相似。\n深度学习还擅长生成与上下文相关的文本，例如回复社交媒体帖子，并模仿特定作者的风格。它还擅长使这些内容对人类具有吸引力—事实上，甚至比人类生成的文本更具吸引力。然而，深度学习不擅长生成正确的回应！例如，我们没有可靠的方法来将医学信息知识库与深度学习模型结合起来，以生成医学上正确的自然语言回应。这是危险的，因为很容易创建对外行人看来具有吸引力但实际上完全不正确的内容。\n另一个问题是，社交媒体上的上下文适当、高度引人入胜的回应可能被大规模使用——比以前见过的任何喷子农场规模大几千倍——来传播虚假信息，制造动荡，鼓励冲突。一般来说，文本生成模型总是在技术上略领先于识别自动生成文本的模型。例如，可以使用一个能够识别人工生成内容的模型来实际改进创建该内容的生成器，直到分类模型无法完成其任务为止。\n尽管存在这些问题，深度学习在自然语言处理中有许多应用：可以用来将文本从一种语言翻译成另一种语言，将长篇文档总结为更快消化的内容，找到感兴趣概念的所有提及等。不幸的是，翻译或总结可能包含完全错误的信息！然而，性能已经足够好，许多人正在使用这些系统——例如，谷歌的在线翻译系统（以及我们所知道的每个其他在线服务）都是基于深度学习的。\n\n\n结合文本和图像\n深度学习将文本和图像结合成一个单一模型的能力通常比大多数人直觉期望的要好得多。例如，一个深度学习模型可以在输入图像上进行训练，输出用英语编写的标题，并且可以学会为新图像自动生成令人惊讶地适当的标题！但是，我们再次提出与前一节讨论的相同警告：不能保证这些标题是正确的。\n由于这个严重问题，我们通常建议深度学习不要作为完全自动化的过程，而是作为模型和人类用户密切互动的过程的一部分。这可能使人类的生产力比完全手动方法高出几个数量级，并且比仅使用人类更准确。\n例如，自动系统可以直接从 CT 扫描中识别潜在的中风患者，并发送高优先级警报，以便快速查看这些扫描。治疗中风只有三个小时的时间窗口，因此这种快速的反馈循环可以挽救生命。同时，所有扫描仍然可以按照通常的方式发送给放射科医生，因此不会减少人类的参与。其他深度学习模型可以自动测量扫描中看到的物品，并将这些测量结果插入报告中，警告放射科医生可能错过的发现，并告诉他们可能相关的其他病例。\n\n\n表格数据\n对于分析时间序列和表格数据，深度学习最近取得了巨大进展。然而，深度学习通常作为多种模型集成的一部分使用。如果您已经有一个正在使用随机森林或梯度提升机（流行的表格建模工具，您很快将了解）的系统，那么切换到或添加深度学习可能不会带来任何显著的改进。\n深度学习确实大大增加了您可以包含的列的种类——例如，包含自然语言（书名、评论等）和高基数分类列（即包含大量离散选择的内容，如邮政编码或产品 ID）。不过，与随机森林或梯度提升机相比，深度学习模型通常需要更长的训练时间，尽管由于提供 GPU 加速的库（如RAPIDS），情况正在改变。我们在第九章中详细介绍了所有这些方法的优缺点。\n\n\n推荐系统\n推荐系统实际上只是一种特殊类型的表格数据。特别是，它们通常具有代表用户的高基数分类变量，以及代表产品（或类似物品）的另一个变量。像亚马逊这样的公司将客户所做的每一次购买都表示为一个巨大的稀疏矩阵，其中客户是行，产品是列。一旦他们以这种格式拥有数据，数据科学家们会应用某种形式的协同过滤来填充矩阵。例如，如果客户 A 购买产品 1 和 10，客户 B 购买产品 1、2、4 和 10，引擎将推荐 A 购买 2 和 4。\n由于深度学习模型擅长处理高基数分类变量，它们非常擅长处理推荐系统。尤其是当将这些变量与其他类型的数据（如自然语言或图像）结合时，它们就像处理表格数据一样发挥作用。它们还可以很好地将所有这些类型的信息与其他元数据（如用户信息、先前交易等）表示为表格进行组合。\n然而，几乎所有的机器学习方法都有一个缺点，那就是它们只告诉你一个特定用户可能喜欢哪些产品，而不是对用户有用的推荐。用户可能喜欢的产品的许多种推荐可能根本不会有任何帮助——例如，如果用户已经熟悉这些产品，或者如果它们只是用户已经购买过的产品的不同包装（例如，当他们已经拥有该套装中的每一件物品时，推荐一个小说的套装）。Jeremy 喜欢读特里·普拉切特的书，有一段时间亚马逊一直在向他推荐特里·普拉切特的书（见图 2-1），这实际上并不是有用的，因为他已经知道这些书了！\n\n\n\n特里·普拉切特的书推荐\n\n\n\n图 2-1. 一个不太有用的推荐\n\n\n\n其他数据类型\n通常，您会发现特定领域的数据类型非常适合现有的类别。例如，蛋白质链看起来很像自然语言文档，因为它们是由复杂关系和意义贯穿整个序列的离散令牌组成的长序列。事实上，使用 NLP 深度学习方法是许多类型蛋白质分析的最先进方法。另一个例子，声音可以表示为频谱图，可以被视为图像；标准的图像深度学习方法在频谱图上表现得非常好。"
  },
  {
    "objectID": "Fastbook/translations/cn/02_production.html#驱动系统方法",
    "href": "Fastbook/translations/cn/02_production.html#驱动系统方法",
    "title": "第二章：从模型到生产",
    "section": "驱动系统方法",
    "text": "驱动系统方法\n许多准确的模型对任何人都没有用，而许多不准确的模型却非常有用。为了确保您的建模工作在实践中有用，您需要考虑您的工作将如何使用。2012 年，Jeremy 与 Margit Zwemer 和 Mike Loukides 一起提出了一种称为驱动系统方法的思考这个问题的方法。\n驱动系统方法，如图 2-2 所示，详细介绍在“设计出色的数据产品”中。基本思想是从考虑您的目标开始，然后考虑您可以采取哪些行动来实现该目标以及您拥有的（或可以获取的）可以帮助的数据，然后构建一个模型，您可以使用该模型确定为实现目标而采取的最佳行动。\n\n\n图 2-2. 驱动系统方法\n考虑自动驾驶汽车中的模型：您希望帮助汽车安全地从 A 点驾驶到 B 点，而无需人为干预。出色的预测建模是解决方案的重要组成部分，但它并不是独立存在的；随着产品变得更加复杂，它会消失在管道中。使用自动驾驶汽车的人完全不知道使其运行的数百（甚至数千）个模型和海量数据。但随着数据科学家构建越来越复杂的产品，他们需要一种系统化的设计方法。\n我们使用数据不仅仅是为了生成更多数据（以预测的形式），而是为了产生可操作的结果。这是 Drivetrain 方法的目标。首先要明确定义一个明确的目标。例如，当谷歌创建其第一个搜索引擎时，考虑了“用户在输入搜索查询时的主要目标是什么？”这导致了谷歌的目标，即“显示最相关的搜索结果”。下一步是考虑您可以拉动的杠杆（即您可以采取的行动）以更好地实现该目标。在谷歌的情况下，这是搜索结果的排名。第三步是考虑他们需要什么新数据来生成这样的排名；他们意识到关于哪些页面链接到哪些其他页面的隐含信息可以用于此目的。\n只有在完成了这前三个步骤之后，我们才开始考虑构建预测模型。我们的目标和可用的杠杆，我们已经拥有的数据以及我们需要收集的额外数据，决定了我们可以构建的模型。这些模型将以杠杆和任何不可控变量作为输入；模型的输出可以结合起来预测我们的目标的最终状态。\n让我们考虑另一个例子：推荐系统。推荐引擎的目标是通过推荐客户不会在没有推荐的情况下购买的物品来推动额外的销售。杠杆是推荐的排名。必须收集新数据以生成将导致新销售的推荐。这将需要进行许多随机实验，以收集关于各种客户的各种推荐的数据。这是很少有组织采取的一步；但是没有它，您就没有所需的信息来根据您的真正目标（更多销售！）优化推荐。\n最后，您可以为购买概率构建两个模型，条件是看到或没有看到推荐。这两个概率之间的差异是给定推荐给客户的效用函数。在算法推荐客户已经拒绝的熟悉书籍（两个组成部分都很小）或者他们本来就会购买的书籍（两个组成部分都很大并互相抵消）的情况下，效用函数会很低。\n正如您所看到的，在实践中，您的模型的实际实施通常需要比仅仅训练一个模型更多！您通常需要运行实验来收集更多数据，并考虑如何将您的模型整合到您正在开发的整个系统中。说到数据，现在让我们专注于如何为您的项目找到数据。"
  },
  {
    "objectID": "Fastbook/translations/cn/02_production.html#数据增强",
    "href": "Fastbook/translations/cn/02_production.html#数据增强",
    "title": "第二章：从模型到生产",
    "section": "数据增强",
    "text": "数据增强\n数据增强指的是创建输入数据的随机变化，使它们看起来不同但不改变数据的含义。对于图像的常见数据增强技术包括旋转、翻转、透视变形、亮度变化和对比度变化。对于我们在这里使用的自然照片图像，我们发现一组标准的增强技术与aug_transforms函数一起提供，效果非常好。\n因为我们的图像现在都是相同大小，我们可以使用 GPU 将这些增强应用于整个批次的图像，这将节省大量时间。要告诉 fastai 我们要在批次上使用这些变换，我们使用batch_tfms参数（请注意，在此示例中我们没有使用RandomResizedCrop，这样您可以更清楚地看到差异；出于同样的原因，我们使用了默认值的两倍的增强量）：\nbears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=8, nrows=2, unique=True)\n\n现在我们已经将数据组装成适合模型训练的格式，让我们使用它来训练一个图像分类器。"
  },
  {
    "objectID": "Fastbook/translations/cn/02_production.html#使用模型进行推断",
    "href": "Fastbook/translations/cn/02_production.html#使用模型进行推断",
    "title": "第二章：从模型到生产",
    "section": "使用模型进行推断",
    "text": "使用模型进行推断\n一旦您拥有一个满意的模型，您需要保存它，以便随后将其复制到一个服务器上，在那里您将在生产中使用它。请记住，模型由两部分组成：架构和训练的参数。保存模型的最简单方法是保存这两部分，因为这样，当您加载模型时，您可以确保具有匹配的架构和参数。要保存这两部分，请使用export方法。\n这种方法甚至保存了如何创建您的DataLoaders的定义。这很重要，因为否则您将不得不重新定义如何转换您的数据以便在生产中使用您的模型。fastai 默认使用验证集DataLoader进行推理，因此不会应用数据增强，这通常是您想要的。\n当您调用export时，fastai 将保存一个名为export.pkl的文件：\nlearn.export()\n让我们通过使用 fastai 添加到 Python 的Path类的ls方法来检查文件是否存在：\npath = Path()\npath.ls(file_exts='.pkl')\n(#1) [Path('export.pkl')]\n您需要这个文件在您部署应用程序的任何地方。现在，让我们尝试在我们的笔记本中创建一个简单的应用程序。\n当我们使用模型进行预测而不是训练时，我们称之为推理。要从导出的文件创建我们的推理学习者，我们使用load_learner（在这种情况下，这并不是真正必要的，因为我们已经在笔记本中有一个工作的Learner；我们在这里这样做是为了让您看到整个过程的始终）：\nlearn_inf = load_learner(path/'export.pkl')\n在进行推理时，通常一次只为一个图像获取预测。要做到这一点，将文件名传递给predict：\nlearn_inf.predict('images/grizzly.jpg')\n('grizzly', tensor(1), tensor([9.0767e-06, 9.9999e-01, 1.5748e-07]))\n这返回了三个东西：以与您最初提供的格式相同的预测类别（在本例中，这是一个字符串），预测类别的索引以及每个类别的概率。最后两个是基于DataLoaders的vocab中类别的顺序；也就是说，所有可能类别的存储列表。在推理时，您可以将DataLoaders作为Learner的属性访问：\nlearn_inf.dls.vocab\n(#3) ['black','grizzly','teddy']\n我们可以看到，如果我们使用predict返回的整数索引到 vocab 中，我们会得到“灰熊”，这是预期的。另外，请注意，如果我们在概率列表中进行索引，我们会看到几乎有 1.00 的概率这是一只灰熊。\n我们知道如何从保存的模型中进行预测，因此我们拥有开始构建我们的应用程序所需的一切。我们可以直接在 Jupyter 笔记本中完成。"
  },
  {
    "objectID": "Fastbook/translations/cn/02_production.html#从模型创建一个笔记本应用",
    "href": "Fastbook/translations/cn/02_production.html#从模型创建一个笔记本应用",
    "title": "第二章：从模型到生产",
    "section": "从模型创建一个笔记本应用",
    "text": "从模型创建一个笔记本应用\n要在应用程序中使用我们的模型，我们可以简单地将predict方法视为常规函数。因此，使用任何应用程序开发人员可用的各种框架和技术都可以创建一个从模型创建的应用程序。\n然而，大多数数据科学家并不熟悉 Web 应用程序开发领域。因此，让我们尝试使用您目前已经了解的东西：事实证明，我们可以仅使用 Jupyter 笔记本创建一个完整的工作 Web 应用程序！使这一切成为可能的两个因素如下：\n\nIPython 小部件(ipywidgets)\nVoilà\n\nIPython 小部件是 GUI 组件，它在 Web 浏览器中将 JavaScript 和 Python 功能结合在一起，并可以在 Jupyter 笔记本中创建和使用。例如，我们在本章前面看到的图像清理器完全是用 IPython 小部件编写的。但是，我们不希望要求我们的应用程序用户自己运行 Jupyter。\n这就是Voilà存在的原因。它是一个使 IPython 小部件应用程序可供最终用户使用的系统，而无需他们使用 Jupyter。Voilà利用了一个事实，即笔记本已经是一种 Web 应用程序，只是另一个复杂的依赖于另一个 Web 应用程序：Jupyter 本身的 Web 应用程序。基本上，它帮助我们自动将我们已经隐式创建的复杂 Web 应用程序（笔记本）转换为一个更简单、更易部署的 Web 应用程序，它的功能类似于普通的 Web 应用程序，而不是笔记本。\n但是我们仍然可以在笔记本中开发的优势，因此使用 ipywidgets，我们可以逐步构建我们的 GUI。我们将使用这种方法创建一个简单的图像分类器。首先，我们需要一个文件上传小部件：\nbtn_upload = widgets.FileUpload()\nbtn_upload\n\n\n\n上传按钮\n\n\n现在我们可以获取图像：\nimg = PILImage.create(btn_upload.data[-1])\n\n\n\n表示图像的输出小部件\n\n\n我们可以使用Output小部件来显示它：\nout_pl = widgets.Output()\nout_pl.clear_output()\nwith out_pl: display(img.to_thumb(128,128))\nout_pl\n\n\n\n表示图像的输出小部件\n\n\n然后我们可以得到我们的预测：\npred,pred_idx,probs = learn_inf.predict(img)\n并使用Label来显示它们：\nlbl_pred = widgets.Label()\nlbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\nlbl_pred\n预测：灰熊；概率：1.0000\n我们需要一个按钮来进行分类。它看起来与上传按钮完全相同：\nbtn_run = widgets.Button(description='Classify')\nbtn_run\n我们还需要一个点击事件处理程序；也就是说，当按下按钮时将调用的函数。我们可以简单地复制之前的代码行：\ndef on_click_classify(change):\n    img = PILImage.create(btn_upload.data[-1])\n    out_pl.clear_output()\n    with out_pl: display(img.to_thumb(128,128))\n    pred,pred_idx,probs = learn_inf.predict(img)\n    lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\n\nbtn_run.on_click(on_click_classify)\n您现在可以通过单击按钮来测试按钮，您应该会看到图像和预测会自动更新！\n现在，我们可以将它们全部放在一个垂直框（VBox）中，以完成我们的 GUI：\nVBox([widgets.Label('Select your bear!'),\n      btn_upload, btn_run, out_pl, lbl_pred])\n\n\n\n整个小部件\n\n\n我们已经编写了所有必要的应用程序代码。下一步是将其转换为我们可以部署的内容。"
  },
  {
    "objectID": "Fastbook/translations/cn/02_production.html#将您的笔记本变成一个真正的应用程序",
    "href": "Fastbook/translations/cn/02_production.html#将您的笔记本变成一个真正的应用程序",
    "title": "第二章：从模型到生产",
    "section": "将您的笔记本变成一个真正的应用程序",
    "text": "将您的笔记本变成一个真正的应用程序\n现在我们在这个 Jupyter 笔记本中已经让一切运转起来了，我们可以创建我们的应用程序。为此，请启动一个新的笔记本，并仅添加创建和显示所需小部件的代码，以及任何要显示的文本的 Markdown。查看书中存储库中的bear_classifier笔记本，看看我们创建的简单笔记本应用程序。\n接下来，如果您尚未安装 Voilà，请将这些行复制到笔记本单元格中并执行：\n!pip install voila\n!jupyter serverextension enable voila --sys-prefix\n以!开头的单元格不包含 Python 代码，而是包含传递给您的 shell（bash，Windows PowerShell 等）的代码。如果您习惯使用命令行，我们将在本书中更详细地讨论这一点，您当然可以直接在终端中键入这两行（不带!前缀）。在这种情况下，第一行安装voila库和应用程序，第二行将其连接到您现有的 Jupyter 笔记本。\nVoilà运行 Jupyter 笔记本，就像您现在使用的 Jupyter 笔记本服务器一样，但它还做了一件非常重要的事情：它删除了所有单元格输入，仅显示输出（包括 ipywidgets），以及您的 Markdown 单元格。因此，剩下的是一个 Web 应用程序！要将您的笔记本视为 Voilà Web 应用程序，请将浏览器 URL 中的“notebooks”一词替换为“voila/render”。您将看到与您的笔记本相同的内容，但没有任何代码单元格。\n当然，您不需要使用 Voilà或 ipywidgets。您的模型只是一个可以调用的函数（pred，pred_idx，probs = learn.predict（img）），因此您可以将其与任何框架一起使用，托管在任何平台上。您可以将在 ipywidgets 和 Voilà中原型设计的内容稍后转换为常规 Web 应用程序。我们在本书中展示这种方法，因为我们认为这是数据科学家和其他不是 Web 开发专家的人从其模型创建应用程序的绝佳方式。\n我们有了我们的应用程序；现在让我们部署它！"
  },
  {
    "objectID": "Fastbook/translations/cn/02_production.html#部署您的应用程序",
    "href": "Fastbook/translations/cn/02_production.html#部署您的应用程序",
    "title": "第二章：从模型到生产",
    "section": "部署您的应用程序",
    "text": "部署您的应用程序\n正如您现在所知，几乎任何有用的深度学习模型都需要 GPU 来训练。那么，在生产中使用该模型需要 GPU 吗？不需要！您几乎可以肯定在生产中不需要 GPU 来提供您的模型。这样做有几个原因：\n\n正如我们所见，GPU 仅在并行执行大量相同工作时才有用。如果您正在进行（比如）图像分类，通常一次只会对一个用户的图像进行分类，而且通常在一张图像中没有足够的工作量可以让 GPU 忙碌足够长的时间以使其非常有效。因此，CPU 通常更具成本效益。\n另一种选择可能是等待一些用户提交他们的图像，然后将它们批量处理并一次性在 GPU 上处理。但是这样会让用户等待，而不是立即得到答案！而且您需要一个高流量的网站才能实现这一点。如果您确实需要这种功能，您可以使用诸如 Microsoft 的ONNX Runtime或AWS SageMaker之类的工具。\n处理 GPU 推理的复杂性很大。特别是，GPU 的内存需要仔细手动管理，您需要一个仔细的排队系统，以确保一次只处理一个批次。\nCPU 服务器的市场竞争要比 GPU 服务器更激烈，因此 CPU 服务器有更便宜的选项可供选择。\n\n由于 GPU 服务的复杂性，许多系统已经出现尝试自动化此过程。然而，管理和运行这些系统也很复杂，通常需要将您的模型编译成专门针对该系统的不同形式。通常最好避免处理这种复杂性，直到/除非您的应用程序变得足够受欢迎，以至于您有明显的财务理由这样做。\n至少对于您的应用程序的初始原型以及您想展示的任何爱好项目，您可以轻松免费托管它们。最佳位置和最佳方式随时间而变化，因此请查看本书网站以获取最新的建议。由于我们在 2020 年初撰写本书，最简单（且免费！）的方法是使用Binder。要在 Binder 上发布您的 Web 应用程序，请按照以下步骤操作：\n\n将您的笔记本添加到GitHub 存储库。\n将该存储库的 URL 粘贴到 Binder 的 URL 字段中，如图 2-4 所示。\n将文件下拉菜单更改为选择 URL。\n在“要打开的 URL”字段中，输入/voila/render/*name*.ipynb（将name替换为您笔记本的名称）。\n单击右下角的剪贴板按钮以复制 URL，并将其粘贴到安全位置。\n单击“启动”。\n\n\n\n\n部署到 Binder\n\n\n\n图 2-4. 部署到 Binder\n第一次执行此操作时，Binder 将花费大约 5 分钟来构建您的站点。在幕后，它正在查找一个可以运行您的应用程序的虚拟机，分配存储空间，并收集所需的文件以用于 Jupyter、您的笔记本以及将您的笔记本呈现为 Web 应用程序。\n最后，一旦启动应用程序运行，它将导航您的浏览器到您的新 Web 应用程序。您可以分享您复制的 URL 以允许其他人访问您的应用程序。\n要了解部署 Web 应用程序的其他（免费和付费）选项，请务必查看书籍网站。\n您可能希望将应用程序部署到移动设备或边缘设备，如树莓派。有许多库和框架允许您将模型直接集成到移动应用程序中。但是，这些方法往往需要许多额外的步骤和样板文件，并且并不总是支持您的模型可能使用的所有 PyTorch 和 fastai 层。此外，您所做的工作将取决于您针对部署的移动设备的类型 - 您可能需要做一些工作以在 iOS 设备上运行，不同的工作以在较新的 Android 设备上运行，不同的工作以在较旧的 Android 设备上运行，等等。相反，我们建议在可能的情况下，将模型本身部署到服务器，并让您的移动或边缘应用程序连接到它作为 Web 服务。\n这种方法有很多优点。初始安装更容易，因为您只需部署一个小型 GUI 应用程序，该应用程序连接到服务器执行所有繁重的工作。更重要的是，核心逻辑的升级可以在您的服务器上进行，而不需要分发给所有用户。您的服务器将拥有比大多数边缘设备更多的内存和处理能力，并且如果您的模型变得更加苛刻，那么扩展这些资源将更容易。您在服务器上拥有的硬件也将更加标准化，并且更容易受到 fastai 和 PyTorch 的支持，因此您不必将模型编译成不同的形式。\n当然也有缺点。你的应用程序将需要网络连接，每次调用模型时都会有一些延迟。（神经网络模型本来就需要一段时间来运行，所以这种额外的网络延迟在实践中可能对用户没有太大影响。事实上，由于你可以在服务器上使用更好的硬件，总体延迟甚至可能比在本地运行时更少！）此外，如果你的应用程序使用敏感数据，你的用户可能会担心采用将数据发送到远程服务器的方法，因此有时隐私考虑将意味着你需要在边缘设备上运行模型（通过在公司防火墙内部设置本地服务器可能可以避免这种情况）。管理复杂性和扩展服务器也可能会带来额外的开销，而如果你的模型在边缘设备上运行，每个用户都会带来自己的计算资源，这将导致随着用户数量的增加更容易扩展（也称为水平扩展）。"
  },
  {
    "objectID": "Fastbook/translations/cn/02_production.html#意想不到的后果和反馈循环",
    "href": "Fastbook/translations/cn/02_production.html#意想不到的后果和反馈循环",
    "title": "第二章：从模型到生产",
    "section": "意想不到的后果和反馈循环",
    "text": "意想不到的后果和反馈循环\n推出模型的最大挑战之一是，您的模型可能会改变其所属系统的行为。例如，考虑一个“预测执法”算法，它预测某些社区的犯罪率更高，导致更多警察被派往这些社区，这可能导致这些社区记录更多犯罪，依此类推。在皇家统计学会的论文“预测和服务？”中，Kristian Lum 和 William Isaac 观察到“预测性执法的命名恰如其分：它预测未来的执法，而不是未来的犯罪。”\n在这种情况下的部分问题是，在存在偏见的情况下（我们将在下一章中深入讨论），反馈循环可能导致该偏见的负面影响变得越来越严重。例如，在美国已经存在着在种族基础上逮捕率存在显著偏见的担忧。根据美国公民自由联盟的说法，“尽管使用率大致相等，黑人因大麻被逮捕的可能性是白人的 3.73 倍。”这种偏见的影响，以及在美国许多地区推出预测性执法算法，导致 Bärí Williams 在纽约时报中写道：“在我的职业生涯中引起如此多兴奋的技术正在以可能意味着在未来几年，我的 7 岁儿子更有可能因为他的种族和我们居住的地方而被无故定性或逮捕，甚至更糟。”\n在推出重要的机器学习系统之前，一个有用的练习是考虑这个问题：“如果它真的很成功会发生什么？”换句话说，如果预测能力非常高，对行为的影响非常显著，那么会发生什么？谁会受到最大影响？最极端的结果可能是什么样的？你怎么知道到底发生了什么？\n这样的思考练习可能会帮助你制定一个更加谨慎的推出计划，配备持续监控系统和人类监督。当然，如果人类监督没有被听取，那么它就没有用，因此确保可靠和有弹性的沟通渠道存在，以便正确的人会意识到问题并有权力解决它们。"
  },
  {
    "objectID": "Fastbook/translations/cn/02_production.html#进一步研究",
    "href": "Fastbook/translations/cn/02_production.html#进一步研究",
    "title": "第二章：从模型到生产",
    "section": "进一步研究",
    "text": "进一步研究\n\n考虑一下驱动器方法如何映射到你感兴趣的项目或问题。\n在什么情况下最好避免某些类型的数据增强？\n对于你有兴趣应用深度学习的项目，考虑一下这个思维实验，“如果它进展得非常顺利会发生什么？”\n开始写博客，撰写你的第一篇博客文章。例如，写一下你认为深度学习在你感兴趣的领域可能有用的地方。"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html",
    "href": "Fastbook/translations/cn/09_tabular.html",
    "title": "第九章：表格建模深入探讨",
    "section": "",
    "text": "表格建模将数据以表格形式（如电子表格或 CSV 文件）呈现。目标是基于其他列中的值来预测一列中的值。在本章中，我们将不仅看深度学习，还将看更一般的机器学习技术，如随机森林，因为根据您的问题，它们可能会给出更好的结果。\n我们将看看我们应该如何预处理和清理数据，以及如何在训练后解释我们模型的结果，但首先我们将看看如何通过使用嵌入将包含类别的列馈送到期望数字的模型中。"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#kaggle-竞赛",
    "href": "Fastbook/translations/cn/09_tabular.html#kaggle-竞赛",
    "title": "第九章：表格建模深入探讨",
    "section": "Kaggle 竞赛",
    "text": "Kaggle 竞赛\nKaggle 是一个非常棒的资源，适合有志成为数据科学家或任何希望提高机器学习技能的人。没有什么比亲自动手实践并获得实时反馈来帮助您提高技能。\nKaggle 提供了以下内容：\n\n有趣的数据集\n关于您的表现的反馈\n排行榜可以看到什么是好的，什么是可能的，以及什么是最先进的\n获奖选手分享有用的技巧和技术的博客文章\n\n到目前为止，我们所有的数据集都可以通过 fastai 的集成数据集系统下载。然而，在本章中我们将使用的数据集只能从 Kaggle 获取。因此，您需要在该网站上注册，然后转到比赛页面。在该页面上点击规则，然后点击我理解并接受。（尽管比赛已经结束，您不会参加，但您仍然需要同意规则才能下载数据。）\n下载 Kaggle 数据集的最简单方法是使用 Kaggle API。您可以通过使用pip安装它，并在笔记本单元格中运行以下命令：\n!pip install kaggle\n使用 Kaggle API 需要一个 API 密钥；要获取一个，点击 Kaggle 网站上的个人资料图片，选择我的账户；然后点击创建新的 API 令牌。这将在您的 PC 上保存一个名为kaggle.json的文件。您需要将此密钥复制到您的 GPU 服务器上。为此，请打开您下载的文件，复制内容，并将其粘贴到与本章相关的笔记本中的以下单引号内（例如，creds =‘{\"username\":\"*xxx*\",\"key\":\"*xxx*\"}’``）：\ncreds = ''\n然后执行此单元格（这只需要运行一次）：\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write(creds)\n    cred_path.chmod(0o600)\n现在您可以从 Kaggle 下载数据集！选择一个路径来下载数据集：\npath = URLs.path('bluebook')\npath\nPath('/home/sgugger/.fastai/archive/bluebook')\n然后使用 Kaggle API 将数据集下载到该路径并解压缩：\nif not path.exists():\n    path.mkdir()\n    api.competition_download_cli('bluebook-for-bulldozers', path=path)\n    file_extract(path/'bluebook-for-bulldozers.zip')\n\npath.ls(file_type='text')\n(#7) [Path('Valid.csv'),Path('Machine_Appendix.csv'),Path('ValidSolution.csv'),P\n &gt; ath('TrainAndValid.csv'),Path('random_forest_benchmark_test.csv'),Path('Test.\n &gt; csv'),Path('median_benchmark.csv')]\n现在我们已经下载了数据集，让我们来看一下！"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#查看数据",
    "href": "Fastbook/translations/cn/09_tabular.html#查看数据",
    "title": "第九章：表格建模深入探讨",
    "section": "查看数据",
    "text": "查看数据\nKaggle 提供了有关我们数据集中某些字段的信息。数据页面解释了train.csv中的关键字段如下：\nSalesID\n销售的唯一标识符。\nMachineID\n机器的唯一标识符。一台机器可以被多次出售。\n销售价格\n机器在拍卖中的售价（仅在train.csv中提供）。\n销售日期\n销售日期。\n在任何数据科学工作中，直接查看数据是很重要的，以确保您了解格式、存储方式、包含的值类型等。即使您已经阅读了数据的描述，实际数据可能并非您所期望的。我们将从将训练集读入 Pandas DataFrame 开始。通常，除非 Pandas 实际耗尽内存并返回错误，否则最好也指定low_memory=False。low_memory参数默认为True，告诉 Pandas 一次只查看几行数据，以确定每列中包含的数据类型。这意味着 Pandas 最终可能会为不同的行使用不同的数据类型，这通常会导致数据处理错误或模型训练问题。\n让我们加载数据并查看列：\ndf = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\ndf.columns\nIndex(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',\n       'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n       'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',\n       'fiModelSeries', 'fiModelDescriptor', 'ProductSize',\n       'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',\n       'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',\n       'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',\n       'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',\n       'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',\n       'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',\n       'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',\n       'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',\n       'Travel_Controls', 'Differential_Type', 'Steering_Controls'],\n      dtype='object')\n这是我们要查看的许多列！尝试浏览数据集，了解每个列中包含的信息类型。我们很快将看到如何“聚焦”于最有趣的部分。\n在这一点上，一个很好的下一步是处理有序列的列。这指的是包含字符串或类似内容的列，但其中这些字符串具有自然排序。例如，这里是ProductSize的级别：\ndf['ProductSize'].unique()\narray([nan, 'Medium', 'Small', 'Large / Medium', 'Mini', 'Large', 'Compact'],\n &gt; dtype=object)\n我们可以告诉 Pandas 这些级别的适当排序方式如下：\nsizes = 'Large','Large / Medium','Medium','Small','Mini','Compact'\ndf['ProductSize'] = df['ProductSize'].astype('category')\ndf['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)\n最重要的数据列是因变量——我们想要预测的变量。请记住，模型的度量是反映预测有多好的函数。重要的是要注意项目使用的度量标准。通常，选择度量标准是项目设置的重要部分。在许多情况下，选择一个好的度量标准将需要不仅仅是选择一个已经存在的变量。这更像是一个设计过程。您应该仔细考虑哪种度量标准，或一组度量标准，实际上衡量了对您重要的模型质量概念。如果没有变量代表该度量标准，您应该看看是否可以从可用的变量构建度量标准。\n然而，在这种情况下，Kaggle 告诉我们要使用的度量标准是实际和预测拍卖价格之间的平方对数误差（RMLSE）。我们只需要进行少量处理即可使用这个度量标准：我们取价格的对数，这样该值的m_rmse将给出我们最终需要的值：\ndep_var = 'SalePrice'\ndf[dep_var] = np.log(df[dep_var])\n我们现在准备探索我们的第一个用于表格数据的机器学习算法：决策树。"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#处理日期",
    "href": "Fastbook/translations/cn/09_tabular.html#处理日期",
    "title": "第九章：表格建模深入探讨",
    "section": "处理日期",
    "text": "处理日期\n我们需要做的第一件数据准备工作是丰富我们对日期的表示。我们刚刚描述的决策树的基本基础是二分 - 将一组分成两组。我们查看序数变量，并根据变量的值是大于（或小于）阈值来划分数据集，我们查看分类变量，并根据变量的级别是否是特定级别来划分数据集。因此，这个算法有一种根据序数和分类数据划分数据集的方法。\n但是这如何适用于常见的数据类型，日期呢？您可能希望将日期视为序数值，因为说一个日期比另一个日期更大是有意义的。然而，日期与大多数序数值有所不同，因为一些日期在某种方面与其他日期有质的不同，这通常与我们建模的系统相关。\n为了帮助我们的算法智能处理日期，我们希望我们的模型不仅知道一个日期是否比另一个日期更近或更早。我们可能希望我们的模型根据日期的星期几、某一天是否是假期、所在月份等来做决策。为此，我们用一组日期元数据列替换每个日期列，例如假期、星期几和月份。这些列提供了我们认为会有用的分类数据。\nfastai 带有一个函数，可以为我们执行此操作-我们只需传递包含日期的列名：\ndf = add_datepart(df, 'saledate')\n在那里的同时，让我们为测试集做同样的事情：\ndf_test = pd.read_csv(path/'Test.csv', low_memory=False)\ndf_test = add_datepart(df_test, 'saledate')\n我们可以看到我们的 DataFrame 中现在有很多新的列：\n' '.join(o for o in df.columns if o.startswith('sale'))\n'saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear\n &gt; saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start\n &gt; saleIs_year_end saleIs_year_start saleElapsed'\n这是一个很好的第一步，但我们需要做更多的清理。为此，我们将使用 fastai 对象TabularPandas和TabularProc。"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#使用-tabularpandas-和-tabularproc",
    "href": "Fastbook/translations/cn/09_tabular.html#使用-tabularpandas-和-tabularproc",
    "title": "第九章：表格建模深入探讨",
    "section": "使用 TabularPandas 和 TabularProc",
    "text": "使用 TabularPandas 和 TabularProc\n第二个预处理步骤是确保我们可以处理字符串和缺失数据。默认情况下，sklearn 都不能处理。相反，我们将使用 fastai 的TabularPandas类，它包装了一个 Pandas DataFrame 并提供了一些便利。为了填充一个TabularPandas，我们将使用两个TabularProc，Categorify和FillMissing。TabularProc类似于常规的Transform，但有以下不同：\n\n它返回传递给它的完全相同的对象，在原地修改对象后返回。\n它在数据首次传入时运行变换，而不是在访问数据时懒惰地运行。\n\nCategorify是一个TabularProc，用数字分类列替换列。FillMissing是一个TabularProc，用列的中位数替换缺失值，并创建一个新的布尔列，对于任何值缺失的行，该列设置为True。这两个变换几乎适用于您将使用的每个表格数据集，因此这是您数据处理的一个很好的起点：\nprocs = [Categorify, FillMissing]\nTabularPandas还将为我们处理数据集的拆分为训练集和验证集。但是，我们需要非常小心处理我们的验证集。我们希望设计它，使其类似于 Kaggle 将用来评判比赛的测试集。\n回想一下验证集和测试集之间的区别，如第一章中所讨论的。验证集是我们从训练中保留的数据，以确保训练过程不会在训练数据上过拟合。测试集是更深层次地被我们自己保留的数据，以确保我们在探索各种模型架构和超参数时不会在验证数据上过拟合。\n我们看不到测试集。但我们确实希望定义我们的验证数据，使其与训练数据具有与测试集相同类型的关系。\n在某些情况下，随机选择数据点的子集就足够了。但这不是这种情况，因为这是一个时间序列。\n如果您查看测试集中表示的日期范围，您会发现它覆盖了 2012 年 5 月的六个月期间，这比训练集中的任何日期都要晚。这是一个很好的设计，因为竞赛赞助商希望确保模型能够预测未来。但这意味着如果我们要有一个有用的验证集，我们也希望验证集比训练集更晚。Kaggle 的训练数据在 2012 年 4 月结束，因此我们将定义一个更窄的训练数据集，其中只包括 2011 年 11 月之前的 Kaggle 训练数据，并且我们将定义一个验证集，其中包括 2011 年 11 月之后的数据。\n为了做到这一点，我们使用np.where，这是一个有用的函数，返回（作为元组的第一个元素）所有True值的索引：\ncond = (df.saleYear&lt;2011) | (df.saleMonth&lt;10)\ntrain_idx = np.where( cond)[0]\nvalid_idx = np.where(~cond)[0]\n\nsplits = (list(train_idx),list(valid_idx))\nTabularPandas需要告诉哪些列是连续的，哪些是分类的。我们可以使用辅助函数cont_cat_split自动处理：\ncont,cat = cont_cat_split(df, 1, dep_var=dep_var)\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)\nTabularPandas的行为很像一个 fastai 的Datasets对象，包括提供train和valid属性：\nlen(to.train),len(to.valid)\n(404710, 7988)\n我们可以看到数据仍然显示为类别的字符串（这里我们只显示了一些列，因为完整的表太大了，无法放在一页上）。\nto.show(3)\n\n\n\n\nstate\nProductGroup\nDrive_System\nEnclosure\nSalePrice\n\n\n\n\n0\nAlabama\nWL\n#na#\nEROPS w AC\n11.097410\n\n\n1\nNorth Carolina\nWL\n#na#\nEROPS w AC\n10.950807\n\n\n2\nNew York\nSSL\n#na#\nOROPS\n9.210340\n\n\n\n然而，底层项目都是数字：\nto.items.head(3)\n\n\n\n\nstate\nProductGroup\nDrive_System\nEnclosure\n\n\n\n\n0\n1\n6\n0\n3\n\n\n1\n33\n6\n0\n3\n\n\n2\n32\n3\n0\n6\n\n\n\n将分类列转换为数字是通过简单地用数字替换每个唯一级别来完成的。与级别相关联的数字是按照它们在列中出现的顺序连续选择的，因此在转换后的分类列中，数字没有特定的含义。唯一的例外是，如果您首先将列转换为 Pandas 有序类别（就像我们之前为ProductSize所做的那样），那么您选择的排序将被使用。我们可以通过查看classes属性来查看映射：\nto.classes['ProductSize']\n(#7) ['#na#','Large','Large / Medium','Medium','Small','Mini','Compact']\n由于处理数据到这一点需要一分钟左右的时间，我们应该保存它，这样以后我们可以继续从这里继续工作，而不必重新运行之前的步骤。fastai 提供了一个使用 Python 的pickle系统保存几乎任何 Python 对象的save方法：\n(path/'to.pkl').save(to)\n以后要读回来，您将键入：\nto = (path/'to.pkl').load()\n现在所有这些预处理都完成了，我们准备创建一个决策树。"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#创建决策树",
    "href": "Fastbook/translations/cn/09_tabular.html#创建决策树",
    "title": "第九章：表格建模深入探讨",
    "section": "创建决策树",
    "text": "创建决策树\n首先，我们定义我们的自变量和因变量：\nxs,y = to.train.xs,to.train.y\nvalid_xs,valid_y = to.valid.xs,to.valid.y\n现在我们的数据都是数字的，没有缺失值，我们可以创建一个决策树：\nm = DecisionTreeRegressor(max_leaf_nodes=4)\nm.fit(xs, y);\n为了简单起见，我们告诉 sklearn 只创建了四个叶节点。要查看它学到了什么，我们可以显示决策树：\ndraw_tree(m, xs, size=7, leaves_parallel=True, precision=2)\n\n理解这幅图片是理解决策树的最好方法之一，所以我们将从顶部开始，逐步解释每个部分。\n顶部节点代表初始模型，在进行任何分割之前，所有数据都在一个组中。这是最简单的模型。这是在不问任何问题的情况下得到的结果，将始终预测值为整个数据集的平均值。在这种情况下，我们可以看到它预测销售价格的对数值为 10.1。它给出了均方误差为 0.48。这个值的平方根是 0.69。（请记住，除非您看到m_rmse，或者均方根误差，否则您看到的值是在取平方根之前的，因此它只是差异的平方的平均值。）我们还可以看到在这个组中有 404,710 条拍卖记录，这是我们训练集的总大小。这里显示的最后一部分信息是找到的最佳分割的决策标准，即基于coupler_system列进行分割。\n向下移动并向左移动，这个节点告诉我们，在coupler_system小于 0.5 的设备拍卖记录中有 360,847 条。这个组中我们的因变量的平均值是 10.21。从初始模型向下移动并向右移动，我们来到了coupler_system大于 0.5 的记录。\n底部行包含我们的叶节点：没有答案出现的节点，因为没有更多问题需要回答。在这一行的最右边是包含coupler_system大于 0.5 的记录的节点。平均值为 9.21，因此我们可以看到决策树算法确实找到了一个单一的二进制决策，将高价值与低价值的拍卖结果分开。仅询问coupler_system预测的平均值为 9.21，而不是 10.1。\n在第一个决策点后返回到顶部节点后，我们可以看到已经进行了第二个二进制决策分割，基于询问YearMade是否小于或等于 1991.5。对于这个条件为真的组（请记住，这是根据coupler_system和YearMade进行的两个二进制决策），平均值为 9.97，在这个组中有 155,724 条拍卖记录。对于这个条件为假的拍卖组，平均值为 10.4，有 205,123 条记录。因此，我们可以看到决策树算法成功地将我们更昂贵的拍卖记录分成了两组，这两组在价值上有显著差异。\n我们可以使用 Terence Parr 强大的dtreeviz 库显示相同的信息：\nsamp_idx = np.random.permutation(len(y))[:500]\ndtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n        fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n        orientation='LR')\n\n这显示了每个分割点数据分布的图表。我们可以清楚地看到我们的YearMade数据存在问题：显然有一些在 1000 年制造的推土机！很可能，这只是一个缺失值代码（在数据中没有出现的值，用作占位符的值，用于在值缺失的情况下）。对于建模目的，1000 是可以的，但正如你所看到的，这个异常值使得我们感兴趣的数值更难以可视化。所以，让我们用 1950 年替换它：\nxs.loc[xs['YearMade']&lt;1900, 'YearMade'] = 1950\nvalid_xs.loc[valid_xs['YearMade']&lt;1900, 'YearMade'] = 1950\n这个改变使得树的可视化中的分割更加清晰，尽管这并没有在模型结果上有任何显著的改变。这是决策树对数据问题有多么弹性的一个很好的例子！\nm = DecisionTreeRegressor(max_leaf_nodes=4).fit(xs, y)\ndtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n        fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n        orientation='LR')\n\n现在让决策树算法构建一个更大的树。在这里，我们没有传递任何停止标准，比如max_leaf_nodes：\nm = DecisionTreeRegressor()\nm.fit(xs, y);\n我们将创建一个小函数来检查我们模型的均方根误差（m_rmse），因为比赛是根据这个来评判的：\ndef r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6)\ndef m_rmse(m, xs, y): return r_mse(m.predict(xs), y)\nm_rmse(m, xs, y)\n0.0\n所以，我们的模型完美了，对吧？不要那么快……记住，我们真的需要检查验证集，以确保我们没有过拟合：\nm_rmse(m, valid_xs, valid_y)\n0.337727\n哎呀——看起来我们可能过拟合得很严重。原因如下：\nm.get_n_leaves(), len(xs)\n(340909, 404710)\n我们的叶节点数几乎和数据点一样多！这似乎有点过于热情。事实上，sklearn 的默认设置允许它继续分裂节点，直到每个叶节点只包含一个项目。让我们改变停止规则，告诉 sklearn 确保每个叶节点至少包含 25 个拍卖记录：\nm = DecisionTreeRegressor(min_samples_leaf=25)\nm.fit(to.train.xs, to.train.y)\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n(0.248562, 0.32368)\n看起来好多了。让我们再次检查叶节点的数量：\nm.get_n_leaves()\n12397\n更加合理！"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#分类变量",
    "href": "Fastbook/translations/cn/09_tabular.html#分类变量",
    "title": "第九章：表格建模深入探讨",
    "section": "分类变量",
    "text": "分类变量\n在前一章中，当使用深度学习网络时，我们通过独热编码处理分类变量，并将其馈送到嵌入层。嵌入层帮助模型发现这些变量不同级别的含义（分类变量的级别没有固有含义，除非我们使用 Pandas 手动指定一个排序）。在决策树中，我们没有嵌入层，那么这些未处理的分类变量如何在决策树中发挥作用呢？例如，像产品代码这样的东西如何使用？\n简短的答案是：它就是有效！想象一种情况，其中一个产品代码在拍卖中比其他任何产品代码都要昂贵得多。在这种情况下，任何二元分割都将导致该产品代码位于某个组中，而该组将比其他组更昂贵。因此，我们简单的决策树构建算法将选择该分割。稍后，在训练过程中，算法将能够进一步分割包含昂贵产品代码的子组，随着时间的推移，树将聚焦于那一个昂贵的产品。\n还可以使用一位编码来替换单个分类变量，其中每一列代表变量的一个可能级别。Pandas 有一个get_dummies方法可以做到这一点。\n然而，实际上并没有证据表明这种方法会改善最终结果。因此，我们通常会尽可能避免使用它，因为它确实会使您的数据集更难处理。在 2019 年，这个问题在 Marvin Wright 和 Inke König 的论文“Splitting on Categorical Predictors in Random Forests”中得到了探讨：\n\n对于名义预测器，标准方法是考虑所有 2^(k − 1) − 1 个k预测类别的 2-分区。然而，这种指数关系会产生大量需要评估的潜在分割，增加了计算复杂性并限制了大多数实现中可能的类别数量。对于二元分类和回归，已经证明按照每个分割中的预测类别进行排序会导致与标准方法完全相同的分割。这减少了计算复杂性，因为对于具有k个类别的名义预测器，只需要考虑k − 1 个分割。\n\n现在您了解了决策树的工作原理，是时候尝试那种最佳的解决方案了：随机森林。"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#创建随机森林",
    "href": "Fastbook/translations/cn/09_tabular.html#创建随机森林",
    "title": "第九章：表格建模深入探讨",
    "section": "创建随机森林",
    "text": "创建随机森林\n我们可以像创建决策树一样创建随机森林，只是现在我们还指定了指示森林中应该有多少树，如何对数据项（行）进行子集化以及如何对字段（列）进行子集化的参数。\n在下面的函数定义中，n_estimators定义了我们想要的树的数量，max_samples定义了每棵树训练时要抽样的行数，max_features定义了在每个分裂点抽样的列数（其中0.5表示“取一半的总列数”）。我们还可以指定何时停止分裂树节点，有效地限制树的深度，通过包含我们在前一节中使用的相同min_samples_leaf参数。最后，我们传递n_jobs=-1告诉 sklearn 使用所有 CPU 并行构建树。通过创建一个小函数，我们可以更快地尝试本章其余部分的变化：\ndef rf(xs, y, n_estimators=40, max_samples=200_000,\n       max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,\n        max_samples=max_samples, max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\nm = rf(xs, y);\n我们的验证 RMSE 现在比我们上次使用DecisionTreeRegressor生成的结果要好得多，后者只使用了所有可用数据生成了一棵树：\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n(0.170896, 0.233502)\n随机森林最重要的特性之一是它对超参数选择不太敏感，比如max_features。您可以将n_estimators设置为尽可能高的数字，以便训练更多的树，树越多，模型就越准确。max_samples通常可以保持默认值，除非您有超过 200,000 个数据点，在这种情况下，将其设置为 200,000 将使其在准确性上有很小影响的情况下更快地训练。max_features=0.5和min_samples_leaf=4通常效果很好，尽管 sklearn 的默认值也很好。\nsklearn 文档展示了一个例子，展示了不同max_features选择的效果，以及树的数量增加。在图中，蓝色曲线使用最少的特征，绿色曲线使用最多的特征（使用所有特征）。正如您在图 9-7 中所看到的，使用较少特征但具有更多树的模型具有最低的错误结果。\n\n\n\nsklearn max_features 图表\n\n\n\n图 9-7. 基于最大特征和树的数量的错误（来源：https://oreil.ly/E0Och）\n为了查看n_estimators的影响，让我们从森林中的每棵树获取预测结果（这些在estimators_属性中）：\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_])\n如您所见，preds.mean(0)给出了与我们的随机森林相同的结果：\nr_mse(preds.mean(0), valid_y)\n0.233502\n让我们看看随着树的数量增加，RMSE 会发生什么变化。如您所见，大约在 30 棵树后，改进水平就会显著减少：\nplt.plot([r_mse(preds[:i+1].mean(0), valid_y) for i in range(40)]);\n\n我们在验证集上的表现比在训练集上差。但这是因为我们过拟合了，还是因为验证集涵盖了不同的时间段，或者两者都有？根据我们已经看到的信息，我们无法确定。然而，随机森林有一个非常聪明的技巧叫做袋外（OOB）误差，可以帮助我们解决这个问题（以及更多！）。"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#袋外误差",
    "href": "Fastbook/translations/cn/09_tabular.html#袋外误差",
    "title": "第九章：表格建模深入探讨",
    "section": "袋外误差",
    "text": "袋外误差\n回想一下，在随机森林中，每棵树都是在训练数据的不同子集上训练的。OOB 错误是一种通过在计算行的错误时仅包括那些行未包含在训练中的树来测量训练数据集中的预测错误的方法。这使我们能够看到模型是否过拟合，而无需单独的验证集。"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#用于预测置信度的树方差",
    "href": "Fastbook/translations/cn/09_tabular.html#用于预测置信度的树方差",
    "title": "第九章：表格建模深入探讨",
    "section": "用于预测置信度的树方差",
    "text": "用于预测置信度的树方差\n我们看到模型如何平均每棵树的预测以获得整体预测——也就是说，一个值的估计。但是我们如何知道估计的置信度？一种简单的方法是使用树之间预测的标准差，而不仅仅是均值。这告诉我们预测的相对置信度。一般来说，我们会更谨慎地使用树给出非常不同结果的行的结果（更高的标准差），而不是在树更一致的情况下使用结果（更低的标准差）。\n在”创建随机森林”中，我们看到如何使用 Python 列表推导来对验证集进行预测，对森林中的每棵树都这样做：\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_])\npreds.shape\n(40, 7988)\n现在我们对验证集中的每棵树和每个拍卖都有一个预测（40 棵树和 7,988 个拍卖）。\n使用这种方法，我们可以获得每个拍卖的所有树的预测的标准差：\npreds_std = preds.std(0)\n以下是前五个拍卖的预测的标准差——也就是验证集的前五行：\npreds_std[:5]\narray([0.21529149, 0.10351274, 0.08901878, 0.28374773, 0.11977206])\n正如您所看到的，预测的置信度差异很大。对于一些拍卖，标准差较低，因为树是一致的。对于其他拍卖，标准差较高，因为树不一致。这是在生产环境中会有用的信息；例如，如果您使用此模型来决定在拍卖中对哪些物品进行竞标，低置信度的预测可能会导致您在竞标之前更仔细地查看物品。"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#特征重要性",
    "href": "Fastbook/translations/cn/09_tabular.html#特征重要性",
    "title": "第九章：表格建模深入探讨",
    "section": "特征重要性",
    "text": "特征重要性\n仅仅知道一个模型能够做出准确的预测通常是不够的，我们还想知道它是如何做出预测的。特征重要性给了我们这种洞察力。我们可以直接从 sklearn 的随机森林中获取这些信息，方法是查看feature_importances_属性。这里有一个简单的函数，我们可以用它将它们放入一个 DataFrame 并对它们进行排序：\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n我们模型的特征重要性显示，前几个最重要的列的重要性得分比其余的要高得多，其中（不出所料）YearMade和ProductSize位于列表的顶部：\nfi = rf_feat_importance(m, xs)\nfi[:10]\n\n\n\n\ncols\nimp\n\n\n\n\n69\nYearMade\n0.182890\n\n\n6\nProductSize\n0.127268\n\n\n30\nCoupler_System\n0.117698\n\n\n7\nfiProductClassDesc\n0.069939\n\n\n66\nModelID\n0.057263\n\n\n77\nsaleElapsed\n0.050113\n\n\n32\nHydraulics_Flow\n0.047091\n\n\n3\nfiSecondaryDesc\n0.041225\n\n\n31\nGrouser_Tracks\n0.031988\n\n\n1\nfiModelDesc\n0.031838\n\n\n\n特征重要性的图表显示了相对重要性更清晰：\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30]);\n\n这些重要性是如何计算的相当简单而优雅。特征重要性算法循环遍历每棵树，然后递归地探索每个分支。在每个分支，它查看用于该分割的特征是什么，以及模型由于该分割而改善了多少。该改善（按该组中的行数加权）被添加到该特征的重要性分数中。这些分数在所有树的所有分支中求和，最后对分数进行归一化，使它们总和为 1。"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#去除低重要性变量",
    "href": "Fastbook/translations/cn/09_tabular.html#去除低重要性变量",
    "title": "第九章：表格建模深入探讨",
    "section": "去除低重要性变量",
    "text": "去除低重要性变量\n看起来我们可以通过去除低重要性的变量来使用列的子集，并且仍然能够获得良好的结果。让我们尝试只保留那些具有特征重要性大于 0.005 的列：\nto_keep = fi[fi.imp&gt;0.005].cols\nlen(to_keep)\n21\n我们可以使用列的这个子集重新训练我们的模型：\nxs_imp = xs[to_keep]\nvalid_xs_imp = valid_xs[to_keep]\nm = rf(xs_imp, y)\n这里是结果：\nm_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y)\n(0.181208, 0.232323)\n我们的准确率大致相同，但我们有更少的列需要研究：\nlen(xs.columns), len(xs_imp.columns)\n(78, 21)\n我们发现，通常改进模型的第一步是简化它——78 列对我们来说太多了，我们无法深入研究它们！此外，在实践中，通常更简单、更易解释的模型更容易推出和维护。\n这也使得我们的特征重要性图更容易解释。让我们再次看一下：\nplot_fi(rf_feat_importance(m, xs_imp));\n\n使这个更难解释的一点是，似乎有一些含义非常相似的变量：例如，ProductGroup和ProductGroupDesc。让我们尝试去除任何冗余特征。"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#去除冗余特征",
    "href": "Fastbook/translations/cn/09_tabular.html#去除冗余特征",
    "title": "第九章：表格建模深入探讨",
    "section": "去除冗余特征",
    "text": "去除冗余特征\n让我们从这里开始：\ncluster_columns(xs_imp)\n\n在这个图表中，最相似的列对是在树的左侧远离“根”处早期合并在一起的。毫不奇怪，ProductGroup和ProductGroupDesc字段很早就合并了，saleYear和saleElapsed，以及fiModelDesc和fiBaseModel也是如此。它们可能是如此密切相关，以至于它们实际上是彼此的同义词。"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#部分依赖",
    "href": "Fastbook/translations/cn/09_tabular.html#部分依赖",
    "title": "第九章：表格建模深入探讨",
    "section": "部分依赖",
    "text": "部分依赖\n正如我们所看到的，最重要的预测变量是ProductSize和YearMade。我们想要了解这些预测变量与销售价格之间的关系。首先，最好检查每个类别的值的计数（由 Pandas 的value_counts方法提供），看看每个类别有多常见：\np = valid_xs_final['ProductSize'].value_counts(sort=False).plot.barh()\nc = to.classes['ProductSize']\nplt.yticks(range(len(c)), c);\n\n最大的组是#na#，这是 fastai 用于缺失值的标签。\n让我们对YearMade做同样的事情。由于这是一个数值特征，我们需要绘制一个直方图，将年份值分组为几个离散的箱：\nax = valid_xs_final['YearMade'].hist()\n\n除了我们用于编码缺失年份值的特殊值 1950 之外，大多数数据都是 1990 年后的。\n现在我们准备看部分依赖图。部分依赖图试图回答这个问题：如果一行除了关注的特征之外没有变化，它会如何影响因变量？\n例如，YearMade 如何影响销售价格，其他条件都相同？为了回答这个问题，我们不能简单地取每个YearMade的平均销售价格。这种方法的问题在于，许多其他因素也会随着年份的变化而变化，比如销售哪些产品、有多少产品带空调、通货膨胀等等。因此，仅仅对具有相同YearMade的所有拍卖品进行平均会捕捉到每个其他字段如何随着YearMade的变化而变化以及这种整体变化如何影响价格的效果。\n相反，我们将YearMade列中的每个值替换为 1950，然后计算每个拍卖品的预测销售价格，并对所有拍卖品进行平均。然后我们对 1951、1952 等年份做同样的操作，直到我们的最终年份 2011。这隔离了仅YearMade的影响（即使通过对一些想象中的记录进行平均，我们分配了一个可能永远不会实际存在的YearMade值以及一些其他值）。"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#数据泄漏",
    "href": "Fastbook/translations/cn/09_tabular.html#数据泄漏",
    "title": "第九章：表格建模深入探讨",
    "section": "数据泄漏",
    "text": "数据泄漏\n在论文“数据挖掘中的泄漏：制定、检测和避免”中，Shachar Kaufman 等人描述了泄漏如下：\n\n关于数据挖掘问题的目标的信息引入，这些信息不应该合法地从中挖掘出来。泄漏的一个微不足道的例子是一个模型将目标本身用作输入，因此得出例如“雨天下雨”的结论。实际上，引入这种非法信息是无意的，并且由数据收集、聚合和准备过程促成。\n\n他们举了一个例子：\n\n在 IBM 的一个实际商业智能项目中，根据其网站上发现的关键词，识别了某些产品的潜在客户。结果证明这是泄漏，因为用于训练的网站内容是在潜在客户已经成为客户的时间点进行采样的，网站包含了 IBM 购买的产品的痕迹，比如“Websphere”这样的词（例如，在关于购买的新闻稿或客户使用的特定产品功能中）。\n\n数据泄漏是微妙的，可以采取多种形式。特别是，缺失值通常代表数据泄漏。\n例如，Jeremy 参加了一个 Kaggle 竞赛，旨在预测哪些研究人员最终会获得研究资助。这些信息是由一所大学提供的，包括成千上万个研究项目的示例，以及有关涉及的研究人员和每个资助是否最终被接受的数据。大学希望能够使用在这次竞赛中开发的模型来排名哪些资助申请最有可能成功，以便优先处理。\nJeremy 使用随机森林对数据进行建模，然后使用特征重要性来找出哪些特征最具预测性。他注意到了三件令人惊讶的事情：\n\n该模型能够在 95%以上的时间内正确预测谁将获得资助。\n显然，毫无意义的标识列是最重要的预测因子。\n星期几和一年中的日期列也具有很高的预测性；例如，大多数在星期日日期的资助申请被接受，许多被接受的资助申请日期在 1 月 1 日。\n\n对于标识列，部分依赖图显示，当信息缺失时，申请几乎总是被拒绝。实际上，事实证明，大学在接受资助申请后才填写了大部分这些信息。通常，对于未被接受的申请，这些信息只是留空。因此，这些信息在申请接收时并不可用，并且不会对预测模型可用——这是数据泄漏。\n同样，成功申请的最终处理通常在一周或一年结束时自动完成。最终处理日期最终出现在数据中，因此，尽管这些信息具有预测性，但实际上在接收申请时并不可用。\n这个例子展示了识别数据泄漏最实用和简单方法，即构建模型，然后执行以下操作：\n\n检查模型的准确性是否过于完美。\n寻找在实践中不合理的重要预测因子。\n寻找在实践中不合理的部分依赖图结果。\n\n回想一下我们的熊探测器，这与我们在第二章中提供的建议相符——通常先构建模型，然后进行数据清理是一个好主意，而不是反过来。模型可以帮助您识别潜在的数据问题。\n它还可以帮助您确定哪些因素影响特定预测，使用树解释器。"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#树解释器",
    "href": "Fastbook/translations/cn/09_tabular.html#树解释器",
    "title": "第九章：表格建模深入探讨",
    "section": "树解释器",
    "text": "树解释器\n在本节开始时，我们说我们想要能够回答五个问题：\n\n我们对使用特定数据行进行预测有多自信？\n对于预测特定数据行，最重要的因素是什么，它们如何影响该预测？\n哪些列是最强的预测因子？\n哪些列在预测目的上实际上是多余的？\n当我们改变这些列时，预测会如何变化？\n\n我们已经处理了其中四个；只剩下第二个问题。要回答这个问题，我们需要使用treeinterpreter库。我们还将使用waterfallcharts库来绘制结果图表。您可以通过在笔记本单元格中运行以下命令来安装这些：\n!pip install treeinterpreter\n!pip install waterfallcharts\n我们已经看到如何计算整个随机森林中的特征重要性。基本思想是查看每个变量对模型改进的贡献，在每棵树的每个分支处，然后将每个变量的所有这些贡献相加。\n我们可以完全相同的方式做，但只针对单个数据行。例如，假设我们正在查看拍卖中的特定物品。我们的模型可能预测这个物品会非常昂贵，我们想知道原因。因此，我们取出那一行数据并将其通过第一棵决策树，查看树中每个点处使用的分割。对于每个分割，我们找到相对于树的父节点的增加或减少。我们对每棵树都这样做，并将每个分割变量的重要性变化相加。\n例如，让我们选择验证集的前几行：\nrow = valid_xs_final.iloc[:5]\n然后我们可以将这些传递给treeinterpreter：\nprediction,bias,contributions = treeinterpreter.predict(m, row.values)\nprediction只是随机森林的预测。bias是基于取因变量的平均值（即每棵树的根模型）的预测。contributions是最有趣的部分-它告诉我们由于每个独立变量的变化而导致的预测总变化。因此，对于每行，contributions加上bias必须等于prediction。让我们只看第一行：\nprediction[0], bias[0], contributions[0].sum()\n(array([9.98234598]), 10.104309759725059, -0.12196378442186026)\n用瀑布图最清晰地显示贡献。这显示了所有独立变量的正负贡献如何相加以创建最终预测，这里标有“净”标签的右侧列：\nwaterfall(valid_xs_final.columns, contributions[0], threshold=0.08,\n          rotation_value=45,formatting='{:,.3f}');\n\n这种信息在生产中最有用，而不是在模型开发过程中。您可以使用它为数据产品的用户提供有关预测背后的基本推理的有用信息。\n现在我们已经介绍了一些经典的机器学习技术来解决这个问题，让我们看看深度学习如何帮助！"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#外推问题",
    "href": "Fastbook/translations/cn/09_tabular.html#外推问题",
    "title": "第九章：表格建模深入探讨",
    "section": "外推问题",
    "text": "外推问题\n让我们考虑一个简单的任务，从显示略带噪音的线性关系的 40 个数据点中进行预测：\nx_lin = torch.linspace(0,20, steps=40)\ny_lin = x_lin + torch.randn_like(x_lin)\nplt.scatter(x_lin, y_lin);\n\n虽然我们只有一个独立变量，但 sklearn 期望独立变量的矩阵，而不是单个向量。因此，我们必须将我们的向量转换为一个具有一列的矩阵。换句话说，我们必须将shape从[40]更改为[40,1]。一种方法是使用unsqueeze方法，在请求的维度上为张量添加一个新的单位轴：\nxs_lin = x_lin.unsqueeze(1)\nx_lin.shape,xs_lin.shape\n(torch.Size([40]), torch.Size([40, 1]))\n更灵活的方法是使用特殊值None切片数组或张量，这会在该位置引入一个额外的单位轴：\nx_lin[:,None].shape\ntorch.Size([40, 1])\n现在我们可以为这些数据创建一个随机森林。我们将只使用前 30 行来训练模型：\nm_lin = RandomForestRegressor().fit(xs_lin[:30],y_lin[:30])\n然后我们将在完整数据集上测试模型。蓝点是训练数据，红点是预测：\nplt.scatter(x_lin, y_lin, 20)\nplt.scatter(x_lin, m_lin.predict(xs_lin), color='red', alpha=0.5);\n\n我们有一个大问题！我们在训练数据范围之外的预测都太低了。你认为这是为什么？\n请记住，随机森林只是对多棵树的预测进行平均。而树只是预测叶子中行的平均值。因此，树和随机森林永远无法预测超出训练数据范围的值。这对于表示随时间变化的数据，如通货膨胀，且希望对未来时间进行预测的数据尤为棘手。您的预测将系统性地过低。\n但问题不仅限于时间变量。随机森林无法对其未见过的数据类型进行外推，从更一般的意义上讲。这就是为什么我们需要确保我们的验证集不包含域外数据。"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#查找域外数据",
    "href": "Fastbook/translations/cn/09_tabular.html#查找域外数据",
    "title": "第九章：表格建模深入探讨",
    "section": "查找域外数据",
    "text": "查找域外数据\n有时很难知道您的测试集是否与训练数据以相同方式分布，或者如果不同，哪些列反映了这种差异。有一种简单的方法可以弄清楚这一点，那就是使用随机森林！\n但在这种情况下，我们不使用随机森林来预测我们实际的因变量。相反，我们尝试预测一行是在验证集还是训练集中。要看到这一点，让我们将训练集和验证集结合起来，创建一个代表每行来自哪个数据集的因变量，使用该数据构建一个随机森林，并获取其特征重要性：\ndf_dom = pd.concat([xs_final, valid_xs_final])\nis_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final))\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:6]\n\n\n\n\ncols\nimp\n\n\n\n\n5\nsaleElapsed\n0.859446\n\n\n9\nSalesID\n0.119325\n\n\n13\nMachineID\n0.014259\n\n\n0\nYearMade\n0.001793\n\n\n8\nfiModelDesc\n0.001740\n\n\n11\nEnclosure\n0.000657\n\n\n\n这显示训练集和验证集之间有三列显着不同：saleElapsed、SalesID 和 MachineID。saleElapsed 的差异相当明显：它是数据集开始和每行之间的天数，因此直接编码了日期。SalesID 的差异表明拍卖销售的标识符可能会随时间递增。MachineID 表明类似的情况可能发生在这些拍卖中出售的个别物品上。\n让我们先获取原始随机森林模型的 RMSE 基线，然后逐个确定移除这些列的影响：\nm = rf(xs_final, y)\nprint('orig', m_rmse(m, valid_xs_final, valid_y))\n\nfor c in ('SalesID','saleElapsed','MachineID'):\n    m = rf(xs_final.drop(c,axis=1), y)\n    print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y))\norig 0.232795\nSalesID 0.23109\nsaleElapsed 0.236221\nMachineID 0.233492\n看起来我们应该能够移除 SalesID 和 MachineID 而不会失去任何准确性。让我们检查一下：\ntime_vars = ['SalesID','MachineID']\nxs_final_time = xs_final.drop(time_vars, axis=1)\nvalid_xs_time = valid_xs_final.drop(time_vars, axis=1)\n\nm = rf(xs_final_time, y)\nm_rmse(m, valid_xs_time, valid_y)\n0.231307\n删除这些变量略微提高了模型的准确性；但更重要的是，这应该使其随时间更具弹性，更易于维护和理解。我们建议对所有数据集尝试构建一个以 is_valid 为因变量的模型，就像我们在这里所做的那样。它通常可以揭示您可能会忽略的微妙的领域转移问题。\n在我们的情况下，可能有助于简单地避免使用旧数据。通常，旧数据显示的关系已经不再有效。让我们尝试只使用最近几年的数据：\nxs['saleYear'].hist();\n\n在这个子集上训练的结果如下：\nfilt = xs['saleYear']&gt;2004\nxs_filt = xs_final_time[filt]\ny_filt = y[filt]\nm = rf(xs_filt, y_filt)\nm_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y)\n(0.17768, 0.230631)\n稍微好一点，这表明您不应该总是使用整个数据集；有时候子集可能更好。\n让我们看看使用神经网络是否有帮助。"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#使用神经网络",
    "href": "Fastbook/translations/cn/09_tabular.html#使用神经网络",
    "title": "第九章：表格建模深入探讨",
    "section": "使用神经网络",
    "text": "使用神经网络\n我们可以使用相同的方法构建一个神经网络模型。让我们首先复制设置 TabularPandas 对象的步骤：\ndf_nn = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\ndf_nn['ProductSize'] = df_nn['ProductSize'].astype('category')\ndf_nn['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)\ndf_nn[dep_var] = np.log(df_nn[dep_var])\ndf_nn = add_datepart(df_nn, 'saledate')\n我们可以通过使用相同的列集合来为我们的神经网络利用我们在随机森林中修剪不需要的列的工作：\ndf_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]]\n在神经网络中，与决策树方法相比，分类列的处理方式大不相同。正如我们在第八章中看到的，在神经网络中，处理分类变量的一个很好的方法是使用嵌入。为了创建嵌入，fastai 需要确定哪些列应该被视为分类变量。它通过比较变量中不同级别的数量与 max_card 参数的值来实现这一点。如果较低，fastai 将把该变量视为分类变量。嵌入大小大于 10,000 通常只应在测试是否有更好的方法来分组变量之后使用，因此我们将使用 9,000 作为我们的 max_card 值：\ncont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)\n然而，在这种情况下，有一个变量绝对不能被视为分类变量：saleElapsed。按定义，分类变量无法在其所见值范围之外进行外推，但我们希望能够预测未来的拍卖销售价格。因此，我们需要将其作为连续变量处理：\ncont_nn.append('saleElapsed')\ncat_nn.remove('saleElapsed')\n让我们来看看我们目前选择的每个分类变量的基数：\ndf_nn_final[cat_nn].nunique()\nYearMade                73\nProductSize              6\nCoupler_System           2\nfiProductClassDesc      74\nModelID               5281\nHydraulics_Flow          3\nfiSecondaryDesc        177\nfiModelDesc           5059\nProductGroup             6\nEnclosure                6\nfiModelDescriptor      140\nDrive_System             4\nHydraulics              12\nTire_Size               17\ndtype: int64\n有关设备“型号”的两个变量，都具有类似非常高的基数，这表明它们可能包含相似的冗余信息。请注意，当分析冗余特征时，我们不一定会注意到这一点，因为这依赖于相似变量按相同顺序排序（即，它们需要具有类似命名的级别）。拥有 5,000 个级别的列意味着我们的嵌入矩阵需要 5,000 列，如果可能的话最好避免。让我们看看删除其中一个这些型号列对随机森林的影响：\nxs_filt2 = xs_filt.drop('fiModelDescriptor', axis=1)\nvalid_xs_time2 = valid_xs_time.drop('fiModelDescriptor', axis=1)\nm2 = rf(xs_filt2, y_filt)\nm_rmse(m, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y)\n(0.176706, 0.230642)\n影响很小，因此我们将其作为神经网络的预测变量移除：\ncat_nn.remove('fiModelDescriptor')\n我们可以像创建随机森林那样创建我们的TabularPandas对象，但有一个非常重要的补充：归一化。随机森林不需要任何归一化——树构建过程只关心变量中值的顺序，而不关心它们的缩放。但正如我们所见，神经网络确实关心这一点。因此，在构建TabularPandas对象时，我们添加Normalize处理器：\nprocs_nn = [Categorify, FillMissing, Normalize]\nto_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn,\n                      splits=splits, y_names=dep_var)\n表格模型和数据通常不需要太多的 GPU 内存，因此我们可以使用更大的批量大小：\ndls = to_nn.dataloaders(1024)\n正如我们讨论过的，为回归模型设置y_range是一个好主意，所以让我们找到我们因变量的最小值和最大值：\ny = to_nn.train.y\ny.min(),y.max()\n(8.465899897028686, 11.863582336583399)\n现在我们可以创建Learner来创建这个表格模型。像往常一样，我们使用特定于应用程序的学习函数，以利用其应用程序定制的默认值。我们将损失函数设置为 MSE，因为这就是这个比赛使用的损失函数。\n默认情况下，对于表格数据，fastai 创建一个具有两个隐藏层的神经网络，分别具有 200 和 100 个激活。这对于小数据集效果很好，但在这里我们有一个相当大的数据集，所以我们将层大小增加到 500 和 250：\nfrom fastai.tabular.all import *\nlearn = tabular_learner(dls, y_range=(8,12), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\nlearn.lr_find()\n(0.005754399299621582, 0.0002754228771664202)\n\n不需要使用fine_tune，所以我们将使用fit_one_cycle进行几个周期的训练，看看效果如何：\nlearn.fit_one_cycle(5, 1e-2)\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.069705\n0.062389\n00:11\n\n\n1\n0.056253\n0.058489\n00:11\n\n\n2\n0.048385\n0.052256\n00:11\n\n\n3\n0.043400\n0.050743\n00:11\n\n\n4\n0.040358\n0.050986\n00:11\n\n\n\n我们可以使用我们的r_mse函数将结果与之前得到的随机森林结果进行比较：\npreds,targs = learn.get_preds()\nr_mse(preds,targs)\n0.2258\n它比随机森林要好得多（尽管训练时间更长，对超参数调整要求更高）。\n在继续之前，让我们保存我们的模型，以防以后想再次使用它：\nlearn.save('nn')\n另一件可以帮助泛化的事情是使用几个模型并平均它们的预测——一个技术，如前面提到的，称为集成。"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#提升",
    "href": "Fastbook/translations/cn/09_tabular.html#提升",
    "title": "第九章：表格建模深入探讨",
    "section": "提升",
    "text": "提升\n到目前为止，我们集成的方法是使用装袋，它涉及将许多模型（每个模型在不同的数据子集上训练）组合起来通过平均它们。正如我们所看到的，当应用于决策树时，这被称为随机森林。\n在另一种重要的集成方法中，称为提升，我们添加模型而不是对它们进行平均。以下是提升的工作原理：\n\n训练一个欠拟合数据集的小模型。\n计算该模型在训练集中的预测。\n从目标中减去预测值；这些被称为残差，代表了训练集中每个点的误差。\n回到第 1 步，但是不要使用原始目标，而是使用残差作为训练的目标。\n继续这样做，直到达到停止标准，比如最大树的数量，或者观察到验证集错误变得更糟。\n\n使用这种方法，每棵新树都将尝试拟合所有先前树的错误。因为我们不断通过从先前树的残差中减去每棵新树的预测来创建新的残差，残差会变得越来越小。\n使用提升树集成进行预测，我们计算每棵树的预测，然后将它们全部加在一起。有许多遵循这种基本方法的模型，以及许多相同模型的名称。梯度提升机（GBMs）和梯度提升决策树（GBDTs）是您最有可能遇到的术语，或者您可能会看到实现这些模型的特定库的名称；在撰写本文时，XGBoost是最受欢迎的。\n请注意，与随机森林不同，使用这种方法，没有什么可以阻止我们过拟合。在随机森林中使用更多树不会导致过拟合，因为每棵树都是独立的。但是在提升集成中，拥有更多树，训练错误就会变得更好，最终您将在验证集上看到过拟合。\n我们不会在这里详细介绍如何训练梯度提升树集成，因为这个领域发展迅速，我们提供的任何指导几乎肯定会在您阅读时过时。在我们撰写本文时，sklearn 刚刚添加了一个HistGradientBoostingRegressor类，提供了出色的性能。对于这个类，以及我们见过的所有梯度提升树方法，有许多要调整的超参数。与随机森林不同，梯度提升树对这些超参数的选择非常敏感；在实践中，大多数人使用一个循环来尝试一系列超参数，找到最适合的那些。\n另一种取得很好结果的技术是在机器学习模型中使用神经网络学习的嵌入。"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#将嵌入与其他方法结合",
    "href": "Fastbook/translations/cn/09_tabular.html#将嵌入与其他方法结合",
    "title": "第九章：表格建模深入探讨",
    "section": "将嵌入与其他方法结合",
    "text": "将嵌入与其他方法结合\n我们在本章开头提到的实体嵌入论文的摘要中指出：“从训练的神经网络中获得的嵌入在作为输入特征时显著提高了所有测试的机器学习方法的性能。”它包括在图 9-8 中显示的非常有趣的表格。\n\n\n\n嵌入与其他方法结合\n\n\n\n图 9-8。使用神经网络嵌入作为其他机器学习方法的输入的效果（由 Cheng Guo 和 Felix Berkhahn 提供）\n这显示了四种建模技术之间的平均百分比误差（MAPE）的比较，其中三种我们已经看过，还有一种是k-最近邻（KNN），这是一种非常简单的基准方法。第一列数字包含在比赛中使用这些方法的结果；第二列显示如果您首先使用具有分类嵌入的神经网络，然后在模型中使用这些分类嵌入而不是原始分类列会发生什么。正如您所看到的，在每种情况下，使用嵌入而不是原始类别可以显著改善模型。\n这是一个非常重要的结果，因为它表明您可以在推断时获得神经网络的性能改进的大部分，而无需使用神经网络。您可以只使用一个嵌入，这实际上只是一个数组查找，以及一个小的决策树集成。\n这些嵌入甚至不需要为组织中的每个模型或任务单独学习。相反，一旦为特定任务的列学习了一组嵌入，它们可以存储在一个中心位置，并在多个模型中重复使用。实际上，我们从与其他大公司的从业者的私下交流中得知，这在许多地方已经发生了。"
  },
  {
    "objectID": "Fastbook/translations/cn/09_tabular.html#进一步研究",
    "href": "Fastbook/translations/cn/09_tabular.html#进一步研究",
    "title": "第九章：表格建模深入探讨",
    "section": "进一步研究",
    "text": "进一步研究\n\n选择一个 Kaggle 上的带表格数据的比赛（当前或过去），并尝试调整本章中所见的技术以获得最佳结果。将您的结果与私人排行榜进行比较。\n自己从头开始实现本章中的决策树算法，并在第一个练习中使用的数据集上尝试它。\n在本章中使用神经网络中的嵌入在随机森林中，并查看是否可以改进我们看到的随机森林结果。\n解释TabularModel源代码的每一行做了什么（除了BatchNorm1d和Dropout层）。"
  },
  {
    "objectID": "Fastbook/translations/cn/18_CAM.html",
    "href": "Fastbook/translations/cn/18_CAM.html",
    "title": "第十八章：使用 CAM 解释 CNN",
    "section": "",
    "text": "现在我们知道如何从头开始构建几乎任何东西，让我们利用这些知识来创建全新（并非常有用！）的功能：类激活图。它让我们对 CNN 为何做出预测有一些见解。\n在这个过程中，我们将学习到 PyTorch 中一个我们之前没有见过的方便功能，hook，并且我们将应用本书中介绍的许多概念。如果你想真正测试你对本书材料的理解，完成本章后，尝试将其放在一边，从头开始重新创建这里的想法（不要偷看！）。"
  },
  {
    "objectID": "Fastbook/translations/cn/18_CAM.html#进一步研究",
    "href": "Fastbook/translations/cn/18_CAM.html#进一步研究",
    "title": "第十八章：使用 CAM 解释 CNN",
    "section": "进一步研究",
    "text": "进一步研究\n\n尝试移除keepdim，看看会发生什么。查阅 PyTorch 文档中的这个参数。为什么我们在这个笔记本中需要它？\n创建一个类似这个的笔记本，但用于 NLP，并用它来找出电影评论中哪些词对于评估特定电影评论的情感最重要。"
  },
  {
    "objectID": "Fastbook/translations/cn/17_foundations.html",
    "href": "Fastbook/translations/cn/17_foundations.html",
    "title": "第十七章：基础神经网络",
    "section": "",
    "text": "本章开始了一段旅程，我们将深入研究我们在前几章中使用的模型的内部。我们将涵盖许多我们以前见过的相同内容，但这一次我们将更加密切地关注实现细节，而不那么密切地关注事物为什么是这样的实际问题。\n我们将从头开始构建一切，仅使用对张量的基本索引。我们将从头开始编写一个神经网络，然后手动实现反向传播，以便我们在调用loss.backward时确切地知道 PyTorch 中发生了什么。我们还将看到如何使用自定义autograd函数扩展 PyTorch，允许我们指定自己的前向和后向计算。"
  },
  {
    "objectID": "Fastbook/translations/cn/17_foundations.html#建模神经元",
    "href": "Fastbook/translations/cn/17_foundations.html#建模神经元",
    "title": "第十七章：基础神经网络",
    "section": "建模神经元",
    "text": "建模神经元\n神经元接收一定数量的输入，并为每个输入设置内部权重。它对这些加权输入求和以产生输出，并添加内部偏置。在数学上，这可以写成\no u t = ∑ i=1 n x i w i + b\n如果我们将输入命名为( x 1 , ⋯ , x n )，我们的权重为( w 1 , ⋯ , w n )，以及我们的偏置b。在代码中，这被翻译为以下内容：\noutput = sum([x*w for x,w in zip(inputs,weights)]) + bias\n然后将此输出馈送到非线性函数中，称为激活函数，然后发送到另一个神经元。在深度学习中，最常见的是修正线性单元，或ReLU，正如我们所见，这是一种花哨的说法：\ndef relu(x): return x if x &gt;= 0 else 0\n然后通过在连续的层中堆叠许多这些神经元来构建深度学习模型。我们创建一个具有一定数量的神经元（称为隐藏大小）的第一层，并将所有输入链接到每个神经元。这样的一层通常称为全连接层或密集层（用于密集连接），或线性层。\n它要求您计算每个input和具有给定weight的每个神经元的点积：\nsum([x*w for x,w in zip(input,weight)])\n如果您对线性代数有一点了解，您可能会记得当您进行矩阵乘法时会发生许多这些点积。更准确地说，如果我们的输入在大小为batch_size乘以n_inputs的矩阵x中，并且如果我们已将神经元的权重分组在大小为n_neurons乘以n_inputs的矩阵w中（每个神经元必须具有与其输入相同数量的权重），以及将所有偏置分组在大小为n_neurons的向量b中，则此全连接层的输出为\ny = x @ w.t() + b\n其中@表示矩阵乘积，w.t()是w的转置矩阵。然后输出y的大小为batch_size乘以n_neurons，在位置(i,j)上我们有这个（对于数学爱好者）：\ny i,j = ∑ k=1 n x i,k w k,j + b j\n或者在代码中：\ny[i,j] = sum([a * b for a,b in zip(x[i,:],w[j,:])]) + b[j]\n转置是必要的，因为在矩阵乘积m @ n的数学定义中，系数(i,j)如下：\nsum([a * b for a,b in zip(m[i,:],n[:,j])])\n所以我们需要的非常基本的操作是矩阵乘法，因为这是神经网络核心中隐藏的内容。"
  },
  {
    "objectID": "Fastbook/translations/cn/17_foundations.html#从头开始的矩阵乘法",
    "href": "Fastbook/translations/cn/17_foundations.html#从头开始的矩阵乘法",
    "title": "第十七章：基础神经网络",
    "section": "从头开始的矩阵乘法",
    "text": "从头开始的矩阵乘法\n让我们编写一个函数，计算两个张量的矩阵乘积，然后再允许我们使用 PyTorch 版本。我们只会在 PyTorch 张量中使用索引：\nimport torch\nfrom torch import tensor\n我们需要三个嵌套的for循环：一个用于行索引，一个用于列索引，一个用于内部求和。ac和ar分别表示a的列数和行数（对于b也是相同的约定），我们通过检查a的列数是否与b的行数相同来确保计算矩阵乘积是可能的：\ndef matmul(a,b):\n    ar,ac = a.shape # n_rows * n_cols\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n    return c\n为了测试这一点，我们假装（使用随机矩阵）我们正在处理一个包含 5 个 MNIST 图像的小批量，将它们展平为28*28向量，然后使用线性模型将它们转换为 10 个激活值：\nm1 = torch.randn(5,28*28)\nm2 = torch.randn(784,10)\n让我们计时我们的函数，使用 Jupyter 的“魔术”命令%time：\n%time t1=matmul(m1, m2)\nCPU times: user 1.15 s, sys: 4.09 ms, total: 1.15 s\nWall time: 1.15 s\n看看这与 PyTorch 内置的@有什么区别？\n%timeit -n 20 t2=m1@m2\n14 µs ± 8.95 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\n正如我们所看到的，在 Python 中三个嵌套循环是一个坏主意！Python 是一种慢速语言，这不会高效。我们在这里看到 PyTorch 比 Python 快大约 100,000 倍——而且这还是在我们开始使用 GPU 之前！\n这种差异是从哪里来的？PyTorch 没有在 Python 中编写矩阵乘法，而是使用 C++来加快速度。通常，当我们在张量上进行计算时，我们需要向量化它们，以便利用 PyTorch 的速度，通常使用两种技术：逐元素算术和广播。"
  },
  {
    "objectID": "Fastbook/translations/cn/17_foundations.html#逐元素算术",
    "href": "Fastbook/translations/cn/17_foundations.html#逐元素算术",
    "title": "第十七章：基础神经网络",
    "section": "逐元素算术",
    "text": "逐元素算术\n所有基本运算符（+、-、*、/、&gt;、&lt;、==）都可以逐元素应用。这意味着如果我们为两个具有相同形状的张量a和b写a+b，我们将得到一个由a和b元素之和组成的张量：\na = tensor([10., 6, -4])\nb = tensor([2., 8, 7])\na + b\ntensor([12., 14.,  3.])\n布尔运算符将返回一个布尔数组：\na &lt; b\ntensor([False,  True,  True])\n如果我们想知道a的每个元素是否小于b中对应的元素，或者两个张量是否相等，我们需要将这些逐元素操作与torch.all结合起来：\n(a &lt; b).all(), (a==b).all()\n(tensor(False), tensor(False))\n像all、sum和mean这样的缩减操作返回只有一个元素的张量，称为秩-0 张量。如果要将其转换为普通的 Python 布尔值或数字，需要调用.item：\n(a + b).mean().item()\n9.666666984558105\n逐元素操作适用于任何秩的张量，只要它们具有相同的形状：\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\nm*m\ntensor([[ 1.,  4.,  9.],\n        [16., 25., 36.],\n        [49., 64., 81.]])\n但是，不能对形状不同的张量执行逐元素操作（除非它们是可广播的，如下一节所讨论的）：\nn = tensor([[1., 2, 3], [4,5,6]])\nm*n\n RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at\n dimension 0\n通过逐元素算术，我们可以去掉我们的三个嵌套循环中的一个：我们可以在将a的第i行和b的第j列对应的张量相乘之前对它们进行求和，这将加快速度，因为内部循环现在将由 PyTorch 以 C 速度执行。\n要访问一列或一行，我们可以简单地写a[i,：]或b[:,j]。:表示在该维度上取所有内容。我们可以限制这个并只取该维度的一个切片，通过传递一个范围，比如1:5，而不仅仅是:。在这种情况下，我们将取第 1 到第 4 列的元素（第二个数字是不包括在内的）。\n一个简化是我们总是可以省略尾随冒号，因此a[i,:]可以缩写为a[i]。考虑到所有这些，我们可以编写我们矩阵乘法的新版本：\ndef matmul(a,b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = (a[i] * b[:,j]).sum()\n    return c\n%timeit -n 20 t3 = matmul(m1,m2)\n1.7 ms ± 88.1 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\n我们已经快了约 700 倍，只是通过删除那个内部的for循环！这只是开始——通过广播，我们可以删除另一个循环并获得更重要的加速。"
  },
  {
    "objectID": "Fastbook/translations/cn/17_foundations.html#广播",
    "href": "Fastbook/translations/cn/17_foundations.html#广播",
    "title": "第十七章：基础神经网络",
    "section": "广播",
    "text": "广播\n正如我们在第四章中讨论的那样，广播是由Numpy 库引入的一个术语，用于描述在算术操作期间如何处理不同秩的张量。例如，显然无法将 3×3 矩阵与 4×5 矩阵相加，但如果我们想将一个标量（可以表示为 1×1 张量）与矩阵相加呢？或者大小为 3 的向量与 3×4 矩阵？在这两种情况下，我们可以找到一种方法来理解这个操作。\n广播为编码规则提供了特定的规则，用于在尝试进行逐元素操作时确定形状是否兼容，以及如何扩展较小形状的张量以匹配较大形状的张量。如果您想要能够编写快速执行的代码，掌握这些规则是至关重要的。在本节中，我们将扩展我们之前对广播的处理，以了解这些规则。\n\n使用标量进行广播\n使用标量进行广播是最简单的广播类型。当我们有一个张量a和一个标量时，我们只需想象一个与a形状相同且填充有该标量的张量，并执行操作：\na = tensor([10., 6, -4])\na &gt; 0\ntensor([ True,  True, False])\n我们如何能够进行这种比较？0被广播以具有与a相同的维度。请注意，这是在不在内存中创建一个充满零的张量的情况下完成的（这将是低效的）。\n如果要通过减去均值（标量）来标准化数据集（矩阵）并除以标准差（另一个标量），这是很有用的：\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\n(m - 5) / 2.73\ntensor([[-1.4652, -1.0989, -0.7326],\n        [-0.3663,  0.0000,  0.3663],\n        [ 0.7326,  1.0989,  1.4652]])\n如果矩阵的每行有不同的均值怎么办？在这种情况下，您需要将一个向量广播到一个矩阵。\n\n\n将向量广播到矩阵\n我们可以将一个向量广播到一个矩阵中：\nc = tensor([10.,20,30])\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\nm.shape,c.shape\n(torch.Size([3, 3]), torch.Size([3]))\nm + c\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n这里，c的元素被扩展为使三行匹配，从而使操作成为可能。同样，PyTorch 实际上并没有在内存中创建三个c的副本。这是由幕后的expand_as方法完成的：\nc.expand_as(m)\ntensor([[10., 20., 30.],\n        [10., 20., 30.],\n        [10., 20., 30.]])\n如果我们查看相应的张量，我们可以请求其storage属性（显示用于张量的内存实际内容）来检查是否存储了无用的数据：\nt = c.expand_as(m)\nt.storage()\n 10.0\n 20.0\n 30.0\n[torch.FloatStorage of size 3]\n尽管张量在官方上有九个元素，但内存中只存储了三个标量。这是可能的，这要归功于给该维度一个 0 步幅的巧妙技巧。在该维度上（这意味着当 PyTorch 通过添加步幅查找下一行时，它不会移动）：\nt.stride(), t.shape\n((0, 1), torch.Size([3, 3]))\n由于m的大小为 3×3，有两种广播的方式。在最后一个维度上进行广播的事实是一种约定，这是来自广播规则的规定，与我们对张量排序的方式无关。如果我们这样做，我们会得到相同的结果：\nc + m\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n实际上，只有通过n，我们才能将大小为n的向量广播到大小为m的矩阵中：\nc = tensor([10.,20,30])\nm = tensor([[1., 2, 3], [4,5,6]])\nc+m\ntensor([[11., 22., 33.],\n        [14., 25., 36.]])\n这不起作用：\nc = tensor([10.,20])\nm = tensor([[1., 2, 3], [4,5,6]])\nc+m\n RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at\n dimension 1\n如果我们想在另一个维度上进行广播，我们必须改变向量的形状，使其成为一个 3×1 矩阵。这可以通过 PyTorch 中的unsqueeze方法来实现：\nc = tensor([10.,20,30])\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\nc = c.unsqueeze(1)\nm.shape,c.shape\n(torch.Size([3, 3]), torch.Size([3, 1]))\n这次，c在列侧进行了扩展：\nc+m\ntensor([[11., 12., 13.],\n        [24., 25., 26.],\n        [37., 38., 39.]])\n与以前一样，只有三个标量存储在内存中：\nt = c.expand_as(m)\nt.storage()\n 10.0\n 20.0\n 30.0\n[torch.FloatStorage of size 3]\n扩展后的张量具有正确的形状，因为列维度的步幅为 0：\nt.stride(), t.shape\n((1, 0), torch.Size([3, 3]))\n使用广播，如果需要添加维度，则默认情况下会在开头添加。在之前进行广播时，PyTorch 在幕后执行了c.unsqueeze(0)：\nc = tensor([10.,20,30])\nc.shape, c.unsqueeze(0).shape,c.unsqueeze(1).shape\n(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))\nunsqueeze命令可以被None索引替换：\nc.shape, c[None,:].shape,c[:,None].shape\n(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))\n您可以始终省略尾随冒号，...表示所有前面的维度：\nc[None].shape,c[...,None].shape\n(torch.Size([1, 3]), torch.Size([3, 1]))\n有了这个，我们可以在我们的矩阵乘法函数中删除另一个for循环。现在，我们不再将a[i]乘以b[:,j]，而是使用广播将a[i]乘以整个矩阵b，然后对结果求和：\ndef matmul(a,b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n#       c[i,j] = (a[i,:]          * b[:,j]).sum() # previous\n        c[i]   = (a[i  ].unsqueeze(-1) * b).sum(dim=0)\n    return c\n%timeit -n 20 t4 = matmul(m1,m2)\n357 µs ± 7.2 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\n现在我们比第一次实现快了 3700 倍！在继续之前，让我们更详细地讨论一下广播规则。\n\n\n广播规则\n在操作两个张量时，PyTorch 会逐个元素地比较它们的形状。它从尾部维度开始，逆向工作，在遇到空维度时添加 1。当以下情况之一为真时，两个维度是兼容的：\n\n它们是相等的。\n其中之一是 1，此时该维度会被广播以使其与其他维度相同。\n\n数组不需要具有相同数量的维度。例如，如果您有一个 256×256×3 的 RGB 值数组，并且想要按不同值缩放图像中的每种颜色，您可以将图像乘以一个具有三个值的一维数组。根据广播规则排列这些数组的尾部轴的大小表明它们是兼容的：\nImage  (3d tensor): 256 x 256 x 3\nScale  (1d tensor):  (1)   (1)  3\nResult (3d tensor): 256 x 256 x 3\n然而，一个大小为 256×256 的 2D 张量与我们的图像不兼容：\nImage  (3d tensor): 256 x 256 x   3\nScale  (1d tensor):  (1)  256 x 256\nError\n在我们早期的例子中，使用了一个 3×3 矩阵和一个大小为 3 的向量，广播是在行上完成的：\nMatrix (2d tensor):   3 x 3\nVector (1d tensor): (1)   3\nResult (2d tensor):   3 x 3\n作为练习，尝试确定何时需要添加维度（以及在何处），以便将大小为64 x 3 x 256 x 256的图像批次与三个元素的向量（一个用于均值，一个用于标准差）进行归一化。\n另一种简化张量操作的有用方法是使用爱因斯坦求和约定。"
  },
  {
    "objectID": "Fastbook/translations/cn/17_foundations.html#爱因斯坦求和",
    "href": "Fastbook/translations/cn/17_foundations.html#爱因斯坦求和",
    "title": "第十七章：基础神经网络",
    "section": "爱因斯坦求和",
    "text": "爱因斯坦求和\n在使用 PyTorch 操作@或torch.matmul之前，我们可以实现矩阵乘法的最后一种方法：爱因斯坦求和（einsum）。这是一种将乘积和求和以一般方式组合的紧凑表示。我们可以写出这样的方程：\nik,kj -&gt; ij\n左侧表示操作数的维度，用逗号分隔。这里我们有两个分别具有两个维度（i,k和k,j）的张量。右侧表示结果维度，所以这里我们有一个具有两个维度i,j的张量。\n爱因斯坦求和符号的规则如下：\n\n重复的索引会被隐式求和。\n每个索引在任何项中最多只能出现两次。\n每个项必须包含相同的非重复索引。\n\n因此，在我们的例子中，由于k是重复的，我们对该索引求和。最终，该公式表示当我们在（i,j）中放入所有第一个张量中的系数（i,k）与第二个张量中的系数（k,j）相乘的总和时得到的矩阵……这就是矩阵乘积！\n以下是我们如何在 PyTorch 中编写这段代码：\ndef matmul(a,b): return torch.einsum('ik,kj-&gt;ij', a, b)\n爱因斯坦求和是一种非常实用的表达涉及索引和乘积和的操作的方式。请注意，您可以在左侧只有一个成员。例如，\ntorch.einsum('ij-&gt;ji', a)\n返回矩阵a的转置。您也可以有三个或更多成员：\ntorch.einsum('bi,ij,bj-&gt;b', a, b, c)\n这将返回一个大小为b的向量，其中第k个坐标是a[k,i] b[i,j] c[k,j]的总和。当您有更多维度时，这种表示特别方便，因为有批次。例如，如果您有两批次的矩阵并且想要计算每批次的矩阵乘积，您可以这样做：\ntorch.einsum('bik,bkj-&gt;bij', a, b)\n让我们回到使用einsum实现的新matmul，看看它的速度：\n%timeit -n 20 t5 = matmul(m1,m2)\n68.7 µs ± 4.06 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\n正如您所看到的，它不仅实用，而且非常快。einsum通常是在 PyTorch 中执行自定义操作的最快方式，而无需深入研究 C++和 CUDA。（但通常不如精心优化的 CUDA 代码快，正如您从“从头开始的矩阵乘法”的结果中看到的。）\n现在我们知道如何从头开始实现矩阵乘法，我们准备构建我们的神经网络——具体来说，是它的前向和后向传递——只使用矩阵乘法。"
  },
  {
    "objectID": "Fastbook/translations/cn/17_foundations.html#定义和初始化一个层",
    "href": "Fastbook/translations/cn/17_foundations.html#定义和初始化一个层",
    "title": "第十七章：基础神经网络",
    "section": "定义和初始化一个层",
    "text": "定义和初始化一个层\n首先我们将以两层神经网络为例。正如我们所看到的，一层可以表示为y = x @ w + b，其中x是我们的输入，y是我们的输出，w是该层的权重（如果我们不像之前那样转置，则大小为输入数量乘以神经元数量），b是偏置向量：\ndef lin(x, w, b): return x @ w + b\n我们可以将第二层叠加在第一层上，但由于数学上两个线性操作的组合是另一个线性操作，只有在中间放入一些非线性的东西才有意义，称为激活函数。正如本章开头提到的，在深度学习应用中，最常用的激活函数是 ReLU，它返回x和0的最大值。\n在本章中，我们实际上不会训练我们的模型，因此我们将为我们的输入和目标使用随机张量。假设我们的输入是大小为 100 的 200 个向量，我们将它们分组成一个批次，我们的目标是 200 个随机浮点数：\nx = torch.randn(200, 100)\ny = torch.randn(200)\n对于我们的两层模型，我们将需要两个权重矩阵和两个偏置向量。假设我们的隐藏大小为 50，输出大小为 1（对于我们的输入之一，相应的输出在这个玩具示例中是一个浮点数）。我们随机初始化权重，偏置为零：\nw1 = torch.randn(100,50)\nb1 = torch.zeros(50)\nw2 = torch.randn(50,1)\nb2 = torch.zeros(1)\n然后我们的第一层的结果就是这样的：\nl1 = lin(x, w1, b1)\nl1.shape\ntorch.Size([200, 50])\n请注意，这个公式适用于我们的输入批次，并返回一个隐藏状态批次：l1是一个大小为 200（我们的批次大小）乘以 50（我们的隐藏大小）的矩阵。\n然而，我们的模型初始化方式存在问题。要理解这一点，我们需要查看l1的均值和标准差（std）：\nl1.mean(), l1.std()\n(tensor(0.0019), tensor(10.1058))\n均值接近零，这是可以理解的，因为我们的输入和权重矩阵的均值都接近零。但标准差，表示我们的激活离均值有多远，从 1 变为 10。这是一个真正的大问题，因为这只是一个层。现代神经网络可以有数百层，因此如果每一层将我们的激活的规模乘以 10，到了最后一层，我们将无法用计算机表示数字。\n实际上，如果我们在x和大小为 100×100 的随机矩阵之间进行 50 次乘法运算，我们将得到这个结果：\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ torch.randn(100,100)\nx[0:5,0:5]\ntensor([[nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan]])\n结果是到处都是nan。也许我们的矩阵的规模太大了，我们需要更小的权重？但如果我们使用太小的权重，我们将遇到相反的问题-我们的激活的规模将从 1 变为 0.1，在 100 层之后，我们将到处都是零：\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ (torch.randn(100,100) * 0.01)\nx[0:5,0:5]\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n因此，我们必须精确地缩放我们的权重矩阵，以使我们的激活的标准差保持在 1。我们可以通过数学计算出要使用的确切值，正如 Xavier Glorot 和 Yoshua Bengio 在“理解训练深度前馈神经网络的困难”中所示。给定层的正确比例是1 / n in，其中n in代表输入的数量。\n在我们的情况下，如果有 100 个输入，我们应该将我们的权重矩阵缩放为 0.1：\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ (torch.randn(100,100) * 0.1)\nx[0:5,0:5]\ntensor([[ 0.7554,  0.6167, -0.1757, -1.5662,  0.5644],\n        [-0.1987,  0.6292,  0.3283, -1.1538,  0.5416],\n        [ 0.6106,  0.2556, -0.0618, -0.9463,  0.4445],\n        [ 0.4484,  0.7144,  0.1164, -0.8626,  0.4413],\n        [ 0.3463,  0.5930,  0.3375, -0.9486,  0.5643]])\n终于，一些既不是零也不是nan的数字！请注意，即使经过了那 50 个虚假层，我们的激活的规模仍然是稳定的：\nx.std()\ntensor(0.7042)\n如果你稍微调整一下 scale 的值，你会注意到即使从 0.1 稍微偏离，你会得到非常小或非常大的数字，因此正确初始化权重非常重要。\n让我们回到我们的神经网络。由于我们稍微改变了我们的输入，我们需要重新定义它们：\nx = torch.randn(200, 100)\ny = torch.randn(200)\n对于我们的权重，我们将使用正确的 scale，这被称为Xavier 初始化（或Glorot 初始化）：\nfrom math import sqrt\nw1 = torch.randn(100,50) / sqrt(100)\nb1 = torch.zeros(50)\nw2 = torch.randn(50,1) / sqrt(50)\nb2 = torch.zeros(1)\n现在如果我们计算第一层的结果，我们可以检查均值和标准差是否受控制：\nl1 = lin(x, w1, b1)\nl1.mean(),l1.std()\n(tensor(-0.0050), tensor(1.0000))\n非常好。现在我们需要经过一个 ReLU，所以让我们定义一个。ReLU 去除负数并用零替换它们，这另一种说法是它将我们的张量夹在零处：\ndef relu(x): return x.clamp_min(0.)\n我们通过这个激活：\nl2 = relu(l1)\nl2.mean(),l2.std()\n(tensor(0.3961), tensor(0.5783))\n现在我们回到原点：我们的激活均值变为 0.4（这是可以理解的，因为我们去除了负数），标准差下降到 0.58。所以像以前一样，经过几层后我们可能最终会得到零：\nx = torch.randn(200, 100)\nfor i in range(50): x = relu(x @ (torch.randn(100,100) * 0.1))\nx[0:5,0:5]\ntensor([[0.0000e+00, 1.9689e-08, 4.2820e-08, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 1.6701e-08, 4.3501e-08, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 1.0976e-08, 3.0411e-08, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 1.8457e-08, 4.9469e-08, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 1.9949e-08, 4.1643e-08, 0.0000e+00, 0.0000e+00]])\n这意味着我们的初始化不正确。为什么？在 Glorot 和 Bengio 撰写他们的文章时，神经网络中最流行的激活函数是双曲正切（tanh，他们使用的那个），而该初始化并没有考虑到我们的 ReLU。幸运的是，有人已经为我们计算出了正确的 scale 供我们使用。在“深入研究整流器：超越人类水平的性能”（我们之前见过的文章，介绍了 ResNet），Kaiming He 等人表明我们应该使用以下 scale 代替：2 / n in，其中n in是我们模型的输入数量。让我们看看这给我们带来了什么：\nx = torch.randn(200, 100)\nfor i in range(50): x = relu(x @ (torch.randn(100,100) * sqrt(2/100)))\nx[0:5,0:5]\ntensor([[0.2871, 0.0000, 0.0000, 0.0000, 0.0026],\n        [0.4546, 0.0000, 0.0000, 0.0000, 0.0015],\n        [0.6178, 0.0000, 0.0000, 0.0180, 0.0079],\n        [0.3333, 0.0000, 0.0000, 0.0545, 0.0000],\n        [0.1940, 0.0000, 0.0000, 0.0000, 0.0096]])\n好了：这次我们的数字不全为零。所以让我们回到我们神经网络的定义，并使用这个初始化（被称为Kaiming 初始化或He 初始化）：\nx = torch.randn(200, 100)\ny = torch.randn(200)\nw1 = torch.randn(100,50) * sqrt(2 / 100)\nb1 = torch.zeros(50)\nw2 = torch.randn(50,1) * sqrt(2 / 50)\nb2 = torch.zeros(1)\n让我们看看通过第一个线性层和 ReLU 后激活的规模：\nl1 = lin(x, w1, b1)\nl2 = relu(l1)\nl2.mean(), l2.std()\n(tensor(0.5661), tensor(0.8339))\n好多了！现在我们的权重已经正确初始化，我们可以定义我们的整个模型：\ndef model(x):\n    l1 = lin(x, w1, b1)\n    l2 = relu(l1)\n    l3 = lin(l2, w2, b2)\n    return l3\n这是前向传播。现在剩下的就是将我们的输出与我们拥有的标签（在这个例子中是随机数）进行比较，使用损失函数。在这种情况下，我们将使用均方误差。（这是一个玩具问题，这是下一步计算梯度所使用的最简单的损失函数。）\n唯一的微妙之处在于我们的输出和目标形状并不完全相同——经过模型后，我们得到这样的输出：\nout = model(x)\nout.shape\ntorch.Size([200, 1])\n为了去掉这个多余的 1 维，我们使用squeeze函数：\ndef mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()\n现在我们准备计算我们的损失：\nloss = mse(out, y)\n前向传播到此结束，现在让我们看一下梯度。"
  },
  {
    "objectID": "Fastbook/translations/cn/17_foundations.html#梯度和反向传播",
    "href": "Fastbook/translations/cn/17_foundations.html#梯度和反向传播",
    "title": "第十七章：基础神经网络",
    "section": "梯度和反向传播",
    "text": "梯度和反向传播\n我们已经看到 PyTorch 通过一个神奇的调用loss.backward计算出我们需要的所有梯度，但让我们探究一下背后发生了什么。\n现在我们需要计算损失相对于模型中所有权重的梯度，即w1、b1、w2和b2中的所有浮点数。为此，我们需要一点数学，具体来说是链式法则。这是指导我们如何计算复合函数导数的微积分规则：\n(g∘f) ‘ ( x ) = g ’ ( f ( x ) ) f ’ ( x )"
  },
  {
    "objectID": "Fastbook/translations/cn/17_foundations.html#重构模型",
    "href": "Fastbook/translations/cn/17_foundations.html#重构模型",
    "title": "第十七章：基础神经网络",
    "section": "重构模型",
    "text": "重构模型\n我们使用的三个函数有两个相关的函数：一个前向传递和一个后向传递。我们可以创建一个类将它们包装在一起，而不是分开编写它们。该类还可以存储后向传递的输入和输出。这样，我们只需要调用backward：\nclass Relu():\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp.clamp_min(0.)\n        return self.out\n\n    def backward(self): self.inp.g = (self.inp&gt;0).float() * self.out.g\n__call__是 Python 中的一个魔术名称，它将使我们的类可调用。当我们键入y = Relu()(x)时，将执行这个操作。我们也可以对我们的线性层和 MSE 损失做同样的操作：\nclass Lin():\n    def __init__(self, w, b): self.w,self.b = w,b\n\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp@self.w + self.b\n        return self.out\n\n    def backward(self):\n        self.inp.g = self.out.g @ self.w.t()\n        self.w.g = self.inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\nclass Mse():\n    def __call__(self, inp, targ):\n        self.inp = inp\n        self.targ = targ\n        self.out = (inp.squeeze() - targ).pow(2).mean()\n        return self.out\n\n    def backward(self):\n        x = (self.inp.squeeze()-self.targ).unsqueeze(-1)\n        self.inp.g = 2.*x/self.targ.shape[0]\n然后我们可以把一切都放在一个模型中，我们用我们的张量w1、b1、w2和b2来初始化：\nclass Model():\n    def __init__(self, w1, b1, w2, b2):\n        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n        self.loss = Mse()\n\n    def __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return self.loss(x, targ)\n\n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\n这种重构和将事物注册为模型的层的好处是，前向和后向传递现在非常容易编写。如果我们想要实例化我们的模型，我们只需要写这个：\nmodel = Model(w1, b1, w2, b2)\n然后前向传递可以这样执行：\nloss = model(x, y)\n然后使用这个进行后向传递：\nmodel.backward()"
  },
  {
    "objectID": "Fastbook/translations/cn/17_foundations.html#转向-pytorch",
    "href": "Fastbook/translations/cn/17_foundations.html#转向-pytorch",
    "title": "第十七章：基础神经网络",
    "section": "转向 PyTorch",
    "text": "转向 PyTorch\n我们编写的Lin、Mse和Relu类有很多共同之处，所以我们可以让它们都继承自同一个基类：\nclass LayerFunction():\n    def __call__(self, *args):\n        self.args = args\n        self.out = self.forward(*args)\n        return self.out\n\n    def forward(self):  raise Exception('not implemented')\n    def bwd(self):      raise Exception('not implemented')\n    def backward(self): self.bwd(self.out, *self.args)\n然后我们只需要在每个子类中实现forward和bwd：\nclass Relu(LayerFunction):\n    def forward(self, inp): return inp.clamp_min(0.)\n    def bwd(self, out, inp): inp.g = (inp&gt;0).float() * out.g\nclass Lin(LayerFunction):\n    def __init__(self, w, b): self.w,self.b = w,b\n\n    def forward(self, inp): return inp@self.w + self.b\n\n    def bwd(self, out, inp):\n        inp.g = out.g @ self.w.t()\n        self.w.g = self.inp.t() @ self.out.g\n        self.b.g = out.g.sum(0)\nclass Mse(LayerFunction):\n    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n    def bwd(self, out, inp, targ):\n        inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]\n我们模型的其余部分可以与以前相同。这越来越接近 PyTorch 的做法。我们需要区分的每个基本函数都被写成一个torch.autograd.Function对象，它有一个forward和一个backward方法。PyTorch 将跟踪我们进行的任何计算，以便能够正确运行反向传播，除非我们将张量的requires_grad属性设置为False。\n编写其中一个（几乎）和编写我们原始类一样容易。不同之处在于我们选择保存什么并将其放入上下文变量中（以确保我们不保存不需要的任何内容），并在backward传递中返回梯度。很少需要编写自己的Function，但如果您需要某些奇特的东西或想要干扰常规函数的梯度，这里是如何编写的：\nfrom torch.autograd import Function\n\nclass MyRelu(Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i.clamp_min(0.)\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i, = ctx.saved_tensors\n        return grad_output * (i&gt;0).float()\n用于构建利用这些Function的更复杂模型的结构是torch.nn.Module。这是所有模型的基本结构，到目前为止您看到的所有神经网络都是从该类中继承的。它主要有助于注册所有可训练的参数，正如我们已经看到的可以在训练循环中使用的那样。\n要实现一个nn.Module，你只需要做以下几步：\n\n确保在初始化时首先调用超类__init__。\n将模型的任何参数定义为具有nn.Parameter属性。\n定义一个forward函数，返回模型的输出。\n\n这里是一个从头开始的线性层的例子：\nimport torch.nn as nn\n\nclass LinearLayer(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(n_out, n_in) * sqrt(2/n_in))\n        self.bias = nn.Parameter(torch.zeros(n_out))\n\n    def forward(self, x): return x @ self.weight.t() + self.bias\n正如您所看到的，这个类会自动跟踪已定义的参数：\nlin = LinearLayer(10,2)\np1,p2 = lin.parameters()\np1.shape,p2.shape\n(torch.Size([2, 10]), torch.Size([2]))\n正是由于nn.Module的这个特性，我们可以只说opt.step，并让优化器循环遍历参数并更新每个参数。\n请注意，在 PyTorch 中，权重存储为一个n_out x n_in矩阵，这就是为什么在前向传递中我们有转置的原因。\n通过使用 PyTorch 中的线性层（也使用 Kaiming 初始化），我们在本章中一直在构建的模型可以这样编写：\nclass Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out))\n        self.loss = mse\n\n    def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ)\nfastai 提供了自己的Module变体，与nn.Module相同，但不需要您调用super().__init__()（它会自动为您执行）：\nclass Model(Module):\n    def __init__(self, n_in, nh, n_out):\n        self.layers = nn.Sequential(\n            nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out))\n        self.loss = mse\n\n    def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ)\n在第十九章中，我们将从这样一个模型开始，看看如何从头开始构建一个训练循环，并将其重构为我们在之前章节中使用的内容。"
  },
  {
    "objectID": "Fastbook/translations/cn/17_foundations.html#进一步研究",
    "href": "Fastbook/translations/cn/17_foundations.html#进一步研究",
    "title": "第十七章：基础神经网络",
    "section": "进一步研究",
    "text": "进一步研究\n\n将 ReLU 实现为 torch.autograd.Function 并用它训练模型。\n如果您对数学感兴趣，请确定数学符号中线性层的梯度。将其映射到本章中的实现。\n了解 PyTorch 中的 unfold 方法，并结合矩阵乘法实现自己的二维卷积函数。然后训练一个使用它的 CNN。\n使用 NumPy 而不是 PyTorch 在本章中实现所有内容。"
  },
  {
    "objectID": "Fastbook/translations/cn/07_sizing_and_tta.html",
    "href": "Fastbook/translations/cn/07_sizing_and_tta.html",
    "title": "第七章：训练一个最先进的模型",
    "section": "",
    "text": "本章介绍了更高级的技术，用于训练图像分类模型并获得最先进的结果。如果您想了解更多关于深度学习的其他应用，并稍后回来，您可以跳过它——后续章节不会假设您已掌握这些材料。\n我们将看一下什么是归一化，一种强大的数据增强技术叫做 Mixup，渐进式调整大小方法，以及测试时间增强。为了展示所有这些，我们将从头开始训练一个模型（不使用迁移学习），使用一个名为 Imagenette 的 ImageNet 子集。它包含了原始 ImageNet 数据集中 10 个非常不同的类别的子集，使得在我们想要进行实验时训练更快。\n这将比我们之前的数据集更难做得好，因为我们使用全尺寸、全彩色的图像，这些图像是不同大小、不同方向、不同光照等对象的照片。因此，在本章中，我们将介绍一些重要的技术，以便充分利用您的数据集，特别是当您从头开始训练，或者使用迁移学习在一个与预训练模型使用的非常不同类型的数据集上训练模型时。"
  },
  {
    "objectID": "Fastbook/translations/cn/07_sizing_and_tta.html#进一步研究",
    "href": "Fastbook/translations/cn/07_sizing_and_tta.html#进一步研究",
    "title": "第七章：训练一个最先进的模型",
    "section": "进一步研究",
    "text": "进一步研究\n\n使用 fastai 文档构建一个函数，将图像裁剪为每个角落的正方形；然后实现一种 TTA 方法，该方法对中心裁剪和这四个裁剪的预测进行平均。有帮助吗？比 fastai 的 TTA 方法更好吗？\n在 arXiv 上找到 Mixup 论文并阅读。选择一两篇介绍 Mixup 变体的较新文章并阅读它们；然后尝试在您的问题上实现它们。\n找到使用 Mixup 训练 Imagenette 的脚本，并将其用作在自己项目上进行长时间训练的示例。执行它并查看是否有帮助。\n阅读侧边栏[“标签平滑，论文”](＃label_smoothing）；然后查看原始论文的相关部分，看看您是否能够理解。不要害怕寻求帮助！"
  },
  {
    "objectID": "Fastbook/translations/cn/19_learner.html",
    "href": "Fastbook/translations/cn/19_learner.html",
    "title": "第十九章：从头开始创建一个 fastai 学习器",
    "section": "",
    "text": "这最后一章（除了结论和在线章节）将会有所不同。它包含的代码比以前的章节要多得多，而叙述要少得多。我们将介绍新的 Python 关键字和库，而不进行讨论。这一章的目的是为您开展一项重要的研究项目。您将看到，我们将从头开始实现 fastai 和 PyTorch API 的许多关键部分，仅建立在我们在第十七章中开发的组件上！这里的关键目标是最终拥有自己的Learner类和一些回调函数，足以训练一个模型在 Imagenette 上，包括我们学习的每个关键技术的示例。在构建Learner的过程中，我们将创建我们自己的Module、Parameter和并行DataLoader的版本，这样您就会对 PyTorch 类的功能有一个很好的了解。\n本章末尾的问卷调查对于本章非常重要。这是我们将指导您探索许多有趣方向的地方，使用本章作为起点。我们建议您在计算机上跟着本章进行学习，并进行大量的实验、网络搜索和其他必要的工作，以了解发生了什么。在本书的其余部分，您已经积累了足够的技能和专业知识来做到这一点，所以我们相信您会做得很好！\n让我们开始手动收集一些数据。"
  },
  {
    "objectID": "Fastbook/translations/cn/19_learner.html#数据集",
    "href": "Fastbook/translations/cn/19_learner.html#数据集",
    "title": "第十九章：从头开始创建一个 fastai 学习器",
    "section": "数据集",
    "text": "数据集\n在 PyTorch 中，Dataset可以是任何支持索引（__getitem__）和len的东西：\nclass Dataset:\n    def __init__(self, fns): self.fns=fns\n    def __len__(self): return len(self.fns)\n    def __getitem__(self, i):\n        im = Image.open(self.fns[i]).resize((64,64)).convert('RGB')\n        y = v2i[self.fns[i].parent.name]\n        return tensor(im).float()/255, tensor(y)\n我们需要一个训练和验证文件名列表传递给Dataset.__init__：\ntrain_filt = L(o.parent.parent.name=='train' for o in files)\ntrain,valid = files[train_filt],files[~train_filt]\nlen(train),len(valid)\n(9469, 3925)\n现在我们可以尝试一下：\ntrain_ds,valid_ds = Dataset(train),Dataset(valid)\nx,y = train_ds[0]\nx.shape,y\n(torch.Size([64, 64, 3]), tensor(0))\nshow_image(x, title=lbls[y]);\n\n正如您所看到的，我们的数据集返回独立变量和因变量作为元组，这正是我们需要的。我们需要将这些整合成一个小批量。通常，可以使用torch.stack来完成这个任务，这就是我们将在这里使用的方法：\ndef collate(idxs, ds):\n    xb,yb = zip(*[ds[i] for i in idxs])\n    return torch.stack(xb),torch.stack(yb)\n这是一个包含两个项目的小批量，用于测试我们的collate：\nx,y = collate([1,2], train_ds)\nx.shape,y\n(torch.Size([2, 64, 64, 3]), tensor([0, 0]))\n现在我们有了数据集和一个整合函数，我们准备创建DataLoader。我们将在这里添加两个东西：一个可选的shuffle用于训练集，以及一个ProcessPoolExecutor来并行进行预处理。并行数据加载器非常重要，因为打开和解码 JPEG 图像是一个缓慢的过程。一个 CPU 核心不足以快速解码图像以使现代 GPU 保持繁忙。这是我们的DataLoader类：\nclass DataLoader:\n    def __init__(self, ds, bs=128, shuffle=False, n_workers=1):\n        self.ds,self.bs,self.shuffle,self.n_workers = ds,bs,shuffle,n_workers\n\n    def __len__(self): return (len(self.ds)-1)//self.bs+1\n\n    def __iter__(self):\n        idxs = L.range(self.ds)\n        if self.shuffle: idxs = idxs.shuffle()\n        chunks = [idxs[n:n+self.bs] for n in range(0, len(self.ds), self.bs)]\n        with ProcessPoolExecutor(self.n_workers) as ex:\n            yield from ex.map(collate, chunks, ds=self.ds)\n让我们尝试一下我们的训练和验证数据集：\nn_workers = min(16, defaults.cpus)\ntrain_dl = DataLoader(train_ds, bs=128, shuffle=True, n_workers=n_workers)\nvalid_dl = DataLoader(valid_ds, bs=256, shuffle=False, n_workers=n_workers)\nxb,yb = first(train_dl)\nxb.shape,yb.shape,len(train_dl)\n(torch.Size([128, 64, 64, 3]), torch.Size([128]), 74)\n这个数据加载器的速度不比 PyTorch 的慢，但它要简单得多。因此，如果您正在调试一个复杂的数据加载过程，不要害怕尝试手动操作，以帮助您准确地了解发生了什么。\n对于归一化，我们需要图像统计数据。通常，可以在一个训练小批量上计算这些数据，因为这里不需要精度：\nstats = [xb.mean((0,1,2)),xb.std((0,1,2))]\nstats\n[tensor([0.4544, 0.4453, 0.4141]), tensor([0.2812, 0.2766, 0.2981])]\n我们的Normalize类只需要存储这些统计数据并应用它们（要查看为什么需要to_device，请尝试将其注释掉，然后查看后面的笔记本中会发生什么）：\nclass Normalize:\n    def __init__(self, stats): self.stats=stats\n    def __call__(self, x):\n        if x.device != self.stats[0].device:\n            self.stats = to_device(self.stats, x.device)\n        return (x-self.stats[0])/self.stats[1]\n我们总是喜欢在笔记本中测试我们构建的一切，一旦我们构建它：\nnorm = Normalize(stats)\ndef tfm_x(x): return norm(x).permute((0,3,1,2))\nt = tfm_x(x)\nt.mean((0,2,3)),t.std((0,2,3))\n(tensor([0.3732, 0.4907, 0.5633]), tensor([1.0212, 1.0311, 1.0131]))\n这里tfm_x不仅仅应用Normalize，还将轴顺序从NHWC排列为NCHW（如果你需要提醒这些首字母缩写指的是什么，请参阅第十三章）。PIL 使用HWC轴顺序，我们不能在 PyTorch 中使用，因此需要这个permute。\n这就是我们模型的数据所需的全部内容。现在我们需要模型本身！"
  },
  {
    "objectID": "Fastbook/translations/cn/19_learner.html#简单的-cnn",
    "href": "Fastbook/translations/cn/19_learner.html#简单的-cnn",
    "title": "第十九章：从头开始创建一个 fastai 学习器",
    "section": "简单的 CNN",
    "text": "简单的 CNN\n正如我们所见，Sequential类使许多架构更容易实现，所以让我们创建一个：\nclass Sequential(Module):\n    def __init__(self, *layers):\n        super().__init__()\n        self.layers = layers\n        self.register_modules(*layers)\n\n    def forward(self, x):\n        for l in self.layers: x = l(x)\n        return x\n这里的forward方法只是依次调用每个层。请注意，我们必须使用我们在Module中定义的register_modules方法，否则layers的内容不会出现在parameters中。"
  },
  {
    "objectID": "Fastbook/translations/cn/19_learner.html#回调",
    "href": "Fastbook/translations/cn/19_learner.html#回调",
    "title": "第十九章：从头开始创建一个 fastai 学习器",
    "section": "回调",
    "text": "回调\n在Learner.__init__中，我们有\nfor cb in cbs: cb.learner = self\n换句话说，每个回调都知道它是在哪个学习器中使用的。这是至关重要的，否则回调无法从学习器中获取信息，或者更改学习器中的内容。因为从学习器中获取信息是如此常见，我们通过将Callback定义为GetAttr的子类，并将默认属性定义为learner，使其更容易：\nclass Callback(GetAttr): _default='learner'\nGetAttr是一个 fastai 类，为您实现了 Python 的标准__getattr__和__dir__方法，因此每当您尝试访问一个不存在的属性时，它会将请求传递给您定义为_default的内容。\n例如，我们希望在fit开始时自动将所有模型参数移动到 GPU。我们可以通过将before_fit定义为self.learner.model.cuda来实现这一点；然而，由于learner是默认属性，并且我们让SetupLearnerCB继承自Callback（它继承自GetAttr），我们可以去掉.learner，只需调用self.model.cuda：\nclass SetupLearnerCB(Callback):\n    def before_batch(self):\n        xb,yb = to_device(self.batch)\n        self.learner.batch = tfm_x(xb),yb\n\n    def before_fit(self): self.model.cuda()\n在SetupLearnerCB中，我们还通过调用to_device(self.batch)将每个小批量移动到 GPU（我们也可以使用更长的to_device(self.learner.batch)）。然而，请注意，在self.learner.batch = tfm_x(xb),yb这一行中，我们不能去掉.learner，因为这里我们是设置属性，而不是获取它。\n在尝试我们的Learner之前，让我们创建一个回调来跟踪和打印进度。否则，我们将无法真正知道它是否正常工作：\nclass TrackResults(Callback):\n    def before_epoch(self): self.accs,self.losses,self.ns = [],[],[]\n\n    def after_epoch(self):\n        n = sum(self.ns)\n        print(self.epoch, self.model.training,\n              sum(self.losses).item()/n, sum(self.accs).item()/n)\n\n    def after_batch(self):\n        xb,yb = self.batch\n        acc = (self.preds.argmax(dim=1)==yb).float().sum()\n        self.accs.append(acc)\n        n = len(xb)\n        self.losses.append(self.loss*n)\n        self.ns.append(n)\n现在我们准备好第一次使用我们的Learner了！\ncbs = [SetupLearnerCB(),TrackResults()]\nlearn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs)\nlearn.fit(1)\n0 True 2.1275552130636814 0.2314922378287042\n\n0 False 1.9942575636942674 0.2991082802547771\n惊人的是，我们可以用如此少的代码实现 fastai 的Learner中的所有关键思想！现在让我们添加一些学习率调度。"
  },
  {
    "objectID": "Fastbook/translations/cn/19_learner.html#调度学习率",
    "href": "Fastbook/translations/cn/19_learner.html#调度学习率",
    "title": "第十九章：从头开始创建一个 fastai 学习器",
    "section": "调度学习率",
    "text": "调度学习率\n如果我们想要获得良好的结果，我们将需要一个 LR finder 和 1cycle 训练。这两个都是退火回调，也就是说，它们在训练过程中逐渐改变超参数。这是LRFinder：\nclass LRFinder(Callback):\n    def before_fit(self):\n        self.losses,self.lrs = [],[]\n        self.learner.lr = 1e-6\n\n    def before_batch(self):\n        if not self.model.training: return\n        self.opt.lr *= 1.2\n\n    def after_batch(self):\n        if not self.model.training: return\n        if self.opt.lr&gt;10 or torch.isnan(self.loss): raise CancelFitException\n        self.losses.append(self.loss.item())\n        self.lrs.append(self.opt.lr)\n这展示了我们如何使用CancelFitException，它本身是一个空类，仅用于表示异常的类型。您可以在Learner中看到这个异常被捕获。（您应该自己添加和测试CancelBatchException，CancelEpochException等。）让我们尝试一下，将其添加到我们的回调列表中：\nlrfind = LRFinder()\nlearn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs+[lrfind])\nlearn.fit(2)\n0 True 2.6336045582954903 0.11014890695955222\n\n0 False 2.230653363853503 0.18318471337579617\n并查看结果：\nplt.plot(lrfind.lrs[:-2],lrfind.losses[:-2])\nplt.xscale('log')\n\n现在我们可以定义我们的OneCycle训练回调：\nclass OneCycle(Callback):\n    def __init__(self, base_lr): self.base_lr = base_lr\n    def before_fit(self): self.lrs = []\n\n    def before_batch(self):\n        if not self.model.training: return\n        n = len(self.dls.train)\n        bn = self.epoch*n + self.num\n        mn = self.n_epochs*n\n        pct = bn/mn\n        pct_start,div_start = 0.25,10\n        if pct&lt;pct_start:\n            pct /= pct_start\n            lr = (1-pct)*self.base_lr/div_start + pct*self.base_lr\n        else:\n            pct = (pct-pct_start)/(1-pct_start)\n            lr = (1-pct)*self.base_lr\n        self.opt.lr = lr\n        self.lrs.append(lr)\n我们将尝试一个 LR 为 0.1：\nonecyc = OneCycle(0.1)\nlearn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs+[onecyc])\n让我们适应一段时间，看看它的样子（我们不会在书中展示所有输出——在笔记本中尝试以查看结果）：\nlearn.fit(8)\n最后，我们将检查学习率是否遵循我们定义的调度（如您所见，我们这里没有使用余弦退火）：\nplt.plot(onecyc.lrs);"
  },
  {
    "objectID": "Fastbook/translations/cn/19_learner.html#进一步研究",
    "href": "Fastbook/translations/cn/19_learner.html#进一步研究",
    "title": "第十九章：从头开始创建一个 fastai 学习器",
    "section": "进一步研究",
    "text": "进一步研究\n\n从头开始编写resnet18（如有需要，请参考第十四章），并在本章中使用Learner进行训练。\n从头开始实现一个批归一化层，并在您的resnet18中使用它。\n为本章编写一个 Mixup 回调。\n向 SGD 添加动量。\n从 fastai（或任何其他库）中挑选几个您感兴趣的特性，并使用本章中创建的对象实现它们。\n选择一篇尚未在 fastai 或 PyTorch 中实现的研究论文，并使用本章中创建的对象进行实现。然后：\n\n将论文移植到 fastai。\n向 fastai 提交拉取请求，或创建自己的扩展模块并发布。\n\n提示：您可能会发现使用nbdev来创建和部署您的软件包很有帮助。"
  },
  {
    "objectID": "Fastbook/03_ethics.html",
    "href": "Fastbook/03_ethics.html",
    "title": "Data Ethics",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n[[chapter_ethics]]"
  },
  {
    "objectID": "Fastbook/03_ethics.html#key-examples-for-data-ethics",
    "href": "Fastbook/03_ethics.html#key-examples-for-data-ethics",
    "title": "Data Ethics",
    "section": "Key Examples for Data Ethics",
    "text": "Key Examples for Data Ethics\nWe are going to start with three specific examples that illustrate three common ethical issues in tech:\n\nRecourse processes—Arkansas’s buggy healthcare algorithms left patients stranded.\nFeedback loops—YouTube’s recommendation system helped unleash a conspiracy theory boom.\nBias—When a traditionally African-American name is searched for on Google, it displays ads for criminal background checks.\n\nIn fact, for every concept that we introduce in this chapter, we are going to provide at least one specific example. For each one, think about what you could have done in this situation, and what kinds of obstructions there might have been to you getting that done. How would you deal with them? What would you look out for?\n\nBugs and Recourse: Buggy Algorithm Used for Healthcare Benefits\nThe Verge investigated software used in over half of the US states to determine how much healthcare people receive, and documented their findings in the article “What Happens When an Algorithm Cuts Your Healthcare”. After implementation of the algorithm in Arkansas, hundreds of people (many with severe disabilities) had their healthcare drastically cut. For instance, Tammy Dobbs, a woman with cerebral palsy who needs an aid to help her to get out of bed, to go to the bathroom, to get food, and more, had her hours of help suddenly reduced by 20 hours a week. She couldn’t get any explanation for why her healthcare was cut. Eventually, a court case revealed that there were mistakes in the software implementation of the algorithm, negatively impacting people with diabetes or cerebral palsy. However, Dobbs and many other people reliant on these healthcare benefits live in fear that their benefits could again be cut suddenly and inexplicably.\n\n\nFeedback Loops: YouTube’s Recommendation System\nFeedback loops can occur when your model is controlling the next round of data you get. The data that is returned quickly becomes flawed by the software itself.\nFor instance, YouTube has 1.9 billion users, who watch over 1 billion hours of YouTube videos a day. Its recommendation algorithm (built by Google), which was designed to optimize watch time, is responsible for around 70% of the content that is watched. But there was a problem: it led to out-of-control feedback loops, leading the New York Times to run the headline “YouTube Unleashed a Conspiracy Theory Boom. Can It Be Contained?”. Ostensibly recommendation systems are predicting what content people will like, but they also have a lot of power in determining what content people even see.\n\n\nBias: Professor Latanya Sweeney “Arrested”\nDr. Latanya Sweeney is a professor at Harvard and director of the university’s data privacy lab. In the paper “Discrimination in Online Ad Delivery” (see &lt;&gt;) she describes her discovery that Googling her name resulted in advertisements saying “Latanya Sweeney, arrested?” even though she is the only known Latanya Sweeney and has never been arrested. However when she Googled other names, such as “Kirsten Lindquist,” she got more neutral ads, even though Kirsten Lindquist has been arrested three times.\n\nBeing a computer scientist, she studied this systematically, and looked at over 2000 names. She found a clear pattern where historically Black names received advertisements suggesting that the person had a criminal record, whereas, white names had more neutral advertisements.\nThis is an example of bias. It can make a big difference to people’s lives—for instance, if a job applicant is Googled it may appear that they have a criminal record when they do not.\n\n\nWhy Does This Matter?\nOne very natural reaction to considering these issues is: “So what? What’s that got to do with me? I’m a data scientist, not a politician. I’m not one of the senior executives at my company who make the decisions about what we do. I’m just trying to build the most predictive model I can.”\nThese are very reasonable questions. But we’re going to try to convince you that the answer is that everybody who is training models absolutely needs to consider how their models will be used, and consider how to best ensure that they are used as positively as possible. There are things you can do. And if you don’t do them, then things can go pretty badly.\nOne particularly hideous example of what happens when technologists focus on technology at all costs is the story of IBM and Nazi Germany. In 2001, a Swiss judge ruled that it was not unreasonable “to deduce that IBM’s technical assistance facilitated the tasks of the Nazis in the commission of their crimes against humanity, acts also involving accountancy and classification by IBM machines and utilized in the concentration camps themselves.”\nIBM, you see, supplied the Nazis with data tabulation products necessary to track the extermination of Jews and other groups on a massive scale. This was driven from the top of the company, with marketing to Hitler and his leadership team. Company President Thomas Watson personally approved the 1939 release of special IBM alphabetizing machines to help organize the deportation of Polish Jews. Pictured in &lt;&gt; is Adolf Hitler (far left) meeting with IBM CEO Tom Watson Sr. (second from left), shortly before Hitler awarded Watson a special “Service to the Reich” medal in 1937.\n\nBut this was not an isolated incident—the organization’s involvement was extensive. IBM and its subsidiaries provided regular training and maintenance onsite at the concentration camps: printing off cards, configuring machines, and repairing them as they broke frequently. IBM set up categorizations on its punch card system for the way that each person was killed, which group they were assigned to, and the logistical information necessary to track them through the vast Holocaust system. IBM’s code for Jews in the concentration camps was 8: some 6,000,000 were killed. Its code for Romanis was 12 (they were labeled by the Nazis as “asocials,” with over 300,000 killed in the Zigeunerlager, or “Gypsy camp”). General executions were coded as 4, death in the gas chambers as 6.\n\nOf course, the project managers and engineers and technicians involved were just living their ordinary lives. Caring for their families, going to the church on Sunday, doing their jobs the best they could. Following orders. The marketers were just doing what they could to meet their business development goals. As Edwin Black, author of IBM and the Holocaust (Dialog Press) observed: “To the blind technocrat, the means were more important than the ends. The destruction of the Jewish people became even less important because the invigorating nature of IBM’s technical achievement was only heightened by the fantastical profits to be made at a time when bread lines stretched across the world.”\nStep back for a moment and consider: How would you feel if you discovered that you had been part of a system that ended up hurting society? Would you be open to finding out? How can you help make sure this doesn’t happen? We have described the most extreme situation here, but there are many negative societal consequences linked to AI and machine learning being observed today, some of which we’ll describe in this chapter.\nIt’s not just a moral burden, either. Sometimes technologists pay very directly for their actions. For instance, the first person who was jailed as a result of the Volkswagen scandal, where the car company was revealed to have cheated on its diesel emissions tests, was not the manager that oversaw the project, or an executive at the helm of the company. It was one of the engineers, James Liang, who just did what he was told.\nOf course, it’s not all bad—if a project you are involved in turns out to make a huge positive impact on even one person, this is going to make you feel pretty great!\nOkay, so hopefully we have convinced you that you ought to care. But what should you do? As data scientists, we’re naturally inclined to focus on making our models better by optimizing some metric or other. But optimizing that metric may not actually lead to better outcomes. And even if it does help create better outcomes, it almost certainly won’t be the only thing that matters. Consider the pipeline of steps that occurs between the development of a model or an algorithm by a researcher or practitioner, and the point at which this work is actually used to make some decision. This entire pipeline needs to be considered as a whole if we’re to have a hope of getting the kinds of outcomes we want.\nNormally there is a very long chain from one end to the other. This is especially true if you are a researcher, where you might not even know if your research will ever get used for anything, or if you’re involved in data collection, which is even earlier in the pipeline. But no one is better placed to inform everyone involved in this chain about the capabilities, constraints, and details of your work than you are. Although there’s no “silver bullet” that can ensure your work is used the right way, by getting involved in the process, and asking the right questions, you can at the very least ensure that the right issues are being considered.\nSometimes, the right response to being asked to do a piece of work is to just say “no.” Often, however, the response we hear is, “If I don’t do it, someone else will.” But consider this: if you’ve been picked for the job, you’re the best person they’ve found to do it—so if you don’t do it, the best person isn’t working on that project. If the first five people they ask all say no too, even better!"
  },
  {
    "objectID": "Fastbook/03_ethics.html#integrating-machine-learning-with-product-design",
    "href": "Fastbook/03_ethics.html#integrating-machine-learning-with-product-design",
    "title": "Data Ethics",
    "section": "Integrating Machine Learning with Product Design",
    "text": "Integrating Machine Learning with Product Design\nPresumably the reason you’re doing this work is because you hope it will be used for something. Otherwise, you’re just wasting your time. So, let’s start with the assumption that your work will end up somewhere. Now, as you are collecting your data and developing your model, you are making lots of decisions. What level of aggregation will you store your data at? What loss function should you use? What validation and training sets should you use? Should you focus on simplicity of implementation, speed of inference, or accuracy of the model? How will your model handle out-of-domain data items? Can it be fine-tuned, or must it be retrained from scratch over time?\nThese are not just algorithm questions. They are data product design questions. But the product managers, executives, judges, journalists, doctors… whoever ends up developing and using the system of which your model is a part will not be well-placed to understand the decisions that you made, let alone change them.\nFor instance, two studies found that Amazon’s facial recognition software produced inaccurate and racially biased results. Amazon claimed that the researchers should have changed the default parameters, without explaining how this would have changed the biased results. Furthermore, it turned out that Amazon was not instructing police departments that used its software to do this either. There was, presumably, a big distance between the researchers that developed these algorithms and the Amazon documentation staff that wrote the guidelines provided to the police. A lack of tight integration led to serious problems for society at large, the police, and Amazon themselves. It turned out that their system erroneously matched 28 members of congress to criminal mugshots! (And the Congresspeople wrongly matched to criminal mugshots were disproportionately people of color, as seen in &lt;&gt;.)\n\nData scientists need to be part of a cross-disciplinary team. And researchers need to work closely with the kinds of people who will end up using their research. Better still is if the domain experts themselves have learned enough to be able to train and debug some models themselves—hopefully there are a few of you reading this book right now!\nThe modern workplace is a very specialized place. Everybody tends to have well-defined jobs to perform. Especially in large companies, it can be hard to know what all the pieces of the puzzle are. Sometimes companies even intentionally obscure the overall project goals that are being worked on, if they know that their employees are not going to like the answers. This is sometimes done by compartmentalising pieces as much as possible.\nIn other words, we’re not saying that any of this is easy. It’s hard. It’s really hard. We all have to do our best. And we have often seen that the people who do get involved in the higher-level context of these projects, and attempt to develop cross-disciplinary capabilities and teams, become some of the most important and well rewarded members of their organizations. It’s the kind of work that tends to be highly appreciated by senior executives, even if it is sometimes considered rather uncomfortable by middle management."
  },
  {
    "objectID": "Fastbook/03_ethics.html#topics-in-data-ethics",
    "href": "Fastbook/03_ethics.html#topics-in-data-ethics",
    "title": "Data Ethics",
    "section": "Topics in Data Ethics",
    "text": "Topics in Data Ethics\nData ethics is a big field, and we can’t cover everything. Instead, we’re going to pick a few topics that we think are particularly relevant:\n\nThe need for recourse and accountability\nFeedback loops\nBias\nDisinformation\n\nLet’s look at each in turn.\n\nRecourse and Accountability\nIn a complex system, it is easy for no one person to feel responsible for outcomes. While this is understandable, it does not lead to good results. In the earlier example of the Arkansas healthcare system in which a bug led to people with cerebral palsy losing access to needed care, the creator of the algorithm blamed government officials, and government officials blamed those who implemented the software. NYU professor Danah Boyd described this phenomenon: “Bureaucracy has often been used to shift or evade responsibility… Today’s algorithmic systems are extending bureaucracy.”\nAn additional reason why recourse is so necessary is because data often contains errors. Mechanisms for audits and error correction are crucial. A database of suspected gang members maintained by California law enforcement officials was found to be full of errors, including 42 babies who had been added to the database when they were less than 1 year old (28 of whom were marked as “admitting to being gang members”). In this case, there was no process in place for correcting mistakes or removing people once they’d been added. Another example is the US credit report system: in a large-scale study of credit reports by the Federal Trade Commission (FTC) in 2012, it was found that 26% of consumers had at least one mistake in their files, and 5% had errors that could be devastating. Yet, the process of getting such errors corrected is incredibly slow and opaque. When public radio reporter Bobby Allyn discovered that he was erroneously listed as having a firearms conviction, it took him “more than a dozen phone calls, the handiwork of a county court clerk and six weeks to solve the problem. And that was only after I contacted the company’s communications department as a journalist.”\nAs machine learning practitioners, we do not always think of it as our responsibility to understand how our algorithms end up being implemented in practice. But we need to.\n\n\nFeedback Loops\nWe explained in &lt;&gt; how an algorithm can interact with its environment to create a feedback loop, making predictions that reinforce actions taken in the real world, which lead to predictions even more pronounced in the same direction. As an example, let’s again consider YouTube’s recommendation system. A couple of years ago the Google team talked about how they had introduced reinforcement learning (closely related to deep learning, but where your loss function represents a result potentially a long time after an action occurs) to improve YouTube’s recommendation system. They described how they used an algorithm that made recommendations such that watch time would be optimized.\nHowever, human beings tend to be drawn to controversial content. This meant that videos about things like conspiracy theories started to get recommended more and more by the recommendation system. Furthermore, it turns out that the kinds of people that are interested in conspiracy theories are also people that watch a lot of online videos! So, they started to get drawn more and more toward YouTube. The increasing number of conspiracy theorists watching videos on YouTube resulted in the algorithm recommending more and more conspiracy theory and other extremist content, which resulted in more extremists watching videos on YouTube, and more people watching YouTube developing extremist views, which led to the algorithm recommending more extremist content… The system was spiraling out of control.\nAnd this phenomenon was not contained to this particular type of content. In June 2019 the New York Times published an article on YouTube’s recommendation system, titled “On YouTube’s Digital Playground, an Open Gate for Pedophiles”. The article started with this chilling story:\n\n: Christiane C. didn’t think anything of it when her 10-year-old daughter and a friend uploaded a video of themselves playing in a backyard pool… A few days later… the video had thousands of views. Before long, it had ticked up to 400,000… “I saw the video again and I got scared by the number of views,” Christiane said. She had reason to be. YouTube’s automated recommendation system… had begun showing the video to users who watched other videos of prepubescent, partially clothed children, a team of researchers has found.\n\n\n: On its own, each video might be perfectly innocent, a home movie, say, made by a child. Any revealing frames are fleeting and appear accidental. But, grouped together, their shared features become unmistakable.\n\nYouTube’s recommendation algorithm had begun curating playlists for pedophiles, picking out innocent home videos that happened to contain prepubescent, partially clothed children.\nNo one at Google planned to create a system that turned family videos into porn for pedophiles. So what happened?\nPart of the problem here is the centrality of metrics in driving a financially important system. When an algorithm has a metric to optimize, as you have seen, it will do everything it can to optimize that number. This tends to lead to all kinds of edge cases, and humans interacting with a system will search for, find, and exploit these edge cases and feedback loops for their advantage.\nThere are signs that this is exactly what has happened with YouTube’s recommendation system. The Guardian ran an article called “How an ex-YouTube Insider Investigated its Secret Algorithm” about Guillaume Chaslot, an ex-YouTube engineer who created AlgoTransparency, which tracks these issues. Chaslot published the chart in &lt;&gt;, following the release of Robert Mueller’s “Report on the Investigation Into Russian Interference in the 2016 Presidential Election.”\n\nRussia Today’s coverage of the Mueller report was an extreme outlier in terms of how many channels were recommending it. This suggests the possibility that Russia Today, a state-owned Russia media outlet, has been successful in gaming YouTube’s recommendation algorithm. Unfortunately, the lack of transparency of systems like this makes it hard to uncover the kinds of problems that we’re discussing.\nOne of our reviewers for this book, Aurélien Géron, led YouTube’s video classification team from 2013 to 2016 (well before the events discussed here). He pointed out that it’s not just feedback loops involving humans that are a problem. There can also be feedback loops without humans! He told us about an example from YouTube:\n\n: One important signal to classify the main topic of a video is the channel it comes from. For example, a video uploaded to a cooking channel is very likely to be a cooking video. But how do we know what topic a channel is about? Well… in part by looking at the topics of the videos it contains! Do you see the loop? For example, many videos have a description which indicates what camera was used to shoot the video. As a result, some of these videos might get classified as videos about “photography.” If a channel has such a misclassified video, it might be classified as a “photography” channel, making it even more likely for future videos on this channel to be wrongly classified as “photography.” This could even lead to runaway virus-like classifications! One way to break this feedback loop is to classify videos with and without the channel signal. Then when classifying the channels, you can only use the classes obtained without the channel signal. This way, the feedback loop is broken.\n\nThere are positive examples of people and organizations attempting to combat these problems. Evan Estola, lead machine learning engineer at Meetup, discussed the example of men expressing more interest than women in tech meetups. taking gender into account could therefore cause Meetup’s algorithm to recommend fewer tech meetups to women, and as a result, fewer women would find out about and attend tech meetups, which could cause the algorithm to suggest even fewer tech meetups to women, and so on in a self-reinforcing feedback loop. So, Evan and his team made the ethical decision for their recommendation algorithm to not create such a feedback loop, by explicitly not using gender for that part of their model. It is encouraging to see a company not just unthinkingly optimize a metric, but consider its impact. According to Evan, “You need to decide which feature not to use in your algorithm… the most optimal algorithm is perhaps not the best one to launch into production.”\nWhile Meetup chose to avoid such an outcome, Facebook provides an example of allowing a runaway feedback loop to run wild. Like YouTube, it tends to radicalize users interested in one conspiracy theory by introducing them to more. As Renee DiResta, a researcher on proliferation of disinformation, writes:\n\n: Once people join a single conspiracy-minded [Facebook] group, they are algorithmically routed to a plethora of others. Join an anti-vaccine group, and your suggestions will include anti-GMO, chemtrail watch, flat Earther (yes, really), and “curing cancer naturally groups. Rather than pulling a user out of the rabbit hole, the recommendation engine pushes them further in.”\n\nIt is extremely important to keep in mind that this kind of behavior can happen, and to either anticipate a feedback loop or take positive action to break it when you see the first signs of it in your own projects. Another thing to keep in mind is bias, which, as we discussed briefly in the previous chapter, can interact with feedback loops in very troublesome ways.\n\n\nBias\nDiscussions of bias online tend to get pretty confusing pretty fast. The word “bias” means so many different things. Statisticians often think when data ethicists are talking about bias that they’re talking about the statistical definition of the term bias. But they’re not. And they’re certainly not talking about the biases that appear in the weights and biases which are the parameters of your model!\nWhat they’re talking about is the social science concept of bias. In “A Framework for Understanding Unintended Consequences of Machine Learning” MIT’s Harini Suresh and John Guttag describe six types of bias in machine learning, summarized in &lt;&gt; from their paper.\n\nWe’ll discuss four of these types of bias, those that we’ve found most helpful in our own work (see the paper for details on the others).\n\nHistorical bias\nHistorical bias comes from the fact that people are biased, processes are biased, and society is biased. Suresh and Guttag say: “Historical bias is a fundamental, structural issue with the first step of the data generation process and can exist even given perfect sampling and feature selection.”\nFor instance, here are a few examples of historical race bias in the US, from the New York Times article “Racial Bias, Even When We Have Good Intentions” by the University of Chicago’s Sendhil Mullainathan:\n\nWhen doctors were shown identical files, they were much less likely to recommend cardiac catheterization (a helpful procedure) to Black patients.\nWhen bargaining for a used car, Black people were offered initial prices $700 higher and received far smaller concessions.\nResponding to apartment rental ads on Craigslist with a Black name elicited fewer responses than with a white name.\nAn all-white jury was 16 percentage points more likely to convict a Black defendant than a white one, but when a jury had one Black member it convicted both at the same rate.\n\nThe COMPAS algorithm, widely used for sentencing and bail decisions in the US, is an example of an important algorithm that, when tested by ProPublica, showed clear racial bias in practice (&lt;&gt;).\n\nAny dataset involving humans can have this kind of bias: medical data, sales data, housing data, political data, and so on. Because underlying bias is so pervasive, bias in datasets is very pervasive. Racial bias even turns up in computer vision, as shown in the example of autocategorized photos shared on Twitter by a Google Photos user shown in &lt;&gt;.\n\nYes, that is showing what you think it is: Google Photos classified a Black user’s photo with their friend as “gorillas”! This algorithmic misstep got a lot of attention in the media. “We’re appalled and genuinely sorry that this happened,” a company spokeswoman said. “There is still clearly a lot of work to do with automatic image labeling, and we’re looking at how we can prevent these types of mistakes from happening in the future.”\nUnfortunately, fixing problems in machine learning systems when the input data has problems is hard. Google’s first attempt didn’t inspire confidence, as coverage by The Guardian suggested (&lt;&gt;).\n\nThese kinds of problems are certainly not limited to just Google. MIT researchers studied the most popular online computer vision APIs to see how accurate they were. But they didn’t just calculate a single accuracy number—instead, they looked at the accuracy across four different groups, as illustrated in &lt;&gt;.\n\nIBM’s system, for instance, had a 34.7% error rate for darker females, versus 0.3% for lighter males—over 100 times more errors! Some people incorrectly reacted to these experiments by claiming that the difference was simply because darker skin is harder for computers to recognize. However, what actually happened was that, after the negative publicity that this result created, all of the companies in question dramatically improved their models for darker skin, such that one year later they were nearly as good as for lighter skin. So what this actually showed is that the developers failed to utilize datasets containing enough darker faces, or test their product with darker faces.\nOne of the MIT researchers, Joy Buolamwini, warned: “We have entered the age of automation overconfident yet underprepared. If we fail to make ethical and inclusive artificial intelligence, we risk losing gains made in civil rights and gender equity under the guise of machine neutrality.”\nPart of the issue appears to be a systematic imbalance in the makeup of popular datasets used for training models. The abstract to the paper “No Classification Without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World” by Shreya Shankar et al. states, “We analyze two large, publicly available image data sets to assess geo-diversity and find that these data sets appear to exhibit an observable amerocentric and eurocentric representation bias. Further, we analyze classifiers trained on these data sets to assess the impact of these training distributions and find strong differences in the relative performance on images from different locales.” &lt;&gt; shows one of the charts from the paper, showing the geographic makeup of what was, at the time (and still are, as this book is being written) the two most important image datasets for training models.\n\nThe vast majority of the images are from the United States and other Western countries, leading to models trained on ImageNet performing worse on scenes from other countries and cultures. For instance, research found that such models are worse at identifying household items (such as soap, spices, sofas, or beds) from lower-income countries. &lt;&gt; shows an image from the paper, “Does Object Recognition Work for Everyone?” by Terrance DeVries et al. of Facebook AI Research that illustrates this point.\n\nIn this example, we can see that the lower-income soap example is a very long way away from being accurate, with every commercial image recognition service predicting “food” as the most likely answer!\nAs we will discuss shortly, in addition, the vast majority of AI researchers and developers are young white men. Most projects that we have seen do most user testing using friends and families of the immediate product development group. Given this, the kinds of problems we just discussed should not be surprising.\nSimilar historical bias is found in the texts used as data for natural language processing models. This crops up in downstream machine learning tasks in many ways. For instance, it was widely reported that until last year Google Translate showed systematic bias in how it translated the Turkish gender-neutral pronoun “o” into English: when applied to jobs which are often associated with males it used “he,” and when applied to jobs which are often associated with females it used “she” (&lt;&gt;).\n\nWe also see this kind of bias in online advertisements. For instance, a study in 2019 by Muhammad Ali et al. found that even when the person placing the ad does not intentionally discriminate, Facebook will show ads to very different audiences based on race and gender. Housing ads with the same text, but picture either a white or a Black family, were shown to racially different audiences.\n\n\nMeasurement bias\nIn the paper “Does Machine Learning Automate Moral Hazard and Error” in American Economic Review, Sendhil Mullainathan and Ziad Obermeyer look at a model that tries to answer the question: using historical electronic health record (EHR) data, what factors are most predictive of stroke? These are the top predictors from the model:\n\nPrior stroke\nCardiovascular disease\nAccidental injury\nBenign breast lump\nColonoscopy\nSinusitis\n\nHowever, only the top two have anything to do with a stroke! Based on what we’ve studied so far, you can probably guess why. We haven’t really measured stroke, which occurs when a region of the brain is denied oxygen due to an interruption in the blood supply. What we’ve measured is who had symptoms, went to a doctor, got the appropriate tests, and received a diagnosis of stroke. Actually having a stroke is not the only thing correlated with this complete list—it’s also correlated with being the kind of person who actually goes to the doctor (which is influenced by who has access to healthcare, can afford their co-pay, doesn’t experience racial or gender-based medical discrimination, and more)! If you are likely to go to the doctor for an accidental injury, then you are likely to also go the doctor when you are having a stroke.\nThis is an example of measurement bias. It occurs when our models make mistakes because we are measuring the wrong thing, or measuring it in the wrong way, or incorporating that measurement into the model inappropriately.\n\n\nAggregation bias\nAggregation bias occurs when models do not aggregate data in a way that incorporates all of the appropriate factors, or when a model does not include the necessary interaction terms, nonlinearities, or so forth. This can particularly occur in medical settings. For instance, the way diabetes is treated is often based on simple univariate statistics and studies involving small groups of heterogeneous people. Analysis of results is often done in a way that does not take account of different ethnicities or genders. However, it turns out that diabetes patients have different complications across ethnicities, and HbA1c levels (widely used to diagnose and monitor diabetes) differ in complex ways across ethnicities and genders. This can result in people being misdiagnosed or incorrectly treated because medical decisions are based on a model that does not include these important variables and interactions.\n\n\nRepresentation bias\nThe abstract of the paper “Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting” by Maria De-Arteaga et al. notes that there is gender imbalance in occupations (e.g., females are more likely to be nurses, and males are more likely to be pastors), and says that: “differences in true positive rates between genders are correlated with existing gender imbalances in occupations, which may compound these imbalances.”\nIn other words, the researchers noticed that models predicting occupation did not only reflect the actual gender imbalance in the underlying population, but actually amplified it! This type of representation bias is quite common, particularly for simple models. When there is some clear, easy-to-see underlying relationship, a simple model will often simply assume that this relationship holds all the time. As &lt;&gt; from the paper shows, for occupations that had a higher percentage of females, the model tended to overestimate the prevalence of that occupation.\n\nFor example, in the training dataset 14.6% of surgeons were women, yet in the model predictions only 11.6% of the true positives were women. The model is thus amplifying the bias existing in the training set.\nNow that we’ve seen that those biases exist, what can we do to mitigate them?\n\n\n\nAddressing different types of bias\nDifferent types of bias require different approaches for mitigation. While gathering a more diverse dataset can address representation bias, this would not help with historical bias or measurement bias. All datasets contain bias. There is no such thing as a completely debiased dataset. Many researchers in the field have been converging on a set of proposals to enable better documentation of the decisions, context, and specifics about how and why a particular dataset was created, what scenarios it is appropriate to use in, and what the limitations are. This way, those using a particular dataset will not be caught off guard by its biases and limitations.\nWe often hear the question—“Humans are biased, so does algorithmic bias even matter?” This comes up so often, there must be some reasoning that makes sense to the people that ask it, but it doesn’t seem very logically sound to us! Independently of whether this is logically sound, it’s important to realize that algorithms (particularly machine learning algorithms!) and people are different. Consider these points about machine learning algorithms:\n\nMachine learning can create feedback loops:: Small amounts of bias can rapidly increase exponentially due to feedback loops.\nMachine learning can amplify bias:: Human bias can lead to larger amounts of machine learning bias.\nAlgorithms & humans are used differently:: Human decision makers and algorithmic decision makers are not used in a plug-and-play interchangeable way in practice.\nTechnology is power:: And with that comes responsibility.\n\nAs the Arkansas healthcare example showed, machine learning is often implemented in practice not because it leads to better outcomes, but because it is cheaper and more efficient. Cathy O’Neill, in her book Weapons of Math Destruction (Crown), described the pattern of how the privileged are processed by people, whereas the poor are processed by algorithms. This is just one of a number of ways that algorithms are used differently than human decision makers. Others include:\n\nPeople are more likely to assume algorithms are objective or error-free (even if they’re given the option of a human override).\nAlgorithms are more likely to be implemented with no appeals process in place.\nAlgorithms are often used at scale.\nAlgorithmic systems are cheap.\n\nEven in the absence of bias, algorithms (and deep learning especially, since it is such an effective and scalable algorithm) can lead to negative societal problems, such as when used for disinformation.\n\n\nDisinformation\nDisinformation has a history stretching back hundreds or even thousands of years. It is not necessarily about getting someone to believe something false, but rather often used to sow disharmony and uncertainty, and to get people to give up on seeking the truth. Receiving conflicting accounts can lead people to assume that they can never know whom or what to trust.\nSome people think disinformation is primarily about false information or fake news, but in reality, disinformation can often contain seeds of truth, or half-truths taken out of context. Ladislav Bittman was an intelligence officer in the USSR who later defected to the US and wrote some books in the 1970s and 1980s on the role of disinformation in Soviet propaganda operations. In The KGB and Soviet Disinformation (Pergamon) he wrote, “Most campaigns are a carefully designed mixture of facts, half-truths, exaggerations, and deliberate lies.”\nIn the US this has hit close to home in recent years, with the FBI detailing a massive disinformation campaign linked to Russia in the 2016 election. Understanding the disinformation that was used in this campaign is very educational. For instance, the FBI found that the Russian disinformation campaign often organized two separate fake “grass roots” protests, one for each side of an issue, and got them to protest at the same time! The Houston Chronicle reported on one of these odd events (&lt;&gt;).\n\n: A group that called itself the “Heart of Texas” had organized it on social media—a protest, they said, against the “Islamization” of Texas. On one side of Travis Street, I found about 10 protesters. On the other side, I found around 50 counterprotesters. But I couldn’t find the rally organizers. No “Heart of Texas.” I thought that was odd, and mentioned it in the article: What kind of group is a no-show at its own event? Now I know why. Apparently, the rally’s organizers were in Saint Petersburg, Russia, at the time. “Heart of Texas” is one of the internet troll groups cited in Special Prosecutor Robert Mueller’s recent indictment of Russians attempting to tamper with the U.S. presidential election.\n\n\nDisinformation often involves coordinated campaigns of inauthentic behavior. For instance, fraudulent accounts may try to make it seem like many people hold a particular viewpoint. While most of us like to think of ourselves as independent-minded, in reality we evolved to be influenced by others in our in-group, and in opposition to those in our out-group. Online discussions can influence our viewpoints, or alter the range of what we consider acceptable viewpoints. Humans are social animals, and as social animals we are extremely influenced by the people around us. Increasingly, radicalization occurs in online environments; influence is coming from people in the virtual space of online forums and social networks.\nDisinformation through autogenerated text is a particularly significant issue, due to the greatly increased capability provided by deep learning. We discuss this issue in depth when we delve into creating language models, in &lt;&gt;.\nOne proposed approach is to develop some form of digital signature, to implement it in a seamless way, and to create norms that we should only trust content that has been verified. The head of the Allen Institute on AI, Oren Etzioni, wrote such a proposal in an article titled “How Will We Prevent AI-Based Forgery?”: “AI is poised to make high-fidelity forgery inexpensive and automated, leading to potentially disastrous consequences for democracy, security, and society. The specter of AI forgery means that we need to act to make digital signatures de rigueur as a means of authentication of digital content.”\nWhilst we can’t hope to discuss all the ethical issues that deep learning, and algorithms more generally, brings up, hopefully this brief introduction has been a useful starting point you can build on. We’ll now move on to the questions of how to identify ethical issues, and what to do about them."
  },
  {
    "objectID": "Fastbook/03_ethics.html#identifying-and-addressing-ethical-issues",
    "href": "Fastbook/03_ethics.html#identifying-and-addressing-ethical-issues",
    "title": "Data Ethics",
    "section": "Identifying and Addressing Ethical Issues",
    "text": "Identifying and Addressing Ethical Issues\nMistakes happen. Finding out about them, and dealing with them, needs to be part of the design of any system that includes machine learning (and many other systems too). The issues raised within data ethics are often complex and interdisciplinary, but it is crucial that we work to address them.\nSo what can we do? This is a big topic, but a few steps towards addressing ethical issues are:\n\nAnalyze a project you are working on.\nImplement processes at your company to find and address ethical risks.\nSupport good policy.\nIncrease diversity.\n\nLet’s walk through each of these steps, starting with analyzing a project you are working on.\n\nAnalyze a Project You Are Working On\nIt’s easy to miss important issues when considering ethical implications of your work. One thing that helps enormously is simply asking the right questions. Rachel Thomas recommends considering the following questions throughout the development of a data project:\n\nShould we even be doing this?\nWhat bias is in the data?\nCan the code and data be audited?\nWhat are the error rates for different sub-groups?\nWhat is the accuracy of a simple rule-based alternative?\nWhat processes are in place to handle appeals or mistakes?\nHow diverse is the team that built it?\n\nThese questions may be able to help you identify outstanding issues, and possible alternatives that are easier to understand and control. In addition to asking the right questions, it’s also important to consider practices and processes to implement.\nOne thing to consider at this stage is what data you are collecting and storing. Data often ends up being used for different purposes than what it was originally collected for. For instance, IBM began selling to Nazi Germany well before the Holocaust, including helping with Germany’s 1933 census conducted by Adolf Hitler, which was effective at identifying far more Jewish people than had previously been recognized in Germany. Similarly, US census data was used to round up Japanese-Americans (who were US citizens) for internment during World War II. It is important to recognize how data and images collected can be weaponized later. Columbia professor Tim Wu wrote that “You must assume that any personal data that Facebook or Android keeps are data that governments around the world will try to get or that thieves will try to steal.”\n\n\nProcesses to Implement\nThe Markkula Center has released An Ethical Toolkit for Engineering/Design Practice that includes some concrete practices to implement at your company, including regularly scheduled sweeps to proactively search for ethical risks (in a manner similar to cybersecurity penetration testing), expanding the ethical circle to include the perspectives of a variety of stakeholders, and considering the terrible people (how could bad actors abuse, steal, misinterpret, hack, destroy, or weaponize what you are building?).\nEven if you don’t have a diverse team, you can still try to pro-actively include the perspectives of a wider group, considering questions such as these (provided by the Markkula Center):\n\nWhose interests, desires, skills, experiences, and values have we simply assumed, rather than actually consulted?\nWho are all the stakeholders who will be directly affected by our product? How have their interests been protected? How do we know what their interests really are—have we asked?\nWho/which groups and individuals will be indirectly affected in significant ways?\nWho might use this product that we didn’t expect to use it, or for purposes we didn’t initially intend?\n\n\nEthical lenses\nAnother useful resource from the Markkula Center is its Conceptual Frameworks in Technology and Engineering Practice. This considers how different foundational ethical lenses can help identify concrete issues, and lays out the following approaches and key questions:\n\nThe rights approach:: Which option best respects the rights of all who have a stake?\nThe justice approach:: Which option treats people equally or proportionately?\nThe utilitarian approach:: Which option will produce the most good and do the least harm?\nThe common good approach:: Which option best serves the community as a whole, not just some members?\nThe virtue approach:: Which option leads me to act as the sort of person I want to be?\n\nMarkkula’s recommendations include a deeper dive into each of these perspectives, including looking at a project through the lenses of its consequences:\n\nWho will be directly affected by this project? Who will be indirectly affected?\nWill the effects in aggregate likely create more good than harm, and what types of good and harm?\nAre we thinking about all relevant types of harm/benefit (psychological, political, environmental, moral, cognitive, emotional, institutional, cultural)?\nHow might future generations be affected by this project?\nDo the risks of harm from this project fall disproportionately on the least powerful in society? Will the benefits go disproportionately to the well-off?\nHave we adequately considered “dual-use”?\n\nThe alternative lens to this is the deontological perspective, which focuses on basic concepts of right and wrong:\n\nWhat rights of others and duties to others must we respect?\nHow might the dignity and autonomy of each stakeholder be impacted by this project?\nWhat considerations of trust and of justice are relevant to this design/project?\nDoes this project involve any conflicting moral duties to others, or conflicting stakeholder rights? How can we prioritize these?\n\nOne of the best ways to help come up with complete and thoughtful answers to questions like these is to ensure that the people asking the questions are diverse.\n\n\n\nThe Power of Diversity\nCurrently, less than 12% of AI researchers are women, according to a study from Element AI. The statistics are similarly dire when it comes to race and age. When everybody on a team has similar backgrounds, they are likely to have similar blindspots around ethical risks. The Harvard Business Review (HBR) has published a number of studies showing many benefits of diverse teams, including:\n\n“How Diversity Can Drive Innovation”\n“Teams Solve Problems Faster When They’re More Cognitively Diverse”\n“Why Diverse Teams Are Smarter”, and\n“Defend Your Research: What Makes a Team Smarter? More Women”\n\nDiversity can lead to problems being identified earlier, and a wider range of solutions being considered. For instance, Tracy Chou was an early engineer at Quora. She wrote of her experiences, describing how she advocated internally for adding a feature that would allow trolls and other bad actors to be blocked. Chou recounts, “I was eager to work on the feature because I personally felt antagonized and abused on the site (gender isn’t an unlikely reason as to why)… But if I hadn’t had that personal perspective, it’s possible that the Quora team wouldn’t have prioritized building a block button so early in its existence.” Harassment often drives people from marginalized groups off online platforms, so this functionality has been important for maintaining the health of Quora’s community.\nA crucial aspect to understand is that women leave the tech industry at over twice the rate that men do, according to the Harvard Business Review (41% of women working in tech leave, compared to 17% of men). An analysis of over 200 books, white papers, and articles found that the reason they leave is that “they’re treated unfairly; underpaid, less likely to be fast-tracked than their male colleagues, and unable to advance.”\nStudies have confirmed a number of the factors that make it harder for women to advance in the workplace. Women receive more vague feedback and personality criticism in performance evaluations, whereas men receive actionable advice tied to business outcomes (which is more useful). Women frequently experience being excluded from more creative and innovative roles, and not receiving high-visibility “stretch” assignments that are helpful in getting promoted. One study found that men’s voices are perceived as more persuasive, fact-based, and logical than women’s voices, even when reading identical scripts.\nReceiving mentorship has been statistically shown to help men advance, but not women. The reason behind this is that when women receive mentorship, it’s advice on how they should change and gain more self-knowledge. When men receive mentorship, it’s public endorsement of their authority. Guess which is more useful in getting promoted?\nAs long as qualified women keep dropping out of tech, teaching more girls to code will not solve the diversity issues plaguing the field. Diversity initiatives often end up focusing primarily on white women, even though women of color face many additional barriers. In interviews with 60 women of color who work in STEM research, 100% had experienced discrimination.\nThe hiring process is particularly broken in tech. One study indicative of the dysfunction comes from Triplebyte, a company that helps place software engineers in companies, conducting a standardized technical interview as part of this process. They have a fascinating dataset: the results of how over 300 engineers did on their exam, coupled with the results of how those engineers did during the interview process for a variety of companies. The number one finding from Triplebyte’s research is that “the types of programmers that each company looks for often have little to do with what the company needs or does. Rather, they reflect company culture and the backgrounds of the founders.”\nThis is a challenge for those trying to break into the world of deep learning, since most companies’ deep learning groups today were founded by academics. These groups tend to look for people “like them”—that is, people that can solve complex math problems and understand dense jargon. They don’t always know how to spot people who are actually good at solving real problems using deep learning.\nThis leaves a big opportunity for companies that are ready to look beyond status and pedigree, and focus on results!\n\n\nFairness, Accountability, and Transparency\nThe professional society for computer scientists, the ACM, runs a data ethics conference called the Conference on Fairness, Accountability, and Transparency. “Fairness, Accountability, and Transparency” which used to go under the acronym FAT but now uses to the less objectionable FAccT. Microsoft has a group focused on “Fairness, Accountability, Transparency, and Ethics” (FATE). In this section, we’ll use “FAccT” to refer to the concepts of Fairness, Accountability, and Transparency.\nFAccT is another lens that you may find useful in considering ethical issues. One useful resource for this is the free online book Fairness and Machine Learning: Limitations and Opportunities by Solon Barocas, Moritz Hardt, and Arvind Narayanan, which “gives a perspective on machine learning that treats fairness as a central concern rather than an afterthought.” It also warns, however, that it “is intentionally narrow in scope… A narrow framing of machine learning ethics might be tempting to technologists and businesses as a way to focus on technical interventions while sidestepping deeper questions about power and accountability. We caution against this temptation.” Rather than provide an overview of the FAccT approach to ethics (which is better done in books such as that one), our focus here will be on the limitations of this kind of narrow framing.\nOne great way to consider whether an ethical lens is complete is to try to come up with an example where the lens and our own ethical intuitions give diverging results. Os Keyes, Jevan Hutson, and Meredith Durbin explored this in a graphic way in their paper “A Mulching Proposal: Analysing and Improving an Algorithmic System for Turning the Elderly into High-Nutrient Slurry”. The paper’s abstract says:\n\n: The ethical implications of algorithmic systems have been much discussed in both HCI and the broader community of those interested in technology design, development and policy. In this paper, we explore the application of one prominent ethical framework - Fairness, Accountability, and Transparency - to a proposed algorithm that resolves various societal issues around food security and population aging. Using various standardised forms of algorithmic audit and evaluation, we drastically increase the algorithm’s adherence to the FAT framework, resulting in a more ethical and beneficent system. We discuss how this might serve as a guide to other researchers or practitioners looking to ensure better ethical outcomes from algorithmic systems in their line of work.\n\nIn this paper, the rather controversial proposal (“Turning the Elderly into High-Nutrient Slurry”) and the results (“drastically increase the algorithm’s adherence to the FAT framework, resulting in a more ethical and beneficent system”) are at odds… to say the least!\nIn philosophy, and especially philosophy of ethics, this is one of the most effective tools: first, come up with a process, definition, set of questions, etc., which is designed to resolve some problem. Then try to come up with an example where that apparent solution results in a proposal that no one would consider acceptable. This can then lead to a further refinement of the solution.\nSo far, we’ve focused on things that you and your organization can do. But sometimes individual or organizational action is not enough. Sometimes, governments also need to consider policy implications."
  },
  {
    "objectID": "Fastbook/03_ethics.html#role-of-policy",
    "href": "Fastbook/03_ethics.html#role-of-policy",
    "title": "Data Ethics",
    "section": "Role of Policy",
    "text": "Role of Policy\nWe often talk to people who are eager for technical or design fixes to be a full solution to the kinds of problems that we’ve been discussing; for instance, a technical approach to debias data, or design guidelines for making technology less addictive. While such measures can be useful, they will not be sufficient to address the underlying problems that have led to our current state. For example, as long as it is incredibly profitable to create addictive technology, companies will continue to do so, regardless of whether this has the side effect of promoting conspiracy theories and polluting our information ecosystem. While individual designers may try to tweak product designs, we will not see substantial changes until the underlying profit incentives change.\n\nThe Effectiveness of Regulation\nTo look at what can cause companies to take concrete action, consider the following two examples of how Facebook has behaved. In 2018, a UN investigation found that Facebook had played a “determining role” in the ongoing genocide of the Rohingya, an ethnic minority in Mynamar described by UN Secretary-General Antonio Guterres as “one of, if not the, most discriminated people in the world.” Local activists had been warning Facebook executives that their platform was being used to spread hate speech and incite violence since as early as 2013. In 2015, they were warned that Facebook could play the same role in Myanmar that the radio broadcasts played during the Rwandan genocide (where a million people were killed). Yet, by the end of 2015, Facebook only employed four contractors that spoke Burmese. As one person close to the matter said, “That’s not 20/20 hindsight. The scale of this problem was significant and it was already apparent.” Zuckerberg promised during the congressional hearings to hire “dozens” to address the genocide in Myanmar (in 2018, years after the genocide had begun, including the destruction by fire of at least 288 villages in northern Rakhine state after August 2017).\nThis stands in stark contrast to Facebook quickly hiring 1,200 people in Germany to try to avoid expensive penalties (of up to 50 million euros) under a new German law against hate speech. Clearly, in this case, Facebook was more reactive to the threat of a financial penalty than to the systematic destruction of an ethnic minority.\nIn an article on privacy issues, Maciej Ceglowski draws parallels with the environmental movement:\n\n: This regulatory project has been so successful in the First World that we risk forgetting what life was like before it. Choking smog of the kind that today kills thousands in Jakarta and Delhi was https://en.wikipedia.org/wiki/Pea_soup_fog[once emblematic of London]. The Cuyahoga River in Ohio used to http://www.ohiohistorycentral.org/w/Cuyahoga_River_Fire[reliably catch fire]. In a particularly horrific example of unforeseen consequences, tetraethyl lead added to gasoline https://en.wikipedia.org/wiki/Lead%E2%80%93crime_hypothesis[raised violent crime rates] worldwide for fifty years. None of these harms could have been fixed by telling people to vote with their wallet, or carefully review the environmental policies of every company they gave their business to, or to stop using the technologies in question. It took coordinated, and sometimes highly technical, regulation across jurisdictional boundaries to fix them. In some cases, like the https://en.wikipedia.org/wiki/Montreal_Protocol[ban on commercial refrigerants] that depleted the ozone layer, that regulation required a worldwide consensus. We’re at the point where we need a similar shift in perspective in our privacy law.\n\n\n\nRights and Policy\nClean air and clean drinking water are public goods which are nearly impossible to protect through individual market decisions, but rather require coordinated regulatory action. Similarly, many of the harms resulting from unintended consequences of misuses of technology involve public goods, such as a polluted information environment or deteriorated ambient privacy. Too often privacy is framed as an individual right, yet there are societal impacts to widespread surveillance (which would still be the case even if it was possible for a few individuals to opt out).\nMany of the issues we are seeing in tech are actually human rights issues, such as when a biased algorithm recommends that Black defendants have longer prison sentences, when particular job ads are only shown to young people, or when police use facial recognition to identify protesters. The appropriate venue to address human rights issues is typically through the law.\nWe need both regulatory and legal changes, and the ethical behavior of individuals. Individual behavior change can’t address misaligned profit incentives, externalities (where corporations reap large profits while offloading their costs and harms to the broader society), or systemic failures. However, the law will never cover all edge cases, and it is important that individual software developers and data scientists are equipped to make ethical decisions in practice.\n\n\nCars: A Historical Precedent\nThe problems we are facing are complex, and there are no simple solutions. This can be discouraging, but we find hope in considering other large challenges that people have tackled throughout history. One example is the movement to increase car safety, covered as a case study in “Datasheets for Datasets” by Timnit Gebru et al. and in the design podcast 99% Invisible. Early cars had no seatbelts, metal knobs on the dashboard that could lodge in people’s skulls during a crash, regular plate glass windows that shattered in dangerous ways, and non-collapsible steering columns that impaled drivers. However, car companies were incredibly resistant to even discussing the idea of safety as something they could help address, and the widespread belief was that cars are just the way they are, and that it was the people using them who caused problems.\nIt took consumer safety activists and advocates decades of work to even change the national conversation to consider that perhaps car companies had some responsibility which should be addressed through regulation. When the collapsible steering column was invented, it was not implemented for several years as there was no financial incentive to do so. Major car company General Motors hired private detectives to try to dig up dirt on consumer safety advocate Ralph Nader. The requirement of seatbelts, crash test dummies, and collapsible steering columns were major victories. It was only in 2011 that car companies were required to start using crash test dummies that would represent the average woman, and not just average men’s bodies; prior to this, women were 40% more likely to be injured in a car crash of the same impact compared to a man. This is a vivid example of the ways that bias, policy, and technology have important consequences."
  },
  {
    "objectID": "Fastbook/03_ethics.html#conclusion",
    "href": "Fastbook/03_ethics.html#conclusion",
    "title": "Data Ethics",
    "section": "Conclusion",
    "text": "Conclusion\nComing from a background of working with binary logic, the lack of clear answers in ethics can be frustrating at first. Yet, the implications of how our work impacts the world, including unintended consequences and the work becoming weaponized by bad actors, are some of the most important questions we can (and should!) consider. Even though there aren’t any easy answers, there are definite pitfalls to avoid and practices to follow to move toward more ethical behavior.\nMany people (including us!) are looking for more satisfying, solid answers about how to address harmful impacts of technology. However, given the complex, far-reaching, and interdisciplinary nature of the problems we are facing, there are no simple solutions. Julia Angwin, former senior reporter at ProPublica who focuses on issues of algorithmic bias and surveillance (and one of the 2016 investigators of the COMPAS recidivism algorithm that helped spark the field of FAccT) said in a 2019 interview:\n\n: I strongly believe that in order to solve a problem, you have to diagnose it, and that we’re still in the diagnosis phase of this. If you think about the turn of the century and industrialization, we had, I don’t know, 30 years of child labor, unlimited work hours, terrible working conditions, and it took a lot of journalist muckraking and advocacy to diagnose the problem and have some understanding of what it was, and then the activism to get laws changed. I feel like we’re in a second industrialization of data information… I see my role as trying to make as clear as possible what the downsides are, and diagnosing them really accurately so that they can be solvable. That’s hard work, and lots more people need to be doing it.\n\nIt’s reassuring that Angwin thinks we are largely still in the diagnosis phase: if your understanding of these problems feels incomplete, that is normal and natural. Nobody has a “cure” yet, although it is vital that we continue working to better understand and address the problems we are facing.\nOne of our reviewers for this book, Fred Monroe, used to work in hedge fund trading. He told us, after reading this chapter, that many of the issues discussed here (distribution of data being dramatically different than what a model was trained on, the impact feedback loops on a model once deployed and at scale, and so forth) were also key issues for building profitable trading models. The kinds of things you need to do to consider societal consequences are going to have a lot of overlap with things you need to do to consider organizational, market, and customer consequences—so thinking carefully about ethics can also help you think carefully about how to make your data product successful more generally!"
  },
  {
    "objectID": "Fastbook/03_ethics.html#questionnaire",
    "href": "Fastbook/03_ethics.html#questionnaire",
    "title": "Data Ethics",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nDoes ethics provide a list of “right answers”?\nHow can working with people of different backgrounds help when considering ethical questions?\nWhat was the role of IBM in Nazi Germany? Why did the company participate as it did? Why did the workers participate?\nWhat was the role of the first person jailed in the Volkswagen diesel scandal?\nWhat was the problem with a database of suspected gang members maintained by California law enforcement officials?\nWhy did YouTube’s recommendation algorithm recommend videos of partially clothed children to pedophiles, even though no employee at Google had programmed this feature?\nWhat are the problems with the centrality of metrics?\nWhy did Meetup.com not include gender in its recommendation system for tech meetups?\nWhat are the six types of bias in machine learning, according to Suresh and Guttag?\nGive two examples of historical race bias in the US.\nWhere are most images in ImageNet from?\nIn the paper “Does Machine Learning Automate Moral Hazard and Error” why is sinusitis found to be predictive of a stroke?\nWhat is representation bias?\nHow are machines and people different, in terms of their use for making decisions?\nIs disinformation the same as “fake news”?\nWhy is disinformation through auto-generated text a particularly significant issue?\nWhat are the five ethical lenses described by the Markkula Center?\nWhere is policy an appropriate tool for addressing data ethics issues?\n\n\nFurther Research:\n\nRead the article “What Happens When an Algorithm Cuts Your Healthcare”. How could problems like this be avoided in the future?\nResearch to find out more about YouTube’s recommendation system and its societal impacts. Do you think recommendation systems must always have feedback loops with negative results? What approaches could Google take to avoid them? What about the government?\nRead the paper “Discrimination in Online Ad Delivery”. Do you think Google should be considered responsible for what happened to Dr. Sweeney? What would be an appropriate response?\nHow can a cross-disciplinary team help avoid negative consequences?\nRead the paper “Does Machine Learning Automate Moral Hazard and Error”. What actions do you think should be taken to deal with the issues identified in this paper?\nRead the article “How Will We Prevent AI-Based Forgery?” Do you think Etzioni’s proposed approach could work? Why?\nComplete the section “Analyze a Project You Are Working On” in this chapter.\nConsider whether your team could be more diverse. If so, what approaches might help?"
  },
  {
    "objectID": "Fastbook/03_ethics.html#deep-learning-in-practice-thats-a-wrap",
    "href": "Fastbook/03_ethics.html#deep-learning-in-practice-thats-a-wrap",
    "title": "Data Ethics",
    "section": "Deep Learning in Practice: That’s a Wrap!",
    "text": "Deep Learning in Practice: That’s a Wrap!\nCongratulations! You’ve made it to the end of the first section of the book. In this section we’ve tried to show you what deep learning can do, and how you can use it to create real applications and products. At this point, you will get a lot more out of the book if you spend some time trying out what you’ve learned. Perhaps you have already been doing this as you go along—in which case, great! If not, that’s no problem either… Now is a great time to start experimenting yourself.\nIf you haven’t been to the book’s website yet, head over there now. It’s really important that you get yourself set up to run the notebooks. Becoming an effective deep learning practitioner is all about practice, so you need to be training models. So, please go get the notebooks running now if you haven’t already! And also have a look on the website for any important updates or notices; deep learning changes fast, and we can’t change the words that are printed in this book, so the website is where you need to look to ensure you have the most up-to-date information.\nMake sure that you have completed the following steps:\n\nConnect to one of the GPU Jupyter servers recommended on the book’s website.\nRun the first notebook yourself.\nUpload an image that you find in the first notebook; then try a few different images of different kinds to see what happens.\nRun the second notebook, collecting your own dataset based on image search queries that you come up with.\nThink about how you can use deep learning to help you with your own projects, including what kinds of data you could use, what kinds of problems may come up, and how you might be able to mitigate these issues in practice.\n\nIn the next section of the book you will learn about how and why deep learning works, instead of just seeing how you can use it in practice. Understanding the how and why is important for both practitioners and researchers, because in this fairly new field nearly every project requires some level of customization and debugging. The better you understand the foundations of deep learning, the better your models will be. These foundations are less important for executives, product managers, and so forth (although still useful, so feel free to keep reading!), but they are critical for anybody who is actually training and deploying models themselves."
  },
  {
    "objectID": "Fastbook/18_CAM.html",
    "href": "Fastbook/18_CAM.html",
    "title": "CNN Interpretation with CAM",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\n[[chapter_cam]]\nNow that we know how to build up pretty much anything from scratch, let’s use that knowledge to create entirely new (and very useful!) functionality: the class activation map. It gives us some insight into why a CNN made the predictions it did.\nIn the process, we’ll learn about one handy feature of PyTorch we haven’t seen before, the hook, and we’ll apply many of the concepts introduced in the rest of the book. If you want to really test out your understanding of the material in this book, after you’ve finished this chapter, try putting it aside and recreating the ideas here yourself from scratch (no peeking!)."
  },
  {
    "objectID": "Fastbook/18_CAM.html#cam-and-hooks",
    "href": "Fastbook/18_CAM.html#cam-and-hooks",
    "title": "CNN Interpretation with CAM",
    "section": "CAM and Hooks",
    "text": "CAM and Hooks\nThe class activation map (CAM) was introduced by Bolei Zhou et al. in “Learning Deep Features for Discriminative Localization”. It uses the output of the last convolutional layer (just before the average pooling layer) together with the predictions to give us a heatmap visualization of why the model made its decision. This is a useful tool for interpretation.\nMore precisely, at each position of our final convolutional layer, we have as many filters as in the last linear layer. We can therefore compute the dot product of those activations with the final weights to get, for each location on our feature map, the score of the feature that was used to make a decision.\nWe’re going to need a way to get access to the activations inside the model while it’s training. In PyTorch this can be done with a hook. Hooks are PyTorch’s equivalent of fastai’s callbacks. However, rather than allowing you to inject code into the training loop like a fastai Learner callback, hooks allow you to inject code into the forward and backward calculations themselves. We can attach a hook to any layer of the model, and it will be executed when we compute the outputs (forward hook) or during backpropagation (backward hook). A forward hook is a function that takes three things—a module, its input, and its output—and it can perform any behavior you want. (fastai also provides a handy HookCallback that we won’t cover here, but take a look at the fastai docs; it makes working with hooks a little easier.)\nTo illustrate, we’ll use the same cats and dogs model we trained in &lt;&gt;:\n\npath = untar_data(URLs.PETS)/'images'\ndef is_cat(x): return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=21,\n    label_func=is_cat, item_tfms=Resize(224))\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.145994\n0.019272\n0.006089\n00:14\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.053405\n0.052540\n0.010825\n00:19\n\n\n\n\n\nTo start, we’ll grab a cat picture and a batch of data:\n\nimg = PILImage.create(image_cat())\nx, = first(dls.test_dl([img]))\n\nFor CAM we want to store the activations of the last convolutional layer. We put our hook function in a class so it has a state that we can access later, and just store a copy of the output:\n\nclass Hook():\n    def hook_func(self, m, i, o): self.stored = o.detach().clone()\n\nWe can then instantiate a Hook and attach it to the layer we want, which is the last layer of the CNN body:\n\nhook_output = Hook()\nhook = learn.model[0].register_forward_hook(hook_output.hook_func)\n\nNow we can grab a batch and feed it through our model:\n\nwith torch.no_grad(): output = learn.model.eval()(x)\n\nAnd we can access our stored activations:\n\nact = hook_output.stored[0]\n\nLet’s also double-check our predictions:\n\nF.softmax(output, dim=-1)\n\ntensor([[0.0010, 0.9990]], device='cuda:0')\n\n\nWe know 0 (for False) is “dog,” because the classes are automatically sorted in fastai, bu we can still double-check by looking at dls.vocab:\n\ndls.vocab\n\n(#2) [False,True]\n\n\nSo, our model is very confident this was a picture of a cat.\nTo do the dot product of our weight matrix (2 by number of activations) with the activations (batch size by activations by rows by cols), we use a custom einsum:\n\nx.shape\n\ntorch.Size([1, 3, 224, 224])\n\n\n\ncam_map = torch.einsum('ck,kij-&gt;cij', learn.model[1][-1].weight, act)\ncam_map.shape\n\ntorch.Size([2, 7, 7])\n\n\nFor each image in our batch, and for each class, we get a 7×7 feature map that tells us where the activations were higher and where they were lower. This will let us see which areas of the pictures influenced the model’s decision.\nFor instance, we can find out which areas made the model decide this animal was a cat (note that we need to decode the input x since it’s been normalized by the DataLoader, and we need to cast to TensorImage since at the time this book is written PyTorch does not maintain types when indexing—this may be fixed by the time you are reading this):\n\nx_dec = TensorImage(dls.train.decode((x,))[0][0])\n_,ax = plt.subplots()\nx_dec.show(ctx=ax)\nax.imshow(cam_map[1].detach().cpu(), alpha=0.6, extent=(0,224,224,0),\n              interpolation='bilinear', cmap='magma');\n\n\n\n\n\n\n\n\nThe areas in bright yellow correspond to high activations and the areas in purple to low activations. In this case, we can see the head and the front paw were the two main areas that made the model decide it was a picture of a cat.\nOnce you’re done with your hook, you should remove it as otherwise it might leak some memory:\n\nhook.remove()\n\nThat’s why it’s usually a good idea to have the Hook class be a context manager, registering the hook when you enter it and removing it when you exit. A context manager is a Python construct that calls __enter__ when the object is created in a with clause, and __exit__ at the end of the with clause. For instance, this is how Python handles the with open(...) as f: construct that you’ll often see for opening files without requiring an explicit close(f) at the end. If we define Hook as follows:\n\nclass Hook():\n    def __init__(self, m):\n        self.hook = m.register_forward_hook(self.hook_func)   \n    def hook_func(self, m, i, o): self.stored = o.detach().clone()\n    def __enter__(self, *args): return self\n    def __exit__(self, *args): self.hook.remove()\n\nwe can safely use it this way:\n\nwith Hook(learn.model[0]) as hook:\n    with torch.no_grad(): output = learn.model.eval()(x.cuda())\n    act = hook.stored\n\nfastai provides this Hook class for you, as well as some other handy classes to make working with hooks easier.\nThis method is useful, but only works for the last layer. Gradient CAM is a variant that addresses this problem."
  },
  {
    "objectID": "Fastbook/18_CAM.html#gradient-cam",
    "href": "Fastbook/18_CAM.html#gradient-cam",
    "title": "CNN Interpretation with CAM",
    "section": "Gradient CAM",
    "text": "Gradient CAM\nThe method we just saw only lets us compute a heatmap with the last activations, since once we have our features, we have to multiply them by the last weight matrix. This won’t work for inner layers in the network. A variant introduced in the paper “Grad-CAM: Why Did You Say That? Visual Explanations from Deep Networks via Gradient-based Localization” in 2016 uses the gradients of the final activation for the desired class. If you remember a little bit about the backward pass, the gradients of the output of the last layer with respect to the input of that layer are equal to the layer weights, since it is a linear layer.\nWith deeper layers, we still want the gradients, but they won’t just be equal to the weights anymore. We have to calculate them. The gradients of every layer are calculated for us by PyTorch during the backward pass, but they’re not stored (except for tensors where requires_grad is True). We can, however, register a hook on the backward pass, which PyTorch will give the gradients to as a parameter, so we can store them there. For this we will use a HookBwd class that works like Hook, but intercepts and stores gradients instead of activations:\n\nclass HookBwd():\n    def __init__(self, m):\n        self.hook = m.register_backward_hook(self.hook_func)   \n    def hook_func(self, m, gi, go): self.stored = go[0].detach().clone()\n    def __enter__(self, *args): return self\n    def __exit__(self, *args): self.hook.remove()\n\nThen for the class index 1 (for True, which is “cat”) we intercept the features of the last convolutional layer as before, and compute the gradients of the output activations of our class. We can’t just call output.backward(), because gradients only make sense with respect to a scalar (which is normally our loss) and output is a rank-2 tensor. But if we pick a single image (we’ll use 0) and a single class (we’ll use 1), then we can calculate the gradients of any weight or activation we like, with respect to that single value, using output[0,cls].backward(). Our hook intercepts the gradients that we’ll use as weights:\n\ncls = 1\nwith HookBwd(learn.model[0]) as hookg:\n    with Hook(learn.model[0]) as hook:\n        output = learn.model.eval()(x.cuda())\n        act = hook.stored\n    output[0,cls].backward()\n    grad = hookg.stored\n\nThe weights for our Grad-CAM are given by the average of our gradients across the feature map. Then it’s exactly the same as before:\n\nw = grad[0].mean(dim=[1,2], keepdim=True)\ncam_map = (w * act[0]).sum(0)\n\n\n_,ax = plt.subplots()\nx_dec.show(ctx=ax)\nax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0),\n              interpolation='bilinear', cmap='magma');\n\n\n\n\n\n\n\n\nThe novelty with Grad-CAM is that we can use it on any layer. For example, here we use it on the output of the second-to-last ResNet group:\n\nwith HookBwd(learn.model[0][-2]) as hookg:\n    with Hook(learn.model[0][-2]) as hook:\n        output = learn.model.eval()(x.cuda())\n        act = hook.stored\n    output[0,cls].backward()\n    grad = hookg.stored\n\n\nw = grad[0].mean(dim=[1,2], keepdim=True)\ncam_map = (w * act[0]).sum(0)\n\nAnd we can now view the activation map for this layer:\n\n_,ax = plt.subplots()\nx_dec.show(ctx=ax)\nax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0),\n              interpolation='bilinear', cmap='magma');"
  },
  {
    "objectID": "Fastbook/18_CAM.html#conclusion",
    "href": "Fastbook/18_CAM.html#conclusion",
    "title": "CNN Interpretation with CAM",
    "section": "Conclusion",
    "text": "Conclusion\nModel interpretation is an area of active research, and we just scraped the surface of what is possible in this brief chapter. Class activation maps give us insight into why a model predicted a certain result by showing the areas of the images that were most responsible for a given prediction. This can help us analyze false positives and figure out what kind of data is missing in our training to avoid them."
  },
  {
    "objectID": "Fastbook/18_CAM.html#questionnaire",
    "href": "Fastbook/18_CAM.html#questionnaire",
    "title": "CNN Interpretation with CAM",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhat is a “hook” in PyTorch?\nWhich layer does CAM use the outputs of?\nWhy does CAM require a hook?\nLook at the source code of the ActivationStats class and see how it uses hooks.\nWrite a hook that stores the activations of a given layer in a model (without peeking, if possible).\nWhy do we call eval before getting the activations? Why do we use no_grad?\nUse torch.einsum to compute the “dog” or “cat” score of each of the locations in the last activation of the body of the model.\nHow do you check which order the categories are in (i.e., the correspondence of index-&gt;category)?\nWhy are we using decode when displaying the input image?\nWhat is a “context manager”? What special methods need to be defined to create one?\nWhy can’t we use plain CAM for the inner layers of a network?\nWhy do we need to register a hook on the backward pass in order to do Grad-CAM?\nWhy can’t we call output.backward() when output is a rank-2 tensor of output activations per image per class?\n\n\nFurther Research\n\nTry removing keepdim and see what happens. Look up this parameter in the PyTorch docs. Why do we need it in this notebook?\nCreate a notebook like this one, but for NLP, and use it to find which words in a movie review are most significant in assessing the sentiment of a particular movie review."
  },
  {
    "objectID": "Fastbook/README_zh.html",
    "href": "Fastbook/README_zh.html",
    "title": "The fastai book",
    "section": "",
    "text": "English / Spanish / Korean / Chinese / Bengali / Indonesian / Italian / Portuguese / Vietnamese / Japanese\n\nThe fastai book\n这些notebook包含了对深度学习，fastai，以及PyTorch的介绍。fastai是一个用于深度学习的分层API；要了解更多信息，请阅读the fastai paper论文。本repo的所有内容的版权都属于Jeremy Howard和Sylvain Gugger，起自2020年。\n这些notebook被用于一个MOOC课程，并且是这本书（目前可供购买）的基础。书籍并没有本稿的GPL限制。\nnotebook 里的代码以及 python 的 .py 文件受到 GPL v3 开源协议的保护；更多详情请查看 LICENSE 文件。\n其余部分（包括 notebook 里的 markdown 单元以及其他文字内容）可以用于重新发布，调整格式或者载体。你可以复制这里的 notebook 或者 fork 本 repo 用于个人目的。但是任何商业传播行为都是不被允许的。我们制作这些资料是用来帮助你学习深度学习的，所以请遵守我们的版权以及上述限制.\n如果你在别的地方发现任何人持有这些资料的副本，请告诉他们，他们的行为是不被允许的，并可能导致法律措施被采取。而且，他们可能会对社区造成伤害。这是因为如果人们忽视我们的版权，我们可能不会再像这样发布新的资料。\n这些资料还属于初稿阶段。如果你在运行这些 notebook 时遇到困难，请尝试在fastai-dev forum搜索或者寻求帮助。请不要因为运行 notebook 中遇到的问题而使用 GitHub issues。\n若果你要为本 repo 创建 pull request，那么你的作品版权将归于 Jeremy Howard 和 Sylvain Gugger。（另外，如果你是对拼写和文本做一些小修改，请标注文件名并对你的修改提供一个简单的说明。因为对 reviewer 来说，越来越难发现哪些地方已经被改正了。非常感谢。）"
  },
  {
    "objectID": "Fastbook/20_conclusion.html",
    "href": "Fastbook/20_conclusion.html",
    "title": "Concluding Thoughts",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\n[[chapter_conclusion]]\nCongratulations! You’ve made it! If you have worked through all of the notebooks to this point, then you have joined the small, but growing group of people that are able to harness the power of deep learning to solve real problems. You may not feel that way yet—in fact you probably don’t. We have seen again and again that students that complete the fast.ai courses dramatically underestimate how effective they are as deep learning practitioners. We’ve also seen that these people are often underestimated by others with a classic academic background. So if you are to rise above your own expectations and the expectations of others, what you do next, after closing this book, is even more important than what you’ve done to get to this point.\nThe most important thing is to keep the momentum going. In fact, as you know from your study of optimizers, momentum is something that can build upon itself! So think about what you can do now to maintain and accelerate your deep learning journey. &lt;&gt; can give you a few ideas.\n\nWe’ve talked a lot in this book about the value of writing, whether it be code or prose. But perhaps you haven’t quite written as much as you had hoped so far. That’s okay! Now is a great chance to turn that around. You have a lot to say, at this point. Perhaps you have tried some experiments on a dataset that other people don’t seem to have looked at in quite the same way. Tell the world about it! Or perhaps thinking about trying out some ideas that occurred to you while you were reading—now is a great time to turn those ideas into code.\nIf you’d like to share your ideas, one fairly low-key place to do so is the fast.ai forums. You will find that the community there is very supportive and helpful, so please do drop by and let us know what you’ve been up to. Or see if you can answer a few questions for those folks who are earlier in their journey than you.\nAnd if you do have some successes, big or small, in your deep learning journey, be sure to let us know! It’s especially helpful if you post about them on the forums, because learning about the successes of other students can be extremely motivating.\nPerhaps the most important approach for many people to stay connected with their learning journey is to build a community around it. For instance, you could try to set up a small deep learning meetup in your local neighborhood, or a study group, or even offer to do a talk at a local meetup about what you’ve learned so far or some particular aspect that interested you. It’s okay that you are not the world’s leading expert just yet—the important thing to remember is that you now know about plenty of stuff that other people don’t, so they are very likely to appreciate your perspective.\nAnother community event which many people find useful is a regular book club or paper reading club. You might find that there are some in your neighbourhood already, and if not you could try to get one started yourself. Even if there is just one other person doing it with you, it will help give you the support and encouragement to get going.\nIf you are not in a geography where it’s easy to get together with like-minded folks in person, drop by the forums, because there are always people starting up virtual study groups. These generally involve a bunch of folks getting together over video chat once a week or so to discuss some deep learning topic.\nHopefully, by this point, you have a few little projects that you’ve put together and experiments that you’ve run. Our recommendation for the next step is to pick one of these and make it as good as you can. Really polish it up into the best piece of work that you can—something you are really proud of. This will force you to go much deeper into a topic, which will really test your understanding and give you the opportunity to see what you can do when you really put your mind to it.\nAlso, you may want to take a look at the fast.ai free online course that covers the same material as this book. Sometimes, seeing the same material in two different ways can really help to crystallize the ideas. In fact, human learning researchers have found that one of the best ways to learn material is to see the same thing from different angles, described in different ways.\nYour final mission, should you choose to accept it, is to take this book and give it to somebody that you know—and get somebody else started on their own deep learning journey!"
  },
  {
    "objectID": "Fastbook/13_convolutions.html",
    "href": "Fastbook/13_convolutions.html",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastai.vision.all import *\nfrom fastbook import *\n\nmatplotlib.rc('image', cmap='Greys')\n[[chapter_convolutions]]\nIn &lt;&gt; we learned how to create a neural network recognizing images. We were able to achieve a bit over 98% accuracy at distinguishing 3s from 7s—but we also saw that fastai’s built-in classes were able to get close to 100%. Let’s start trying to close the gap.\nIn this chapter, we will begin by digging into what convolutions are and building a CNN from scratch. We will then study a range of techniques to improve training stability and learn all the tweaks the library usually applies for us to get great results."
  },
  {
    "objectID": "Fastbook/13_convolutions.html#the-magic-of-convolutions",
    "href": "Fastbook/13_convolutions.html#the-magic-of-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "The Magic of Convolutions",
    "text": "The Magic of Convolutions\nOne of the most powerful tools that machine learning practitioners have at their disposal is feature engineering. A feature is a transformation of the data which is designed to make it easier to model. For instance, the add_datepart function that we used for our tabular dataset preprocessing in &lt;&gt; added date features to the Bulldozers dataset. What kinds of features might we be able to create from images?\n\njargon: Feature engineering: Creating new transformations of the input data in order to make it easier to model.\n\nIn the context of an image, a feature is a visually distinctive attribute. For example, the number 7 is characterized by a horizontal edge near the top of the digit, and a top-right to bottom-left diagonal edge underneath that. On the other hand, the number 3 is characterized by a diagonal edge in one direction at the top left and bottom right of the digit, the opposite diagonal at the bottom left and top right, horizontal edges at the middle, top, and bottom, and so forth. So what if we could extract information about where the edges occur in each image, and then use that information as our features, instead of raw pixels?\nIt turns out that finding the edges in an image is a very common task in computer vision, and is surprisingly straightforward. To do it, we use something called a convolution. A convolution requires nothing more than multiplication, and addition—two operations that are responsible for the vast majority of work that we will see in every single deep learning model in this book!\nA convolution applies a kernel across an image. A kernel is a little matrix, such as the 3×3 matrix in the top right of &lt;&gt;.\n\nThe 7×7 grid to the left is the image we’re going to apply the kernel to. The convolution operation multiplies each element of the kernel by each element of a 3×3 block of the image. The results of these multiplications are then added together. The diagram in &lt;&gt; shows an example of applying a kernel to a single location in the image, the 3×3 block around cell 18.\nLet’s do this with code. First, we create a little 3×3 matrix like so:\n\ntop_edge = tensor([[-1,-1,-1],\n                   [ 0, 0, 0],\n                   [ 1, 1, 1]]).float()\n\nWe’re going to call this our kernel (because that’s what fancy computer vision researchers call these). And we’ll need an image, of course:\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\n#hide\nPath.BASE_PATH = path\n\n\nim3 = Image.open(path/'train'/'3'/'12.png')\nshow_image(im3);\n\n\n\n\n\n\n\n\nNow we’re going to take the top 3×3-pixel square of our image, and multiply each of those values by each item in our kernel. Then we’ll add them up, like so:\n\nim3_t = tensor(im3)\nim3_t[0:3,0:3] * top_edge\n\ntensor([[-0., -0., -0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\n\n\n\n(im3_t[0:3,0:3] * top_edge).sum()\n\ntensor(0.)\n\n\nNot very interesting so far—all the pixels in the top-left corner are white. But let’s pick a couple of more interesting spots:\n\n#hide_output\ndf = pd.DataFrame(im3_t[:10,:20])\ndf.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n5\n0\n0\n0\n12\n99\n91\n142\n155\n246\n182\n155\n155\n155\n155\n131\n52\n0\n0\n0\n0\n\n\n6\n0\n0\n0\n138\n254\n254\n254\n254\n254\n254\n254\n254\n254\n254\n254\n252\n210\n122\n33\n0\n\n\n7\n0\n0\n0\n220\n254\n254\n254\n235\n189\n189\n189\n189\n150\n189\n205\n254\n254\n254\n75\n0\n\n\n8\n0\n0\n0\n35\n74\n35\n35\n25\n0\n0\n0\n0\n0\n0\n13\n224\n254\n254\n153\n0\n\n\n9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n90\n254\n254\n247\n53\n0\n\n\n\n\n\n\nThere’s a top edge at cell 5,8. Let’s repeat our calculation there:\n\n(im3_t[4:7,6:9] * top_edge).sum()\n\ntensor(762.)\n\n\nThere’s a right edge at cell 8,18. What does that give us?:\n\n(im3_t[7:10,17:20] * top_edge).sum()\n\ntensor(-29.)\n\n\nAs you can see, this little calculation is returning a high number where the 3×3-pixel square represents a top edge (i.e., where there are low values at the top of the square, and high values immediately underneath). That’s because the -1 values in our kernel have little impact in that case, but the 1 values have a lot.\nLet’s look a tiny bit at the math. The filter will take any window of size 3×3 in our images, and if we name the pixel values like this:\n\\[\\begin{matrix} a1 & a2 & a3 \\\\ a4 & a5 & a6 \\\\ a7 & a8 & a9 \\end{matrix}\\]\nit will return \\(-a1-a2-a3+a7+a8+a9\\). If we are in a part of the image where \\(a1\\), \\(a2\\), and \\(a3\\) add up to the same as \\(a7\\), \\(a8\\), and \\(a9\\), then the terms will cancel each other out and we will get 0. However, if \\(a7\\) is greater than \\(a1\\), \\(a8\\) is greater than \\(a2\\), and \\(a9\\) is greater than \\(a3\\), we will get a bigger number as a result. So this filter detects horizontal edges—more precisely, edges where we go from bright parts of the image at the top to darker parts at the bottom.\nChanging our filter to have the row of 1s at the top and the -1s at the bottom would detect horizontal edges that go from dark to light. Putting the 1s and -1s in columns versus rows would give us filters that detect vertical edges. Each set of weights will produce a different kind of outcome.\nLet’s create a function to do this for one location, and check it matches our result from before:\n\ndef apply_kernel(row, col, kernel):\n    return (im3_t[row-1:row+2,col-1:col+2] * kernel).sum()\n\n\napply_kernel(5,7,top_edge)\n\ntensor(762.)\n\n\nBut note that we can’t apply it to the corner (e.g., location 0,0), since there isn’t a complete 3×3 square there.\n\nMapping a Convolution Kernel\nWe can map apply_kernel() across the coordinate grid. That is, we’ll be taking our 3×3 kernel, and applying it to each 3×3 section of our image. For instance, &lt;&gt; shows the positions a 3×3 kernel can be applied to in the first row of a 5×5 image.\n\nTo get a grid of coordinates we can use a nested list comprehension, like so:\n\n[[(i,j) for j in range(1,5)] for i in range(1,5)]\n\n[[(1, 1), (1, 2), (1, 3), (1, 4)],\n [(2, 1), (2, 2), (2, 3), (2, 4)],\n [(3, 1), (3, 2), (3, 3), (3, 4)],\n [(4, 1), (4, 2), (4, 3), (4, 4)]]\n\n\n\nnote: Nested List Comprehensions: Nested list comprehensions are used a lot in Python, so if you haven’t seen them before, take a few minutes to make sure you understand what’s happening here, and experiment with writing your own nested list comprehensions.\n\nHere’s the result of applying our kernel over a coordinate grid:\n\nrng = range(1,27)\ntop_edge3 = tensor([[apply_kernel(i,j,top_edge) for j in rng] for i in rng])\n\nshow_image(top_edge3);\n\n\n\n\n\n\n\n\nLooking good! Our top edges are black, and bottom edges are white (since they are the opposite of top edges). Now that our image contains negative numbers too, matplotlib has automatically changed our colors so that white is the smallest number in the image, black the highest, and zeros appear as gray.\nWe can try the same thing for left edges:\n\nleft_edge = tensor([[-1,1,0],\n                    [-1,1,0],\n                    [-1,1,0]]).float()\n\nleft_edge3 = tensor([[apply_kernel(i,j,left_edge) for j in rng] for i in rng])\n\nshow_image(left_edge3);\n\n\n\n\n\n\n\n\nAs we mentioned before, a convolution is the operation of applying such a kernel over a grid in this way. In the paper “A Guide to Convolution Arithmetic for Deep Learning” there are many great diagrams showing how image kernels can be applied. Here’s an example from the paper showing (at the bottom) a light blue 4×4 image, with a dark blue 3×3 kernel being applied, creating a 2×2 green output activation map at the top.\n\nLook at the shape of the result. If the original image has a height of h and a width of w, how many 3×3 windows can we find? As you can see from the example, there are h-2 by w-2 windows, so the image we get has a result as a height of h-2 and a width of w-2.\nWe won’t implement this convolution function from scratch, but use PyTorch’s implementation instead (it is way faster than anything we could do in Python).\n\n\nConvolutions in PyTorch\nConvolution is such an important and widely used operation that PyTorch has it built in. It’s called F.conv2d (recall that F is a fastai import from torch.nn.functional, as recommended by PyTorch). The PyTorch docs tell us that it includes these parameters:\n\ninput:: input tensor of shape (minibatch, in_channels, iH, iW)\nweight:: filters of shape (out_channels, in_channels, kH, kW)\n\nHere iH,iW is the height and width of the image (i.e., 28,28), and kH,kW is the height and width of our kernel (3,3). But apparently PyTorch is expecting rank-4 tensors for both these arguments, whereas currently we only have rank-2 tensors (i.e., matrices, or arrays with two axes).\nThe reason for these extra axes is that PyTorch has a few tricks up its sleeve. The first trick is that PyTorch can apply a convolution to multiple images at the same time. That means we can call it on every item in a batch at once!\nThe second trick is that PyTorch can apply multiple kernels at the same time. So let’s create the diagonal-edge kernels too, and then stack all four of our edge kernels into a single tensor:\n\ndiag1_edge = tensor([[ 0,-1, 1],\n                     [-1, 1, 0],\n                     [ 1, 0, 0]]).float()\ndiag2_edge = tensor([[ 1,-1, 0],\n                     [ 0, 1,-1],\n                     [ 0, 0, 1]]).float()\n\nedge_kernels = torch.stack([left_edge, top_edge, diag1_edge, diag2_edge])\nedge_kernels.shape\n\ntorch.Size([4, 3, 3])\n\n\nTo test this, we’ll need a DataLoader and a sample mini-batch. Let’s use the data block API:\n\nmnist = DataBlock((ImageBlock(cls=PILImageBW), CategoryBlock), \n                  get_items=get_image_files, \n                  splitter=GrandparentSplitter(),\n                  get_y=parent_label)\n\ndls = mnist.dataloaders(path)\nxb,yb = first(dls.valid)\nxb.shape\n\ntorch.Size([64, 1, 28, 28])\n\n\nBy default, fastai puts data on the GPU when using data blocks. Let’s move it to the CPU for our examples:\n\nxb,yb = to_cpu(xb),to_cpu(yb)\n\nOne batch contains 64 images, each of 1 channel, with 28×28 pixels. F.conv2d can handle multichannel (i.e., color) images too. A channel is a single basic color in an image—for regular full-color images there are three channels, red, green, and blue. PyTorch represents an image as a rank-3 tensor, with dimensions [channels, rows, columns].\nWe’ll see how to handle more than one channel later in this chapter. Kernels passed to F.conv2d need to be rank-4 tensors: [channels_in, features_out, rows, columns]. edge_kernels is currently missing one of these. We need to tell PyTorch that the number of input channels in the kernel is one, which we can do by inserting an axis of size one (this is known as a unit axis) in the first location, where the PyTorch docs show in_channels is expected. To insert a unit axis into a tensor, we use the unsqueeze method:\n\nedge_kernels.shape,edge_kernels.unsqueeze(1).shape\n\n(torch.Size([4, 3, 3]), torch.Size([4, 1, 3, 3]))\n\n\nThis is now the correct shape for edge_kernels. Let’s pass this all to conv2d:\n\nedge_kernels = edge_kernels.unsqueeze(1)\n\n\nbatch_features = F.conv2d(xb, edge_kernels)\nbatch_features.shape\n\ntorch.Size([64, 4, 26, 26])\n\n\nThe output shape shows we gave 64 images in the mini-batch, 4 kernels, and 26×26 edge maps (we started with 28×28 images, but lost one pixel from each side as discussed earlier). We can see we get the same results as when we did this manually:\n\nshow_image(batch_features[0,0]);\n\n\n\n\n\n\n\n\nThe most important trick that PyTorch has up its sleeve is that it can use the GPU to do all this work in parallel—that is, applying multiple kernels, to multiple images, across multiple channels. Doing lots of work in parallel is critical to getting GPUs to work efficiently; if we did each of these operations one at a time, we’d often run hundreds of times slower (and if we used our manual convolution loop from the previous section, we’d be millions of times slower!). Therefore, to become a strong deep learning practitioner, one skill to practice is giving your GPU plenty of work to do at a time.\nIt would be nice to not lose those two pixels on each axis. The way we do that is to add padding, which is simply additional pixels added around the outside of our image. Most commonly, pixels of zeros are added.\n\n\nStrides and Padding\nWith appropriate padding, we can ensure that the output activation map is the same size as the original image, which can make things a lot simpler when we construct our architectures. &lt;&gt; shows how adding padding allows us to apply the kernels in the image corners.\n\nWith a 5×5 input, 4×4 kernel, and 2 pixels of padding, we end up with a 6×6 activation map, as we can see in &lt;&gt;.\n\nIf we add a kernel of size ks by ks (with ks an odd number), the necessary padding on each side to keep the same shape is ks//2. An even number for ks would require a different amount of padding on the top/bottom and left/right, but in practice we almost never use an even filter size.\nSo far, when we have applied the kernel to the grid, we have moved it one pixel over at a time. But we can jump further; for instance, we could move over two pixels after each kernel application, as in &lt;&gt;. This is known as a stride-2 convolution. The most common kernel size in practice is 3×3, and the most common padding is 1. As you’ll see, stride-2 convolutions are useful for decreasing the size of our outputs, and stride-1 convolutions are useful for adding layers without changing the output size.\n\nIn an image of size h by w, using a padding of 1 and a stride of 2 will give us a result of size (h+1)//2 by (w+1)//2. The general formula for each dimension is (n + 2*pad - ks)//stride + 1, where pad is the padding, ks, the size of our kernel, and stride is the stride.\nLet’s now take a look at how the pixel values of the result of our convolutions are computed.\n\n\nUnderstanding the Convolution Equations\nTo explain the math behind convolutions, fast.ai student Matt Kleinsmith came up with the very clever idea of showing CNNs from different viewpoints. In fact, it’s so clever, and so helpful, we’re going to show it here too!\nHere’s our 3×3 pixel image, with each pixel labeled with a letter:\n\nAnd here’s our kernel, with each weight labeled with a Greek letter:\n\nSince the filter fits in the image four times, we have four results:\n\n&lt;&gt; shows how we applied the kernel to each section of the image to yield each result.\n\nThe equation view is in &lt;&gt;.\n\nNotice that the bias term, b, is the same for each section of the image. You can consider the bias as part of the filter, just like the weights (α, β, γ, δ) are part of the filter.\nHere’s an interesting insight—a convolution can be represented as a special kind of matrix multiplication, as illustrated in &lt;&gt;. The weight matrix is just like the ones from traditional neural networks. However, this weight matrix has two special properties:\n\nThe zeros shown in gray are untrainable. This means that they’ll stay zero throughout the optimization process.\nSome of the weights are equal, and while they are trainable (i.e., changeable), they must remain equal. These are called shared weights.\n\nThe zeros correspond to the pixels that the filter can’t touch. Each row of the weight matrix corresponds to one application of the filter.\n\nNow that we understand what a convolution is, let’s use them to build a neural net."
  },
  {
    "objectID": "Fastbook/13_convolutions.html#our-first-convolutional-neural-network",
    "href": "Fastbook/13_convolutions.html#our-first-convolutional-neural-network",
    "title": "Convolutional Neural Networks",
    "section": "Our First Convolutional Neural Network",
    "text": "Our First Convolutional Neural Network\nThere is no reason to believe that some particular edge filters are the most useful kernels for image recognition. Furthermore, we’ve seen that in later layers convolutional kernels become complex transformations of features from lower levels, but we don’t have a good idea of how to manually construct these.\nInstead, it would be best to learn the values of the kernels. We already know how to do this—SGD! In effect, the model will learn the features that are useful for classification.\nWhen we use convolutions instead of (or in addition to) regular linear layers we create a convolutional neural network (CNN).\n\nCreating the CNN\nLet’s go back to the basic neural network we had in &lt;&gt;. It was defined like this:\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28,30),\n    nn.ReLU(),\n    nn.Linear(30,1)\n)\n\nWe can view a model’s definition:\n\nsimple_net\n\nSequential(\n  (0): Linear(in_features=784, out_features=30, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=30, out_features=1, bias=True)\n)\n\n\nWe now want to create a similar architecture to this linear model, but using convolutional layers instead of linear. nn.Conv2d is the module equivalent of F.conv2d. It’s more convenient than F.conv2d when creating an architecture, because it creates the weight matrix for us automatically when we instantiate it.\nHere’s a possible architecture:\n\nbroken_cnn = sequential(\n    nn.Conv2d(1,30, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(30,1, kernel_size=3, padding=1)\n)\n\nOne thing to note here is that we didn’t need to specify 28×28 as the input size. That’s because a linear layer needs a weight in the weight matrix for every pixel, so it needs to know how many pixels there are, but a convolution is applied over each pixel automatically. The weights only depend on the number of input and output channels and the kernel size, as we saw in the previous section.\nThink about what the output shape is going to be, then let’s try it and see:\n\nbroken_cnn(xb).shape\n\ntorch.Size([64, 1, 28, 28])\n\n\nThis is not something we can use to do classification, since we need a single output activation per image, not a 28×28 map of activations. One way to deal with this is to use enough stride-2 convolutions such that the final layer is size 1. That is, after one stride-2 convolution the size will be 14×14, after two it will be 7×7, then 4×4, 2×2, and finally size 1.\nLet’s try that now. First, we’ll define a function with the basic parameters we’ll use in each convolution:\n\ndef conv(ni, nf, ks=3, act=True):\n    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n\n\nimportant: Refactoring: Refactoring parts of your neural networks like this makes it much less likely you’ll get errors due to inconsistencies in your architectures, and makes it more obvious to the reader which parts of your layers are actually changing.\n\nWhen we use a stride-2 convolution, we often increase the number of features at the same time. This is because we’re decreasing the number of activations in the activation map by a factor of 4; we don’t want to decrease the capacity of a layer by too much at a time.\n\njargon: channels and features: These two terms are largely used interchangeably, and refer to the size of the second axis of a weight matrix, which is, the number of activations per grid cell after a convolution. Features is never used to refer to the input data, but channels can refer to either the input data (generally channels are colors) or activations inside the network.\n\nHere is how we can build a simple CNN:\n\nsimple_cnn = sequential(\n    conv(1 ,4),            #14x14\n    conv(4 ,8),            #7x7\n    conv(8 ,16),           #4x4\n    conv(16,32),           #2x2\n    conv(32,2, act=False), #1x1\n    Flatten(),\n)\n\n\nj: I like to add comments like the ones here after each convolution to show how large the activation map will be after each layer. These comments assume that the input size is 28*28\n\nNow the network outputs two activations, which map to the two possible levels in our labels:\n\nsimple_cnn(xb).shape\n\ntorch.Size([64, 2])\n\n\nWe can now create our Learner:\n\nlearn = Learner(dls, simple_cnn, loss_func=F.cross_entropy, metrics=accuracy)\n\nTo see exactly what’s going on in the model, we can use summary:\n\nlearn.summary()\n\nSequential (Input shape: ['64 x 1 x 28 x 28'])\n================================================================\nLayer (type)         Output Shape         Param #    Trainable \n================================================================\nConv2d               64 x 4 x 14 x 14     40         True      \n________________________________________________________________\nReLU                 64 x 4 x 14 x 14     0          False     \n________________________________________________________________\nConv2d               64 x 8 x 7 x 7       296        True      \n________________________________________________________________\nReLU                 64 x 8 x 7 x 7       0          False     \n________________________________________________________________\nConv2d               64 x 16 x 4 x 4      1,168      True      \n________________________________________________________________\nReLU                 64 x 16 x 4 x 4      0          False     \n________________________________________________________________\nConv2d               64 x 32 x 2 x 2      4,640      True      \n________________________________________________________________\nReLU                 64 x 32 x 2 x 2      0          False     \n________________________________________________________________\nConv2d               64 x 2 x 1 x 1       578        True      \n________________________________________________________________\nFlatten              64 x 2               0          False     \n________________________________________________________________\n\nTotal params: 6,722\nTotal trainable params: 6,722\nTotal non-trainable params: 0\n\nOptimizer used: &lt;function Adam at 0x7fbc9c258cb0&gt;\nLoss function: &lt;function cross_entropy at 0x7fbca9ba0170&gt;\n\nCallbacks:\n  - TrainEvalCallback\n  - Recorder\n  - ProgressCallback\n\n\nNote that the output of the final Conv2d layer is 64x2x1x1. We need to remove those extra 1x1 axes; that’s what Flatten does. It’s basically the same as PyTorch’s squeeze method, but as a module.\nLet’s see if this trains! Since this is a deeper network than we’ve built from scratch before, we’ll use a lower learning rate and more epochs:\n\nlearn.fit_one_cycle(2, 0.01)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.072684\n0.045110\n0.990186\n00:05\n\n\n1\n0.022580\n0.030775\n0.990186\n00:05\n\n\n\n\n\nSuccess! It’s getting closer to the resnet18 result we had, although it’s not quite there yet, and it’s taking more epochs, and we’re needing to use a lower learning rate. We still have a few more tricks to learn, but we’re getting closer and closer to being able to create a modern CNN from scratch.\n\n\nUnderstanding Convolution Arithmetic\nWe can see from the summary that we have an input of size 64x1x28x28. The axes are batch,channel,height,width. This is often represented as NCHW (where N refers to batch size). Tensorflow, on the other hand, uses NHWC axis order. The first layer is:\n\nm = learn.model[0]\nm\n\nSequential(\n  (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (1): ReLU()\n)\n\n\nSo we have 1 input channel, 4 output channels, and a 3×3 kernel. Let’s check the weights of the first convolution:\n\nm[0].weight.shape\n\ntorch.Size([4, 1, 3, 3])\n\n\nThe summary shows we have 40 parameters, and 4*1*3*3 is 36. What are the other four parameters? Let’s see what the bias contains:\n\nm[0].bias.shape\n\ntorch.Size([4])\n\n\nWe can now use this information to clarify our statement in the previous section: “When we use a stride-2 convolution, we often increase the number of features because we’re decreasing the number of activations in the activation map by a factor of 4; we don’t want to decrease the capacity of a layer by too much at a time.”\nThere is one bias for each channel. (Sometimes channels are called features or filters when they are not input channels.) The output shape is 64x4x14x14, and this will therefore become the input shape to the next layer. The next layer, according to summary, has 296 parameters. Let’s ignore the batch axis to keep things simple. So for each of 14*14=196 locations we are multiplying 296-8=288 weights (ignoring the bias for simplicity), so that’s 196*288=56_448 multiplications at this layer. The next layer will have 7*7*(1168-16)=56_448 multiplications.\nWhat happened here is that our stride-2 convolution halved the grid size from 14x14 to 7x7, and we doubled the number of filters from 8 to 16, resulting in no overall change in the amount of computation. If we left the number of channels the same in each stride-2 layer, the amount of computation being done in the net would get less and less as it gets deeper. But we know that the deeper layers have to compute semantically rich features (such as eyes or fur), so we wouldn’t expect that doing less computation would make sense.\nAnother way to think of this is based on receptive fields.\n\n\nReceptive Fields\nThe receptive field is the area of an image that is involved in the calculation of a layer. On the book’s website, you’ll find an Excel spreadsheet called conv-example.xlsx that shows the calculation of two stride-2 convolutional layers using an MNIST digit. Each layer has a single kernel. &lt;&gt; shows what we see if we click on one of the cells in the conv2 section, which shows the output of the second convolutional layer, and click trace precedents.\n\nHere, the cell with the green border is the cell we clicked on, and the blue highlighted cells are its precedents—that is, the cells used to calculate its value. These cells are the corresponding 3×3 area of cells from the input layer (on the left), and the cells from the filter (on the right). Let’s now click trace precedents again, to see what cells are used to calculate these inputs. &lt;&gt; shows what happens.\n\nIn this example, we have just two convolutional layers, each of stride 2, so this is now tracing right back to the input image. We can see that a 7×7 area of cells in the input layer is used to calculate the single green cell in the Conv2 layer. This 7×7 area is the receptive field in the input of the green activation in Conv2. We can also see that a second filter kernel is needed now, since we have two layers.\nAs you see from this example, the deeper we are in the network (specifically, the more stride-2 convs we have before a layer), the larger the receptive field for an activation in that layer. A large receptive field means that a large amount of the input image is used to calculate each activation in that layer is. We now know that in the deeper layers of the network we have semantically rich features, corresponding to larger receptive fields. Therefore, we’d expect that we’d need more weights for each of our features to handle this increasing complexity. This is another way of saying the same thing we mentioned in the previous section: when we introduce a stride-2 conv in our network, we should also increase the number of channels.\nWhen writing this particular chapter, we had a lot of questions we needed answers for, to be able to explain CNNs to you as best we could. Believe it or not, we found most of the answers on Twitter. We’re going to take a quick break to talk to you about that now, before we move on to color images.\n\n\nA Note About Twitter\nWe are not, to say the least, big users of social networks in general. But our goal in writing this book is to help you become the best deep learning practitioner you can, and we would be remiss not to mention how important Twitter has been in our own deep learning journeys.\nYou see, there’s another part of Twitter, far away from Donald Trump and the Kardashians, which is the part of Twitter where deep learning researchers and practitioners talk shop every day. As we were writing this section, Jeremy wanted to double-check that what we were saying about stride-2 convolutions was accurate, so he asked on Twitter:\n\nA few minutes later, this answer popped up:\n\nChristian Szegedy is the first author of Inception, the 2014 ImageNet winner and source of many key insights used in modern neural networks. Two hours later, this appeared:\n\nDo you recognize that name? You saw it in &lt;&gt;, when we were talking about the Turing Award winners who established the foundations of deep learning today!\nJeremy also asked on Twitter for help checking our description of label smoothing in &lt;&gt; was accurate, and got a response again from directly from Christian Szegedy (label smoothing was originally introduced in the Inception paper):\n\nMany of the top people in deep learning today are Twitter regulars, and are very open about interacting with the wider community. One good way to get started is to look at a list of Jeremy’s recent Twitter likes, or Sylvain’s. That way, you can see a list of Twitter users that we think have interesting and useful things to say.\nTwitter is the main way we both stay up to date with interesting papers, software releases, and other deep learning news. For making connections with the deep learning community, we recommend getting involved both in the fast.ai forums and on Twitter.\nThat said, let’s get back to the meat of this chapter. Up until now, we have only shown you examples of pictures in black and white, with one value per pixel. In practice, most colored images have three values per pixel to define their color. We’ll look at working with color images next."
  },
  {
    "objectID": "Fastbook/13_convolutions.html#color-images",
    "href": "Fastbook/13_convolutions.html#color-images",
    "title": "Convolutional Neural Networks",
    "section": "Color Images",
    "text": "Color Images\nA colour picture is a rank-3 tensor:\n\nim = image2tensor(Image.open(image_bear()))\nim.shape\n\ntorch.Size([3, 1000, 846])\n\n\n\nshow_image(im);\n\n\n\n\n\n\n\n\nThe first axis contains the channels, red, green, and blue:\n\n_,axs = subplots(1,3)\nfor bear,ax,color in zip(im,axs,('Reds','Greens','Blues')):\n    show_image(255-bear, ax=ax, cmap=color)\n\n\n\n\n\n\n\n\nWe saw what the convolution operation was for one filter on one channel of the image (our examples were done on a square). A convolutional layer will take an image with a certain number of channels (three for the first layer for regular RGB color images) and output an image with a different number of channels. Like our hidden size that represented the numbers of neurons in a linear layer, we can decide to have as many filters as we want, and each of them will be able to specialize, some to detect horizontal edges, others to detect vertical edges and so forth, to give something like we studied in &lt;&gt;.\nIn one sliding window, we have a certain number of channels and we need as many filters (we don’t use the same kernel for all the channels). So our kernel doesn’t have a size of 3 by 3, but ch_in (for channels in) is 3 by 3. On each channel, we multiply the elements of our window by the elements of the coresponding filter, then sum the results (as we saw before) and sum over all the filters. In the example given in &lt;&gt;, the result of our conv layer on that window is red + green + blue.\n\nSo, in order to apply a convolution to a color picture we require a kernel tensor with a size that matches the first axis. At each location, the corresponding parts of the kernel and the image patch are multiplied together.\nThese are then all added together, to produce a single number, for each grid location, for each output feature, as shown in &lt;&gt;.\n\nThen we have ch_out filters like this, so in the end, the result of our convolutional layer will be a batch of images with ch_out channels and a height and width given by the formula outlined earlier. This give us ch_out tensors of size ch_in x ks x ks that we represent in one big tensor of four dimensions. In PyTorch, the order of the dimensions for those weights is ch_out x ch_in x ks x ks.\nAdditionally, we may want to have a bias for each filter. In the preceding example, the final result for our convolutional layer would be \\(y_{R} + y_{G} + y_{B} + b\\) in that case. Like in a linear layer, there are as many bias as we have kernels, so the biases is a vector of size ch_out.\nThere are no special mechanisms required when setting up a CNN for training with color images. Just make sure your first layer has three inputs.\nThere are lots of ways of processing color images. For instance, you can change them to black and white, change from RGB to HSV (hue, saturation, and value) color space, and so forth. In general, it turns out experimentally that changing the encoding of colors won’t make any difference to your model results, as long as you don’t lose information in the transformation. So, transforming to black and white is a bad idea, since it removes the color information entirely (and this can be critical; for instance, a pet breed may have a distinctive color); but converting to HSV generally won’t make any difference.\nNow you know what those pictures in &lt;&gt; of “what a neural net learns” from the Zeiler and Fergus paper mean! This is their picture of some of the layer 1 weights which we showed:\n\nThis is taking the three slices of the convolutional kernel, for each output feature, and displaying them as images. We can see that even though the creators of the neural net never explicitly created kernels to find edges, for instance, the neural net automatically discovered these features using SGD.\nNow let’s see how we can train these CNNs, and show you all the techniques fastai uses under the hood for efficient training."
  },
  {
    "objectID": "Fastbook/13_convolutions.html#improving-training-stability",
    "href": "Fastbook/13_convolutions.html#improving-training-stability",
    "title": "Convolutional Neural Networks",
    "section": "Improving Training Stability",
    "text": "Improving Training Stability\nSince we are so good at recognizing 3s from 7s, let’s move on to something harder—recognizing all 10 digits. That means we’ll need to use MNIST instead of MNIST_SAMPLE:\n\npath = untar_data(URLs.MNIST)\n\n\n#hide\nPath.BASE_PATH = path\n\n\npath.ls()\n\n(#2) [Path('testing'),Path('training')]\n\n\nThe data is in two folders named training and testing, so we have to tell GrandparentSplitter about that (it defaults to train and valid). We did do that in the get_dls function, which we create to make it easy to change our batch size later:\n\ndef get_dls(bs=64):\n    return DataBlock(\n        blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), \n        get_items=get_image_files, \n        splitter=GrandparentSplitter('training','testing'),\n        get_y=parent_label,\n        batch_tfms=Normalize()\n    ).dataloaders(path, bs=bs)\n\ndls = get_dls()\n\nRemember, it’s always a good idea to look at your data before you use it:\n\ndls.show_batch(max_n=9, figsize=(4,4))\n\n\n\n\n\n\n\n\nNow that we have our data ready, we can train a simple model on it.\n\nA Simple Baseline\nEarlier in this chapter, we built a model based on a conv function like this:\n\ndef conv(ni, nf, ks=3, act=True):\n    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n\nLet’s start with a basic CNN as a baseline. We’ll use the same one as earlier, but with one tweak: we’ll use more activations. Since we have more numbers to differentiate, it’s likely we will need to learn more filters.\nAs we discussed, we generally want to double the number of filters each time we have a stride-2 layer. One way to increase the number of filters throughout our network is to double the number of activations in the first layer–then every layer after that will end up twice as big as in the previous version as well.\nBut there is a subtle problem with this. Consider the kernel that is being applied to each pixel. By default, we use a 3×3-pixel kernel. That means that there are a total of 3×3 = 9 pixels that the kernel is being applied to at each location. Previously, our first layer had four output filters. That meant that there were four values being computed from nine pixels at each location. Think about what happens if we double this output to eight filters. Then when we apply our kernel we will be using nine pixels to calculate eight numbers. That means it isn’t really learning much at all: the output size is almost the same as the input size. Neural networks will only create useful features if they’re forced to do so—that is, if the number of outputs from an operation is significantly smaller than the number of inputs.\nTo fix this, we can use a larger kernel in the first layer. If we use a kernel of 5×5 pixels then there are 25 pixels being used at each kernel application. Creating eight filters from this will mean the neural net will have to find some useful features:\n\ndef simple_cnn():\n    return sequential(\n        conv(1 ,8, ks=5),        #14x14\n        conv(8 ,16),             #7x7\n        conv(16,32),             #4x4\n        conv(32,64),             #2x2\n        conv(64,10, act=False),  #1x1\n        Flatten(),\n    )\n\nAs you’ll see in a moment, we can look inside our models while they’re training in order to try to find ways to make them train better. To do this we use the ActivationStats callback, which records the mean, standard deviation, and histogram of activations of every trainable layer (as we’ve seen, callbacks are used to add behavior to the training loop; we’ll explore how they work in &lt;&gt;):\n\nfrom fastai.callback.hook import *\n\nWe want to train quickly, so that means training at a high learning rate. Let’s see how we go at 0.06:\n\ndef fit(epochs=1):\n    learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy,\n                    metrics=accuracy, cbs=ActivationStats(with_hist=True))\n    learn.fit(epochs, 0.06)\n    return learn\n\n\nlearn = fit()\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.307071\n2.305865\n0.113500\n00:16\n\n\n\n\n\nThis didn’t train at all well! Let’s find out why.\nOne handy feature of the callbacks passed to Learner is that they are made available automatically, with the same name as the callback class, except in snake_case. So, our ActivationStats callback can be accessed through activation_stats. I’m sure you remember learn.recorder… can you guess how that is implemented? That’s right, it’s a callback called Recorder!\nActivationStats includes some handy utilities for plotting the activations during training. plot_layer_stats(idx) plots the mean and standard deviation of the activations of layer number idx, along with the percentage of activations near zero. Here’s the first layer’s plot:\n\nlearn.activation_stats.plot_layer_stats(0)\n\n\n\n\n\n\n\n\nGenerally our model should have a consistent, or at least smooth, mean and standard deviation of layer activations during training. Activations near zero are particularly problematic, because it means we have computation in the model that’s doing nothing at all (since multiplying by zero gives zero). When you have some zeros in one layer, they will therefore generally carry over to the next layer… which will then create more zeros. Here’s the penultimate layer of our network:\n\nlearn.activation_stats.plot_layer_stats(-2)\n\n\n\n\n\n\n\n\nAs expected, the problems get worse towards the end of the network, as the instability and zero activations compound over layers. Let’s look at what we can do to make training more stable.\n\n\nIncrease Batch Size\nOne way to make training more stable is to increase the batch size. Larger batches have gradients that are more accurate, since they’re calculated from more data. On the downside, though, a larger batch size means fewer batches per epoch, which means less opportunities for your model to update weights. Let’s see if a batch size of 512 helps:\n\ndls = get_dls(512)\n\n\nlearn = fit()\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.309385\n2.302744\n0.113500\n00:08\n\n\n\n\n\nLet’s see what the penultimate layer looks like:\n\nlearn.activation_stats.plot_layer_stats(-2)\n\n\n\n\n\n\n\n\nAgain, we’ve got most of our activations near zero. Let’s see what else we can do to improve training stability.\n\n\n1cycle Training\nOur initial weights are not well suited to the task we’re trying to solve. Therefore, it is dangerous to begin training with a high learning rate: we may very well make the training diverge instantly, as we’ve seen. We probably don’t want to end training with a high learning rate either, so that we don’t skip over a minimum. But we want to train at a high learning rate for the rest of the training period, because we’ll be able to train more quickly that way. Therefore, we should change the learning rate during training, from low, to high, and then back to low again.\nLeslie Smith (yes, the same guy that invented the learning rate finder!) developed this idea in his article “Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates”. He designed a schedule for learning rate separated into two phases: one where the learning rate grows from the minimum value to the maximum value (warmup), and one where it decreases back to the minimum value (annealing). Smith called this combination of approaches 1cycle training.\n1cycle training allows us to use a much higher maximum learning rate than other types of training, which gives two benefits:\n\nBy training with higher learning rates, we train faster—a phenomenon Smith named super-convergence.\nBy training with higher learning rates, we overfit less because we skip over the sharp local minima to end up in a smoother (and therefore more generalizable) part of the loss.\n\nThe second point is an interesting and subtle one; it is based on the observation that a model that generalizes well is one whose loss would not change very much if you changed the input by a small amount. If a model trains at a large learning rate for quite a while, and can find a good loss when doing so, it must have found an area that also generalizes well, because it is jumping around a lot from batch to batch (that is basically the definition of a high learning rate). The problem is that, as we have discussed, just jumping to a high learning rate is more likely to result in diverging losses, rather than seeing your losses improve. So we don’t jump straight to a high learning rate. Instead, we start at a low learning rate, where our losses do not diverge, and we allow the optimizer to gradually find smoother and smoother areas of our parameters by gradually going to higher and higher learning rates.\nThen, once we have found a nice smooth area for our parameters, we want to find the very best part of that area, which means we have to bring our learning rates down again. This is why 1cycle training has a gradual learning rate warmup, and a gradual learning rate cooldown. Many researchers have found that in practice this approach leads to more accurate models and trains more quickly. That is why it is the approach that is used by default for fine_tune in fastai.\nIn &lt;&gt; we’ll learn all about momentum in SGD. Briefly, momentum is a technique where the optimizer takes a step not only in the direction of the gradients, but also that continues in the direction of previous steps. Leslie Smith introduced the idea of cyclical momentums in “A Disciplined Approach to Neural Network Hyper-Parameters: Part 1”. It suggests that the momentum varies in the opposite direction of the learning rate: when we are at high learning rates, we use less momentum, and we use more again in the annealing phase.\nWe can use 1cycle training in fastai by calling fit_one_cycle:\n\ndef fit(epochs=1, lr=0.06):\n    learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy,\n                    metrics=accuracy, cbs=ActivationStats(with_hist=True))\n    learn.fit_one_cycle(epochs, lr)\n    return learn\n\n\nlearn = fit()\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.210838\n0.084827\n0.974300\n00:08\n\n\n\n\n\nWe’re finally making some progress! It’s giving us a reasonable accuracy now.\nWe can view the learning rate and momentum throughout training by calling plot_sched on learn.recorder. learn.recorder (as the name suggests) records everything that happens during training, including losses, metrics, and hyperparameters such as learning rate and momentum:\n\nlearn.recorder.plot_sched()\n\n\n\n\n\n\n\n\nSmith’s original 1cycle paper used a linear warmup and linear annealing. As you can see, we adapted the approach in fastai by combining it with another popular approach: cosine annealing. fit_one_cycle provides the following parameters you can adjust:\n\nlr_max:: The highest learning rate that will be used (this can also be a list of learning rates for each layer group, or a Python slice object containing the first and last layer group learning rates)\ndiv:: How much to divide lr_max by to get the starting learning rate\ndiv_final:: How much to divide lr_max by to get the ending learning rate\npct_start:: What percentage of the batches to use for the warmup\nmoms:: A tuple (mom1,mom2,mom3) where mom1 is the initial momentum, mom2 is the minimum momentum, and mom3 is the final momentum\n\nLet’s take a look at our layer stats again:\n\nlearn.activation_stats.plot_layer_stats(-2)\n\n\n\n\n\n\n\n\nThe percentage of near-zero weights is getting much better, although it’s still quite high.\nWe can see even more about what’s going on in our training using color_dim, passing it a layer index:\n\nlearn.activation_stats.color_dim(-2)\n\n\n\n\n\n\n\n\ncolor_dim was developed by fast.ai in conjunction with a student, Stefano Giomo. Stefano, who refers to the idea as the colorful dimension, provides an in-depth explanation of the history and details behind the method. The basic idea is to create a histogram of the activations of a layer, which we would hope would follow a smooth pattern such as the normal distribution (colorful_dist).\n\nTo create color_dim, we take the histogram shown on the left here, and convert it into just the colored representation shown at the bottom. Then we flip it on its side, as shown on the right. We found that the distribution is clearer if we take the log of the histogram values. Then, Stefano describes:\n\n: The final plot for each layer is made by stacking the histogram of the activations from each batch along the horizontal axis. So each vertical slice in the visualisation represents the histogram of activations for a single batch. The color intensity corresponds to the height of the histogram, in other words the number of activations in each histogram bin.\n\n&lt;&gt; shows how this all fits together.\n\nThis illustrates why log(f) is more colorful than f when f follows a normal distribution because taking a log changes the Gaussian in a quadratic, which isn’t as narrow.\nSo with that in mind, let’s take another look at the result for the penultimate layer:\n\nlearn.activation_stats.color_dim(-2)\n\n\n\n\n\n\n\n\nThis shows a classic picture of “bad training.” We start with nearly all activations at zero—that’s what we see at the far left, with all the dark blue. The bright yellow at the bottom represents the near-zero activations. Then, over the first few batches we see the number of nonzero activations exponentially increasing. But it goes too far, and collapses! We see the dark blue return, and the bottom becomes bright yellow again. It almost looks like training restarts from scratch. Then we see the activations increase again, and collapse again. After repeating this a few times, eventually we see a spread of activations throughout the range.\nIt’s much better if training can be smooth from the start. The cycles of exponential increase and then collapse tend to result in a lot of near-zero activations, resulting in slow training and poor final results. One way to solve this problem is to use batch normalization.\n\n\nBatch Normalization\nTo fix the slow training and poor final results we ended up with in the previous section, we need to fix the initial large percentage of near-zero activations, and then try to maintain a good distribution of activations throughout training.\nSergey Ioffe and Christian Szegedy presented a solution to this problem in the 2015 paper “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift”. In the abstract, they describe just the problem that we’ve seen:\n\n: Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization… We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs.\n\nTheir solution, they say is:\n\n: Making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization.\n\nThe paper caused great excitement as soon as it was released, because it included the chart in &lt;&gt;, which clearly demonstrated that batch normalization could train a model that was even more accurate than the current state of the art (the Inception architecture) and around 5x faster.\n\nBatch normalization (often just called batchnorm) works by taking an average of the mean and standard deviations of the activations of a layer and using those to normalize the activations. However, this can cause problems because the network might want some activations to be really high in order to make accurate predictions. So they also added two learnable parameters (meaning they will be updated in the SGD step), usually called gamma and beta. After normalizing the activations to get some new activation vector y, a batchnorm layer returns gamma*y + beta.\nThat’s why our activations can have any mean or variance, independent from the mean and standard deviation of the results of the previous layer. Those statistics are learned separately, making training easier on our model. The behavior is different during training and validation: during training, we use the mean and standard deviation of the batch to normalize the data, while during validation we instead use a running mean of the statistics calculated during training.\nLet’s add a batchnorm layer to conv:\n\ndef conv(ni, nf, ks=3, act=True):\n    layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)]\n    if act: layers.append(nn.ReLU())\n    layers.append(nn.BatchNorm2d(nf))\n    return nn.Sequential(*layers)\n\nand fit our model:\n\nlearn = fit()\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.130036\n0.055021\n0.986400\n00:10\n\n\n\n\n\nThat’s a great result! Let’s take a look at color_dim:\n\nlearn.activation_stats.color_dim(-4)\n\n\n\n\n\n\n\n\nThis is just what we hope to see: a smooth development of activations, with no “crashes.” Batchnorm has really delivered on its promise here! In fact, batchnorm has been so successful that we see it (or something very similar) in nearly all modern neural networks.\nAn interesting observation about models containing batch normalization layers is that they tend to generalize better than models that don’t contain them. Although we haven’t as yet seen a rigorous analysis of what’s going on here, most researchers believe that the reason for this is that batch normalization adds some extra randomness to the training process. Each mini-batch will have a somewhat different mean and standard deviation than other mini-batches. Therefore, the activations will be normalized by different values each time. In order for the model to make accurate predictions, it will have to learn to become robust to these variations. In general, adding additional randomization to the training process often helps.\nSince things are going so well, let’s train for a few more epochs and see how it goes. In fact, let’s increase the learning rate, since the abstract of the batchnorm paper claimed we should be able to “train at much higher learning rates”:\n\nlearn = fit(5, lr=0.1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.191731\n0.121738\n0.960900\n00:11\n\n\n1\n0.083739\n0.055808\n0.981800\n00:10\n\n\n2\n0.053161\n0.044485\n0.987100\n00:10\n\n\n3\n0.034433\n0.030233\n0.990200\n00:10\n\n\n4\n0.017646\n0.025407\n0.991200\n00:10\n\n\n\n\n\nAt this point, I think it’s fair to say we know how to recognize digits! It’s time to move on to something harder…"
  },
  {
    "objectID": "Fastbook/13_convolutions.html#conclusions",
    "href": "Fastbook/13_convolutions.html#conclusions",
    "title": "Convolutional Neural Networks",
    "section": "Conclusions",
    "text": "Conclusions\nWe’ve seen that convolutions are just a type of matrix multiplication, with two constraints on the weight matrix: some elements are always zero, and some elements are tied (forced to always have the same value). In &lt;&gt; we saw the eight requirements from the 1986 book Parallel Distributed Processing; one of them was “A pattern of connectivity among units.” That’s exactly what these constraints do: they enforce a certain pattern of connectivity.\nThese constraints allow us to use far fewer parameters in our model, without sacrificing the ability to represent complex visual features. That means we can train deeper models faster, with less overfitting. Although the universal approximation theorem shows that it should be possible to represent anything in a fully connected network in one hidden layer, we’ve seen now that in practice we can train much better models by being thoughtful about network architecture.\nConvolutions are by far the most common pattern of connectivity we see in neural nets (along with regular linear layers, which we refer to as fully connected), but it’s likely that many more will be discovered.\nWe’ve also seen how to interpret the activations of layers in the network to see whether training is going well or not, and how batchnorm helps regularize the training and makes it smoother. In the next chapter, we will use both of those layers to build the most popular architecture in computer vision: a residual network."
  },
  {
    "objectID": "Fastbook/13_convolutions.html#questionnaire",
    "href": "Fastbook/13_convolutions.html#questionnaire",
    "title": "Convolutional Neural Networks",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhat is a “feature”?\nWrite out the convolutional kernel matrix for a top edge detector.\nWrite out the mathematical operation applied by a 3×3 kernel to a single pixel in an image.\nWhat is the value of a convolutional kernel apply to a 3×3 matrix of zeros?\nWhat is “padding”?\nWhat is “stride”?\nCreate a nested list comprehension to complete any task that you choose.\nWhat are the shapes of the input and weight parameters to PyTorch’s 2D convolution?\nWhat is a “channel”?\nWhat is the relationship between a convolution and a matrix multiplication?\nWhat is a “convolutional neural network”?\nWhat is the benefit of refactoring parts of your neural network definition?\nWhat is Flatten? Where does it need to be included in the MNIST CNN? Why?\nWhat does “NCHW” mean?\nWhy does the third layer of the MNIST CNN have 7*7*(1168-16) multiplications?\nWhat is a “receptive field”?\nWhat is the size of the receptive field of an activation after two stride 2 convolutions? Why?\nRun conv-example.xlsx yourself and experiment with trace precedents.\nHave a look at Jeremy or Sylvain’s list of recent Twitter “like”s, and see if you find any interesting resources or ideas there.\nHow is a color image represented as a tensor?\nHow does a convolution work with a color input?\nWhat method can we use to see that data in DataLoaders?\nWhy do we double the number of filters after each stride-2 conv?\nWhy do we use a larger kernel in the first conv with MNIST (with simple_cnn)?\nWhat information does ActivationStats save for each layer?\nHow can we access a learner’s callback after training?\nWhat are the three statistics plotted by plot_layer_stats? What does the x-axis represent?\nWhy are activations near zero problematic?\nWhat are the upsides and downsides of training with a larger batch size?\nWhy should we avoid using a high learning rate at the start of training?\nWhat is 1cycle training?\nWhat are the benefits of training with a high learning rate?\nWhy do we want to use a low learning rate at the end of training?\nWhat is “cyclical momentum”?\nWhat callback tracks hyperparameter values during training (along with other information)?\nWhat does one column of pixels in the color_dim plot represent?\nWhat does “bad training” look like in color_dim? Why?\nWhat trainable parameters does a batch normalization layer contain?\nWhat statistics are used to normalize in batch normalization during training? How about during validation?\nWhy do models with batch normalization layers generalize better?\n\n\nFurther Research\n\nWhat features other than edge detectors have been used in computer vision (especially before deep learning became popular)?\nThere are other normalization layers available in PyTorch. Try them out and see what works best. Learn about why other normalization layers have been developed, and how they differ from batch normalization.\nTry moving the activation function after the batch normalization layer in conv. Does it make a difference? See what you can find out about what order is recommended, and why."
  },
  {
    "objectID": "Fastbook/02_production.html",
    "href": "Fastbook/02_production.html",
    "title": "From Model to Production",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n[[chapter_production]]\nThe six lines of code we saw in &lt;&gt; are just one small part of the process of using deep learning in practice. In this chapter, we’re going to use a computer vision example to look at the end-to-end process of creating a deep learning application. More specifically, we’re going to build a bear classifier! In the process, we’ll discuss the capabilities and constraints of deep learning, explore how to create datasets, look at possible gotchas when using deep learning in practice, and more. Many of the key points will apply equally well to other deep learning problems, such as those in &lt;&gt;. If you work through a problem similar in key respects to our example problems, we expect you to get excellent results with little code, quickly.\nLet’s start with how you should frame your problem."
  },
  {
    "objectID": "Fastbook/02_production.html#the-practice-of-deep-learning",
    "href": "Fastbook/02_production.html#the-practice-of-deep-learning",
    "title": "From Model to Production",
    "section": "The Practice of Deep Learning",
    "text": "The Practice of Deep Learning\nWe’ve seen that deep learning can solve a lot of challenging problems quickly and with little code. As a beginner, there’s a sweet spot of problems that are similar enough to our example problems that you can very quickly get extremely useful results. However, deep learning isn’t magic! The same 6 lines of code won’t work for every problem anyone can think of today. Underestimating the constraints and overestimating the capabilities of deep learning may lead to frustratingly poor results, at least until you gain some experience and can solve the problems that arise. Conversely, overestimating the constraints and underestimating the capabilities of deep learning may mean you do not attempt a solvable problem because you talk yourself out of it.\nWe often talk to people who underestimate both the constraints and the capabilities of deep learning. Both of these can be problems: underestimating the capabilities means that you might not even try things that could be very beneficial, and underestimating the constraints might mean that you fail to consider and react to important issues.\nThe best thing to do is to keep an open mind. If you remain open to the possibility that deep learning might solve part of your problem with less data or complexity than you expect, then it is possible to design a process where you can find the specific capabilities and constraints related to your particular problem as you work through the process. This doesn’t mean making any risky bets — we will show you how you can gradually roll out models so that they don’t create significant risks, and can even backtest them prior to putting them in production.\n\nStarting Your Project\nSo where should you start your deep learning journey? The most important thing is to ensure that you have some project to work on—it is only through working on your own projects that you will get real experience building and using models. When selecting a project, the most important consideration is data availability. Regardless of whether you are doing a project just for your own learning or for practical application in your organization, you want something where you can get started quickly. We have seen many students, researchers, and industry practitioners waste months or years while they attempt to find their perfect dataset. The goal is not to find the “perfect” dataset or project, but just to get started and iterate from there.\nIf you take this approach, then you will be on your third iteration of learning and improving while the perfectionists are still in the planning stages!\nWe also suggest that you iterate from end to end in your project; that is, don’t spend months fine-tuning your model, or polishing the perfect GUI, or labelling the perfect dataset… Instead, complete every step as well as you can in a reasonable amount of time, all the way to the end. For instance, if your final goal is an application that runs on a mobile phone, then that should be what you have after each iteration. But perhaps in the early iterations you take some shortcuts, for instance by doing all of the processing on a remote server, and using a simple responsive web application. By completing the project end to end, you will see where the trickiest bits are, and which bits make the biggest difference to the final result.\nAs you work through this book, we suggest that you complete lots of small experiments, by running and adjusting the notebooks we provide, at the same time that you gradually develop your own projects. That way, you will be getting experience with all of the tools and techniques that we’re explaining, as we discuss them.\n\ns: To make the most of this book, take the time to experiment between each chapter, be it on your own project or by exploring the notebooks we provide. Then try rewriting those notebooks from scratch on a new dataset. It’s only by practicing (and failing) a lot that you will get an intuition of how to train a model.\n\nBy using the end-to-end iteration approach you will also get a better understanding of how much data you really need. For instance, you may find you can only easily get 200 labeled data items, and you can’t really know until you try whether that’s enough to get the performance you need for your application to work well in practice.\nIn an organizational context you will be able to show your colleagues that your idea can really work by showing them a real working prototype. We have repeatedly observed that this is the secret to getting good organizational buy-in for a project.\nSince it is easiest to get started on a project where you already have data available, that means it’s probably easiest to get started on a project related to something you are already doing, because you already have data about things that you are doing. For instance, if you work in the music business, you may have access to many recordings. If you work as a radiologist, you probably have access to lots of medical images. If you are interested in wildlife preservation, you may have access to lots of images of wildlife.\nSometimes, you have to get a bit creative. Maybe you can find some previous machine learning project, such as a Kaggle competition, that is related to your field of interest. Sometimes, you have to compromise. Maybe you can’t find the exact data you need for the precise project you have in mind; but you might be able to find something from a similar domain, or measured in a different way, tackling a slightly different problem. Working on these kinds of similar projects will still give you a good understanding of the overall process, and may help you identify other shortcuts, data sources, and so forth.\nEspecially when you are just starting out with deep learning, it’s not a good idea to branch out into very different areas, to places that deep learning has not been applied to before. That’s because if your model does not work at first, you will not know whether it is because you have made a mistake, or if the very problem you are trying to solve is simply not solvable with deep learning. And you won’t know where to look to get help. Therefore, it is best at first to start with something where you can find an example online where somebody has had good results with something that is at least somewhat similar to what you are trying to achieve, or where you can convert your data into a format similar to what someone else has used before (such as creating an image from your data). Let’s have a look at the state of deep learning, just so you know what kinds of things deep learning is good at right now.\n\n\nThe State of Deep Learning\nLet’s start by considering whether deep learning can be any good at the problem you are looking to work on. This section provides a summary of the state of deep learning at the start of 2020. However, things move very fast, and by the time you read this some of these constraints may no longer exist. We will try to keep the book’s website up-to-date; in addition, a Google search for “what can AI do now” is likely to provide current information.\n\nComputer vision\nThere are many domains in which deep learning has not been used to analyze images yet, but those where it has been tried have nearly universally shown that computers can recognize what items are in an image at least as well as people can—even specially trained people, such as radiologists. This is known as object recognition. Deep learning is also good at recognizing where objects in an image are, and can highlight their locations and name each found object. This is known as object detection (there is also a variant of this that we saw in &lt;&gt;, where every pixel is categorized based on what kind of object it is part of—this is called segmentation). Deep learning algorithms are generally not good at recognizing images that are significantly different in structure or style to those used to train the model. For instance, if there were no black-and-white images in the training data, the model may do poorly on black-and-white images. Similarly, if the training data did not contain hand-drawn images, then the model will probably do poorly on hand-drawn images. There is no general way to check what types of images are missing in your training set, but we will show in this chapter some ways to try to recognize when unexpected image types arise in the data when the model is being used in production (this is known as checking for out-of-domain data).\nOne major challenge for object detection systems is that image labelling can be slow and expensive. There is a lot of work at the moment going into tools to try to make this labelling faster and easier, and to require fewer handcrafted labels to train accurate object detection models. One approach that is particularly helpful is to synthetically generate variations of input images, such as by rotating them or changing their brightness and contrast; this is called data augmentation and also works well for text and other types of models. We will be discussing it in detail in this chapter.\nAnother point to consider is that although your problem might not look like a computer vision problem, it might be possible with a little imagination to turn it into one. For instance, if what you are trying to classify are sounds, you might try converting the sounds into images of their acoustic waveforms and then training a model on those images.\n\n\nText (natural language processing)\nComputers are very good at classifying both short and long documents based on categories such as spam or not spam, sentiment (e.g., is the review positive or negative), author, source website, and so forth. We are not aware of any rigorous work done in this area to compare them to humans, but anecdotally it seems to us that deep learning performance is similar to human performance on these tasks. Deep learning is also very good at generating context-appropriate text, such as replies to social media posts, and imitating a particular author’s style. It’s good at making this content compelling to humans too—in fact, even more compelling than human-generated text. However, deep learning is currently not good at generating correct responses! We don’t currently have a reliable way to, for instance, combine a knowledge base of medical information with a deep learning model for generating medically correct natural language responses. This is very dangerous, because it is so easy to create content that appears to a layman to be compelling, but actually is entirely incorrect.\nAnother concern is that context-appropriate, highly compelling responses on social media could be used at massive scale—thousands of times greater than any troll farm previously seen—to spread disinformation, create unrest, and encourage conflict. As a rule of thumb, text generation models will always be technologically a bit ahead of models recognizing automatically generated text. For instance, it is possible to use a model that can recognize artificially generated content to actually improve the generator that creates that content, until the classification model is no longer able to complete its task.\nDespite these issues, deep learning has many applications in NLP: it can be used to translate text from one language to another, summarize long documents into something that can be digested more quickly, find all mentions of a concept of interest, and more. Unfortunately, the translation or summary could well include completely incorrect information! However, the performance is already good enough that many people are using these systems—for instance, Google’s online translation system (and every other online service we are aware of) is based on deep learning.\n\n\nCombining text and images\nThe ability of deep learning to combine text and images into a single model is, generally, far better than most people intuitively expect. For example, a deep learning model can be trained on input images with output captions written in English, and can learn to generate surprisingly appropriate captions automatically for new images! But again, we have the same warning that we discussed in the previous section: there is no guarantee that these captions will actually be correct.\nBecause of this serious issue, we generally recommend that deep learning be used not as an entirely automated process, but as part of a process in which the model and a human user interact closely. This can potentially make humans orders of magnitude more productive than they would be with entirely manual methods, and actually result in more accurate processes than using a human alone. For instance, an automatic system can be used to identify potential stroke victims directly from CT scans, and send a high-priority alert to have those scans looked at quickly. There is only a three-hour window to treat strokes, so this fast feedback loop could save lives. At the same time, however, all scans could continue to be sent to radiologists in the usual way, so there would be no reduction in human input. Other deep learning models could automatically measure items seen on the scans, and insert those measurements into reports, warning the radiologists about findings that they may have missed, and telling them about other cases that might be relevant.\n\n\nTabular data\nFor analyzing time series and tabular data, deep learning has recently been making great strides. However, deep learning is generally used as part of an ensemble of multiple types of model. If you already have a system that is using random forests or gradient boosting machines (popular tabular modeling tools that you will learn about soon), then switching to or adding deep learning may not result in any dramatic improvement. Deep learning does greatly increase the variety of columns that you can include—for example, columns containing natural language (book titles, reviews, etc.), and high-cardinality categorical columns (i.e., something that contains a large number of discrete choices, such as zip code or product ID). On the down side, deep learning models generally take longer to train than random forests or gradient boosting machines, although this is changing thanks to libraries such as RAPIDS, which provides GPU acceleration for the whole modeling pipeline. We cover the pros and cons of all these methods in detail in &lt;&gt;.\n\n\nRecommendation systems\nRecommendation systems are really just a special type of tabular data. In particular, they generally have a high-cardinality categorical variable representing users, and another one representing products (or something similar). A company like Amazon represents every purchase that has ever been made by its customers as a giant sparse matrix, with customers as the rows and products as the columns. Once they have the data in this format, data scientists apply some form of collaborative filtering to fill in the matrix. For example, if customer A buys products 1 and 10, and customer B buys products 1, 2, 4, and 10, the engine will recommend that A buy 2 and 4. Because deep learning models are good at handling high-cardinality categorical variables, they are quite good at handling recommendation systems. They particularly come into their own, just like for tabular data, when combining these variables with other kinds of data, such as natural language or images. They can also do a good job of combining all of these types of information with additional metadata represented as tables, such as user information, previous transactions, and so forth.\nHowever, nearly all machine learning approaches have the downside that they only tell you what products a particular user might like, rather than what recommendations would be helpful for a user. Many kinds of recommendations for products a user might like may not be at all helpful—for instance, if the user is already familiar with the products, or if they are simply different packagings of products they have already purchased (such as a boxed set of novels, when they already have each of the items in that set). Jeremy likes reading books by Terry Pratchett, and for a while Amazon was recommending nothing but Terry Pratchett books to him (see &lt;&gt;), which really wasn’t helpful because he already was aware of these books!\n\n\n\nOther data types\nOften you will find that domain-specific data types fit very nicely into existing categories. For instance, protein chains look a lot like natural language documents, in that they are long sequences of discrete tokens with complex relationships and meaning throughout the sequence. And indeed, it does turn out that using NLP deep learning methods is the current state-of-the-art approach for many types of protein analysis. As another example, sounds can be represented as spectrograms, which can be treated as images; standard deep learning approaches for images turn out to work really well on spectrograms.\n\n\n\nThe Drivetrain Approach\nThere are many accurate models that are of no use to anyone, and many inaccurate models that are highly useful. To ensure that your modeling work is useful in practice, you need to consider how your work will be used. In 2012 Jeremy, along with Margit Zwemer and Mike Loukides, introduced a method called the Drivetrain Approach for thinking about this issue.\nThe Drivetrain Approach, illustrated in &lt;&gt;, was described in detail in “Designing Great Data Products”. The basic idea is to start with considering your objective, then think about what actions you can take to meet that objective and what data you have (or can acquire) that can help, and then build a model that you can use to determine the best actions to take to get the best results in terms of your objective.\n\nConsider a model in an autonomous vehicle: you want to help a car drive safely from point A to point B without human intervention. Great predictive modeling is an important part of the solution, but it doesn’t stand on its own; as products become more sophisticated, it disappears into the plumbing. Someone using a self-driving car is completely unaware of the hundreds (if not thousands) of models and the petabytes of data that make it work. But as data scientists build increasingly sophisticated products, they need a systematic design approach.\nWe use data not just to generate more data (in the form of predictions), but to produce actionable outcomes. That is the goal of the Drivetrain Approach. Start by defining a clear objective. For instance, Google, when creating their first search engine, considered “What is the user’s main objective in typing in a search query?” This led them to their objective, which was to “show the most relevant search result.” The next step is to consider what levers you can pull (i.e., what actions you can take) to better achieve that objective. In Google’s case, that was the ranking of the search results. The third step was to consider what new data they would need to produce such a ranking; they realized that the implicit information regarding which pages linked to which other pages could be used for this purpose. Only after these first three steps do we begin thinking about building the predictive models. Our objective and available levers, what data we already have and what additional data we will need to collect, determine the models we can build. The models will take both the levers and any uncontrollable variables as their inputs; the outputs from the models can be combined to predict the final state for our objective.\nLet’s consider another example: recommendation systems. The objective of a recommendation engine is to drive additional sales by surprising and delighting the customer with recommendations of items they would not have purchased without the recommendation. The lever is the ranking of the recommendations. New data must be collected to generate recommendations that will cause new sales. This will require conducting many randomized experiments in order to collect data about a wide range of recommendations for a wide range of customers. This is a step that few organizations take; but without it, you don’t have the information you need to actually optimize recommendations based on your true objective (more sales!).\nFinally, you could build two models for purchase probabilities, conditional on seeing or not seeing a recommendation. The difference between these two probabilities is a utility function for a given recommendation to a customer. It will be low in cases where the algorithm recommends a familiar book that the customer has already rejected (both components are small) or a book that they would have bought even without the recommendation (both components are large and cancel each other out).\nAs you can see, in practice often the practical implementation of your models will require a lot more than just training a model! You’ll often need to run experiments to collect more data, and consider how to incorporate your models into the overall system you’re developing. Speaking of data, let’s now focus on how to find data for your project."
  },
  {
    "objectID": "Fastbook/02_production.html#gathering-data",
    "href": "Fastbook/02_production.html#gathering-data",
    "title": "From Model to Production",
    "section": "Gathering Data",
    "text": "Gathering Data\nFor many types of projects, you may be able to find all the data you need online. The project we’ll be completing in this chapter is a bear detector. It will discriminate between three types of bear: grizzly, black, and teddy bears. There are many images on the internet of each type of bear that we can use. We just need a way to find them and download them. We’ve provided a tool you can use for this purpose, so you can follow along with this chapter and create your own image recognition application for whatever kinds of objects you’re interested in. In the fast.ai course, thousands of students have presented their work in the course forums, displaying everything from hummingbird varieties in Trinidad to bus types in Panama—one student even created an application that would help his fiancée recognize his 16 cousins during Christmas vacation!\nAt the time of writing, Bing Image Search is the best option we know of for finding and downloading images. It’s free for up to 1,000 queries per month, and each query can download up to 150 images. However, something better might have come along between when we wrote this and when you’re reading the book, so be sure to check out the book’s website for our current recommendation.\n\nimportant: Keeping in Touch With the Latest Services: Services that can be used for creating datasets come and go all the time, and their features, interfaces, and pricing change regularly too. In this section, we’ll show how to use the Bing Image Search API available at the time this book was written. We’ll be providing more options and more up to date information on the book’s website, so be sure to have a look there now to get the most current information on how to download images from the web to create a dataset for deep learning."
  },
  {
    "objectID": "Fastbook/02_production.html#from-data-to-dataloaders",
    "href": "Fastbook/02_production.html#from-data-to-dataloaders",
    "title": "From Model to Production",
    "section": "From Data to DataLoaders",
    "text": "From Data to DataLoaders\nDataLoaders is a thin class that just stores whatever DataLoader objects you pass to it, and makes them available as train and valid. Although it’s a very simple class, it’s very important in fastai: it provides the data for your model. The key functionality in DataLoaders is provided with just these four lines of code (it has some other minor functionality we’ll skip over for now):\nclass DataLoaders(GetAttr):\n    def __init__(self, *loaders): self.loaders = loaders\n    def __getitem__(self, i): return self.loaders[i]\n    train,valid = add_props(lambda i,self: self[i])\n\njargon: DataLoaders: A fastai class that stores multiple DataLoader objects you pass to it, normally a train and a valid, although it’s possible to have as many as you like. The first two are made available as properties.\n\nLater in the book you’ll also learn about the Dataset and Datasets classes, which have the same relationship.\nTo turn our downloaded data into a DataLoaders object we need to tell fastai at least four things:\n\nWhat kinds of data we are working with\nHow to get the list of items\nHow to label these items\nHow to create the validation set\n\nSo far we have seen a number of factory methods for particular combinations of these things, which are convenient when you have an application and data structure that happen to fit into those predefined methods. For when you don’t, fastai has an extremely flexible system called the data block API. With this API you can fully customize every stage of the creation of your DataLoaders. Here is what we need to create a DataLoaders for the dataset that we just downloaded:\n\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\n\nLet’s look at each of these arguments in turn. First we provide a tuple where we specify what types we want for the independent and dependent variables:\nblocks=(ImageBlock, CategoryBlock)\nThe independent variable is the thing we are using to make predictions from, and the dependent variable is our target. In this case, our independent variables are images, and our dependent variables are the categories (type of bear) for each image. We will see many other types of block in the rest of this book.\nFor this DataLoaders our underlying items will be file paths. We have to tell fastai how to get a list of those files. The get_image_files function takes a path, and returns a list of all of the images in that path (recursively, by default):\nget_items=get_image_files\nOften, datasets that you download will already have a validation set defined. Sometimes this is done by placing the images for the training and validation sets into different folders. Sometimes it is done by providing a CSV file in which each filename is listed along with which dataset it should be in. There are many ways that this can be done, and fastai provides a very general approach that allows you to use one of its predefined classes for this, or to write your own. In this case, however, we simply want to split our training and validation sets randomly. However, we would like to have the same training/validation split each time we run this notebook, so we fix the random seed (computers don’t really know how to create random numbers at all, but simply create lists of numbers that look random; if you provide the same starting point for that list each time—called the seed—then you will get the exact same list each time):\nsplitter=RandomSplitter(valid_pct=0.2, seed=42)\nThe independent variable is often referred to as x and the dependent variable is often referred to as y. Here, we are telling fastai what function to call to create the labels in our dataset:\nget_y=parent_label\nparent_label is a function provided by fastai that simply gets the name of the folder a file is in. Because we put each of our bear images into folders based on the type of bear, this is going to give us the labels that we need.\nOur images are all different sizes, and this is a problem for deep learning: we don’t feed the model one image at a time but several of them (what we call a mini-batch). To group them in a big array (usually called a tensor) that is going to go through our model, they all need to be of the same size. So, we need to add a transform which will resize these images to the same size. Item transforms are pieces of code that run on each individual item, whether it be an image, category, or so forth. fastai includes many predefined transforms; we use the Resize transform here:\nitem_tfms=Resize(128)\nThis command has given us a DataBlock object. This is like a template for creating a DataLoaders. We still need to tell fastai the actual source of our data—in this case, the path where the images can be found:\n\ndls = bears.dataloaders(path)\n\nA DataLoaders includes validation and training DataLoaders. DataLoader is a class that provides batches of a few items at a time to the GPU. We’ll be learning a lot more about this class in the next chapter. When you loop through a DataLoader fastai will give you 64 (by default) items at a time, all stacked up into a single tensor. We can take a look at a few of those items by calling the show_batch method on a DataLoader:\n\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\n\n\nBy default Resize crops the images to fit a square shape of the size requested, using the full width or height. This can result in losing some important details. Alternatively, you can ask fastai to pad the images with zeros (black), or squish/stretch them:\n\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\n\n\n\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros'))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\n\n\nAll of these approaches seem somewhat wasteful, or problematic. If we squish or stretch the images they end up as unrealistic shapes, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy. If we crop the images then we remove some of the features that allow us to perform recognition. For instance, if we were trying to recognize breeds of dog or cat, we might end up cropping out a key part of the body or the face necessary to distinguish between similar breeds. If we pad the images then we have a whole lot of empty space, which is just wasted computation for our model and results in a lower effective resolution for the part of the image we actually use.\nInstead, what we normally do in practice is to randomly select part of the image, and crop to just that part. On each epoch (which is one complete pass through all of our images in the dataset) we randomly select a different part of each image. This means that our model can learn to focus on, and recognize, different features in our images. It also reflects how images work in the real world: different photos of the same thing may be framed in slightly different ways.\nIn fact, an entirely untrained neural network knows nothing whatsoever about how images behave. It doesn’t even recognize that when an object is rotated by one degree, it still is a picture of the same thing! So actually training the neural network with examples of images where the objects are in slightly different places and slightly different sizes helps it to understand the basic concept of what an object is, and how it can be represented in an image.\nHere’s another example where we replace Resize with RandomResizedCrop, which is the transform that provides the behavior we just described. The most important parameter to pass in is min_scale, which determines how much of the image to select at minimum each time:\n\nbears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=4, nrows=1, unique=True)\n\n\n\n\n\n\n\n\nWe used unique=True to have the same image repeated with different versions of this RandomResizedCrop transform. This is a specific example of a more general technique, called data augmentation.\n\nData Augmentation\nData augmentation refers to creating random variations of our input data, such that they appear different, but do not actually change the meaning of the data. Examples of common data augmentation techniques for images are rotation, flipping, perspective warping, brightness changes and contrast changes. For natural photo images such as the ones we are using here, a standard set of augmentations that we have found work pretty well are provided with the aug_transforms function. Because our images are now all the same size, we can apply these augmentations to an entire batch of them using the GPU, which will save a lot of time. To tell fastai we want to use these transforms on a batch, we use the batch_tfms parameter (note that we’re not using RandomResizedCrop in this example, so you can see the differences more clearly; we’re also using double the amount of augmentation compared to the default, for the same reason):\n\nbears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=8, nrows=2, unique=True)\n\n\n\n\n\n\n\n\nNow that we have assembled our data in a format fit for model training, let’s actually train an image classifier using it."
  },
  {
    "objectID": "Fastbook/02_production.html#training-your-model-and-using-it-to-clean-your-data",
    "href": "Fastbook/02_production.html#training-your-model-and-using-it-to-clean-your-data",
    "title": "From Model to Production",
    "section": "Training Your Model, and Using It to Clean Your Data",
    "text": "Training Your Model, and Using It to Clean Your Data\nTime to use the same lines of code as in &lt;&gt; to train our bear classifier.\nWe don’t have a lot of data for our problem (150 pictures of each sort of bear at most), so to train our model, we’ll use RandomResizedCrop with an image size of 224 px, which is fairly standard for image classification, and default aug_transforms:\n\nbears = bears.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\ndls = bears.dataloaders(path)\n\nWe can now create our Learner and fine-tune it in the usual way:\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.235733\n0.212541\n0.087302\n00:05\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.213371\n0.112450\n0.023810\n00:05\n\n\n1\n0.173855\n0.072306\n0.023810\n00:06\n\n\n2\n0.147096\n0.039068\n0.015873\n00:06\n\n\n3\n0.123984\n0.026801\n0.015873\n00:06\n\n\n\n\n\nNow let’s see whether the mistakes the model is making are mainly thinking that grizzlies are teddies (that would be bad for safety!), or that grizzlies are black bears, or something else. To visualize this, we can create a confusion matrix:\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\nThe rows represent all the black, grizzly, and teddy bears in our dataset, respectively. The columns represent the images which the model predicted as black, grizzly, and teddy bears, respectively. Therefore, the diagonal of the matrix shows the images which were classified correctly, and the off-diagonal cells represent those which were classified incorrectly. This is one of the many ways that fastai allows you to view the results of your model. It is (of course!) calculated using the validation set. With the color-coding, the goal is to have white everywhere except the diagonal, where we want dark blue. Our bear classifier isn’t making many mistakes!\nIt’s helpful to see where exactly our errors are occurring, to see whether they’re due to a dataset problem (e.g., images that aren’t bears at all, or are labeled incorrectly, etc.), or a model problem (perhaps it isn’t handling images taken with unusual lighting, or from a different angle, etc.). To do this, we can sort our images by their loss.\nThe loss is a number that is higher if the model is incorrect (especially if it’s also confident of its incorrect answer), or if it’s correct, but not confident of its correct answer. In a couple of chapters we’ll learn in depth how loss is calculated and used in the training process. For now, plot_top_losses shows us the images with the highest loss in our dataset. As the title of the output says, each image is labeled with four things: prediction, actual (target label), loss, and probability. The probability here is the confidence level, from zero to one, that the model has assigned to its prediction:\n\ninterp.plot_top_losses(5, nrows=1)\n\n\n\n\n\n\n\n\nThis output shows that the image with the highest loss is one that has been predicted as “grizzly” with high confidence. However, it’s labeled (based on our Bing image search) as “black.” We’re not bear experts, but it sure looks to us like this label is incorrect! We should probably change its label to “grizzly.”\nThe intuitive approach to doing data cleaning is to do it before you train a model. But as you’ve seen in this case, a model can actually help you find data issues more quickly and easily. So, we normally prefer to train a quick and simple model first, and then use it to help us with data cleaning.\nfastai includes a handy GUI for data cleaning called ImageClassifierCleaner that allows you to choose a category and the training versus validation set and view the highest-loss images (in order), along with menus to allow images to be selected for removal or relabeling:\n\n#hide_output\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n#hide\n# for idx in cleaner.delete(): cleaner.fns[idx].unlink()\n# for idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\nWe can see that amongst our “black bears” is an image that contains two bears: one grizzly, one black. So, we should choose &lt;Delete&gt; in the menu under this image. ImageClassifierCleaner doesn’t actually do the deleting or changing of labels for you; it just returns the indices of items to change. So, for instance, to delete (unlink) all images selected for deletion, we would run:\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nTo move images for which we’ve selected a different category, we would run:\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\ns: Cleaning the data and getting it ready for your model are two of the biggest challenges for data scientists; they say it takes 90% of their time. The fastai library aims to provide tools that make it as easy as possible.\n\nWe’ll be seeing more examples of model-driven data cleaning throughout this book. Once we’ve cleaned up our data, we can retrain our model. Try it yourself, and see if your accuracy improves!\n\nnote: No Need for Big Data: After cleaning the dataset using these steps, we generally are seeing 100% accuracy on this task. We even see that result when we download a lot fewer images than the 150 per class we’re using here. As you can see, the common complaint that you need massive amounts of data to do deep learning can be a very long way from the truth!\n\nNow that we have trained our model, let’s see how we can deploy it to be used in practice."
  },
  {
    "objectID": "Fastbook/02_production.html#turning-your-model-into-an-online-application",
    "href": "Fastbook/02_production.html#turning-your-model-into-an-online-application",
    "title": "From Model to Production",
    "section": "Turning Your Model into an Online Application",
    "text": "Turning Your Model into an Online Application\nWe are now going to look at what it takes to turn this model into a working online application. We will just go as far as creating a basic working prototype; we do not have the scope in this book to teach you all the details of web application development generally.\n\nUsing the Model for Inference\nOnce you’ve got a model you’re happy with, you need to save it, so that you can then copy it over to a server where you’ll use it in production. Remember that a model consists of two parts: the architecture and the trained parameters. The easiest way to save the model is to save both of these, because that way when you load a model you can be sure that you have the matching architecture and parameters. To save both parts, use the export method.\nThis method even saves the definition of how to create your DataLoaders. This is important, because otherwise you would have to redefine how to transform your data in order to use your model in production. fastai automatically uses your validation set DataLoader for inference by default, so your data augmentation will not be applied, which is generally what you want.\nWhen you call export, fastai will save a file called “export.pkl”:\n\nlearn.export()\n\nLet’s check that the file exists, by using the ls method that fastai adds to Python’s Path class:\n\npath = Path()\npath.ls(file_exts='.pkl')\n\n(#1) [Path('export.pkl')]\n\n\nYou’ll need this file wherever you deploy your app to. For now, let’s try to create a simple app within our notebook.\nWhen we use a model for getting predictions, instead of training, we call it inference. To create our inference learner from the exported file, we use load_learner (in this case, this isn’t really necessary, since we already have a working Learner in our notebook; we’re just doing it here so you can see the whole process end-to-end):\n\nlearn_inf = load_learner(path/'export.pkl')\n\nWhen we’re doing inference, we’re generally just getting predictions for one image at a time. To do this, pass a filename to predict:\n\nlearn_inf.predict('images/grizzly.jpg')\n\n\n\n\n('grizzly', tensor(1), tensor([9.0767e-06, 9.9999e-01, 1.5748e-07]))\n\n\nThis has returned three things: the predicted category in the same format you originally provided (in this case that’s a string), the index of the predicted category, and the probabilities of each category. The last two are based on the order of categories in the vocab of the DataLoaders; that is, the stored list of all possible categories. At inference time, you can access the DataLoaders as an attribute of the Learner:\n\nlearn_inf.dls.vocab\n\n(#3) ['black','grizzly','teddy']\n\n\nWe can see here that if we index into the vocab with the integer returned by predict then we get back “grizzly,” as expected. Also, note that if we index into the list of probabilities, we see a nearly 1.00 probability that this is a grizzly.\nWe know how to make predictions from our saved model, so we have everything we need to start building our app. We can do it directly in a Jupyter notebook.\n\n\nCreating a Notebook App from the Model\nTo use our model in an application, we can simply treat the predict method as a regular function. Therefore, creating an app from the model can be done using any of the myriad of frameworks and techniques available to application developers.\nHowever, most data scientists are not familiar with the world of web application development. So let’s try using something that you do, at this point, know: it turns out that we can create a complete working web application using nothing but Jupyter notebooks! The two things we need to make this happen are:\n\nIPython widgets (ipywidgets)\nVoilà\n\nIPython widgets are GUI components that bring together JavaScript and Python functionality in a web browser, and can be created and used within a Jupyter notebook. For instance, the image cleaner that we saw earlier in this chapter is entirely written with IPython widgets. However, we don’t want to require users of our application to run Jupyter themselves.\nThat is why Voilà exists. It is a system for making applications consisting of IPython widgets available to end users, without them having to use Jupyter at all. Voilà is taking advantage of the fact that a notebook already is a kind of web application, just a rather complex one that depends on another web application: Jupyter itself. Essentially, it helps us automatically convert the complex web application we’ve already implicitly made (the notebook) into a simpler, easier-to-deploy web application, which functions like a normal web application rather than like a notebook.\nBut we still have the advantage of developing in a notebook, so with ipywidgets, we can build up our GUI step by step. We will use this approach to create a simple image classifier. First, we need a file upload widget:\n\n#hide_output\nbtn_upload = widgets.FileUpload()\nbtn_upload\n\n\n\n\n\nNow we can grab the image:\n\n#hide\n# For the book, we can't actually click an upload button, so we fake it\nbtn_upload = SimpleNamespace(data = ['images/grizzly.jpg'])\n\n\nimg = PILImage.create(btn_upload.data[-1])\n\n\nWe can use an Output widget to display it:\n\n#hide_output\nout_pl = widgets.Output()\nout_pl.clear_output()\nwith out_pl: display(img.to_thumb(128,128))\nout_pl\n\n\nThen we can get our predictions:\n\npred,pred_idx,probs = learn_inf.predict(img)\n\n\n\n\nand use a Label to display them:\n\n#hide_output\nlbl_pred = widgets.Label()\nlbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\nlbl_pred\n\n\n\n\nPrediction: grizzly; Probability: 1.0000\nWe’ll need a button to do the classification. It looks exactly like the upload button:\n\n#hide_output\nbtn_run = widgets.Button(description='Classify')\nbtn_run\n\n\n\n\nWe’ll also need a click event handler; that is, a function that will be called when it’s pressed. We can just copy over the lines of code from above:\n\ndef on_click_classify(change):\n    img = PILImage.create(btn_upload.data[-1])\n    out_pl.clear_output()\n    with out_pl: display(img.to_thumb(128,128))\n    pred,pred_idx,probs = learn_inf.predict(img)\n    lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\n\nbtn_run.on_click(on_click_classify)\n\nYou can test the button now by pressing it, and you should see the image and predictions update automatically!\nWe can now put them all in a vertical box (VBox) to complete our GUI:\n\n#hide\n#Putting back btn_upload to a widget for next cell\nbtn_upload = widgets.FileUpload()\n\n\n#hide_output\nVBox([widgets.Label('Select your bear!'), \n      btn_upload, btn_run, out_pl, lbl_pred])\n\n\n\n\n\nWe have written all the code necessary for our app. The next step is to convert it into something we can deploy.\n\n\nTurning Your Notebook into a Real App\n\n#hide\n# !pip install voila\n# !jupyter serverextension enable --sys-prefix voila \n\nNow that we have everything working in this Jupyter notebook, we can create our application. To do this, start a new notebook and add to it only the code needed to create and show the widgets that you need, and markdown for any text that you want to appear. Have a look at the bear_classifier notebook in the book’s repo to see the simple notebook application we created.\nNext, install Voilà if you haven’t already, by copying these lines into a notebook cell and executing it:\n!pip install voila\n!jupyter serverextension enable --sys-prefix voila\nCells that begin with a ! do not contain Python code, but instead contain code that is passed to your shell (bash, Windows PowerShell, etc.). If you are comfortable using the command line, which we’ll discuss more later in this book, you can of course simply type these two lines (without the ! prefix) directly into your terminal. In this case, the first line installs the voila library and application, and the second connects it to your existing Jupyter notebook.\nVoilà runs Jupyter notebooks just like the Jupyter notebook server you are using now does, but it also does something very important: it removes all of the cell inputs, and only shows output (including ipywidgets), along with your markdown cells. So what’s left is a web application! To view your notebook as a Voilà web application, replace the word “notebooks” in your browser’s URL with: “voila/render”. You will see the same content as your notebook, but without any of the code cells.\nOf course, you don’t need to use Voilà or ipywidgets. Your model is just a function you can call (pred,pred_idx,probs = learn.predict(img)), so you can use it with any framework, hosted on any platform. And you can take something you’ve prototyped in ipywidgets and Voilà and later convert it into a regular web application. We’re showing you this approach in the book because we think it’s a great way for data scientists and other folks that aren’t web development experts to create applications from their models.\nWe have our app, now let’s deploy it!\n\n\nDeploying your app\nAs you now know, you need a GPU to train nearly any useful deep learning model. So, do you need a GPU to use that model in production? No! You almost certainly do not need a GPU to serve your model in production. There are a few reasons for this:\n\nAs we’ve seen, GPUs are only useful when they do lots of identical work in parallel. If you’re doing (say) image classification, then you’ll normally be classifying just one user’s image at a time, and there isn’t normally enough work to do in a single image to keep a GPU busy for long enough for it to be very efficient. So, a CPU will often be more cost-effective.\nAn alternative could be to wait for a few users to submit their images, and then batch them up and process them all at once on a GPU. But then you’re asking your users to wait, rather than getting answers straight away! And you need a high-volume site for this to be workable. If you do need this functionality, you can use a tool such as Microsoft’s ONNX Runtime, or AWS Sagemaker\nThe complexities of dealing with GPU inference are significant. In particular, the GPU’s memory will need careful manual management, and you’ll need a careful queueing system to ensure you only process one batch at a time.\nThere’s a lot more market competition in CPU than GPU servers, as a result of which there are much cheaper options available for CPU servers.\n\nBecause of the complexity of GPU serving, many systems have sprung up to try to automate this. However, managing and running these systems is also complex, and generally requires compiling your model into a different form that’s specialized for that system. It’s typically preferable to avoid dealing with this complexity until/unless your app gets popular enough that it makes clear financial sense for you to do so.\nFor at least the initial prototype of your application, and for any hobby projects that you want to show off, you can easily host them for free. The best place and the best way to do this will vary over time, so check the book’s website for the most up-to-date recommendations. As we’re writing this book in early 2020 the simplest (and free!) approach is to use Binder. To publish your web app on Binder, you follow these steps:\n\nAdd your notebook to a GitHub repository.\nPaste the URL of that repo into Binder’s URL, as shown in &lt;&gt;.\nChange the File dropdown to instead select URL.\nIn the “URL to open” field, enter /voila/render/name.ipynb (replacing name with the name of for your notebook).\nClick the clickboard button at the bottom right to copy the URL and paste it somewhere safe.\nClick Launch.\n\n\nThe first time you do this, Binder will take around 5 minutes to build your site. Behind the scenes, it is finding a virtual machine that can run your app, allocating storage, collecting the files needed for Jupyter, for your notebook, and for presenting your notebook as a web application.\nFinally, once it has started the app running, it will navigate your browser to your new web app. You can share the URL you copied to allow others to access your app as well.\nFor other (both free and paid) options for deploying your web app, be sure to take a look at the book’s website.\nYou may well want to deploy your application onto mobile devices, or edge devices such as a Raspberry Pi. There are a lot of libraries and frameworks that allow you to integrate a model directly into a mobile application. However, these approaches tend to require a lot of extra steps and boilerplate, and do not always support all the PyTorch and fastai layers that your model might use. In addition, the work you do will depend on what kind of mobile devices you are targeting for deployment—you might need to do some work to run on iOS devices, different work to run on newer Android devices, different work for older Android devices, etc. Instead, we recommend wherever possible that you deploy the model itself to a server, and have your mobile or edge application connect to it as a web service.\nThere are quite a few upsides to this approach. The initial installation is easier, because you only have to deploy a small GUI application, which connects to the server to do all the heavy lifting. More importantly perhaps, upgrades of that core logic can happen on your server, rather than needing to be distributed to all of your users. Your server will have a lot more memory and processing capacity than most edge devices, and it is far easier to scale those resources if your model becomes more demanding. The hardware that you will have on a server is also going to be more standard and more easily supported by fastai and PyTorch, so you don’t have to compile your model into a different form.\nThere are downsides too, of course. Your application will require a network connection, and there will be some latency each time the model is called. (It takes a while for a neural network model to run anyway, so this additional network latency may not make a big difference to your users in practice. In fact, since you can use better hardware on the server, the overall latency may even be less than if it were running locally!) Also, if your application uses sensitive data then your users may be concerned about an approach which sends that data to a remote server, so sometimes privacy considerations will mean that you need to run the model on the edge device (it may be possible to avoid this by having an on-premise server, such as inside a company’s firewall). Managing the complexity and scaling the server can create additional overhead too, whereas if your model runs on the edge devices then each user is bringing their own compute resources, which leads to easier scaling with an increasing number of users (also known as horizontal scaling).\n\nA: I’ve had a chance to see up close how the mobile ML landscape is changing in my work. We offer an iPhone app that depends on computer vision, and for years we ran our own computer vision models in the cloud. This was the only way to do it then since those models needed significant memory and compute resources and took minutes to process inputs. This approach required building not only the models (fun!) but also the infrastructure to ensure a certain number of “compute worker machines” were absolutely always running (scary), that more machines would automatically come online if traffic increased, that there was stable storage for large inputs and outputs, that the iOS app could know and tell the user how their job was doing, etc. Nowadays Apple provides APIs for converting models to run efficiently on device and most iOS devices have dedicated ML hardware, so that’s the strategy we use for our newer models. It’s still not easy but in our case it’s worth it, for a faster user experience and to worry less about servers. What works for you will depend, realistically, on the user experience you’re trying to create and what you personally find is easy to do. If you really know how to run servers, do it. If you really know how to build native mobile apps, do that. There are many roads up the hill.\n\nOverall, we’d recommend using a simple CPU-based server approach where possible, for as long as you can get away with it. If you’re lucky enough to have a very successful application, then you’ll be able to justify the investment in more complex deployment approaches at that time.\nCongratulations, you have successfully built a deep learning model and deployed it! Now is a good time to take a pause and think about what could go wrong."
  },
  {
    "objectID": "Fastbook/02_production.html#how-to-avoid-disaster",
    "href": "Fastbook/02_production.html#how-to-avoid-disaster",
    "title": "From Model to Production",
    "section": "How to Avoid Disaster",
    "text": "How to Avoid Disaster\nIn practice, a deep learning model will be just one piece of a much bigger system. As we discussed at the start of this chapter, a data product requires thinking about the entire end-to-end process, from conception to use in production. In this book, we can’t hope to cover all the complexity of managing deployed data products, such as managing multiple versions of models, A/B testing, canarying, refreshing the data (should we just grow and grow our datasets all the time, or should we regularly remove some of the old data?), handling data labeling, monitoring all this, detecting model rot, and so forth. In this section we will give an overview of some of the most important issues to consider; for a more detailed discussion of deployment issues we refer to you to the excellent Building Machine Learning Powered Applications by Emmanuel Ameisen (O’Reilly)\nOne of the biggest issues to consider is that understanding and testing the behavior of a deep learning model is much more difficult than with most other code you write. With normal software development you can analyze the exact steps that the software is taking, and carefully study which of these steps match the desired behavior that you are trying to create. But with a neural network the behavior emerges from the model’s attempt to match the training data, rather than being exactly defined.\nThis can result in disaster! For instance, let’s say we really were rolling out a bear detection system that will be attached to video cameras around campsites in national parks, and will warn campers of incoming bears. If we used a model trained with the dataset we downloaded there would be all kinds of problems in practice, such as:\n\nWorking with video data instead of images\nHandling nighttime images, which may not appear in this dataset\nDealing with low-resolution camera images\nEnsuring results are returned fast enough to be useful in practice\nRecognizing bears in positions that are rarely seen in photos that people post online (for example from behind, partially covered by bushes, or when a long way away from the camera)\n\nA big part of the issue is that the kinds of photos that people are most likely to upload to the internet are the kinds of photos that do a good job of clearly and artistically displaying their subject matter—which isn’t the kind of input this system is going to be getting. So, we may need to do a lot of our own data collection and labelling to create a useful system.\nThis is just one example of the more general problem of out-of-domain data. That is to say, there may be data that our model sees in production which is very different to what it saw during training. There isn’t really a complete technical solution to this problem; instead, we have to be careful about our approach to rolling out the technology.\nThere are other reasons we need to be careful too. One very common problem is domain shift, where the type of data that our model sees changes over time. For instance, an insurance company may use a deep learning model as part of its pricing and risk algorithm, but over time the types of customers that the company attracts, and the types of risks they represent, may change so much that the original training data is no longer relevant.\nOut-of-domain data and domain shift are examples of a larger problem: that you can never fully understand the entire behaviour of your neural network. They have far too many parameters to be able to analytically understand all of their possible behaviors. This is the natural downside of their best feature—their flexibility, which enables them to solve complex problems where we may not even be able to fully specify our preferred solution approaches. The good news, however, is that there are ways to mitigate these risks using a carefully thought-out process. The details of this will vary depending on the details of the problem you are solving, but we will attempt to lay out here a high-level approach, summarized in &lt;&gt;, which we hope will provide useful guidance.\n\nWhere possible, the first step is to use an entirely manual process, with your deep learning model approach running in parallel but not being used directly to drive any actions. The humans involved in the manual process should look at the deep learning outputs and check whether they make sense. For instance, with our bear classifier a park ranger could have a screen displaying video feeds from all the cameras, with any possible bear sightings simply highlighted in red. The park ranger would still be expected to be just as alert as before the model was deployed; the model is simply helping to check for problems at this point.\nThe second step is to try to limit the scope of the model, and have it carefully supervised by people. For instance, do a small geographically and time-constrained trial of the model-driven approach. Rather than rolling our bear classifier out in every national park throughout the country, we could pick a single observation post, for a one-week period, and have a park ranger check each alert before it goes out.\nThen, gradually increase the scope of your rollout. As you do so, ensure that you have really good reporting systems in place, to make sure that you are aware of any significant changes to the actions being taken compared to your manual process. For instance, if the number of bear alerts doubles or halves after rollout of the new system in some location, we should be very concerned. Try to think about all the ways in which your system could go wrong, and then think about what measure or report or picture could reflect that problem, and ensure that your regular reporting includes that information.\n\nJ: I started a company 20 years ago called Optimal Decisions that used machine learning and optimization to help giant insurance companies set their pricing, impacting tens of billions of dollars of risks. We used the approaches described here to manage the potential downsides of something going wrong. Also, before we worked with our clients to put anything in production, we tried to simulate the impact by testing the end-to-end system on their previous year’s data. It was always quite a nerve-wracking process, putting these new algorithms into production, but every rollout was successful.\n\n\nUnforeseen Consequences and Feedback Loops\nOne of the biggest challenges in rolling out a model is that your model may change the behaviour of the system it is a part of. For instance, consider a “predictive policing” algorithm that predicts more crime in certain neighborhoods, causing more police officers to be sent to those neighborhoods, which can result in more crimes being recorded in those neighborhoods, and so on. In the Royal Statistical Society paper “To Predict and Serve?”, Kristian Lum and William Isaac observe that: “predictive policing is aptly named: it is predicting future policing, not future crime.”\nPart of the issue in this case is that in the presence of bias (which we’ll discuss in depth in the next chapter), feedback loops can result in negative implications of that bias getting worse and worse. For instance, there are concerns that this is already happening in the US, where there is significant bias in arrest rates on racial grounds. According to the ACLU, “despite roughly equal usage rates, Blacks are 3.73 times more likely than whites to be arrested for marijuana.” The impact of this bias, along with the rollout of predictive policing algorithms in many parts of the US, led Bärí Williams to write in the New York Times: “The same technology that’s the source of so much excitement in my career is being used in law enforcement in ways that could mean that in the coming years, my son, who is 7 now, is more likely to be profiled or arrested—or worse—for no reason other than his race and where we live.”\nA helpful exercise prior to rolling out a significant machine learning system is to consider this question: “What would happen if it went really, really well?” In other words, what if the predictive power was extremely high, and its ability to influence behavior was extremely significant? In that case, who would be most impacted? What would the most extreme results potentially look like? How would you know what was really going on?\nSuch a thought exercise might help you to construct a more careful rollout plan, with ongoing monitoring systems and human oversight. Of course, human oversight isn’t useful if it isn’t listened to, so make sure that there are reliable and resilient communication channels so that the right people will be aware of issues, and will have the power to fix them."
  },
  {
    "objectID": "Fastbook/02_production.html#get-writing",
    "href": "Fastbook/02_production.html#get-writing",
    "title": "From Model to Production",
    "section": "Get Writing!",
    "text": "Get Writing!\nOne of the things our students have found most helpful to solidify their understanding of this material is to write it down. There is no better test of your understanding of a topic than attempting to teach it to somebody else. This is helpful even if you never show your writing to anybody—but it’s even better if you share it! So we recommend that, if you haven’t already, you start a blog. Now that you’ve completed Chapter 2 and have learned how to train and deploy models, you’re well placed to write your first blog post about your deep learning journey. What’s surprised you? What opportunities do you see for deep learning in your field? What obstacles do you see?\nRachel Thomas, cofounder of fast.ai, wrote in the article “Why You (Yes, You) Should Blog”:\n____\nThe top advice I would give my younger self would be to start blogging sooner. Here are some reasons to blog:\n\n* It’s like a resume, only better. I know of a few people who have had blog posts lead to job offers!\n* Helps you learn. Organizing knowledge always helps me synthesize my own ideas. One of the tests of whether you understand something is whether you can explain it to someone else. A blog post is a great way to do that.\n* I’ve gotten invitations to conferences and invitations to speak from my blog posts. I was invited to the TensorFlow Dev Summit (which was awesome!) for writing a blog post about how I don’t like TensorFlow.\n* Meet new people. I’ve met several people who have responded to blog posts I wrote.\n* Saves time. Any time you answer a question multiple times through email, you should turn it into a blog post, which makes it easier for you to share the next time someone asks.\n____\nPerhaps her most important tip is this:\n\n: You are best positioned to help people one step behind you. The material is still fresh in your mind. Many experts have forgotten what it was like to be a beginner (or an intermediate) and have forgotten why the topic is hard to understand when you first hear it. The context of your particular background, your particular style, and your knowledge level will give a different twist to what you’re writing about.\n\nWe’ve provided full details on how to set up a blog in &lt;&gt;. If you don’t have a blog already, take a look at that now, because we’ve got a really great approach set up for you to start blogging for free, with no ads—and you can even use Jupyter Notebook!"
  },
  {
    "objectID": "Fastbook/02_production.html#questionnaire",
    "href": "Fastbook/02_production.html#questionnaire",
    "title": "From Model to Production",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nProvide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.\nWhere do text models currently have a major deficiency?\nWhat are possible negative societal implications of text generation models?\nIn situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?\nWhat kind of tabular data is deep learning particularly good at?\nWhat’s a key downside of directly using a deep learning model for recommendation systems?\nWhat are the steps of the Drivetrain Approach?\nHow do the steps of the Drivetrain Approach map to a recommendation system?\nCreate an image recognition model using data you curate, and deploy it on the web.\nWhat is DataLoaders?\nWhat four things do we need to tell fastai to create DataLoaders?\nWhat does the splitter parameter to DataBlock do?\nHow do we ensure a random split always gives the same validation set?\nWhat letters are often used to signify the independent and dependent variables?\nWhat’s the difference between the crop, pad, and squish resize approaches? When might you choose one over the others?\nWhat is data augmentation? Why is it needed?\nWhat is the difference between item_tfms and batch_tfms?\nWhat is a confusion matrix?\nWhat does export save?\nWhat is it called when we use a model for getting predictions, instead of training?\nWhat are IPython widgets?\nWhen might you want to use CPU for deployment? When might GPU be better?\nWhat are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC?\nWhat are three examples of problems that could occur when rolling out a bear warning system in practice?\nWhat is “out-of-domain data”?\nWhat is “domain shift”?\nWhat are the three steps in the deployment process?\n\n\nFurther Research\n\nConsider how the Drivetrain Approach maps to a project or problem you’re interested in.\nWhen might it be best to avoid certain types of data augmentation?\nFor a project you’re interested in applying deep learning to, consider the thought experiment “What would happen if it went really, really well?”\nStart a blog, and write your first blog post. For instance, write about what you think deep learning might be useful for in a domain you’re interested in."
  },
  {
    "objectID": "Fastbook/README_bn.html",
    "href": "Fastbook/README_bn.html",
    "title": "fastai বই",
    "section": "",
    "text": "English / Spanish / Korean / Chinese / Bengali / Indonesian / Italian / Portuguese / Vietnamese / Japanese\n\nfastai বই\nএই নোটবুকগুলি প্রারম্ভিক ডীপ লার্নিং, fastai, এবং PyTorch কভার করে। fastai হল ডীপ লার্নিং -এর জন্য একটি স্তরবিশিষ্ট API; আরও জানার জন্য fastai পেপারটি দেখুন। এই রেপোর অন্তর্ভুক্ত সমস্তকিছুর স্বত্ব ২০২০ সাল থেকে জেরেমি হাওয়ার্ড ও সিলভ্যাঁ গাগার দ্বারা রক্ষিত।\nএই নোটবুকগুলি একটি MOOC -এ ব্যবহৃত হয় এবং এগুলি এই বইটার ভিত্তি, যা এখন কিনতে পাওয়া যাচ্ছে। এই খসড়ার অন্তর্ভুক্ত বিষয়বস্তু GPL লাইসেন্সের আওতাধীন, কিন্তু বইটি GPL লাইসেন্সের বাঁধাধরার আওতাভুক্ত নয়।\nএই নোটবুকগুলিতে ব্যবহৃত কোড এবং পাইথন .py ফাইলগুলি GPL v3 লাইসেন্সের আওতাধীন। আরও জানার জন্য LICENSE ফাইলটি দেখুন।\nবাদবাকি সমস্তকিছুর (নোটবুকের সমস্ত মার্কডাউন সেল এবং অন্যান্য টেক্সটের) কোনরকম পুনর্বিতরণ বা তাদের আকার বা মাধ্যমের কোনরকম পরিবর্তন অনুমোদিত নয়। তবে এই নোটবুকগুলিকে ব্যক্তিগত ব্যবহারের প্রয়োজনে কপি করা যেতে পারে, বা রেপোটিকে ফর্ক করা যেতে পারে। এই নোটবুকগুলি ব্যবসায়িক উদ্দেশ্যে ব্যবহার করা বা কোনভাবে সম্প্রচার করা নিষিদ্ধ। আমরা আপনাদের ডীপ লার্নিং শেখায় সাহায্য করার জন্য এই মেটেরিয়ালগুলি বিনামূল্যে উপলব্ধ করছি। সুতরাং আমাদের স্বত্বাধিকারকে সম্মান করুন, এবং বিধিনিষেধগুলিকে মান্যতা দিন।\nযদি আপনি এই মেটেরিয়ালগুলি বা তাদের কোন কপিকে অন্য কোথাও কারও দ্বারা হোস্টেড হতে দেখেন, তাহলে তাঁদের জানান যে তাঁদের কার্যকলাপ অনুমোদিত নয়, এবং তাঁদের বিরুদ্ধে আইনী ব্যবস্থা নেওয়া হতে পারে। উপরন্তু, তাঁরা ব্যবহারকারী গোষ্ঠীর ক্ষতিসাধন করবেন, কারণ আমাদের স্বত্বাধিকার ক্ষতিগ্রস্ত হলে আমরা এইভাবে অতিরিক্ত মেটেরিয়াল প্রকাশ করার আগে অনেকবার ভাববো।\nএইটা একটা শুরুর দিকের খসড়া। যদি আপনার নোটবুকগুলি রান করতে কোনরকম সমস্যা হয়, তাহলে fastai-dev ফোরামে উত্তরের খোঁজ করুন, অথবা সাহায্যের প্রয়োজন হলে সাহায্য চান। নোটবুকগুলি রান করতে কোনরকম সমস্যা হলে তা জানানোর জন্য অনুগ্রহ করে গিটহাব ইস্যুর ব্যবহার করবেন না।\nআপনি যদি এই রেপোতে কোন পুল রিকোয়েস্ট তৈরি করেন, তাহলে আপনি সেই কাজের স্বত্বাধিকার জেরেমি হাওয়ার্ড এবং সিলভ্যাঁ গাগারকে অর্পণ করছেন। (আরও জেনে রাখুন- আপনি যদি বানান বা লেখায় ছোট আকারের কোন ভুল সংশোধন করতে চান, তাহলে অনুগ্রহ করে ফাইলের নাম এবং আপনার করা পরিবর্তনের একটা অতিসংক্ষিপ্ত বিবরণ যোগ করুন। রিভিউয়ারদের পক্ষে বিভিন্ন ফাইল এবং সম্পর্কিত পরিবর্তনের হিসেব রাখা কঠিন হয়ে দাঁড়াচ্ছে। ধন্যবাদ।)"
  },
  {
    "objectID": "Fastbook/19_learner.html",
    "href": "Fastbook/19_learner.html",
    "title": "A fastai Learner from Scratch",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\nThis final chapter (other than the conclusion and the online chapters) is going to look a bit different. It contains far more code and far less prose than the previous chapters. We will introduce new Python keywords and libraries without discussing them. This chapter is meant to be the start of a significant research project for you. You see, we are going to implement many of the key pieces of the fastai and PyTorch APIs from scratch, building on nothing other than the components that we developed in &lt;&gt;! The key goal here is to end up with your own Learner class, and some callbacks—enough to be able to train a model on Imagenette, including examples of each of the key techniques we’ve studied. On the way to building Learner, we will create our own version of Module, Parameter, and parallel DataLoader so you have a very good idea of what those PyTorch classes do.\nThe end-of-chapter questionnaire is particularly important for this chapter. This is where we will be pointing you in the many interesting directions that you could take, using this chapter as your starting point. We suggest that you follow along with this chapter on your computer, and do lots of experiments, web searches, and whatever else you need to understand what’s going on. You’ve built up the skills and expertise to do this in the rest of this book, so we think you are going to do great!\nLet’s begin by gathering (manually) some data."
  },
  {
    "objectID": "Fastbook/19_learner.html#data",
    "href": "Fastbook/19_learner.html#data",
    "title": "A fastai Learner from Scratch",
    "section": "Data",
    "text": "Data\nHave a look at the source to untar_data to see how it works. We’ll use it here to access the 160-pixel version of Imagenette for use in this chapter:\n\npath = untar_data(URLs.IMAGENETTE_160)\n\nTo access the image files, we can use get_image_files:\n\nt = get_image_files(path)\nt[0]\n\nPath('/home/jhoward/.fastai/data/imagenette2-160/val/n03417042/n03417042_3752.JPEG')\n\n\nOr we could do the same thing using just Python’s standard library, with glob:\n\nfrom glob import glob\nfiles = L(glob(f'{path}/**/*.JPEG', recursive=True)).map(Path)\nfiles[0]\n\nPath('/home/jhoward/.fastai/data/imagenette2-160/val/n03417042/n03417042_3752.JPEG')\n\n\nIf you look at the source for get_image_files, you’ll see it uses Python’s os.walk; this is a faster and more flexible function than glob, so be sure to try it out.\nWe can open an image with the Python Imaging Library’s Image class:\n\nim = Image.open(files[0])\nim\n\n\n\n\n\n\n\n\n\nim_t = tensor(im)\nim_t.shape\n\ntorch.Size([160, 213, 3])\n\n\nThat’s going to be the basis of our independent variable. For our dependent variable, we can use Path.parent from pathlib. First we’ll need our vocab:\n\nlbls = files.map(Self.parent.name()).unique(); lbls\n\n(#10) ['n03417042','n03445777','n03888257','n03394916','n02979186','n03000684','n03425413','n01440764','n03028079','n02102040']\n\n\n…and the reverse mapping, thanks to L.val2idx:\n\nv2i = lbls.val2idx(); v2i\n\n{'n03417042': 0,\n 'n03445777': 1,\n 'n03888257': 2,\n 'n03394916': 3,\n 'n02979186': 4,\n 'n03000684': 5,\n 'n03425413': 6,\n 'n01440764': 7,\n 'n03028079': 8,\n 'n02102040': 9}\n\n\nThat’s all the pieces we need to put together our Dataset.\n\nDataset\nA Dataset in PyTorch can be anything that supports indexing (__getitem__) and len:\n\nclass Dataset:\n    def __init__(self, fns): self.fns=fns\n    def __len__(self): return len(self.fns)\n    def __getitem__(self, i):\n        im = Image.open(self.fns[i]).resize((64,64)).convert('RGB')\n        y = v2i[self.fns[i].parent.name]\n        return tensor(im).float()/255, tensor(y)\n\nWe need a list of training and validation filenames to pass to Dataset.__init__:\n\ntrain_filt = L(o.parent.parent.name=='train' for o in files)\ntrain,valid = files[train_filt],files[~train_filt]\nlen(train),len(valid)\n\n(9469, 3925)\n\n\nNow we can try it out:\n\ntrain_ds,valid_ds = Dataset(train),Dataset(valid)\nx,y = train_ds[0]\nx.shape,y\n\n(torch.Size([64, 64, 3]), tensor(0))\n\n\n\nshow_image(x, title=lbls[y]);\n\n\n\n\n\n\n\n\nAs you see, our dataset is returning the independent and dependent variables as a tuple, which is just what we need. We’ll need to be able to collate these into a mini-batch. Generally this is done with torch.stack, which is what we’ll use here:\n\ndef collate(idxs, ds): \n    xb,yb = zip(*[ds[i] for i in idxs])\n    return torch.stack(xb),torch.stack(yb)\n\nHere’s a mini-batch with two items, for testing our collate:\n\nx,y = collate([1,2], train_ds)\nx.shape,y\n\n(torch.Size([2, 64, 64, 3]), tensor([0, 0]))\n\n\nNow that we have a dataset and a collation function, we’re ready to create DataLoader. We’ll add two more things here: an optional shuffle for the training set, and a ProcessPoolExecutor to do our preprocessing in parallel. A parallel data loader is very important, because opening and decoding a JPEG image is a slow process. One CPU core is not enough to decode images fast enough to keep a modern GPU busy. Here’s our DataLoader class:\n\nclass DataLoader:\n    def __init__(self, ds, bs=128, shuffle=False, n_workers=1):\n        self.ds,self.bs,self.shuffle,self.n_workers = ds,bs,shuffle,n_workers\n\n    def __len__(self): return (len(self.ds)-1)//self.bs+1\n\n    def __iter__(self):\n        idxs = L.range(self.ds)\n        if self.shuffle: idxs = idxs.shuffle()\n        chunks = [idxs[n:n+self.bs] for n in range(0, len(self.ds), self.bs)]\n        with ProcessPoolExecutor(self.n_workers) as ex:\n            yield from ex.map(collate, chunks, ds=self.ds)\n\nLet’s try it out with our training and validation datasets:\n\nn_workers = min(16, defaults.cpus)\ntrain_dl = DataLoader(train_ds, bs=128, shuffle=True, n_workers=n_workers)\nvalid_dl = DataLoader(valid_ds, bs=256, shuffle=False, n_workers=n_workers)\nxb,yb = first(train_dl)\nxb.shape,yb.shape,len(train_dl)\n\n(torch.Size([128, 64, 64, 3]), torch.Size([128]), 74)\n\n\nThis data loader is not much slower than PyTorch’s, but it’s far simpler. So if you’re debugging a complex data loading process, don’t be afraid to try doing things manually to help you see exactly what’s going on.\nFor normalization, we’ll need image statistics. Generally it’s fine to calculate these on a single training mini-batch, since precision isn’t needed here:\n\nstats = [xb.mean((0,1,2)),xb.std((0,1,2))]\nstats\n\n[tensor([0.4544, 0.4453, 0.4141]), tensor([0.2812, 0.2766, 0.2981])]\n\n\nOur Normalize class just needs to store these stats and apply them (to see why the to_device is needed, try commenting it out, and see what happens later in this notebook):\n\nclass Normalize:\n    def __init__(self, stats): self.stats=stats\n    def __call__(self, x):\n        if x.device != self.stats[0].device:\n            self.stats = to_device(self.stats, x.device)\n        return (x-self.stats[0])/self.stats[1]\n\nWe always like to test everything we build in a notebook, as soon as we build it:\n\nnorm = Normalize(stats)\ndef tfm_x(x): return norm(x).permute((0,3,1,2))\n\n\nt = tfm_x(x)\nt.mean((0,2,3)),t.std((0,2,3))\n\n(tensor([0.3732, 0.4907, 0.5633]), tensor([1.0212, 1.0311, 1.0131]))\n\n\nHere tfm_x isn’t just applying Normalize, but is also permuting the axis order from NHWC to NCHW (see &lt;&gt; if you need a reminder of what these acronyms refer to). PIL uses HWC axis order, which we can’t use with PyTorch, hence the need for this permute.\nThat’s all we need for the data for our model. So now we need the model itself!"
  },
  {
    "objectID": "Fastbook/19_learner.html#module-and-parameter",
    "href": "Fastbook/19_learner.html#module-and-parameter",
    "title": "A fastai Learner from Scratch",
    "section": "Module and Parameter",
    "text": "Module and Parameter\nTo create a model, we’ll need Module. To create Module, we’ll need Parameter, so let’s start there. Recall that in &lt;&gt; we said that the Parameter class “doesn’t actually add any functionality (other than automatically calling requires_grad_ for us). It’s only used as a”marker” to show what to include in parameters.” Here’s a definition which does exactly that:\n\nclass Parameter(Tensor):\n    def __new__(self, x): return Tensor._make_subclass(Parameter, x, True)\n    def __init__(self, *args, **kwargs): self.requires_grad_()\n\nThe implementation here is a bit awkward: we have to define the special __new__ Python method and use the internal PyTorch method _make_subclass because, as at the time of writing, PyTorch doesn’t otherwise work correctly with this kind of subclassing or provide an officially supported API to do this. This may have been fixed by the time you read this, so look on the book’s website to see if there are updated details.\nOur Parameter now behaves just like a tensor, as we wanted:\n\nParameter(tensor(3.))\n\ntensor(3., requires_grad=True)\n\n\nNow that we have this, we can define Module:\n\nclass Module:\n    def __init__(self):\n        self.hook,self.params,self.children,self._training = None,[],[],False\n        \n    def register_parameters(self, *ps): self.params += ps\n    def register_modules   (self, *ms): self.children += ms\n        \n    @property\n    def training(self): return self._training\n    @training.setter\n    def training(self,v):\n        self._training = v\n        for m in self.children: m.training=v\n            \n    def parameters(self):\n        return self.params + sum([m.parameters() for m in self.children], [])\n\n    def __setattr__(self,k,v):\n        super().__setattr__(k,v)\n        if isinstance(v,Parameter): self.register_parameters(v)\n        if isinstance(v,Module):    self.register_modules(v)\n        \n    def __call__(self, *args, **kwargs):\n        res = self.forward(*args, **kwargs)\n        if self.hook is not None: self.hook(res, args)\n        return res\n    \n    def cuda(self):\n        for p in self.parameters(): p.data = p.data.cuda()\n\nThe key functionality is in the definition of parameters:\nself.params + sum([m.parameters() for m in self.children], [])\nThis means that we can ask any Module for its parameters, and it will return them, including all its child modules (recursively). But how does it know what its parameters are? It’s thanks to implementing Python’s special __setattr__ method, which is called for us any time Python sets an attribute on a class. Our implementation includes this line:\nif isinstance(v,Parameter): self.register_parameters(v)\nAs you see, this is where we use our new Parameter class as a “marker”—anything of this class is added to our params.\nPython’s __call__ allows us to define what happens when our object is treated as a function; we just call forward (which doesn’t exist here, so it’ll need to be added by subclasses). Before we do, we’ll call a hook, if it’s defined. Now you can see that PyTorch hooks aren’t doing anything fancy at all—they’re just calling any hooks that have been registered.\nOther than these pieces of functionality, our Module also provides cuda and training attributes, which we’ll use shortly.\nNow we can create our first Module, which is ConvLayer:\n\nclass ConvLayer(Module):\n    def __init__(self, ni, nf, stride=1, bias=True, act=True):\n        super().__init__()\n        self.w = Parameter(torch.zeros(nf,ni,3,3))\n        self.b = Parameter(torch.zeros(nf)) if bias else None\n        self.act,self.stride = act,stride\n        init = nn.init.kaiming_normal_ if act else nn.init.xavier_normal_\n        init(self.w)\n    \n    def forward(self, x):\n        x = F.conv2d(x, self.w, self.b, stride=self.stride, padding=1)\n        if self.act: x = F.relu(x)\n        return x\n\nWe’re not implementing F.conv2d from scratch, since you should have already done that (using unfold) in the questionnaire in &lt;&gt;. Instead, we’re just creating a small class that wraps it up along with bias and weight initialization. Let’s check that it works correctly with Module.parameters:\n\nl = ConvLayer(3, 4)\nlen(l.parameters())\n\n2\n\n\nAnd that we can call it (which will result in forward being called):\n\nxbt = tfm_x(xb)\nr = l(xbt)\nr.shape\n\ntorch.Size([128, 4, 64, 64])\n\n\nIn the same way, we can implement Linear:\n\nclass Linear(Module):\n    def __init__(self, ni, nf):\n        super().__init__()\n        self.w = Parameter(torch.zeros(nf,ni))\n        self.b = Parameter(torch.zeros(nf))\n        nn.init.xavier_normal_(self.w)\n    \n    def forward(self, x): return x@self.w.t() + self.b\n\nand test if it works:\n\nl = Linear(4,2)\nr = l(torch.ones(3,4))\nr.shape\n\ntorch.Size([3, 2])\n\n\nLet’s also create a testing module to check that if we include multiple parameters as attributes, they are all correctly registered:\n\nclass T(Module):\n    def __init__(self):\n        super().__init__()\n        self.c,self.l = ConvLayer(3,4),Linear(4,2)\n\nSince we have a conv layer and a linear layer, each of which has weights and biases, we’d expect four parameters in total:\n\nt = T()\nlen(t.parameters())\n\n4\n\n\nWe should also find that calling cuda on this class puts all these parameters on the GPU:\n\nt.cuda()\nt.l.w.device\n\ndevice(type='cuda', index=5)\n\n\nWe can now use those pieces to create a CNN.\n\nSimple CNN\nAs we’ve seen, a Sequential class makes many architectures easier to implement, so let’s make one:\n\nclass Sequential(Module):\n    def __init__(self, *layers):\n        super().__init__()\n        self.layers = layers\n        self.register_modules(*layers)\n\n    def forward(self, x):\n        for l in self.layers: x = l(x)\n        return x\n\nThe forward method here just calls each layer in turn. Note that we have to use the register_modules method we defined in Module, since otherwise the contents of layers won’t appear in parameters.\n\nimportant: All The Code is Here: Remember that we’re not using any PyTorch functionality for modules here; we’re defining everything ourselves. So if you’re not sure what register_modules does, or why it’s needed, have another look at our code for Module to see what we wrote!\n\nWe can create a simplified AdaptivePool that only handles pooling to a 1×1 output, and flattens it as well, by just using mean:\n\nclass AdaptivePool(Module):\n    def forward(self, x): return x.mean((2,3))\n\nThat’s enough for us to create a CNN!\n\ndef simple_cnn():\n    return Sequential(\n        ConvLayer(3 ,16 ,stride=2), #32\n        ConvLayer(16,32 ,stride=2), #16\n        ConvLayer(32,64 ,stride=2), # 8\n        ConvLayer(64,128,stride=2), # 4\n        AdaptivePool(),\n        Linear(128, 10)\n    )\n\nLet’s see if our parameters are all being registered correctly:\n\nm = simple_cnn()\nlen(m.parameters())\n\n10\n\n\nNow we can try adding a hook. Note that we’ve only left room for one hook in Module; you could make it a list, or use something like Pipeline to run a few as a single function:\n\ndef print_stats(outp, inp): print (outp.mean().item(),outp.std().item())\nfor i in range(4): m.layers[i].hook = print_stats\n\nr = m(xbt)\nr.shape\n\n0.5239089727401733 0.8776043057441711\n0.43470510840415955 0.8347987532615662\n0.4357188045978546 0.7621666193008423\n0.46562111377716064 0.7416611313819885\n\n\ntorch.Size([128, 10])\n\n\nWe have data and model. Now we need a loss function."
  },
  {
    "objectID": "Fastbook/19_learner.html#loss",
    "href": "Fastbook/19_learner.html#loss",
    "title": "A fastai Learner from Scratch",
    "section": "Loss",
    "text": "Loss\nWe’ve already seen how to define “negative log likelihood”:\n\ndef nll(input, target): return -input[range(target.shape[0]), target].mean()\n\nWell actually, there’s no log here, since we’re using the same definition as PyTorch. That means we need to put the log together with softmax:\n\ndef log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\n\nsm = log_softmax(r); sm[0][0]\n\ntensor(-1.2790, grad_fn=&lt;SelectBackward&gt;)\n\n\nCombining these gives us our cross-entropy loss:\n\nloss = nll(sm, yb)\nloss\n\ntensor(2.5666, grad_fn=&lt;NegBackward&gt;)\n\n\nNote that the formula:\n\\[\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)\\]\ngives a simplification when we compute the log softmax, which was previously defined as (x.exp()/(x.exp().sum(-1))).log():\n\ndef log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()\nsm = log_softmax(r); sm[0][0]\n\ntensor(-1.2790, grad_fn=&lt;SelectBackward&gt;)\n\n\nThen, there is a more stable way to compute the log of the sum of exponentials, called the LogSumExp trick. The idea is to use the following formula:\n\\[\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )\\]\nwhere \\(a\\) is the maximum of \\(x_{j}\\).\nHere’s the same thing in code:\n\nx = torch.rand(5)\na = x.max()\nx.exp().sum().log() == a + (x-a).exp().sum().log()\n\ntensor(True)\n\n\nWe’ll put that into a function:\n\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\n\nlogsumexp(r)[0]\n\ntensor(3.9784, grad_fn=&lt;SelectBackward&gt;)\n\n\nso we can use it for our log_softmax function:\n\ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n\nWhich gives the same result as before:\n\nsm = log_softmax(r); sm[0][0]\n\ntensor(-1.2790, grad_fn=&lt;SelectBackward&gt;)\n\n\nWe can use these to create cross_entropy:\n\ndef cross_entropy(preds, yb): return nll(log_softmax(preds), yb).mean()\n\nLet’s now combine all those pieces together to create a Learner."
  },
  {
    "objectID": "Fastbook/19_learner.html#learner",
    "href": "Fastbook/19_learner.html#learner",
    "title": "A fastai Learner from Scratch",
    "section": "Learner",
    "text": "Learner\nWe have data, a model, and a loss function; we only need one more thing before we can fit a model, and that’s an optimizer! Here’s SGD:\n\nclass SGD:\n    def __init__(self, params, lr, wd=0.): store_attr()\n    def step(self):\n        for p in self.params:\n            p.data -= (p.grad.data + p.data*self.wd) * self.lr\n            p.grad.data.zero_()\n\nAs we’ve seen in this book, life is easier with a Learner. The Learner class needs to know our training and validation sets, which means we need DataLoaders to store them. We don’t need any other functionality, just a place to store them and access them:\n\nclass DataLoaders:\n    def __init__(self, *dls): self.train,self.valid = dls\n\ndls = DataLoaders(train_dl,valid_dl)\n\nNow we’re ready to create our Learner class:\n\nclass Learner:\n    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=SGD):\n        store_attr()\n        for cb in cbs: cb.learner = self\n\n    def one_batch(self):\n        self('before_batch')\n        xb,yb = self.batch\n        self.preds = self.model(xb)\n        self.loss = self.loss_func(self.preds, yb)\n        if self.model.training:\n            self.loss.backward()\n            self.opt.step()\n        self('after_batch')\n\n    def one_epoch(self, train):\n        self.model.training = train\n        self('before_epoch')\n        dl = self.dls.train if train else self.dls.valid\n        for self.num,self.batch in enumerate(progress_bar(dl, leave=False)):\n            self.one_batch()\n        self('after_epoch')\n    \n    def fit(self, n_epochs):\n        self('before_fit')\n        self.opt = self.opt_func(self.model.parameters(), self.lr)\n        self.n_epochs = n_epochs\n        try:\n            for self.epoch in range(n_epochs):\n                self.one_epoch(True)\n                self.one_epoch(False)\n        except CancelFitException: pass\n        self('after_fit')\n        \n    def __call__(self,name):\n        for cb in self.cbs: getattr(cb,name,noop)()\n\nThis is the largest class we’ve created in the book, but each method is quite small, so by looking at each in turn you should be able to follow what’s going on.\nThe main method we’ll be calling is fit. This loops with:\nfor self.epoch in range(n_epochs)\nand at each epoch calls self.one_epoch for each of train=True and then train=False. Then self.one_epoch calls self.one_batch for each batch in dls.train or dls.valid, as appropriate (after wrapping the DataLoader in fastprogress.progress_bar. Finally, self.one_batch follows the usual set of steps to fit one mini-batch that we’ve seen throughout this book.\nBefore and after each step, Learner calls self, which calls __call__ (which is standard Python functionality). __call__ uses getattr(cb,name) on each callback in self.cbs, which is a Python built-in function that returns the attribute (a method, in this case) with the requested name. So, for instance, self('before_fit') will call cb.before_fit() for each callback where that method is defined.\nAs you can see, Learner is really just using our standard training loop, except that it’s also calling callbacks at appropriate times. So let’s define some callbacks!\n\nCallbacks\nIn Learner.__init__ we have:\nfor cb in cbs: cb.learner = self\nIn other words, every callback knows what learner it is used in. This is critical, since otherwise a callback can’t get information from the learner, or change things in the learner. Because getting information from the learner is so common, we make that easier by defining Callback as a subclass of GetAttr, with a default attribute of learner:\n\nclass Callback(GetAttr): _default='learner'\n\nGetAttr is a fastai class that implements Python’s standard __getattr__ and __dir__ methods for you, such that any time you try to access an attribute that doesn’t exist, it passes the request along to whatever you have defined as _default.\nFor instance, we want to move all model parameters to the GPU automatically at the start of fit. We could do this by defining before_fit as self.learner.model.cuda(); however, because learner is the default attribute, and we have SetupLearnerCB inherit from Callback (which inherits from GetAttr), we can remove the .learner and just call self.model.cuda():\n\nclass SetupLearnerCB(Callback):\n    def before_batch(self):\n        xb,yb = to_device(self.batch)\n        self.learner.batch = tfm_x(xb),yb\n\n    def before_fit(self): self.model.cuda()\n\nIn SetupLearnerCB we also move each mini-batch to the GPU, by calling to_device(self.batch) (we could also have used the longer to_device(self.learner.batch). Note however that in the line self.learner.batch = tfm_x(xb),yb we can’t remove .learner, because here we’re setting the attribute, not getting it.\nBefore we try our Learner out, let’s create a callback to track and print progress. Otherwise we won’t really know if it’s working properly:\n\nclass TrackResults(Callback):\n    def before_epoch(self): self.accs,self.losses,self.ns = [],[],[]\n        \n    def after_epoch(self):\n        n = sum(self.ns)\n        print(self.epoch, self.model.training,\n              sum(self.losses).item()/n, sum(self.accs).item()/n)\n        \n    def after_batch(self):\n        xb,yb = self.batch\n        acc = (self.preds.argmax(dim=1)==yb).float().sum()\n        self.accs.append(acc)\n        n = len(xb)\n        self.losses.append(self.loss*n)\n        self.ns.append(n)\n\nNow we’re ready to use our Learner for the first time!\n\ncbs = [SetupLearnerCB(),TrackResults()]\nlearn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs)\nlearn.fit(1)\n\n\n\n\n0 True 2.1275552130636814 0.2314922378287042\n\n\n\n\n\n0 False 1.9942575636942674 0.2991082802547771\n\n\nIt’s quite amazing to realize that we can implement all the key ideas from fastai’s Learner in so little code! Let’s now add some learning rate scheduling.\n\n\nScheduling the Learning Rate\nIf we’re going to get good results, we’ll want an LR finder and 1cycle training. These are both annealing callbacks—that is, they are gradually changing hyperparameters as we train. Here’s LRFinder:\n\nclass LRFinder(Callback):\n    def before_fit(self):\n        self.losses,self.lrs = [],[]\n        self.learner.lr = 1e-6\n        \n    def before_batch(self):\n        if not self.model.training: return\n        self.opt.lr *= 1.2\n\n    def after_batch(self):\n        if not self.model.training: return\n        if self.opt.lr&gt;10 or torch.isnan(self.loss): raise CancelFitException\n        self.losses.append(self.loss.item())\n        self.lrs.append(self.opt.lr)\n\nThis shows how we’re using CancelFitException, which is itself an empty class, only used to signify the type of exception. You can see in Learner that this exception is caught. (You should add and test CancelBatchException, CancelEpochException, etc. yourself.) Let’s try it out, by adding it to our list of callbacks:\n\nlrfind = LRFinder()\nlearn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs+[lrfind])\nlearn.fit(2)\n\n\n\n\n0 True 2.6336045582954903 0.11014890695955222\n\n\n\n\n\n0 False 2.230653363853503 0.18318471337579617\n\n\n\n    \n        \n      \n      16.22% [12/74 00:02&lt;00:12]\n    \n    \n\n\nAnd take a look at the results:\n\nplt.plot(lrfind.lrs[:-2],lrfind.losses[:-2])\nplt.xscale('log')\n\n\n\n\n\n\n\n\nNow we can define our OneCycle training callback:\n\nclass OneCycle(Callback):\n    def __init__(self, base_lr): self.base_lr = base_lr\n    def before_fit(self): self.lrs = []\n\n    def before_batch(self):\n        if not self.model.training: return\n        n = len(self.dls.train)\n        bn = self.epoch*n + self.num\n        mn = self.n_epochs*n\n        pct = bn/mn\n        pct_start,div_start = 0.25,10\n        if pct&lt;pct_start:\n            pct /= pct_start\n            lr = (1-pct)*self.base_lr/div_start + pct*self.base_lr\n        else:\n            pct = (pct-pct_start)/(1-pct_start)\n            lr = (1-pct)*self.base_lr\n        self.opt.lr = lr\n        self.lrs.append(lr)\n\nWe’ll try an LR of 0.1:\n\nonecyc = OneCycle(0.1)\nlearn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs+[onecyc])\n\nLet’s fit for a while and see how it looks (we won’t show all the output in the book—try it in the notebook to see the results):\n\n#hide_output\nlearn.fit(8)\n\nFinally, we’ll check that the learning rate followed the schedule we defined (as you see, we’re not using cosine annealing here):\n\nplt.plot(onecyc.lrs);"
  },
  {
    "objectID": "Fastbook/19_learner.html#conclusion",
    "href": "Fastbook/19_learner.html#conclusion",
    "title": "A fastai Learner from Scratch",
    "section": "Conclusion",
    "text": "Conclusion\nWe have explored how the key concepts of the fastai library are implemented by re-implementing them in this chapter. Since it’s mostly full of code, you should definitely try to experiment with it by looking at the corresponding notebook on the book’s website. Now that you know how it’s built, as a next step be sure to check out the intermediate and advanced tutorials in the fastai documentation to learn how to customize every bit of the library."
  },
  {
    "objectID": "Fastbook/19_learner.html#questionnaire",
    "href": "Fastbook/19_learner.html#questionnaire",
    "title": "A fastai Learner from Scratch",
    "section": "Questionnaire",
    "text": "Questionnaire\n\ntip: Experiments: For the questions here that ask you to explain what some function or class is, you should also complete your own code experiments.\n\n\nWhat is glob?\nHow do you open an image with the Python imaging library?\nWhat does L.map do?\nWhat does Self do?\nWhat is L.val2idx?\nWhat methods do you need to implement to create your own Dataset?\nWhy do we call convert when we open an image from Imagenette?\nWhat does ~ do? How is it useful for splitting training and validation sets?\nDoes ~ work with the L or Tensor classes? What about NumPy arrays, Python lists, or pandas DataFrames?\nWhat is ProcessPoolExecutor?\nHow does L.range(self.ds) work?\nWhat is __iter__?\nWhat is first?\nWhat is permute? Why is it needed?\nWhat is a recursive function? How does it help us define the parameters method?\nWrite a recursive function that returns the first 20 items of the Fibonacci sequence.\nWhat is super?\nWhy do subclasses of Module need to override forward instead of defining __call__?\nIn ConvLayer, why does init depend on act?\nWhy does Sequential need to call register_modules?\nWrite a hook that prints the shape of every layer’s activations.\nWhat is “LogSumExp”?\nWhy is log_softmax useful?\nWhat is GetAttr? How is it helpful for callbacks?\nReimplement one of the callbacks in this chapter without inheriting from Callback or GetAttr.\nWhat does Learner.__call__ do?\nWhat is getattr? (Note the case difference to GetAttr!)\nWhy is there a try block in fit?\nWhy do we check for model.training in one_batch?\nWhat is store_attr?\nWhat is the purpose of TrackResults.before_epoch?\nWhat does model.cuda do? How does it work?\nWhy do we need to check model.training in LRFinder and OneCycle?\nUse cosine annealing in OneCycle.\n\n\nFurther Research\n\nWrite resnet18 from scratch (refer to &lt;&gt; as needed), and train it with the Learner in this chapter.\nImplement a batchnorm layer from scratch and use it in your resnet18.\nWrite a Mixup callback for use in this chapter.\nAdd momentum to SGD.\nPick a few features that you’re interested in from fastai (or any other library) and implement them in this chapter.\nPick a research paper that’s not yet implemented in fastai or PyTorch and implement it in this chapter.\n\n\nPort it over to fastai.\nSubmit a pull request to fastai, or create your own extension module and release it.\nHint: you may find it helpful to use nbdev to create and deploy your package."
  },
  {
    "objectID": "Fastbook/06_multicat.html",
    "href": "Fastbook/06_multicat.html",
    "title": "Other Computer Vision Problems",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\n[[chapter_multicat]]\nIn the previous chapter you learned some important practical techniques for training models in practice. Considerations like selecting learning rates and the number of epochs are very important to getting good results.\nIn this chapter we are going to look at two other types of computer vision problems: multi-label classification and regression. The first one is when you want to predict more than one label per image (or sometimes none at all), and the second is when your labels are one or several numbers—a quantity instead of a category.\nIn the process will study more deeply the output activations, targets, and loss functions in deep learning models."
  },
  {
    "objectID": "Fastbook/06_multicat.html#multi-label-classification",
    "href": "Fastbook/06_multicat.html#multi-label-classification",
    "title": "Other Computer Vision Problems",
    "section": "Multi-Label Classification",
    "text": "Multi-Label Classification\nMulti-label classification refers to the problem of identifying the categories of objects in images that may not contain exactly one type of object. There may be more than one kind of object, or there may be no objects at all in the classes that you are looking for.\nFor instance, this would have been a great approach for our bear classifier. One problem with the bear classifier that we rolled out in &lt;&gt; was that if a user uploaded something that wasn’t any kind of bear, the model would still say it was either a grizzly, black, or teddy bear—it had no ability to predict “not a bear at all.” In fact, after we have completed this chapter, it would be a great exercise for you to go back to your image classifier application, and try to retrain it using the multi-label technique, then test it by passing in an image that is not of any of your recognized classes.\nIn practice, we have not seen many examples of people training multi-label classifiers for this purpose—but we very often see both users and developers complaining about this problem. It appears that this simple solution is not at all widely understood or appreciated! Because in practice it is probably more common to have some images with zero matches or more than one match, we should probably expect in practice that multi-label classifiers are more widely applicable than single-label classifiers.\nFirst, let’s see what a multi-label dataset looks like, then we’ll explain how to get it ready for our model. You’ll see that the architecture of the model does not change from the last chapter; only the loss function does. Let’s start with the data.\n\nThe Data\nFor our example we are going to use the PASCAL dataset, which can have more than one kind of classified object per image.\nWe begin by downloading and extracting the dataset as per usual:\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.PASCAL_2007)\n\nThis dataset is different from the ones we have seen before, in that it is not structured by filename or folder but instead comes with a CSV (comma-separated values) file telling us what labels to use for each image. We can inspect the CSV file by reading it into a Pandas DataFrame:\n\ndf = pd.read_csv(path/'train.csv')\ndf.head()\n\n\n\n\n\n\n\n\nfname\nlabels\nis_valid\n\n\n\n\n0\n000005.jpg\nchair\nTrue\n\n\n1\n000007.jpg\ncar\nTrue\n\n\n2\n000009.jpg\nhorse person\nTrue\n\n\n3\n000012.jpg\ncar\nFalse\n\n\n4\n000016.jpg\nbicycle\nTrue\n\n\n\n\n\n\n\nAs you can see, the list of categories in each image is shown as a space-delimited string.\n\n\nSidebar: Pandas and DataFrames\nNo, it’s not actually a panda! Pandas is a Python library that is used to manipulate and analyze tabular and time series data. The main class is DataFrame, which represents a table of rows and columns. You can get a DataFrame from a CSV file, a database table, Python dictionaries, and many other sources. In Jupyter, a DataFrame is output as a formatted table, as shown here.\nYou can access rows and columns of a DataFrame with the iloc property, as if it were a matrix:\n\ndf.iloc[:,0]\n\n0       000005.jpg\n1       000007.jpg\n2       000009.jpg\n3       000012.jpg\n4       000016.jpg\n           ...    \n5006    009954.jpg\n5007    009955.jpg\n5008    009958.jpg\n5009    009959.jpg\n5010    009961.jpg\nName: fname, Length: 5011, dtype: object\n\n\n\ndf.iloc[0,:]\n# Trailing :s are always optional (in numpy, pytorch, pandas, etc.),\n#   so this is equivalent:\ndf.iloc[0]\n\nfname       000005.jpg\nlabels           chair\nis_valid          True\nName: 0, dtype: object\n\n\nYou can also grab a column by name by indexing into a DataFrame directly:\n\ndf['fname']\n\n0       000005.jpg\n1       000007.jpg\n2       000009.jpg\n3       000012.jpg\n4       000016.jpg\n           ...    \n5006    009954.jpg\n5007    009955.jpg\n5008    009958.jpg\n5009    009959.jpg\n5010    009961.jpg\nName: fname, Length: 5011, dtype: object\n\n\nYou can create new columns and do calculations using columns:\n\ntmp_df = pd.DataFrame({'a':[1,2], 'b':[3,4]})\ntmp_df\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n3\n\n\n1\n2\n4\n\n\n\n\n\n\n\n\ntmp_df['c'] = tmp_df['a']+tmp_df['b']\ntmp_df\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n0\n1\n3\n4\n\n\n1\n2\n4\n6\n\n\n\n\n\n\n\nPandas is a fast and flexible library, and an important part of every data scientist’s Python toolbox. Unfortunately, its API can be rather confusing and surprising, so it takes a while to get familiar with it. If you haven’t used Pandas before, we’d suggest going through a tutorial; we are particularly fond of the book Python for Data Analysis by Wes McKinney, the creator of Pandas (O’Reilly). It also covers other important libraries like matplotlib and numpy. We will try to briefly describe Pandas functionality we use as we come across it, but will not go into the level of detail of McKinney’s book.\n\n\nEnd sidebar\nNow that we have seen what the data looks like, let’s make it ready for model training.\n\n\nConstructing a DataBlock\nHow do we convert from a DataFrame object to a DataLoaders object? We generally suggest using the data block API for creating a DataLoaders object, where possible, since it provides a good mix of flexibility and simplicity. Here we will show you the steps that we take to use the data blocks API to construct a DataLoaders object in practice, using this dataset as an example.\nAs we have seen, PyTorch and fastai have two main classes for representing and accessing a training set or validation set:\n\nDataset:: A collection that returns a tuple of your independent and dependent variable for a single item\nDataLoader:: An iterator that provides a stream of mini-batches, where each mini-batch is a tuple of a batch of independent variables and a batch of dependent variables\n\nOn top of these, fastai provides two classes for bringing your training and validation sets together:\n\nDatasets:: An object that contains a training Dataset and a validation Dataset\nDataLoaders:: An object that contains a training DataLoader and a validation DataLoader\n\nSince a DataLoader builds on top of a Dataset and adds additional functionality to it (collating multiple items into a mini-batch), it’s often easiest to start by creating and testing Datasets, and then look at DataLoaders after that’s working.\nWhen we create a DataBlock, we build up gradually, step by step, and use the notebook to check our data along the way. This is a great way to make sure that you maintain momentum as you are coding, and that you keep an eye out for any problems. It’s easy to debug, because you know that if a problem arises, it is in the line of code you just typed!\nLet’s start with the simplest case, which is a data block created with no parameters:\n\ndblock = DataBlock()\n\nWe can create a Datasets object from this. The only thing needed is a source—in this case, our DataFrame:\n\ndsets = dblock.datasets(df)\n\nThis contains a train and a valid dataset, which we can index into:\n\nlen(dsets.train),len(dsets.valid)\n\n(4009, 1002)\n\n\n\nx,y = dsets.train[0]\nx,y\n\n(fname       008663.jpg\n labels      car person\n is_valid         False\n Name: 4346, dtype: object,\n fname       008663.jpg\n labels      car person\n is_valid         False\n Name: 4346, dtype: object)\n\n\nAs you can see, this simply returns a row of the DataFrame, twice. This is because by default, the data block assumes we have two things: input and target. We are going to need to grab the appropriate fields from the DataFrame, which we can do by passing get_x and get_y functions:\n\nx['fname']\n\n'008663.jpg'\n\n\n\ndblock = DataBlock(get_x = lambda r: r['fname'], get_y = lambda r: r['labels'])\ndsets = dblock.datasets(df)\ndsets.train[0]\n\n('005620.jpg', 'aeroplane')\n\n\nAs you can see, rather than defining a function in the usual way, we are using Python’s lambda keyword. This is just a shortcut for defining and then referring to a function. The following more verbose approach is identical:\n\ndef get_x(r): return r['fname']\ndef get_y(r): return r['labels']\ndblock = DataBlock(get_x = get_x, get_y = get_y)\ndsets = dblock.datasets(df)\ndsets.train[0]\n\n('002549.jpg', 'tvmonitor')\n\n\nLambda functions are great for quickly iterating, but they are not compatible with serialization, so we advise you to use the more verbose approach if you want to export your Learner after training (lambdas are fine if you are just experimenting).\nWe can see that the independent variable will need to be converted into a complete path, so that we can open it as an image, and the dependent variable will need to be split on the space character (which is the default for Python’s split function) so that it becomes a list:\n\ndef get_x(r): return path/'train'/r['fname']\ndef get_y(r): return r['labels'].split(' ')\ndblock = DataBlock(get_x = get_x, get_y = get_y)\ndsets = dblock.datasets(df)\ndsets.train[0]\n\n(Path('/home/jhoward/.fastai/data/pascal_2007/train/002844.jpg'), ['train'])\n\n\nTo actually open the image and do the conversion to tensors, we will need to use a set of transforms; block types will provide us with those. We can use the same block types that we have used previously, with one exception: the ImageBlock will work fine again, because we have a path that points to a valid image, but the CategoryBlock is not going to work. The problem is that block returns a single integer, but we need to be able to have multiple labels for each item. To solve this, we use a MultiCategoryBlock. This type of block expects to receive a list of strings, as we have in this case, so let’s test it out:\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   get_x = get_x, get_y = get_y)\ndsets = dblock.datasets(df)\ndsets.train[0]\n\n(PILImage mode=RGB size=500x375,\n TensorMultiCategory([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]))\n\n\nAs you can see, our list of categories is not encoded in the same way that it was for the regular CategoryBlock. In that case, we had a single integer representing which category was present, based on its location in our vocab. In this case, however, we instead have a list of zeros, with a one in any position where that category is present. For example, if there is a one in the second and fourth positions, then that means that vocab items two and four are present in this image. This is known as one-hot encoding. The reason we can’t easily just use a list of category indices is that each list would be a different length, and PyTorch requires tensors, where everything has to be the same length.\n\njargon: One-hot encoding: Using a vector of zeros, with a one in each location that is represented in the data, to encode a list of integers.\n\nLet’s check what the categories represent for this example (we are using the convenient torch.where function, which tells us all of the indices where our condition is true or false):\n\nidxs = torch.where(dsets.train[0][1]==1.)[0]\ndsets.train.vocab[idxs]\n\n(#1) ['dog']\n\n\nWith NumPy arrays, PyTorch tensors, and fastai’s L class, we can index directly using a list or vector, which makes a lot of code (such as this example) much clearer and more concise.\nWe have ignored the column is_valid up until now, which means that DataBlock has been using a random split by default. To explicitly choose the elements of our validation set, we need to write a function and pass it to splitter (or use one of fastai’s predefined functions or classes). It will take the items (here our whole DataFrame) and must return two (or more) lists of integers:\n\ndef splitter(df):\n    train = df.index[~df['is_valid']].tolist()\n    valid = df.index[df['is_valid']].tolist()\n    return train,valid\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y)\n\ndsets = dblock.datasets(df)\ndsets.train[0]\n\n(PILImage mode=RGB size=500x333,\n TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n\n\nAs we have discussed, a DataLoader collates the items from a Dataset into a mini-batch. This is a tuple of tensors, where each tensor simply stacks the items from that location in the Dataset item.\nNow that we have confirmed that the individual items look okay, there’s one more step we need to ensure we can create our DataLoaders, which is to ensure that every item is of the same size. To do this, we can use RandomResizedCrop:\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y,\n                   item_tfms = RandomResizedCrop(128, min_scale=0.35))\ndls = dblock.dataloaders(df)\n\nAnd now we can display a sample of our data:\n\ndls.show_batch(nrows=1, ncols=3)\n\n\n\n\n\n\n\n\nRemember that if anything goes wrong when you create your DataLoaders from your DataBlock, or if you want to view exactly what happens with your DataBlock, you can use the summary method we presented in the last chapter.\nOur data is now ready for training a model. As we will see, nothing is going to change when we create our Learner, but behind the scenes, the fastai library will pick a new loss function for us: binary cross-entropy.\n\n\nBinary Cross-Entropy\nNow we’ll create our Learner. We saw in &lt;&gt; that a Learner object contains four main things: the model, a DataLoaders object, an Optimizer, and the loss function to use. We already have our DataLoaders, we can leverage fastai’s resnet models (which we’ll learn how to create from scratch later), and we know how to create an SGD optimizer. So let’s focus on ensuring we have a suitable loss function. To do this, let’s use vision_learner to create a Learner, so we can look at its activations:\n\nlearn = vision_learner(dls, resnet18)\n\nWe also saw that the model in a Learner is generally an object of a class inheriting from nn.Module, and that we can call it using parentheses and it will return the activations of a model. You should pass it your independent variable, as a mini-batch. We can try it out by grabbing a mini batch from our DataLoader and then passing it to the model:\n\nx,y = to_cpu(dls.train.one_batch())\nactivs = learn.model(x)\nactivs.shape\n\ntorch.Size([64, 20])\n\n\nThink about why activs has this shape—we have a batch size of 64, and we need to calculate the probability of each of 20 categories. Here’s what one of those activations looks like:\n\nactivs[0]\n\nTensorBase([-1.4608,  0.9895,  0.5279, -1.0224, -1.4174, -0.1778, -0.4821, -0.2561,  0.6638,  0.1715,  2.3625,  4.2209,  1.0515,  4.5342,  0.5485,  1.0585, -0.7959,  2.2770, -1.9935,  1.9646],\n       grad_fn=&lt;AliasBackward0&gt;)\n\n\n\nnote: Getting Model Activations: Knowing how to manually get a mini-batch and pass it into a model, and look at the activations and loss, is really important for debugging your model. It is also very helpful for learning, so that you can see exactly what is going on.\n\nThey aren’t yet scaled to between 0 and 1, but we learned how to do that in &lt;&gt;, using the sigmoid function. We also saw how to calculate a loss based on this—this is our loss function from &lt;&gt;, with the addition of log as discussed in the last chapter:\n\ndef binary_cross_entropy(inputs, targets):\n    inputs = inputs.sigmoid()\n    return -torch.where(targets==1, inputs, 1-inputs).log().mean()\n\nNote that because we have a one-hot-encoded dependent variable, we can’t directly use nll_loss or softmax (and therefore we can’t use cross_entropy):\n\nsoftmax, as we saw, requires that all predictions sum to 1, and tends to push one activation to be much larger than the others (due to the use of exp); however, we may well have multiple objects that we’re confident appear in an image, so restricting the maximum sum of activations to 1 is not a good idea. By the same reasoning, we may want the sum to be less than 1, if we don’t think any of the categories appear in an image.\nnll_loss, as we saw, returns the value of just one activation: the single activation corresponding with the single label for an item. This doesn’t make sense when we have multiple labels.\n\nOn the other hand, the binary_cross_entropy function, which is just mnist_loss along with log, provides just what we need, thanks to the magic of PyTorch’s elementwise operations. Each activation will be compared to each target for each column, so we don’t have to do anything to make this function work for multiple columns.\n\nj: One of the things I really like about working with libraries like PyTorch, with broadcasting and elementwise operations, is that quite frequently I find I can write code that works equally well for a single item or a batch of items, without changes. binary_cross_entropy is a great example of this. By using these operations, we don’t have to write loops ourselves, and can rely on PyTorch to do the looping we need as appropriate for the rank of the tensors we’re working with.\n\nPyTorch already provides this function for us. In fact, it provides a number of versions, with rather confusing names!\nF.binary_cross_entropy and its module equivalent nn.BCELoss calculate cross-entropy on a one-hot-encoded target, but do not include the initial sigmoid. Normally for one-hot-encoded targets you’ll want F.binary_cross_entropy_with_logits (or nn.BCEWithLogitsLoss), which do both sigmoid and binary cross-entropy in a single function, as in the preceding example.\nThe equivalent for single-label datasets (like MNIST or the Pet dataset), where the target is encoded as a single integer, is F.nll_loss or nn.NLLLoss for the version without the initial softmax, and F.cross_entropy or nn.CrossEntropyLoss for the version with the initial softmax.\nSince we have a one-hot-encoded target, we will use BCEWithLogitsLoss:\n\nloss_func = nn.BCEWithLogitsLoss()\nloss = loss_func(activs, y)\nloss\n\nTensorMultiCategory(1.0524, grad_fn=&lt;AliasBackward0&gt;)\n\n\nWe don’t actually need to tell fastai to use this loss function (although we can if we want) since it will be automatically chosen for us. fastai knows that the DataLoaders has multiple category labels, so it will use nn.BCEWithLogitsLoss by default.\nOne change compared to the last chapter is the metric we use: because this is a multilabel problem, we can’t use the accuracy function. Why is that? Well, accuracy was comparing our outputs to our targets like so:\ndef accuracy(inp, targ, axis=-1):\n    \"Compute accuracy with `targ` when `pred` is bs * n_classes\"\n    pred = inp.argmax(dim=axis)\n    return (pred == targ).float().mean()\nThe class predicted was the one with the highest activation (this is what argmax does). Here it doesn’t work because we could have more than one prediction on a single image. After applying the sigmoid to our activations (to make them between 0 and 1), we need to decide which ones are 0s and which ones are 1s by picking a threshold. Each value above the threshold will be considered as a 1, and each value lower than the threshold will be considered a 0:\ndef accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):\n    \"Compute accuracy when `inp` and `targ` are the same size.\"\n    if sigmoid: inp = inp.sigmoid()\n    return ((inp&gt;thresh)==targ.bool()).float().mean()\nIf we pass accuracy_multi directly as a metric, it will use the default value for threshold, which is 0.5. We might want to adjust that default and create a new version of accuracy_multi that has a different default. To help with this, there is a function in Python called partial. It allows us to bind a function with some arguments or keyword arguments, making a new version of that function that, whenever it is called, always includes those arguments. For instance, here is a simple function taking two arguments:\n\ndef say_hello(name, say_what=\"Hello\"): return f\"{say_what} {name}.\"\nsay_hello('Jeremy'),say_hello('Jeremy', 'Ahoy!')\n\n('Hello Jeremy.', 'Ahoy! Jeremy.')\n\n\nWe can switch to a French version of that function by using partial:\n\nf = partial(say_hello, say_what=\"Bonjour\")\nf(\"Jeremy\"),f(\"Sylvain\")\n\n('Bonjour Jeremy.', 'Bonjour Sylvain.')\n\n\nWe can now train our model. Let’s try setting the accuracy threshold to 0.2 for our metric:\n\nlearn = vision_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)\n\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/jhoward/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n0.942999\n0.698309\n0.230896\n00:05\n\n\n1\n0.822529\n0.567567\n0.287151\n00:04\n\n\n2\n0.604535\n0.200134\n0.818327\n00:04\n\n\n3\n0.359754\n0.123086\n0.945558\n00:04\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n0.133748\n0.116784\n0.943725\n00:05\n\n\n1\n0.117125\n0.107055\n0.950837\n00:05\n\n\n2\n0.098062\n0.103551\n0.950877\n00:05\n\n\n\n\n\nPicking a threshold is important. If you pick a threshold that’s too low, you’ll often be failing to select correctly labeled objects. We can see this by changing our metric, and then calling validate, which returns the validation loss and metrics:\n\nlearn.metrics = partial(accuracy_multi, thresh=0.1)\nlearn.validate()\n\n\n\n\n(#2) [0.10477833449840546,0.9314740300178528]\n\n\nIf you pick a threshold that’s too high, you’ll only be selecting the objects for which your model is very confident:\n\nlearn.metrics = partial(accuracy_multi, thresh=0.99)\nlearn.validate()\n\n\n\n\n(#2) [0.10477833449840546,0.9429482221603394]\n\n\nWe can find the best threshold by trying a few levels and seeing what works best. This is much faster if we just grab the predictions once:\n\npreds,targs = learn.get_preds()\n\n\n\n\nThen we can call the metric directly. Note that by default get_preds applies the output activation function (sigmoid, in this case) for us, so we’ll need to tell accuracy_multi to not apply it:\n\naccuracy_multi(preds, targs, thresh=0.9, sigmoid=False)\n\nTensorImage(0.9567)\n\n\nWe can now use this approach to find the best threshold level:\n\nxs = torch.linspace(0.05,0.95,29)\naccs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs]\nplt.plot(xs,accs);\n\n\n\n\n\n\n\n\nIn this case, we’re using the validation set to pick a hyperparameter (the threshold), which is the purpose of the validation set. Sometimes students have expressed their concern that we might be overfitting to the validation set, since we’re trying lots of values to see which is the best. However, as you see in the plot, changing the threshold in this case results in a smooth curve, so we’re clearly not picking some inappropriate outlier. This is a good example of where you have to be careful of the difference between theory (don’t try lots of hyperparameter values or you might overfit the validation set) versus practice (if the relationship is smooth, then it’s fine to do this).\nThis concludes the part of this chapter dedicated to multi-label classification. Next, we’ll take a look at a regression problem."
  },
  {
    "objectID": "Fastbook/06_multicat.html#regression",
    "href": "Fastbook/06_multicat.html#regression",
    "title": "Other Computer Vision Problems",
    "section": "Regression",
    "text": "Regression\nIt’s easy to think of deep learning models as being classified into domains, like computer vision, NLP, and so forth. And indeed, that’s how fastai classifies its applications—largely because that’s how most people are used to thinking of things.\nBut really, that’s hiding a more interesting and deeper perspective. A model is defined by its independent and dependent variables, along with its loss function. That means that there’s really a far wider array of models than just the simple domain-based split. Perhaps we have an independent variable that’s an image, and a dependent that’s text (e.g., generating a caption from an image); or perhaps we have an independent variable that’s text and dependent that’s an image (e.g., generating an image from a caption—which is actually possible for deep learning to do!); or perhaps we’ve got images, texts, and tabular data as independent variables, and we’re trying to predict product purchases… the possibilities really are endless.\nTo be able to move beyond fixed applications, to crafting your own novel solutions to novel problems, it helps to really understand the data block API (and maybe also the mid-tier API, which we’ll see later in the book). As an example, let’s consider the problem of image regression. This refers to learning from a dataset where the independent variable is an image, and the dependent variable is one or more floats. Often we see people treat image regression as a whole separate application—but as you’ll see here, we can treat it as just another CNN on top of the data block API.\nWe’re going to jump straight to a somewhat tricky variant of image regression, because we know you’re ready for it! We’re going to do a key point model. A key point refers to a specific location represented in an image—in this case, we’ll use images of people and we’ll be looking for the center of the person’s face in each image. That means we’ll actually be predicting two values for each image: the row and column of the face center.\n\nAssemble the Data\nWe will use the Biwi Kinect Head Pose dataset for this section. We’ll begin by downloading the dataset as usual:\n\npath = untar_data(URLs.BIWI_HEAD_POSE)\n\n\n#hide\nPath.BASE_PATH = path\n\nLet’s see what we’ve got!\n\npath.ls().sorted()\n\n(#50) [Path('01'),Path('01.obj'),Path('02'),Path('02.obj'),Path('03'),Path('03.obj'),Path('04'),Path('04.obj'),Path('05'),Path('05.obj')...]\n\n\nThere are 24 directories numbered from 01 to 24 (they correspond to the different people photographed), and a corresponding .obj file for each (we won’t need them here). Let’s take a look inside one of these directories:\n\n(path/'01').ls().sorted()\n\n(#1000) [Path('01/depth.cal'),Path('01/frame_00003_pose.txt'),Path('01/frame_00003_rgb.jpg'),Path('01/frame_00004_pose.txt'),Path('01/frame_00004_rgb.jpg'),Path('01/frame_00005_pose.txt'),Path('01/frame_00005_rgb.jpg'),Path('01/frame_00006_pose.txt'),Path('01/frame_00006_rgb.jpg'),Path('01/frame_00007_pose.txt')...]\n\n\nInside the subdirectories, we have different frames, each of them come with an image (_rgb.jpg) and a pose file (_pose.txt). We can easily get all the image files recursively with get_image_files, then write a function that converts an image filename to its associated pose file:\n\nimg_files = get_image_files(path)\ndef img2pose(x): return Path(f'{str(x)[:-7]}pose.txt')\nimg2pose(img_files[0])\n\nPath('13/frame_00349_pose.txt')\n\n\nLet’s take a look at our first image:\n\nim = PILImage.create(img_files[0])\nim.shape\n\n(480, 640)\n\n\n\nim.to_thumb(160)\n\n\n\n\n\n\n\n\nThe Biwi dataset website used to explain the format of the pose text file associated with each image, which shows the location of the center of the head. The details of this aren’t important for our purposes, so we’ll just show the function we use to extract the head center point:\n\ncal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)\ndef get_ctr(f):\n    ctr = np.genfromtxt(img2pose(f), skip_header=3)\n    c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n    c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n    return tensor([c1,c2])\n\nThis function returns the coordinates as a tensor of two items:\n\nget_ctr(img_files[0])\n\ntensor([384.6370, 259.4787])\n\n\nWe can pass this function to DataBlock as get_y, since it is responsible for labeling each item. We’ll resize the images to half their input size, just to speed up training a bit.\nOne important point to note is that we should not just use a random splitter. The reason for this is that the same people appear in multiple images in this dataset, but we want to ensure that our model can generalize to people that it hasn’t seen yet. Each folder in the dataset contains the images for one person. Therefore, we can create a splitter function that returns true for just one person, resulting in a validation set containing just that person’s images.\nThe only other difference from the previous data block examples is that the second block is a PointBlock. This is necessary so that fastai knows that the labels represent coordinates; that way, it knows that when doing data augmentation, it should do the same augmentation to these coordinates as it does to the images:\n\nbiwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    get_y=get_ctr,\n    splitter=FuncSplitter(lambda o: o.parent.name=='13'),\n    batch_tfms=aug_transforms(size=(240,320)), \n)\n\n\nimportant: Points and Data Augmentation: We’re not aware of other libraries (except for fastai) that automatically and correctly apply data augmentation to coordinates. So, if you’re working with another library, you may need to disable data augmentation for these kinds of problems.\n\nBefore doing any modeling, we should look at our data to confirm it seems okay:\n\ndls = biwi.dataloaders(path)\ndls.show_batch(max_n=9, figsize=(8,6))\n\n\n\n\n\n\n\n\nThat’s looking good! As well as looking at the batch visually, it’s a good idea to also look at the underlying tensors (especially as a student; it will help clarify your understanding of what your model is really seeing):\n\nxb,yb = dls.one_batch()\nxb.shape,yb.shape\n\n(torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2]))\n\n\nMake sure that you understand why these are the shapes for our mini-batches.\nHere’s an example of one row from the dependent variable:\n\nyb[0]\n\nTensorPoint([[-0.3375,  0.2193]], device='cuda:6')\n\n\nAs you can see, we haven’t had to use a separate image regression application; all we’ve had to do is label the data, and tell fastai what kinds of data the independent and dependent variables represent.\nIt’s the same for creating our Learner. We will use the same function as before, with one new parameter, and we will be ready to train our model.\n\n\nTraining a Model\nAs usual, we can use vision_learner to create our Learner. Remember way back in &lt;&gt; how we used y_range to tell fastai the range of our targets? We’ll do the same here (coordinates in fastai and PyTorch are always rescaled between -1 and +1):\n\nlearn = vision_learner(dls, resnet18, y_range=(-1,1))\n\ny_range is implemented in fastai using sigmoid_range, which is defined as:\n\ndef sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo\n\nThis is set as the final layer of the model, if y_range is defined. Take a moment to think about what this function does, and why it forces the model to output activations in the range (lo,hi).\nHere’s what it looks like:\n\nplot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4)\n\n/home/jhoward/anaconda3/lib/python3.7/site-packages/fastbook/__init__.py:55: UserWarning: Not providing a value for linspace's steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at  /pytorch/aten/src/ATen/native/RangeFactories.cpp:23.)\n  x = torch.linspace(min,max)\n\n\n\n\n\n\n\n\n\nWe didn’t specify a loss function, which means we’re getting whatever fastai chooses as the default. Let’s see what it picked for us:\n\ndls.loss_func\n\nFlattenedLoss of MSELoss()\n\n\nThis makes sense, since when coordinates are used as the dependent variable, most of the time we’re likely to be trying to predict something as close as possible; that’s basically what MSELoss (mean squared error loss) does. If you want to use a different loss function, you can pass it to vision_learner using the loss_func parameter.\nNote also that we didn’t specify any metrics. That’s because the MSE is already a useful metric for this task (although it’s probably more interpretable after we take the square root).\nWe can pick a good learning rate with the learning rate finder:\n\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.005754399299621582, lr_steep=0.033113110810518265)\n\n\n\n\n\n\n\n\n\nWe’ll try an LR of 1e-2:\n\nlr = 1e-2\nlearn.fine_tune(3, lr)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.049630\n0.007602\n00:42\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.008714\n0.004291\n00:53\n\n\n1\n0.003213\n0.000715\n00:53\n\n\n2\n0.001482\n0.000036\n00:53\n\n\n\n\n\nGenerally when we run this we get a loss of around 0.0001, which corresponds to an average coordinate prediction error of:\n\nmath.sqrt(0.0001)\n\n0.01\n\n\nThis sounds very accurate! But it’s important to take a look at our results with Learner.show_results. The left side are the actual (ground truth) coordinates and the right side are our model’s predictions:\n\nlearn.show_results(ds_idx=1, nrows=3, figsize=(6,8))\n\n\n\n\n\n\n\n\n\n\n\nIt’s quite amazing that with just a few minutes of computation we’ve created such an accurate key points model, and without any special domain-specific application. This is the power of building on flexible APIs, and using transfer learning! It’s particularly striking that we’ve been able to use transfer learning so effectively even between totally different tasks; our pretrained model was trained to do image classification, and we fine-tuned for image regression."
  },
  {
    "objectID": "Fastbook/06_multicat.html#conclusion",
    "href": "Fastbook/06_multicat.html#conclusion",
    "title": "Other Computer Vision Problems",
    "section": "Conclusion",
    "text": "Conclusion\nIn problems that are at first glance completely different (single-label classification, multi-label classification, and regression), we end up using the same model with just different numbers of outputs. The loss function is the one thing that changes, which is why it’s important to double-check that you are using the right loss function for your problem.\nfastai will automatically try to pick the right one from the data you built, but if you are using pure PyTorch to build your DataLoaders, make sure you think hard when you have to decide on your choice of loss function, and remember that you most probably want:\n\nnn.CrossEntropyLoss for single-label classification\nnn.BCEWithLogitsLoss for multi-label classification\nnn.MSELoss for regression"
  },
  {
    "objectID": "Fastbook/06_multicat.html#questionnaire",
    "href": "Fastbook/06_multicat.html#questionnaire",
    "title": "Other Computer Vision Problems",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nHow could multi-label classification improve the usability of the bear classifier?\nHow do we encode the dependent variable in a multi-label classification problem?\nHow do you access the rows and columns of a DataFrame as if it was a matrix?\nHow do you get a column by name from a DataFrame?\nWhat is the difference between a Dataset and DataLoader?\nWhat does a Datasets object normally contain?\nWhat does a DataLoaders object normally contain?\nWhat does lambda do in Python?\nWhat are the methods to customize how the independent and dependent variables are created with the data block API?\nWhy is softmax not an appropriate output activation function when using a one hot encoded target?\nWhy is nll_loss not an appropriate loss function when using a one-hot-encoded target?\nWhat is the difference between nn.BCELoss and nn.BCEWithLogitsLoss?\nWhy can’t we use regular accuracy in a multi-label problem?\nWhen is it okay to tune a hyperparameter on the validation set?\nHow is y_range implemented in fastai? (See if you can implement it yourself and test it without peeking!)\nWhat is a regression problem? What loss function should you use for such a problem?\nWhat do you need to do to make sure the fastai library applies the same data augmentation to your input images and your target point coordinates?\n\n\nFurther Research\n\nRead a tutorial about Pandas DataFrames and experiment with a few methods that look interesting to you. See the book’s website for recommended tutorials.\nRetrain the bear classifier using multi-label classification. See if you can make it work effectively with images that don’t contain any bears, including showing that information in the web application. Try an image with two different kinds of bears. Check whether the accuracy on the single-label dataset is impacted using multi-label classification."
  },
  {
    "objectID": "Fastbook/app_jupyter.html",
    "href": "Fastbook/app_jupyter.html",
    "title": "Appendix: Jupyter Notebook 101",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\nfrom fastbook import *\n[appendix] [role=“Jupyter notebook 101”]\nYou can read this tutorial in the book, but we strongly suggest reading it in a (yes, you guessed it) Jupyter Notebook. This way, you will be able to actually try the different commands we will introduce here. If you followed one of our tutorials in the previous section, you should have been left in the course folder. Just click on nbs then dl1 and you should find the tutorial named 00_notebook_tutorial. Click on it to open a new tab and you’ll be ready to go.\nIf you are on your personal machine, clone the course repository and navigate inside before following the same steps."
  },
  {
    "objectID": "Fastbook/app_jupyter.html#introduction",
    "href": "Fastbook/app_jupyter.html#introduction",
    "title": "Appendix: Jupyter Notebook 101",
    "section": "Introduction",
    "text": "Introduction\nLet’s build up from the basics: what is a Jupyter Notebook? Well, we wrote this book using Jupyter Notebooks. A notebook is a document made of cells. You can write in some of them (markdown cells) or you can perform calculations in Python (code cells) and run them like this:\n\n1+1\n\n2\n\n\nCool, huh? This combination of prose and code makes Jupyter Notebook ideal for experimentation: we can see the rationale for each experiment, the code, and the results in one comprehensive document.\nOther renowned institutions in academia and industry use Jupyter Notebook, including Google, Microsoft, IBM, Bloomberg, Berkeley and NASA among others. Even Nobel-winning economists use Jupyter Notebooks for their experiments and some suggest that Jupyter Notebooks will be the new format for research papers."
  },
  {
    "objectID": "Fastbook/app_jupyter.html#writing",
    "href": "Fastbook/app_jupyter.html#writing",
    "title": "Appendix: Jupyter Notebook 101",
    "section": "Writing",
    "text": "Writing\nA type of cell in which you can write text is called a Markdown cell. Markdown is a very popular markup language. To specify that a cell is Markdown you need to click in the drop-down menu in the toolbar and select Markdown.\nClick on the the ‘+’ button on the left and select Markdown from the toolbar. Now you can type your first Markdown cell. Write ‘My first markdown cell’ and press run.\n\n\n\nadd\n\n\nYou should see something like this:\nMy first markdown cell\nNow try making your first Code cell: follow the same steps as before but don’t change the cell type (when you add a cell its default type is Code). Type something like 3/2. You should see ‘1.5’ as output.\n\n3/2\n\n1.5"
  },
  {
    "objectID": "Fastbook/app_jupyter.html#modes",
    "href": "Fastbook/app_jupyter.html#modes",
    "title": "Appendix: Jupyter Notebook 101",
    "section": "Modes",
    "text": "Modes\nIf you made a mistake in your Markdown cell and you have already run it, you will notice that you cannot edit it just by clicking on it. This is because you are in Command Mode. Jupyter Notebooks have two distinct modes:\n\nEdit Mode:: Allows you to edit a cell’s content.\nCommand Mode:: Allows you to edit the notebook as a whole and use keyboard shortcuts but not edit a cell’s content.\n\nYou can toggle between these two by either pressing ESC and Enter or clicking outside a cell or inside it (you need to double click if it’s a Markdown cell). You can always tell which mode you’re on: the current cell will have a green border in Edit Mode and a blue border in Command Mode. Try it!"
  },
  {
    "objectID": "Fastbook/app_jupyter.html#other-important-considerations",
    "href": "Fastbook/app_jupyter.html#other-important-considerations",
    "title": "Appendix: Jupyter Notebook 101",
    "section": "Other Important Considerations",
    "text": "Other Important Considerations\nYour notebook is autosaved every 120 seconds. If you want to manually save it you can just press the save button on the upper left corner or press s in Command Mode.\n\n\n\nSave\n\n\nTo know if your kernel (the Python engine executing your instructions behind the scenes) is computing or not, you can check the dot in your upper right corner. If the dot is full, it means that the kernel is working. If not, it is idle. You can place the mouse on it and the state of the kernel will be displayed.\n\n\n\nBusy\n\n\nThere are a couple of shortcuts you must know about which we use all the time (always in Command Mode). These are:\n\nShift+Enter:: Run the code or markdown on a cell\nUp Arrow+Down Arrow:: Toggle across cells\nb:: Create new cell\n0+0:: Reset Kernel\n\nYou can find more shortcuts by typing h (for help).\nYou may need to use a terminal in a Jupyter Notebook environment (for example to git pull on a repository). That is very easy to do: just press ‘New’ in your Home directory and ‘Terminal’. Don’t know how to use the Terminal? We made a tutorial for that as well. You can find it here.\n\n\n\nTerminal\n\n\nThat’s it. This is all you need to know to use Jupyter Notebooks. That said, we have more tips and tricks below, so don’t jump to the next section just yet."
  },
  {
    "objectID": "Fastbook/app_jupyter.html#markdown-formatting",
    "href": "Fastbook/app_jupyter.html#markdown-formatting",
    "title": "Appendix: Jupyter Notebook 101",
    "section": "Markdown Formatting",
    "text": "Markdown Formatting\n\nItalics, Bold, Strikethrough, Inline, Blockquotes and Links\nThe five most important concepts to format your code appropriately when using Markdown are:\n\nItalics:: Surround your text with _ or *.\nBold:: Surround your text with __ or **.\ninline:: Surround your text with `.\nblockquote:: Place &gt; before your text.\nLinks:: Surround the text you want to link with [] and place the link adjacent to the text, surrounded with ().\n\n\n\nHeadings\nNotice that including a hashtag before the text in a markdown cell makes the text a heading. The number of hashtags you include will determine the priority of the header (# is level one, ## is level two, ### is level three and #### is level four). We will add three new cells with the + button on the left to see how every level of heading looks.\nIn the notebook, double click on some headings and find out what level they are!\n\n\nLists\nThere are three types of lists in markdown.\nOrdered list:\n\nStep 1\n\nStep 1B\n\nStep 3\n\nUnordered list\n\nlearning rate\ncycle length\nweight decay\n\nTask list\n\nLearn Jupyter Notebooks\n\nWriting\nModes\nOther Considerations\n\nChange the world\n\nIn the notebook, double click on them to see how they are built!"
  },
  {
    "objectID": "Fastbook/app_jupyter.html#code-capabilities",
    "href": "Fastbook/app_jupyter.html#code-capabilities",
    "title": "Appendix: Jupyter Notebook 101",
    "section": "Code Capabilities",
    "text": "Code Capabilities\nCode cells are different than Markdown cells in that they have an output cell. This means that we can keep the results of our code within the notebook and share them. Let’s say we want to show a graph that explains the result of an experiment. We can just run the necessary cells and save the notebook. The output will be there when we open it again! Try it out by running the next four cells.\n\n# Import necessary libraries\nfrom fastai.vision.all import * \nimport matplotlib.pyplot as plt\n\n\nfrom PIL import Image\n\n\na = 1\nb = a + 1\nc = b + a + 1\nd = c + b + a + 1\na, b, c ,d\n\n(1, 2, 4, 8)\n\n\n\nplt.plot([a,b,c,d])\nplt.show()\n\n\n\n\n\n\n\n\nWe can also print images while experimenting.\n\nImage.open(image_cat())"
  },
  {
    "objectID": "Fastbook/app_jupyter.html#running-the-app-locally",
    "href": "Fastbook/app_jupyter.html#running-the-app-locally",
    "title": "Appendix: Jupyter Notebook 101",
    "section": "Running the App Locally",
    "text": "Running the App Locally\nYou may be running Jupyter Notebook from an interactive coding environment like Gradient, Sagemaker or Salamander. You can also run a Jupyter Notebook server from your local computer. What’s more, if you have installed Anaconda you don’t even need to install Jupyter (if not, just pip install jupyter).\nYou just need to run jupyter notebook in your terminal. Remember to run it from a folder that contains all the folders/files you will want to access. You will be able to open, view, and edit files located within the directory in which you run this command but not files in parent directories.\nIf a browser tab does not open automatically once you run the command, you should CTRL+CLICK the link starting with ‘http://localhost:’ and this will open a new tab in your default browser."
  },
  {
    "objectID": "Fastbook/app_jupyter.html#creating-a-notebook",
    "href": "Fastbook/app_jupyter.html#creating-a-notebook",
    "title": "Appendix: Jupyter Notebook 101",
    "section": "Creating a Notebook",
    "text": "Creating a Notebook\nNow that you have your own Jupyter Notebook server running, you will probably want to write your own notebook. Click on ‘New’ in the upper left corner and ‘Python 3’ in the drop-down list (we are going to use a Python kernel for all our experiments).\n\n\n\nnew_notebook"
  },
  {
    "objectID": "Fastbook/app_jupyter.html#shortcuts-and-tricks",
    "href": "Fastbook/app_jupyter.html#shortcuts-and-tricks",
    "title": "Appendix: Jupyter Notebook 101",
    "section": "Shortcuts and Tricks",
    "text": "Shortcuts and Tricks\nHere is a list of useful tricks when in a Jupyter Notebook. Make sure you learn them early and use them as often as you can!\n\nCommand Mode Shortcuts\nThere are a couple of useful keyboard shortcuts in Command Mode that you can leverage to make Jupyter Notebook faster to use. Remember that you can switch back and forth between Command Mode and Edit Mode with Esc and Enter.\n\nm:: Convert cell to Markdown\ny:: Convert cell to Code\nd+d:: Delete cell\no:: Toggle between hide or show output\nShift+Arrow up/Arrow down:: Select multiple cells. Once you have selected them you can operate on them like a batch (run, copy, paste etc).\nShift+M:: Merge selected cells\n\n\n\nCell Tricks\nThere are also some tricks that you can code into a cell:\n\n?function-name:: Shows the definition and docstring for that function\n??function-name:: Shows the source code for that function\ndoc(function-name):: Shows the definition, docstring and links to the documentation of the function (only works with fastai library imported)\nShift+Tab (press once):: See which parameters to pass to a function\nShift+Tab (press three times):: Get additional information on the method\n\n\n\nLine Magics\nLine magics are functions that you can run on cells. They should be at the beginning of a line and take as an argument the rest of the line from where they are called. You call them by placing a ‘%’ sign before the command. The most useful ones are:\n\n%matplotlib inline:: Ensures that all matplotlib plots will be plotted in the output cell within the notebook and will be kept in the notebook when saved.\n\nThis command is always called together at the beginning of every notebook of the fast.ai course.\n%matplotlib inline\n\n%timeit:: Runs a line ten thousand times and displays the average time it took to run.\n\n\n%timeit [i+1 for i in range(1000)]\n\n56.1 µs ± 592 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\n\n%debug: Inspects a function which is showing an error using the Python debugger. If you type this in a cell just after an error, you will be directed to a console where you can inspect the values of all the variables."
  },
  {
    "objectID": "Fastbook/04_mnist_basics.html",
    "href": "Fastbook/04_mnist_basics.html",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastai.vision.all import *\nfrom fastbook import *\n\nmatplotlib.rc('image', cmap='Greys')\n[[chapter_mnist_basics]]\nHaving seen what it looks like to actually train a variety of models in Chapter 2, let’s now look under the hood and see exactly what is going on. We’ll start by using computer vision to introduce fundamental tools and concepts for deep learning.\nTo be exact, we’ll discuss the roles of arrays and tensors and of broadcasting, a powerful technique for using them expressively. We’ll explain stochastic gradient descent (SGD), the mechanism for learning by updating weights automatically. We’ll discuss the choice of a loss function for our basic classification task, and the role of mini-batches. We’ll also describe the math that a basic neural network is actually doing. Finally, we’ll put all these pieces together.\nIn future chapters we’ll do deep dives into other applications as well, and see how these concepts and tools generalize. But this chapter is about laying foundation stones. To be frank, that also makes this one of the hardest chapters, because of how these concepts all depend on each other. Like an arch, all the stones need to be in place for the structure to stay up. Also like an arch, once that happens, it’s a powerful structure that can support other things. But it requires some patience to assemble.\nLet’s begin. The first step is to consider how images are represented in a computer."
  },
  {
    "objectID": "Fastbook/04_mnist_basics.html#pixels-the-foundations-of-computer-vision",
    "href": "Fastbook/04_mnist_basics.html#pixels-the-foundations-of-computer-vision",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "Pixels: The Foundations of Computer Vision",
    "text": "Pixels: The Foundations of Computer Vision\nIn order to understand what happens in a computer vision model, we first have to understand how computers handle images. We’ll use one of the most famous datasets in computer vision, MNIST, for our experiments. MNIST contains images of handwritten digits, collected by the National Institute of Standards and Technology and collated into a machine learning dataset by Yann Lecun and his colleagues. Lecun used MNIST in 1998 in Lenet-5, the first computer system to demonstrate practically useful recognition of handwritten digit sequences. This was one of the most important breakthroughs in the history of AI."
  },
  {
    "objectID": "Fastbook/04_mnist_basics.html#sidebar-tenacity-and-deep-learning",
    "href": "Fastbook/04_mnist_basics.html#sidebar-tenacity-and-deep-learning",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "Sidebar: Tenacity and Deep Learning",
    "text": "Sidebar: Tenacity and Deep Learning\nThe story of deep learning is one of tenacity and grit by a handful of dedicated researchers. After early hopes (and hype!) neural networks went out of favor in the 1990’s and 2000’s, and just a handful of researchers kept trying to make them work well. Three of them, Yann Lecun, Yoshua Bengio, and Geoffrey Hinton, were awarded the highest honor in computer science, the Turing Award (generally considered the “Nobel Prize of computer science”), in 2018 after triumphing despite the deep skepticism and disinterest of the wider machine learning and statistics community.\nGeoff Hinton has told of how even academic papers showing dramatically better results than anything previously published would be rejected by top journals and conferences, just because they used a neural network. Yann Lecun’s work on convolutional neural networks, which we will study in the next section, showed that these models could read handwritten text—something that had never been achieved before. However, his breakthrough was ignored by most researchers, even as it was used commercially to read 10% of the checks in the US!\nIn addition to these three Turing Award winners, there are many other researchers who have battled to get us to where we are today. For instance, Jurgen Schmidhuber (who many believe should have shared in the Turing Award) pioneered many important ideas, including working with his student Sepp Hochreiter on the long short-term memory (LSTM) architecture (widely used for speech recognition and other text modeling tasks, and used in the IMDb example in &lt;&gt;). Perhaps most important of all, Paul Werbos in 1974 invented back-propagation for neural networks, the technique shown in this chapter and used universally for training neural networks (Werbos 1994). His development was almost entirely ignored for decades, but today it is considered the most important foundation of modern AI.\nThere is a lesson here for all of us! On your deep learning journey you will face many obstacles, both technical, and (even more difficult) posed by people around you who don’t believe you’ll be successful. There’s one guaranteed way to fail, and that’s to stop trying. We’ve seen that the only consistent trait amongst every fast.ai student that’s gone on to be a world-class practitioner is that they are all very tenacious."
  },
  {
    "objectID": "Fastbook/04_mnist_basics.html#end-sidebar",
    "href": "Fastbook/04_mnist_basics.html#end-sidebar",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "End sidebar",
    "text": "End sidebar\nFor this initial tutorial we are just going to try to create a model that can classify any image as a 3 or a 7. So let’s download a sample of MNIST that contains images of just these digits:\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\n#hide\nPath.BASE_PATH = path\n\nWe can see what’s in this directory by using ls, a method added by fastai. This method returns an object of a special fastai class called L, which has all the same functionality of Python’s built-in list, plus a lot more. One of its handy features is that, when printed, it displays the count of items, before listing the items themselves (if there are more than 10 items, it just shows the first few):\n\npath.ls()\n\n(#9) [Path('cleaned.csv'),Path('item_list.txt'),Path('trained_model.pkl'),Path('models'),Path('valid'),Path('labels.csv'),Path('export.pkl'),Path('history.csv'),Path('train')]\n\n\nThe MNIST dataset follows a common layout for machine learning datasets: separate folders for the training set and the validation set (and/or test set). Let’s see what’s inside the training set:\n\n(path/'train').ls()\n\n(#2) [Path('train/7'),Path('train/3')]\n\n\nThere’s a folder of 3s, and a folder of 7s. In machine learning parlance, we say that “3” and “7” are the labels (or targets) in this dataset. Let’s take a look in one of these folders (using sorted to ensure we all get the same order of files):\n\nthrees = (path/'train'/'3').ls().sorted()\nsevens = (path/'train'/'7').ls().sorted()\nthrees\n\n(#6131) [Path('train/3/10.png'),Path('train/3/10000.png'),Path('train/3/10011.png'),Path('train/3/10031.png'),Path('train/3/10034.png'),Path('train/3/10042.png'),Path('train/3/10052.png'),Path('train/3/1007.png'),Path('train/3/10074.png'),Path('train/3/10091.png')...]\n\n\nAs we might expect, it’s full of image files. Let’s take a look at one now. Here’s an image of a handwritten number 3, taken from the famous MNIST dataset of handwritten numbers:\n\nim3_path = threes[1]\nim3 = Image.open(im3_path)\nim3\n\n\n\n\n\n\n\n\nHere we are using the Image class from the Python Imaging Library (PIL), which is the most widely used Python package for opening, manipulating, and viewing images. Jupyter knows about PIL images, so it displays the image for us automatically.\nIn a computer, everything is represented as a number. To view the numbers that make up this image, we have to convert it to a NumPy array or a PyTorch tensor. For instance, here’s what a section of the image looks like, converted to a NumPy array:\n\narray(im3)[4:10,4:10]\n\narray([[  0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,  29],\n       [  0,   0,   0,  48, 166, 224],\n       [  0,  93, 244, 249, 253, 187],\n       [  0, 107, 253, 253, 230,  48],\n       [  0,   3,  20,  20,  15,   0]], dtype=uint8)\n\n\nThe 4:10 indicates we requested the rows from index 4 (included) to 10 (not included) and the same for the columns. NumPy indexes from top to bottom and left to right, so this section is located in the top-left corner of the image. Here’s the same thing as a PyTorch tensor:\n\ntensor(im3)[4:10,4:10]\n\ntensor([[  0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,  29],\n        [  0,   0,   0,  48, 166, 224],\n        [  0,  93, 244, 249, 253, 187],\n        [  0, 107, 253, 253, 230,  48],\n        [  0,   3,  20,  20,  15,   0]], dtype=torch.uint8)\n\n\nWe can slice the array to pick just the part with the top of the digit in it, and then use a Pandas DataFrame to color-code the values using a gradient, which shows us clearly how the image is created from the pixel values:\n\n#hide_output\nim3_t = tensor(im3)\ndf = pd.DataFrame(im3_t[4:15,4:22])\ndf.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n29\n150\n195\n254\n255\n254\n176\n193\n150\n96\n0\n0\n0\n\n\n2\n0\n0\n0\n48\n166\n224\n253\n253\n234\n196\n253\n253\n253\n253\n233\n0\n0\n0\n\n\n3\n0\n93\n244\n249\n253\n187\n46\n10\n8\n4\n10\n194\n253\n253\n233\n0\n0\n0\n\n\n4\n0\n107\n253\n253\n230\n48\n0\n0\n0\n0\n0\n192\n253\n253\n156\n0\n0\n0\n\n\n5\n0\n3\n20\n20\n15\n0\n0\n0\n0\n0\n43\n224\n253\n245\n74\n0\n0\n0\n\n\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n249\n253\n245\n126\n0\n0\n0\n0\n\n\n7\n0\n0\n0\n0\n0\n0\n0\n14\n101\n223\n253\n248\n124\n0\n0\n0\n0\n0\n\n\n8\n0\n0\n0\n0\n0\n11\n166\n239\n253\n253\n253\n187\n30\n0\n0\n0\n0\n0\n\n\n9\n0\n0\n0\n0\n0\n16\n248\n250\n253\n253\n253\n253\n232\n213\n111\n2\n0\n0\n\n\n10\n0\n0\n0\n0\n0\n0\n0\n43\n98\n98\n208\n253\n253\n253\n253\n187\n22\n0\n\n\n\n\n\n\nYou can see that the background white pixels are stored as the number 0, black is the number 255, and shades of gray are between the two. The entire image contains 28 pixels across and 28 pixels down, for a total of 784 pixels. (This is much smaller than an image that you would get from a phone camera, which has millions of pixels, but is a convenient size for our initial learning and experiments. We will build up to bigger, full-color images soon.)\nSo, now you’ve seen what an image looks like to a computer, let’s recall our goal: create a model that can recognize 3s and 7s. How might you go about getting a computer to do that?\n\nWarning: Stop and Think!: Before you read on, take a moment to think about how a computer might be able to recognize these two different digits. What kinds of features might it be able to look at? How might it be able to identify these features? How could it combine them together? Learning works best when you try to solve problems yourself, rather than just reading somebody else’s answers; so step away from this book for a few minutes, grab a piece of paper and pen, and jot some ideas down…"
  },
  {
    "objectID": "Fastbook/04_mnist_basics.html#first-try-pixel-similarity",
    "href": "Fastbook/04_mnist_basics.html#first-try-pixel-similarity",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "First Try: Pixel Similarity",
    "text": "First Try: Pixel Similarity\nSo, here is a first idea: how about we find the average pixel value for every pixel of the 3s, then do the same for the 7s. This will give us two group averages, defining what we might call the “ideal” 3 and 7. Then, to classify an image as one digit or the other, we see which of these two ideal digits the image is most similar to. This certainly seems like it should be better than nothing, so it will make a good baseline.\n\njargon: Baseline: A simple model which you are confident should perform reasonably well. It should be very simple to implement, and very easy to test, so that you can then test each of your improved ideas, and make sure they are always better than your baseline. Without starting with a sensible baseline, it is very difficult to know whether your super-fancy models are actually any good. One good approach to creating a baseline is doing what we have done here: think of a simple, easy-to-implement model. Another good approach is to search around to find other people that have solved similar problems to yours, and download and run their code on your dataset. Ideally, try both of these!\n\nStep one for our simple model is to get the average of pixel values for each of our two groups. In the process of doing this, we will learn a lot of neat Python numeric programming tricks!\nLet’s create a tensor containing all of our 3s stacked together. We already know how to create a tensor containing a single image. To create a tensor containing all the images in a directory, we will first use a Python list comprehension to create a plain list of the single image tensors.\nWe will use Jupyter to do some little checks of our work along the way—in this case, making sure that the number of returned items seems reasonable:\n\nseven_tensors = [tensor(Image.open(o)) for o in sevens]\nthree_tensors = [tensor(Image.open(o)) for o in threes]\nlen(three_tensors),len(seven_tensors)\n\n(6131, 6265)\n\n\n\nnote: List Comprehensions: List and dictionary comprehensions are a wonderful feature of Python. Many Python programmers use them every day, including the authors of this book—they are part of “idiomatic Python.” But programmers coming from other languages may have never seen them before. There are a lot of great tutorials just a web search away, so we won’t spend a long time discussing them now. Here is a quick explanation and example to get you started. A list comprehension looks like this: new_list = [f(o) for o in a_list if o&gt;0]. This will return every element of a_list that is greater than 0, after passing it to the function f. There are three parts here: the collection you are iterating over (a_list), an optional filter (if o&gt;0), and something to do to each element (f(o)). It’s not only shorter to write but way faster than the alternative ways of creating the same list with a loop.\n\nWe’ll also check that one of the images looks okay. Since we now have tensors (which Jupyter by default will print as values), rather than PIL images (which Jupyter by default will display as images), we need to use fastai’s show_image function to display it:\n\nshow_image(three_tensors[1]);\n\n\n\n\n\n\n\n\nFor every pixel position, we want to compute the average over all the images of the intensity of that pixel. To do this we first combine all the images in this list into a single three-dimensional tensor. The most common way to describe such a tensor is to call it a rank-3 tensor. We often need to stack up individual tensors in a collection into a single tensor. Unsurprisingly, PyTorch comes with a function called stack that we can use for this purpose.\nSome operations in PyTorch, such as taking a mean, require us to cast our integer types to float types. Since we’ll be needing this later, we’ll also cast our stacked tensor to float now. Casting in PyTorch is as simple as typing the name of the type you wish to cast to, and treating it as a method.\nGenerally when images are floats, the pixel values are expected to be between 0 and 1, so we will also divide by 255 here:\n\nstacked_sevens = torch.stack(seven_tensors).float()/255\nstacked_threes = torch.stack(three_tensors).float()/255\nstacked_threes.shape\n\ntorch.Size([6131, 28, 28])\n\n\nPerhaps the most important attribute of a tensor is its shape. This tells you the length of each axis. In this case, we can see that we have 6,131 images, each of size 28×28 pixels. There is nothing specifically about this tensor that says that the first axis is the number of images, the second is the height, and the third is the width—the semantics of a tensor are entirely up to us, and how we construct it. As far as PyTorch is concerned, it is just a bunch of numbers in memory.\nThe length of a tensor’s shape is its rank:\n\nlen(stacked_threes.shape)\n\n3\n\n\nIt is really important for you to commit to memory and practice these bits of tensor jargon: rank is the number of axes or dimensions in a tensor; shape is the size of each axis of a tensor.\n\nA: Watch out because the term “dimension” is sometimes used in two ways. Consider that we live in “three-dimensonal space” where a physical position can be described by a 3-vector v. But according to PyTorch, the attribute v.ndim (which sure looks like the “number of dimensions” of v) equals one, not three! Why? Because v is a vector, which is a tensor of rank one, meaning that it has only one axis (even if that axis has a length of three). In other words, sometimes dimension is used for the size of an axis (“space is three-dimensional”); other times, it is used for the rank, or the number of axes (“a matrix has two dimensions”). When confused, I find it helpful to translate all statements into terms of rank, axis, and length, which are unambiguous terms.\n\nWe can also get a tensor’s rank directly with ndim:\n\nstacked_threes.ndim\n\n3\n\n\nFinally, we can compute what the ideal 3 looks like. We calculate the mean of all the image tensors by taking the mean along dimension 0 of our stacked, rank-3 tensor. This is the dimension that indexes over all the images.\nIn other words, for every pixel position, this will compute the average of that pixel over all images. The result will be one value for every pixel position, or a single image. Here it is:\n\nmean3 = stacked_threes.mean(0)\nshow_image(mean3);\n\n\n\n\n\n\n\n\nAccording to this dataset, this is the ideal number 3! (You may not like it, but this is what peak number 3 performance looks like.) You can see how it’s very dark where all the images agree it should be dark, but it becomes wispy and blurry where the images disagree.\nLet’s do the same thing for the 7s, but put all the steps together at once to save some time:\n\nmean7 = stacked_sevens.mean(0)\nshow_image(mean7);\n\n\n\n\n\n\n\n\nLet’s now pick an arbitrary 3 and measure its distance from our “ideal digits.”\n\nstop: Stop and Think!: How would you calculate how similar a particular image is to each of our ideal digits? Remember to step away from this book and jot down some ideas before you move on! Research shows that recall and understanding improves dramatically when you are engaged with the learning process by solving problems, experimenting, and trying new ideas yourself\n\nHere’s a sample 3:\n\na_3 = stacked_threes[1]\nshow_image(a_3);\n\n\n\n\n\n\n\n\nHow can we determine its distance from our ideal 3? We can’t just add up the differences between the pixels of this image and the ideal digit. Some differences will be positive while others will be negative, and these differences will cancel out, resulting in a situation where an image that is too dark in some places and too light in others might be shown as having zero total differences from the ideal. That would be misleading!\nTo avoid this, there are two main ways data scientists measure distance in this context:\n\nTake the mean of the absolute value of differences (absolute value is the function that replaces negative values with positive values). This is called the mean absolute difference or L1 norm\nTake the mean of the square of differences (which makes everything positive) and then take the square root (which undoes the squaring). This is called the root mean squared error (RMSE) or L2 norm.\n\n\nimportant: It’s Okay to Have Forgotten Your Math: In this book we generally assume that you have completed high school math, and remember at least some of it… But everybody forgets some things! It all depends on what you happen to have had reason to practice in the meantime. Perhaps you have forgotten what a square root is, or exactly how they work. No problem! Any time you come across a maths concept that is not explained fully in this book, don’t just keep moving on; instead, stop and look it up. Make sure you understand the basic idea, how it works, and why we might be using it. One of the best places to refresh your understanding is Khan Academy. For instance, Khan Academy has a great introduction to square roots.\n\nLet’s try both of these now:\n\ndist_3_abs = (a_3 - mean3).abs().mean()\ndist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\ndist_3_abs,dist_3_sqr\n\n(tensor(0.1114), tensor(0.2021))\n\n\n\ndist_7_abs = (a_3 - mean7).abs().mean()\ndist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\ndist_7_abs,dist_7_sqr\n\n(tensor(0.1586), tensor(0.3021))\n\n\nIn both cases, the distance between our 3 and the “ideal” 3 is less than the distance to the ideal 7. So our simple model will give the right prediction in this case.\nPyTorch already provides both of these as loss functions. You’ll find these inside torch.nn.functional, which the PyTorch team recommends importing as F (and is available by default under that name in fastai):\n\nF.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt()\n\n(tensor(0.1586), tensor(0.3021))\n\n\nHere mse stands for mean squared error, and l1 refers to the standard mathematical jargon for mean absolute value (in math it’s called the L1 norm).\n\nS: Intuitively, the difference between L1 norm and mean squared error (MSE) is that the latter will penalize bigger mistakes more heavily than the former (and be more lenient with small mistakes).\n\n\nJ: When I first came across this “L1” thingie, I looked it up to see what on earth it meant. I found on Google that it is a vector norm using absolute value, so looked up vector norm and started reading: Given a vector space V over a field F of the real or complex numbers, a norm on V is a nonnegative-valued any function p: V → [0,+∞) with the following properties: For all a ∈ F and all u, v ∈ V, p(u + v) ≤ p(u) + p(v)… Then I stopped reading. “Ugh, I’ll never understand math!” I thought, for the thousandth time. Since then I’ve learned that every time these complex mathy bits of jargon come up in practice, it turns out I can replace them with a tiny bit of code! Like, the L1 loss is just equal to (a-b).abs().mean(), where a and b are tensors. I guess mathy folks just think differently than me… I’ll make sure in this book that every time some mathy jargon comes up, I’ll give you the little bit of code it’s equal to as well, and explain in common-sense terms what’s going on.\n\nWe just completed various mathematical operations on PyTorch tensors. If you’ve done some numeric programming in NumPy before, you may recognize these as being similar to NumPy arrays. Let’s have a look at those two very important data structures.\n\nNumPy Arrays and PyTorch Tensors\nNumPy is the most widely used library for scientific and numeric programming in Python. It provides very similar functionality and a very similar API to that provided by PyTorch; however, it does not support using the GPU or calculating gradients, which are both critical for deep learning. Therefore, in this book we will generally use PyTorch tensors instead of NumPy arrays, where possible.\n(Note that fastai adds some features to NumPy and PyTorch to make them a bit more similar to each other. If any code in this book doesn’t work on your computer, it’s possible that you forgot to include a line like this at the start of your notebook: from fastai.vision.all import *.)\nBut what are arrays and tensors, and why should you care?\nPython is slow compared to many languages. Anything fast in Python, NumPy, or PyTorch is likely to be a wrapper for a compiled object written (and optimized) in another language—specifically C. In fact, NumPy arrays and PyTorch tensors can finish computations many thousands of times faster than using pure Python.\nA NumPy array is a multidimensional table of data, with all items of the same type. Since that can be any type at all, they can even be arrays of arrays, with the innermost arrays potentially being different sizes—this is called a “jagged array.” By “multidimensional table” we mean, for instance, a list (dimension of one), a table or matrix (dimension of two), a “table of tables” or “cube” (dimension of three), and so forth. If the items are all of some simple type such as integer or float, then NumPy will store them as a compact C data structure in memory. This is where NumPy shines. NumPy has a wide variety of operators and methods that can run computations on these compact structures at the same speed as optimized C, because they are written in optimized C.\nA PyTorch tensor is nearly the same thing as a NumPy array, but with an additional restriction that unlocks some additional capabilities. It’s the same in that it, too, is a multidimensional table of data, with all items of the same type. However, the restriction is that a tensor cannot use just any old type—it has to use a single basic numeric type for all components. For example, a PyTorch tensor cannot be jagged. It is always a regularly shaped multidimensional rectangular structure.\nThe vast majority of methods and operators supported by NumPy on these structures are also supported by PyTorch, but PyTorch tensors have additional capabilities. One major capability is that these structures can live on the GPU, in which case their computation will be optimized for the GPU and can run much faster (given lots of values to work on). In addition, PyTorch can automatically calculate derivatives of these operations, including combinations of operations. As you’ll see, it would be impossible to do deep learning in practice without this capability.\n\nS: If you don’t know what C is, don’t worry as you won’t need it at all. In a nutshell, it’s a low-level (low-level means more similar to the language that computers use internally) language that is very fast compared to Python. To take advantage of its speed while programming in Python, try to avoid as much as possible writing loops, and replace them by commands that work directly on arrays or tensors.\n\nPerhaps the most important new coding skill for a Python programmer to learn is how to effectively use the array/tensor APIs. We will be showing lots more tricks later in this book, but here’s a summary of the key things you need to know for now.\nTo create an array or tensor, pass a list (or list of lists, or list of lists of lists, etc.) to array() or tensor():\n\ndata = [[1,2,3],[4,5,6]]\narr = array (data)\ntns = tensor(data)\n\n\narr  # numpy\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\n\ntns  # pytorch\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\nAll the operations that follow are shown on tensors, but the syntax and results for NumPy arrays is identical.\nYou can select a row (note that, like lists in Python, tensors are 0-indexed so 1 refers to the second row/column):\n\ntns[1]\n\ntensor([4, 5, 6])\n\n\nor a column, by using : to indicate all of the first axis (we sometimes refer to the dimensions of tensors/arrays as axes):\n\ntns[:,1]\n\ntensor([2, 5])\n\n\nYou can combine these with Python slice syntax ([start:end] with end being excluded) to select part of a row or column:\n\ntns[1,1:3]\n\ntensor([5, 6])\n\n\nAnd you can use the standard operators such as +, -, *, /:\n\ntns+1\n\ntensor([[2, 3, 4],\n        [5, 6, 7]])\n\n\nTensors have a type:\n\ntns.type()\n\n'torch.LongTensor'\n\n\nAnd will automatically change type as needed, for example from int to float:\n\ntns*1.5\n\ntensor([[1.5000, 3.0000, 4.5000],\n        [6.0000, 7.5000, 9.0000]])\n\n\nSo, is our baseline model any good? To quantify this, we must define a metric."
  },
  {
    "objectID": "Fastbook/04_mnist_basics.html#computing-metrics-using-broadcasting",
    "href": "Fastbook/04_mnist_basics.html#computing-metrics-using-broadcasting",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "Computing Metrics Using Broadcasting",
    "text": "Computing Metrics Using Broadcasting\nRecall that a metric is a number that is calculated based on the predictions of our model, and the correct labels in our dataset, in order to tell us how good our model is. For instance, we could use either of the functions we saw in the previous section, mean squared error, or mean absolute error, and take the average of them over the whole dataset. However, neither of these are numbers that are very understandable to most people; in practice, we normally use accuracy as the metric for classification models.\nAs we’ve discussed, we want to calculate our metric over a validation set. This is so that we don’t inadvertently overfit—that is, train a model to work well only on our training data. This is not really a risk with the pixel similarity model we’re using here as a first try, since it has no trained components, but we’ll use a validation set anyway to follow normal practices and to be ready for our second try later.\nTo get a validation set we need to remove some of the data from training entirely, so it is not seen by the model at all. As it turns out, the creators of the MNIST dataset have already done this for us. Do you remember how there was a whole separate directory called valid? That’s what this directory is for!\nSo to start with, let’s create tensors for our 3s and 7s from that directory. These are the tensors we will use to calculate a metric measuring the quality of our first-try model, which measures distance from an ideal image:\n\nvalid_3_tens = torch.stack([tensor(Image.open(o)) \n                            for o in (path/'valid'/'3').ls()])\nvalid_3_tens = valid_3_tens.float()/255\nvalid_7_tens = torch.stack([tensor(Image.open(o)) \n                            for o in (path/'valid'/'7').ls()])\nvalid_7_tens = valid_7_tens.float()/255\nvalid_3_tens.shape,valid_7_tens.shape\n\n(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))\n\n\nIt’s good to get in the habit of checking shapes as you go. Here we see two tensors, one representing the 3s validation set of 1,010 images of size 28×28, and one representing the 7s validation set of 1,028 images of size 28×28.\nWe ultimately want to write a function, is_3, that will decide if an arbitrary image is a 3 or a 7. It will do this by deciding which of our two “ideal digits” this arbitrary image is closer to. For that we need to define a notion of distance—that is, a function that calculates the distance between two images.\nWe can write a simple function that calculates the mean absolute error using an expression very similar to the one we wrote in the last section:\n\ndef mnist_distance(a,b): return (a-b).abs().mean((-1,-2))\nmnist_distance(a_3, mean3)\n\ntensor(0.1114)\n\n\nThis is the same value we previously calculated for the distance between these two images, the ideal 3 mean3 and the arbitrary sample 3 a_3, which are both single-image tensors with a shape of [28,28].\nBut in order to calculate a metric for overall accuracy, we will need to calculate the distance to the ideal 3 for every image in the validation set. How do we do that calculation? We could write a loop over all of the single-image tensors that are stacked within our validation set tensor, valid_3_tens, which has a shape of [1010,28,28] representing 1,010 images. But there is a better way.\nSomething very interesting happens when we take this exact same distance function, designed for comparing two single images, but pass in as an argument valid_3_tens, the tensor that represents the 3s validation set:\n\nvalid_3_dist = mnist_distance(valid_3_tens, mean3)\nvalid_3_dist, valid_3_dist.shape\n\n(tensor([0.1050, 0.1526, 0.1186,  ..., 0.1122, 0.1170, 0.1086]),\n torch.Size([1010]))\n\n\nInstead of complaining about shapes not matching, it returned the distance for every single image as a vector (i.e., a rank-1 tensor) of length 1,010 (the number of 3s in our validation set). How did that happen?\nTake another look at our function mnist_distance, and you’ll see we have there the subtraction (a-b). The magic trick is that PyTorch, when it tries to perform a simple subtraction operation between two tensors of different ranks, will use broadcasting. That is, it will automatically expand the tensor with the smaller rank to have the same size as the one with the larger rank. Broadcasting is an important capability that makes tensor code much easier to write.\nAfter broadcasting so the two argument tensors have the same rank, PyTorch applies its usual logic for two tensors of the same rank: it performs the operation on each corresponding element of the two tensors, and returns the tensor result. For instance:\n\ntensor([1,2,3]) + tensor(1)\n\ntensor([2, 3, 4])\n\n\nSo in this case, PyTorch treats mean3, a rank-2 tensor representing a single image, as if it were 1,010 copies of the same image, and then subtracts each of those copies from each 3 in our validation set. What shape would you expect this tensor to have? Try to figure it out yourself before you look at the answer below:\n\n(valid_3_tens-mean3).shape\n\ntorch.Size([1010, 28, 28])\n\n\nWe are calculating the difference between our “ideal 3” and each of the 1,010 3s in the validation set, for each of 28×28 images, resulting in the shape [1010,28,28].\nThere are a couple of important points about how broadcasting is implemented, which make it valuable not just for expressivity but also for performance:\n\nPyTorch doesn’t actually copy mean3 1,010 times. It pretends it were a tensor of that shape, but doesn’t actually allocate any additional memory\nIt does the whole calculation in C (or, if you’re using a GPU, in CUDA, the equivalent of C on the GPU), tens of thousands of times faster than pure Python (up to millions of times faster on a GPU!).\n\nThis is true of all broadcasting and elementwise operations and functions done in PyTorch. It’s the most important technique for you to know to create efficient PyTorch code.\nNext in mnist_distance we see abs. You might be able to guess now what this does when applied to a tensor. It applies the method to each individual element in the tensor, and returns a tensor of the results (that is, it applies the method “elementwise”). So in this case, we’ll get back 1,010 matrices of absolute values.\nFinally, our function calls mean((-1,-2)). The tuple (-1,-2) represents a range of axes. In Python, -1 refers to the last element, and -2 refers to the second-to-last. So in this case, this tells PyTorch that we want to take the mean ranging over the values indexed by the last two axes of the tensor. The last two axes are the horizontal and vertical dimensions of an image. After taking the mean over the last two axes, we are left with just the first tensor axis, which indexes over our images, which is why our final size was (1010). In other words, for every image, we averaged the intensity of all the pixels in that image.\nWe’ll be learning lots more about broadcasting throughout this book, especially in &lt;&gt;, and will be practicing it regularly too.\nWe can use mnist_distance to figure out whether an image is a 3 or not by using the following logic: if the distance between the digit in question and the ideal 3 is less than the distance to the ideal 7, then it’s a 3. This function will automatically do broadcasting and be applied elementwise, just like all PyTorch functions and operators:\n\ndef is_3(x): return mnist_distance(x,mean3) &lt; mnist_distance(x,mean7)\n\nLet’s test it on our example case:\n\nis_3(a_3), is_3(a_3).float()\n\n(tensor(True), tensor(1.))\n\n\nNote that when we convert the Boolean response to a float, we get 1.0 for True and 0.0 for False. Thanks to broadcasting, we can also test it on the full validation set of 3s:\n\nis_3(valid_3_tens)\n\ntensor([True, True, True,  ..., True, True, True])\n\n\nNow we can calculate the accuracy for each of the 3s and 7s by taking the average of that function for all 3s and its inverse for all 7s:\n\naccuracy_3s =      is_3(valid_3_tens).float() .mean()\naccuracy_7s = (1 - is_3(valid_7_tens).float()).mean()\n\naccuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2\n\n(tensor(0.9168), tensor(0.9854), tensor(0.9511))\n\n\nThis looks like a pretty good start! We’re getting over 90% accuracy on both 3s and 7s, and we’ve seen how to define a metric conveniently using broadcasting.\nBut let’s be honest: 3s and 7s are very different-looking digits. And we’re only classifying 2 out of the 10 possible digits so far. So we’re going to need to do better!\nTo do better, perhaps it is time to try a system that does some real learning—that is, that can automatically modify itself to improve its performance. In other words, it’s time to talk about the training process, and SGD."
  },
  {
    "objectID": "Fastbook/04_mnist_basics.html#stochastic-gradient-descent-sgd",
    "href": "Fastbook/04_mnist_basics.html#stochastic-gradient-descent-sgd",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "Stochastic Gradient Descent (SGD)",
    "text": "Stochastic Gradient Descent (SGD)\nDo you remember the way that Arthur Samuel described machine learning, which we quoted in &lt;&gt;?\n\n: Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would “learn” from its experience.\n\nAs we discussed, this is the key to allowing us to have a model that can get better and better—that can learn. But our pixel similarity approach does not really do this. We do not have any kind of weight assignment, or any way of improving based on testing the effectiveness of a weight assignment. In other words, we can’t really improve our pixel similarity approach by modifying a set of parameters. In order to take advantage of the power of deep learning, we will first have to represent our task in the way that Arthur Samuel described it.\nInstead of trying to find the similarity between an image and an “ideal image,” we could instead look at each individual pixel and come up with a set of weights for each one, such that the highest weights are associated with those pixels most likely to be black for a particular category. For instance, pixels toward the bottom right are not very likely to be activated for a 7, so they should have a low weight for a 7, but they are likely to be activated for an 8, so they should have a high weight for an 8. This can be represented as a function and set of weight values for each possible category—for instance the probability of being the number 8:\ndef pr_eight(x,w): return (x*w).sum()\nHere we are assuming that x is the image, represented as a vector—in other words, with all of the rows stacked up end to end into a single long line. And we are assuming that the weights are a vector w. If we have this function, then we just need some way to update the weights to make them a little bit better. With such an approach, we can repeat that step a number of times, making the weights better and better, until they are as good as we can make them.\nWe want to find the specific values for the vector w that causes the result of our function to be high for those images that are actually 8s, and low for those images that are not. Searching for the best vector w is a way to search for the best function for recognising 8s. (Because we are not yet using a deep neural network, we are limited by what our function can actually do—we are going to fix that constraint later in this chapter.)\nTo be more specific, here are the steps that we are going to require, to turn this function into a machine learning classifier:\n\nInitialize the weights.\nFor each image, use these weights to predict whether it appears to be a 3 or a 7.\nBased on these predictions, calculate how good the model is (its loss).\nCalculate the gradient, which measures for each weight, how changing that weight would change the loss\nStep (that is, change) all the weights based on that calculation.\nGo back to the step 2, and repeat the process.\nIterate until you decide to stop the training process (for instance, because the model is good enough or you don’t want to wait any longer).\n\nThese seven steps, illustrated in &lt;&gt;, are the key to the training of all deep learning models. That deep learning turns out to rely entirely on these steps is extremely surprising and counterintuitive. It’s amazing that this process can solve such complex problems. But, as you’ll see, it really does!\n\n#id gradient_descent\n#caption The gradient descent process\n#alt Graph showing the steps for Gradient Descent\ngv('''\ninit-&gt;predict-&gt;loss-&gt;gradient-&gt;step-&gt;stop\nstep-&gt;predict[label=repeat]\n''')\n\n\n\n\n\n\n\n\nThere are many different ways to do each of these seven steps, and we will be learning about them throughout the rest of this book. These are the details that make a big difference for deep learning practitioners, but it turns out that the general approach to each one generally follows some basic principles. Here are a few guidelines:\n\nInitialize:: We initialize the parameters to random values. This may sound surprising. There are certainly other choices we could make, such as initializing them to the percentage of times that pixel is activated for that category—but since we already know that we have a routine to improve these weights, it turns out that just starting with random weights works perfectly well.\nLoss:: This is what Samuel referred to when he spoke of testing the effectiveness of any current weight assignment in terms of actual performance. We need some function that will return a number that is small if the performance of the model is good (the standard approach is to treat a small loss as good, and a large loss as bad, although this is just a convention).\nStep:: A simple way to figure out whether a weight should be increased a bit, or decreased a bit, would be just to try it: increase the weight by a small amount, and see if the loss goes up or down. Once you find the correct direction, you could then change that amount by a bit more, and a bit less, until you find an amount that works well. However, this is slow! As we will see, the magic of calculus allows us to directly figure out in which direction, and by roughly how much, to change each weight, without having to try all these small changes. The way to do this is by calculating gradients. This is just a performance optimization, we would get exactly the same results by using the slower manual process as well.\nStop:: Once we’ve decided how many epochs to train the model for (a few suggestions for this were given in the earlier list), we apply that decision. This is where that decision is applied. For our digit classifier, we would keep training until the accuracy of the model started getting worse, or we ran out of time.\n\nBefore applying these steps to our image classification problem, let’s illustrate what they look like in a simpler case. First we will define a very simple function, the quadratic—let’s pretend that this is our loss function, and x is a weight parameter of the function:\n\ndef f(x): return x**2\n\nHere is a graph of that function:\n\nplot_function(f, 'x', 'x**2')\n\n\n\n\n\n\n\n\nThe sequence of steps we described earlier starts by picking some random value for a parameter, and calculating the value of the loss:\n\nplot_function(f, 'x', 'x**2')\nplt.scatter(-1.5, f(-1.5), color='red');\n\n\n\n\n\n\n\n\nNow we look to see what would happen if we increased or decreased our parameter by a little bit—the adjustment. This is simply the slope at a particular point:\n\nWe can change our weight by a little in the direction of the slope, calculate our loss and adjustment again, and repeat this a few times. Eventually, we will get to the lowest point on our curve:\n\nThis basic idea goes all the way back to Isaac Newton, who pointed out that we can optimize arbitrary functions in this way. Regardless of how complicated our functions become, this basic approach of gradient descent will not significantly change. The only minor changes we will see later in this book are some handy ways we can make it faster, by finding better steps.\n\nCalculating Gradients\nThe one magic step is the bit where we calculate the gradients. As we mentioned, we use calculus as a performance optimization; it allows us to more quickly calculate whether our loss will go up or down when we adjust our parameters up or down. In other words, the gradients will tell us how much we have to change each weight to make our model better.\nYou may remember from your high school calculus class that the derivative of a function tells you how much a change in its parameters will change its result. If not, don’t worry, lots of us forget calculus once high school is behind us! But you will have to have some intuitive understanding of what a derivative is before you continue, so if this is all very fuzzy in your head, head over to Khan Academy and complete the lessons on basic derivatives. You won’t have to know how to calculate them yourselves, you just have to know what a derivative is.\nThe key point about a derivative is this: for any function, such as the quadratic function we saw in the previous section, we can calculate its derivative. The derivative is another function. It calculates the change, rather than the value. For instance, the derivative of the quadratic function at the value 3 tells us how rapidly the function changes at the value 3. More specifically, you may recall that gradient is defined as rise/run, that is, the change in the value of the function, divided by the change in the value of the parameter. When we know how our function will change, then we know what we need to do to make it smaller. This is the key to machine learning: having a way to change the parameters of a function to make it smaller. Calculus provides us with a computational shortcut, the derivative, which lets us directly calculate the gradients of our functions.\nOne important thing to be aware of is that our function has lots of weights that we need to adjust, so when we calculate the derivative we won’t get back one number, but lots of them—a gradient for every weight. But there is nothing mathematically tricky here; you can calculate the derivative with respect to one weight, and treat all the other ones as constant, then repeat that for each other weight. This is how all of the gradients are calculated, for every weight.\nWe mentioned just now that you won’t have to calculate any gradients yourself. How can that be? Amazingly enough, PyTorch is able to automatically compute the derivative of nearly any function! What’s more, it does it very fast. Most of the time, it will be at least as fast as any derivative function that you can create by hand. Let’s see an example.\nFirst, let’s pick a tensor value which we want gradients at:\n\nxt = tensor(3.).requires_grad_()\n\nNotice the special method requires_grad_? That’s the magical incantation we use to tell PyTorch that we want to calculate gradients with respect to that variable at that value. It is essentially tagging the variable, so PyTorch will remember to keep track of how to compute gradients of the other, direct calculations on it that you will ask for.\n\na: This API might throw you off if you’re coming from math or physics. In those contexts the “gradient” of a function is just another function (i.e., its derivative), so you might expect gradient-related APIs to give you a new function. But in deep learning, “gradients” usually means the value of a function’s derivative at a particular argument value. The PyTorch API also puts the focus on the argument, not the function you’re actually computing the gradients of. It may feel backwards at first, but it’s just a different perspective.\n\nNow we calculate our function with that value. Notice how PyTorch prints not just the value calculated, but also a note that it has a gradient function it’ll be using to calculate our gradients when needed:\n\nyt = f(xt)\nyt\n\ntensor(9., grad_fn=&lt;PowBackward0&gt;)\n\n\nFinally, we tell PyTorch to calculate the gradients for us:\n\nyt.backward()\n\nThe “backward” here refers to backpropagation, which is the name given to the process of calculating the derivative of each layer. We’ll see how this is done exactly in chapter &lt;&gt;, when we calculate the gradients of a deep neural net from scratch. This is called the “backward pass” of the network, as opposed to the “forward pass,” which is where the activations are calculated. Life would probably be easier if backward was just called calculate_grad, but deep learning folks really do like to add jargon everywhere they can!\nWe can now view the gradients by checking the grad attribute of our tensor:\n\nxt.grad\n\ntensor(6.)\n\n\nIf you remember your high school calculus rules, the derivative of x**2 is 2*x, and we have x=3, so the gradients should be 2*3=6, which is what PyTorch calculated for us!\nNow we’ll repeat the preceding steps, but with a vector argument for our function:\n\nxt = tensor([3.,4.,10.]).requires_grad_()\nxt\n\ntensor([ 3.,  4., 10.], requires_grad=True)\n\n\nAnd we’ll add sum to our function so it can take a vector (i.e., a rank-1 tensor), and return a scalar (i.e., a rank-0 tensor):\n\ndef f(x): return (x**2).sum()\n\nyt = f(xt)\nyt\n\ntensor(125., grad_fn=&lt;SumBackward0&gt;)\n\n\nOur gradients are 2*xt, as we’d expect!\n\nyt.backward()\nxt.grad\n\ntensor([ 6.,  8., 20.])\n\n\nThe gradients only tell us the slope of our function, they don’t actually tell us exactly how far to adjust the parameters. But it gives us some idea of how far; if the slope is very large, then that may suggest that we have more adjustments to do, whereas if the slope is very small, that may suggest that we are close to the optimal value.\n\n\nStepping With a Learning Rate\nDeciding how to change our parameters based on the values of the gradients is an important part of the deep learning process. Nearly all approaches start with the basic idea of multiplying the gradient by some small number, called the learning rate (LR). The learning rate is often a number between 0.001 and 0.1, although it could be anything. Often, people select a learning rate just by trying a few, and finding which results in the best model after training (we’ll show you a better approach later in this book, called the learning rate finder). Once you’ve picked a learning rate, you can adjust your parameters using this simple function:\nw -= gradient(w) * lr\nThis is known as stepping your parameters, using an optimizer step. Notice how we subtract the gradient * lr from the parameter to update it. This allows us to adjust the parameter in the direction of the slope by increasing the parameter when the slope is negative and decreasing the parameter when the slope is positive. We want to adjust our parameters in the direction of the slope because our goal in deep learning is to minimize the loss.\nIf you pick a learning rate that’s too low, it can mean having to do a lot of steps. &lt;&gt; illustrates that.\n\nBut picking a learning rate that’s too high is even worse—it can actually result in the loss getting worse, as we see in &lt;&gt;!\n\nIf the learning rate is too high, it may also “bounce” around, rather than actually diverging; &lt;&gt; shows how this has the result of taking many steps to train successfully.\n\nNow let’s apply all of this in an end-to-end example.\n\n\nAn End-to-End SGD Example\nWe’ve seen how to use gradients to find a minimum. Now it’s time to look at an SGD example and see how finding a minimum can be used to train a model to fit data better.\nLet’s start with a simple, synthetic, example model. Imagine you were measuring the speed of a roller coaster as it went over the top of a hump. It would start fast, and then get slower as it went up the hill; it would be slowest at the top, and it would then speed up again as it went downhill. You want to build a model of how the speed changes over time. If you were measuring the speed manually every second for 20 seconds, it might look something like this:\n\ntime = torch.arange(0,20).float(); time\n\ntensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.])\n\n\n\nspeed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1\nplt.scatter(time,speed);\n\n\n\n\n\n\n\n\nWe’ve added a bit of random noise, since measuring things manually isn’t precise. This means it’s not that easy to answer the question: what was the roller coaster’s speed? Using SGD we can try to find a function that matches our observations. We can’t consider every possible function, so let’s use a guess that it will be quadratic; i.e., a function of the form a*(time**2)+(b*time)+c.\nWe want to distinguish clearly between the function’s input (the time when we are measuring the coaster’s speed) and its parameters (the values that define which quadratic we’re trying). So, let’s collect the parameters in one argument and thus separate the input, t, and the parameters, params, in the function’s signature:\n\ndef f(t, params):\n    a,b,c = params\n    return a*(t**2) + (b*t) + c\n\nIn other words, we’ve restricted the problem of finding the best imaginable function that fits the data, to finding the best quadratic function. This greatly simplifies the problem, since every quadratic function is fully defined by the three parameters a, b, and c. Thus, to find the best quadratic function, we only need to find the best values for a, b, and c.\nIf we can solve this problem for the three parameters of a quadratic function, we’ll be able to apply the same approach for other, more complex functions with more parameters—such as a neural net. Let’s find the parameters for f first, and then we’ll come back and do the same thing for the MNIST dataset with a neural net.\nWe need to define first what we mean by “best.” We define this precisely by choosing a loss function, which will return a value based on a prediction and a target, where lower values of the function correspond to “better” predictions. It is important for loss functions to return lower values when predictions are more accurate, as the SGD procedure we defined earlier will try to minimize this loss. For continuous data, it’s common to use mean squared error:\n\ndef mse(preds, targets): return ((preds-targets)**2).mean()\n\nNow, let’s work through our 7 step process.\n\nStep 1: Initialize the parameters\nFirst, we initialize the parameters to random values, and tell PyTorch that we want to track their gradients, using requires_grad_:\n\nparams = torch.randn(3).requires_grad_()\n\n\n#hide\norig_params = params.clone()\n\n\n\nStep 2: Calculate the predictions\nNext, we calculate the predictions:\n\npreds = f(time, params)\n\nLet’s create a little function to see how close our predictions are to our targets, and take a look:\n\ndef show_preds(preds, ax=None):\n    if ax is None: ax=plt.subplots()[1]\n    ax.scatter(time, speed)\n    ax.scatter(time, to_np(preds), color='red')\n    ax.set_ylim(-300,100)\n\n\nshow_preds(preds)\n\n\n\n\n\n\n\n\nThis doesn’t look very close—our random parameters suggest that the roller coaster will end up going backwards, since we have negative speeds!\n\n\nStep 3: Calculate the loss\nWe calculate the loss as follows:\n\nloss = mse(preds, speed)\nloss\n\ntensor(25823.8086, grad_fn=&lt;MeanBackward0&gt;)\n\n\nOur goal is now to improve this. To do that, we’ll need to know the gradients.\n\n\nStep 4: Calculate the gradients\nThe next step is to calculate the gradients. In other words, calculate an approximation of how the parameters need to change:\n\nloss.backward()\nparams.grad\n\ntensor([-53195.8594,  -3419.7146,   -253.8908])\n\n\n\nparams.grad * 1e-5\n\ntensor([-0.5320, -0.0342, -0.0025])\n\n\nWe can use these gradients to improve our parameters. We’ll need to pick a learning rate (we’ll discuss how to do that in practice in the next chapter; for now we’ll just use 1e-5, or 0.00001):\n\nparams\n\ntensor([-0.7658, -0.7506,  1.3525], requires_grad=True)\n\n\n\n\nStep 5: Step the weights.\nNow we need to update the parameters based on the gradients we just calculated:\n\nlr = 1e-5\nparams.data -= lr * params.grad.data\nparams.grad = None\n\n\na: Understanding this bit depends on remembering recent history. To calculate the gradients we call backward on the loss. But this loss was itself calculated by mse, which in turn took preds as an input, which was calculated using f taking as an input params, which was the object on which we originally called requires_grad_—which is the original call that now allows us to call backward on loss. This chain of function calls represents the mathematical composition of functions, which enables PyTorch to use calculus’s chain rule under the hood to calculate these gradients.\n\nLet’s see if the loss has improved:\n\npreds = f(time,params)\nmse(preds, speed)\n\ntensor(5435.5366, grad_fn=&lt;MeanBackward0&gt;)\n\n\nAnd take a look at the plot:\n\nshow_preds(preds)\n\n\n\n\n\n\n\n\nWe need to repeat this a few times, so we’ll create a function to apply one step:\n\ndef apply_step(params, prn=True):\n    preds = f(time, params)\n    loss = mse(preds, speed)\n    loss.backward()\n    params.data -= lr * params.grad.data\n    params.grad = None\n    if prn: print(loss.item())\n    return preds\n\n\n\nStep 6: Repeat the process\nNow we iterate. By looping and performing many improvements, we hope to reach a good result:\n\nfor i in range(10): apply_step(params)\n\n5435.53662109375\n1577.4495849609375\n847.3780517578125\n709.22265625\n683.0757446289062\n678.12451171875\n677.1839599609375\n677.0025024414062\n676.96435546875\n676.9537353515625\n\n\n\n#hide\nparams = orig_params.detach().requires_grad_()\n\nThe loss is going down, just as we hoped! But looking only at these loss numbers disguises the fact that each iteration represents an entirely different quadratic function being tried, on the way to finding the best possible quadratic function. We can see this process visually if, instead of printing out the loss function, we plot the function at every step. Then we can see how the shape is approaching the best possible quadratic function for our data:\n\n_,axs = plt.subplots(1,4,figsize=(12,3))\nfor ax in axs: show_preds(apply_step(params, False), ax)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nStep 7: stop\nWe just decided to stop after 10 epochs arbitrarily. In practice, we would watch the training and validation losses and our metrics to decide when to stop, as we’ve discussed.\n\n\n\nSummarizing Gradient Descent\n\n#hide_input\n#id gradient_descent\n#caption The gradient descent process\n#alt Graph showing the steps for Gradient Descent\ngv('''\ninit-&gt;predict-&gt;loss-&gt;gradient-&gt;step-&gt;stop\nstep-&gt;predict[label=repeat]\n''')\n\n\n\n\n\n\n\n\nTo summarize, at the beginning, the weights of our model can be random (training from scratch) or come from a pretrained model (transfer learning). In the first case, the output we will get from our inputs won’t have anything to do with what we want, and even in the second case, it’s very likely the pretrained model won’t be very good at the specific task we are targeting. So the model will need to learn better weights.\nWe begin by comparing the outputs the model gives us with our targets (we have labeled data, so we know what result the model should give) using a loss function, which returns a number that we want to make as low as possible by improving our weights. To do this, we take a few data items (such as images) from the training set and feed them to our model. We compare the corresponding targets using our loss function, and the score we get tells us how wrong our predictions were. We then change the weights a little bit to make it slightly better.\nTo find how to change the weights to make the loss a bit better, we use calculus to calculate the gradients. (Actually, we let PyTorch do it for us!) Let’s consider an analogy. Imagine you are lost in the mountains with your car parked at the lowest point. To find your way back to it, you might wander in a random direction, but that probably wouldn’t help much. Since you know your vehicle is at the lowest point, you would be better off going downhill. By always taking a step in the direction of the steepest downward slope, you should eventually arrive at your destination. We use the magnitude of the gradient (i.e., the steepness of the slope) to tell us how big a step to take; specifically, we multiply the gradient by a number we choose called the learning rate to decide on the step size. We then iterate until we have reached the lowest point, which will be our parking lot, then we can stop.\nAll of that we just saw can be transposed directly to the MNIST dataset, except for the loss function. Let’s now see how we can define a good training objective."
  },
  {
    "objectID": "Fastbook/04_mnist_basics.html#the-mnist-loss-function",
    "href": "Fastbook/04_mnist_basics.html#the-mnist-loss-function",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "The MNIST Loss Function",
    "text": "The MNIST Loss Function\nWe already have our independent variables x—these are the images themselves. We’ll concatenate them all into a single tensor, and also change them from a list of matrices (a rank-3 tensor) to a list of vectors (a rank-2 tensor). We can do this using view, which is a PyTorch method that changes the shape of a tensor without changing its contents. -1 is a special parameter to view that means “make this axis as big as necessary to fit all the data”:\n\ntrain_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n\nWe need a label for each image. We’ll use 1 for 3s and 0 for 7s:\n\ntrain_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\ntrain_x.shape,train_y.shape\n\n(torch.Size([12396, 784]), torch.Size([12396, 1]))\n\n\nA Dataset in PyTorch is required to return a tuple of (x,y) when indexed. Python provides a zip function which, when combined with list, provides a simple way to get this functionality:\n\ndset = list(zip(train_x,train_y))\nx,y = dset[0]\nx.shape,y\n\n(torch.Size([784]), tensor([1]))\n\n\n\nvalid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\nvalid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\nvalid_dset = list(zip(valid_x,valid_y))\n\nNow we need an (initially random) weight for every pixel (this is the initialize step in our seven-step process):\n\ndef init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\n\nweights = init_params((28*28,1))\n\nThe function weights*pixels won’t be flexible enough—it is always equal to 0 when the pixels are equal to 0 (i.e., its intercept is 0). You might remember from high school math that the formula for a line is y=w*x+b; we still need the b. We’ll initialize it to a random number too:\n\nbias = init_params(1)\n\nIn neural networks, the w in the equation y=w*x+b is called the weights, and the b is called the bias. Together, the weights and bias make up the parameters.\n\njargon: Parameters: The weights and biases of a model. The weights are the w in the equation w*x+b, and the biases are the b in that equation.\n\nWe can now calculate a prediction for one image:\n\n(train_x[0]*weights.T).sum() + bias\n\ntensor([20.2336], grad_fn=&lt;AddBackward0&gt;)\n\n\nWhile we could use a Python for loop to calculate the prediction for each image, that would be very slow. Because Python loops don’t run on the GPU, and because Python is a slow language for loops in general, we need to represent as much of the computation in a model as possible using higher-level functions.\nIn this case, there’s an extremely convenient mathematical operation that calculates w*x for every row of a matrix—it’s called matrix multiplication. &lt;&gt; shows what matrix multiplication looks like.\n\nThis image shows two matrices, A and B, being multiplied together. Each item of the result, which we’ll call AB, contains each item of its corresponding row of A multiplied by each item of its corresponding column of B, added together. For instance, row 1, column 2 (the yellow dot with a red border) is calculated as \\(a_{1,1} * b_{1,2} + a_{1,2} * b_{2,2}\\). If you need a refresher on matrix multiplication, we suggest you take a look at the Intro to Matrix Multiplication on Khan Academy, since this is the most important mathematical operation in deep learning.\nIn Python, matrix multiplication is represented with the @ operator. Let’s try it:\n\ndef linear1(xb): return xb@weights + bias\npreds = linear1(train_x)\npreds\n\ntensor([[20.2336],\n        [17.0644],\n        [15.2384],\n        ...,\n        [18.3804],\n        [23.8567],\n        [28.6816]], grad_fn=&lt;AddBackward0&gt;)\n\n\nThe first element is the same as we calculated before, as we’d expect. This equation, batch@weights + bias, is one of the two fundamental equations of any neural network (the other one is the activation function, which we’ll see in a moment).\nLet’s check our accuracy. To decide if an output represents a 3 or a 7, we can just check whether it’s greater than 0.0, so our accuracy for each item can be calculated (using broadcasting, so no loops!) with:\n\ncorrects = (preds&gt;0.0).float() == train_y\ncorrects\n\ntensor([[ True],\n        [ True],\n        [ True],\n        ...,\n        [False],\n        [False],\n        [False]])\n\n\n\ncorrects.float().mean().item()\n\n0.4912068545818329\n\n\nNow let’s see what the change in accuracy is for a small change in one of the weights (note that we have to ask PyTorch not to calculate gradients as we do this, which is what with torch.no_grad() is doing here):\n\nwith torch.no_grad(): weights[0] *= 1.0001\n\n\npreds = linear1(train_x)\n((preds&gt;0.0).float() == train_y).float().mean().item()\n\n0.4912068545818329\n\n\nAs we’ve seen, we need gradients in order to improve our model using SGD, and in order to calculate gradients we need some loss function that represents how good our model is. That is because the gradients are a measure of how that loss function changes with small tweaks to the weights.\nSo, we need to choose a loss function. The obvious approach would be to use accuracy, which is our metric, as our loss function as well. In this case, we would calculate our prediction for each image, collect these values to calculate an overall accuracy, and then calculate the gradients of each weight with respect to that overall accuracy.\nUnfortunately, we have a significant technical problem here. The gradient of a function is its slope, or its steepness, which can be defined as rise over run—that is, how much the value of the function goes up or down, divided by how much we changed the input. We can write this in mathematically as: (y_new - y_old) / (x_new - x_old). This gives us a good approximation of the gradient when x_new is very similar to x_old, meaning that their difference is very small. But accuracy only changes at all when a prediction changes from a 3 to a 7, or vice versa. The problem is that a small change in weights from x_old to x_new isn’t likely to cause any prediction to change, so (y_new - y_old) will almost always be 0. In other words, the gradient is 0 almost everywhere.\nA very small change in the value of a weight will often not actually change the accuracy at all. This means it is not useful to use accuracy as a loss function—if we do, most of the time our gradients will actually be 0, and the model will not be able to learn from that number.\n\nS: In mathematical terms, accuracy is a function that is constant almost everywhere (except at the threshold, 0.5), so its derivative is nil almost everywhere (and infinity at the threshold). This then gives gradients that are 0 or infinite, which are useless for updating the model.\n\nInstead, we need a loss function which, when our weights result in slightly better predictions, gives us a slightly better loss. So what does a “slightly better prediction” look like, exactly? Well, in this case, it means that if the correct answer is a 3 the score is a little higher, or if the correct answer is a 7 the score is a little lower.\nLet’s write such a function now. What form does it take?\nThe loss function receives not the images themselves, but the predictions from the model. Let’s make one argument, prds, of values between 0 and 1, where each value is the prediction that an image is a 3. It is a vector (i.e., a rank-1 tensor), indexed over the images.\nThe purpose of the loss function is to measure the difference between predicted values and the true values — that is, the targets (aka labels). Let’s make another argument, trgts, with values of 0 or 1 which tells whether an image actually is a 3 or not. It is also a vector (i.e., another rank-1 tensor), indexed over the images.\nSo, for instance, suppose we had three images which we knew were a 3, a 7, and a 3. And suppose our model predicted with high confidence (0.9) that the first was a 3, with slight confidence (0.4) that the second was a 7, and with fair confidence (0.2), but incorrectly, that the last was a 7. This would mean our loss function would receive these values as its inputs:\n\ntrgts  = tensor([1,0,1])\nprds   = tensor([0.9, 0.4, 0.2])\n\nHere’s a first try at a loss function that measures the distance between predictions and targets:\n\ndef mnist_loss(predictions, targets):\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\nWe’re using a new function, torch.where(a,b,c). This is the same as running the list comprehension [b[i] if a[i] else c[i] for i in range(len(a))], except it works on tensors, at C/CUDA speed. In plain English, this function will measure how distant each prediction is from 1 if it should be 1, and how distant it is from 0 if it should be 0, and then it will take the mean of all those distances.\n\nnote: Read the Docs: It’s important to learn about PyTorch functions like this, because looping over tensors in Python performs at Python speed, not C/CUDA speed! Try running help(torch.where) now to read the docs for this function, or, better still, look it up on the PyTorch documentation site.\n\nLet’s try it on our prds and trgts:\n\ntorch.where(trgts==1, 1-prds, prds)\n\ntensor([0.1000, 0.4000, 0.8000])\n\n\nYou can see that this function returns a lower number when predictions are more accurate, when accurate predictions are more confident (higher absolute values), and when inaccurate predictions are less confident. In PyTorch, we always assume that a lower value of a loss function is better. Since we need a scalar for the final loss, mnist_loss takes the mean of the previous tensor:\n\nmnist_loss(prds,trgts)\n\ntensor(0.4333)\n\n\nFor instance, if we change our prediction for the one “false” target from 0.2 to 0.8 the loss will go down, indicating that this is a better prediction:\n\nmnist_loss(tensor([0.9, 0.4, 0.8]),trgts)\n\ntensor(0.2333)\n\n\nOne problem with mnist_loss as currently defined is that it assumes that predictions are always between 0 and 1. We need to ensure, then, that this is actually the case! As it happens, there is a function that does exactly that—let’s take a look.\n\nSigmoid\nThe sigmoid function always outputs a number between 0 and 1. It’s defined as follows:\n\ndef sigmoid(x): return 1/(1+torch.exp(-x))\n\nPytorch defines an accelerated version for us, so we don’t really need our own. This is an important function in deep learning, since we often want to ensure values are between 0 and 1. This is what it looks like:\n\nplot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)\n\n\n\n\n\n\n\n\nAs you can see, it takes any input value, positive or negative, and smooshes it onto an output value between 0 and 1. It’s also a smooth curve that only goes up, which makes it easier for SGD to find meaningful gradients.\nLet’s update mnist_loss to first apply sigmoid to the inputs:\n\ndef mnist_loss(predictions, targets):\n    predictions = predictions.sigmoid()\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\nNow we can be confident our loss function will work, even if the predictions are not between 0 and 1. All that is required is that a higher prediction corresponds to higher confidence an image is a 3.\nHaving defined a loss function, now is a good moment to recapitulate why we did this. After all, we already had a metric, which was overall accuracy. So why did we define a loss?\nThe key difference is that the metric is to drive human understanding and the loss is to drive automated learning. To drive automated learning, the loss must be a function that has a meaningful derivative. It can’t have big flat sections and large jumps, but instead must be reasonably smooth. This is why we designed a loss function that would respond to small changes in confidence level. This requirement means that sometimes it does not really reflect exactly what we are trying to achieve, but is rather a compromise between our real goal and a function that can be optimized using its gradient. The loss function is calculated for each item in our dataset, and then at the end of an epoch the loss values are all averaged and the overall mean is reported for the epoch.\nMetrics, on the other hand, are the numbers that we really care about. These are the values that are printed at the end of each epoch that tell us how our model is really doing. It is important that we learn to focus on these metrics, rather than the loss, when judging the performance of a model.\n\n\nSGD and Mini-Batches\nNow that we have a loss function that is suitable for driving SGD, we can consider some of the details involved in the next phase of the learning process, which is to change or update the weights based on the gradients. This is called an optimization step.\nIn order to take an optimization step we need to calculate the loss over one or more data items. How many should we use? We could calculate it for the whole dataset, and take the average, or we could calculate it for a single data item. But neither of these is ideal. Calculating it for the whole dataset would take a very long time. Calculating it for a single item would not use much information, so it would result in a very imprecise and unstable gradient. That is, you’d be going to the trouble of updating the weights, but taking into account only how that would improve the model’s performance on that single item.\nSo instead we take a compromise between the two: we calculate the average loss for a few data items at a time. This is called a mini-batch. The number of data items in the mini-batch is called the batch size. A larger batch size means that you will get a more accurate and stable estimate of your dataset’s gradients from the loss function, but it will take longer, and you will process fewer mini-batches per epoch. Choosing a good batch size is one of the decisions you need to make as a deep learning practitioner to train your model quickly and accurately. We will talk about how to make this choice throughout this book.\nAnother good reason for using mini-batches rather than calculating the gradient on individual data items is that, in practice, we nearly always do our training on an accelerator such as a GPU. These accelerators only perform well if they have lots of work to do at a time, so it’s helpful if we can give them lots of data items to work on. Using mini-batches is one of the best ways to do this. However, if you give them too much data to work on at once, they run out of memory—making GPUs happy is also tricky!\nAs we saw in our discussion of data augmentation in &lt;&gt;, we get better generalization if we can vary things during training. One simple and effective thing we can vary is what data items we put in each mini-batch. Rather than simply enumerating our dataset in order for every epoch, instead what we normally do is randomly shuffle it on every epoch, before we create mini-batches. PyTorch and fastai provide a class that will do the shuffling and mini-batch collation for you, called DataLoader.\nA DataLoader can take any Python collection and turn it into an iterator over mini-batches, like so:\n\ncoll = range(15)\ndl = DataLoader(coll, batch_size=5, shuffle=True)\nlist(dl)\n\n[tensor([ 3, 12,  8, 10,  2]),\n tensor([ 9,  4,  7, 14,  5]),\n tensor([ 1, 13,  0,  6, 11])]\n\n\nFor training a model, we don’t just want any Python collection, but a collection containing independent and dependent variables (that is, the inputs and targets of the model). A collection that contains tuples of independent and dependent variables is known in PyTorch as a Dataset. Here’s an example of an extremely simple Dataset:\n\nds = L(enumerate(string.ascii_lowercase))\nds\n\n(#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7, 'h'),(8, 'i'),(9, 'j')...]\n\n\nWhen we pass a Dataset to a DataLoader we will get back mini-batches which are themselves tuples of tensors representing batches of independent and dependent variables:\n\ndl = DataLoader(ds, batch_size=6, shuffle=True)\nlist(dl)\n\n[(tensor([17, 18, 10, 22,  8, 14]), ('r', 's', 'k', 'w', 'i', 'o')),\n (tensor([20, 15,  9, 13, 21, 12]), ('u', 'p', 'j', 'n', 'v', 'm')),\n (tensor([ 7, 25,  6,  5, 11, 23]), ('h', 'z', 'g', 'f', 'l', 'x')),\n (tensor([ 1,  3,  0, 24, 19, 16]), ('b', 'd', 'a', 'y', 't', 'q')),\n (tensor([2, 4]), ('c', 'e'))]\n\n\nWe are now ready to write our first training loop for a model using SGD!"
  },
  {
    "objectID": "Fastbook/04_mnist_basics.html#putting-it-all-together",
    "href": "Fastbook/04_mnist_basics.html#putting-it-all-together",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nIt’s time to implement the process we saw in &lt;&gt;. In code, our process will be implemented something like this for each epoch:\nfor x,y in dl:\n    pred = model(x)\n    loss = loss_func(pred, y)\n    loss.backward()\n    parameters -= parameters.grad * lr\nFirst, let’s re-initialize our parameters:\n\nweights = init_params((28*28,1))\nbias = init_params(1)\n\nA DataLoader can be created from a Dataset:\n\ndl = DataLoader(dset, batch_size=256)\nxb,yb = first(dl)\nxb.shape,yb.shape\n\n(torch.Size([256, 784]), torch.Size([256, 1]))\n\n\nWe’ll do the same for the validation set:\n\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n\nLet’s create a mini-batch of size 4 for testing:\n\nbatch = train_x[:4]\nbatch.shape\n\ntorch.Size([4, 784])\n\n\n\npreds = linear1(batch)\npreds\n\ntensor([[-11.1002],\n        [  5.9263],\n        [  9.9627],\n        [ -8.1484]], grad_fn=&lt;AddBackward0&gt;)\n\n\n\nloss = mnist_loss(preds, train_y[:4])\nloss\n\ntensor(0.5006, grad_fn=&lt;MeanBackward0&gt;)\n\n\nNow we can calculate the gradients:\n\nloss.backward()\nweights.grad.shape,weights.grad.mean(),bias.grad\n\n(torch.Size([784, 1]), tensor(-0.0001), tensor([-0.0008]))\n\n\nLet’s put that all in a function:\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\nand test it:\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(),bias.grad\n\n(tensor(-0.0002), tensor([-0.0015]))\n\n\nBut look what happens if we call it twice:\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(),bias.grad\n\n(tensor(-0.0003), tensor([-0.0023]))\n\n\nThe gradients have changed! The reason for this is that loss.backward actually adds the gradients of loss to any gradients that are currently stored. So, we have to set the current gradients to 0 first:\n\nweights.grad.zero_()\nbias.grad.zero_();\n\n\nnote: Inplace Operations: Methods in PyTorch whose names end in an underscore modify their objects in place. For instance, bias.zero_() sets all elements of the tensor bias to 0.\n\nOur only remaining step is to update the weights and biases based on the gradient and learning rate. When we do so, we have to tell PyTorch not to take the gradient of this step too—otherwise things will get very confusing when we try to compute the derivative at the next batch! If we assign to the data attribute of a tensor then PyTorch will not take the gradient of that step. Here’s our basic training loop for an epoch:\n\ndef train_epoch(model, lr, params):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\nWe also want to check how we’re doing, by looking at the accuracy of the validation set. To decide if an output represents a 3 or a 7, we can just check whether it’s greater than 0. So our accuracy for each item can be calculated (using broadcasting, so no loops!) with:\n\n(preds&gt;0.0).float() == train_y[:4]\n\ntensor([[False],\n        [ True],\n        [ True],\n        [False]])\n\n\nThat gives us this function to calculate our validation accuracy:\n\ndef batch_accuracy(xb, yb):\n    preds = xb.sigmoid()\n    correct = (preds&gt;0.5) == yb\n    return correct.float().mean()\n\nWe can check it works:\n\nbatch_accuracy(linear1(batch), train_y[:4])\n\ntensor(0.5000)\n\n\nand then put the batches together:\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\n\nvalidate_epoch(linear1)\n\n0.5219\n\n\nThat’s our starting point. Let’s train for one epoch, and see if the accuracy improves:\n\nlr = 1.\nparams = weights,bias\ntrain_epoch(linear1, lr, params)\nvalidate_epoch(linear1)\n\n0.6883\n\n\nThen do a few more:\n\nfor i in range(20):\n    train_epoch(linear1, lr, params)\n    print(validate_epoch(linear1), end=' ')\n\n0.8314 0.9017 0.9227 0.9349 0.9438 0.9501 0.9535 0.9564 0.9594 0.9618 0.9613 0.9638 0.9643 0.9652 0.9662 0.9677 0.9687 0.9691 0.9691 0.9696 \n\n\nLooking good! We’re already about at the same accuracy as our “pixel similarity” approach, and we’ve created a general-purpose foundation we can build on. Our next step will be to create an object that will handle the SGD step for us. In PyTorch, it’s called an optimizer.\n\nCreating an Optimizer\nBecause this is such a general foundation, PyTorch provides some useful classes to make it easier to implement. The first thing we can do is replace our linear1 function with PyTorch’s nn.Linear module. A module is an object of a class that inherits from the PyTorch nn.Module class. Objects of this class behave identically to standard Python functions, in that you can call them using parentheses and they will return the activations of a model.\nnn.Linear does the same thing as our init_params and linear together. It contains both the weights and biases in a single class. Here’s how we replicate our model from the previous section:\n\nlinear_model = nn.Linear(28*28,1)\n\nEvery PyTorch module knows what parameters it has that can be trained; they are available through the parameters method:\n\nw,b = linear_model.parameters()\nw.shape,b.shape\n\n(torch.Size([1, 784]), torch.Size([1]))\n\n\nWe can use this information to create an optimizer:\n\nclass BasicOptim:\n    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n    def step(self, *args, **kwargs):\n        for p in self.params: p.data -= p.grad.data * self.lr\n\n    def zero_grad(self, *args, **kwargs):\n        for p in self.params: p.grad = None\n\nWe can create our optimizer by passing in the model’s parameters:\n\nopt = BasicOptim(linear_model.parameters(), lr)\n\nOur training loop can now be simplified to:\n\ndef train_epoch(model):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n\nOur validation function doesn’t need to change at all:\n\nvalidate_epoch(linear_model)\n\n0.4157\n\n\nLet’s put our little training loop in a function, to make things simpler:\n\ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=' ')\n\nThe results are the same as in the previous section:\n\ntrain_model(linear_model, 20)\n\n0.4932 0.8618 0.8203 0.9102 0.9331 0.9468 0.9555 0.9629 0.9658 0.9673 0.9687 0.9707 0.9726 0.9751 0.9761 0.9761 0.9775 0.978 0.9785 0.9785 \n\n\nfastai provides the SGD class which, by default, does the same thing as our BasicOptim:\n\nlinear_model = nn.Linear(28*28,1)\nopt = SGD(linear_model.parameters(), lr)\ntrain_model(linear_model, 20)\n\n0.4932 0.852 0.8335 0.9116 0.9326 0.9473 0.9555 0.9624 0.9648 0.9668 0.9692 0.9712 0.9731 0.9746 0.9761 0.9765 0.9775 0.978 0.9785 0.9785 \n\n\nfastai also provides Learner.fit, which we can use instead of train_model. To create a Learner we first need to create a DataLoaders, by passing in our training and validation DataLoaders:\n\ndls = DataLoaders(dl, valid_dl)\n\nTo create a Learner without using an application (such as vision_learner) we need to pass in all the elements that we’ve created in this chapter: the DataLoaders, the model, the optimization function (which will be passed the parameters), the loss function, and optionally any metrics to print:\n\nlearn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\nNow we can call fit:\n\nlearn.fit(10, lr=lr)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.636857\n0.503549\n0.495584\n00:00\n\n\n1\n0.545725\n0.170281\n0.866045\n00:00\n\n\n2\n0.199223\n0.184893\n0.831207\n00:00\n\n\n3\n0.086580\n0.107836\n0.911187\n00:00\n\n\n4\n0.045185\n0.078481\n0.932777\n00:00\n\n\n5\n0.029108\n0.062792\n0.946516\n00:00\n\n\n6\n0.022560\n0.053017\n0.955348\n00:00\n\n\n7\n0.019687\n0.046500\n0.962218\n00:00\n\n\n8\n0.018252\n0.041929\n0.965162\n00:00\n\n\n9\n0.017402\n0.038573\n0.967615\n00:00\n\n\n\n\n\nAs you can see, there’s nothing magic about the PyTorch and fastai classes. They are just convenient pre-packaged pieces that make your life a bit easier! (They also provide a lot of extra functionality we’ll be using in future chapters.)\nWith these classes, we can now replace our linear model with a neural network."
  },
  {
    "objectID": "Fastbook/04_mnist_basics.html#adding-a-nonlinearity",
    "href": "Fastbook/04_mnist_basics.html#adding-a-nonlinearity",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "Adding a Nonlinearity",
    "text": "Adding a Nonlinearity\nSo far we have a general procedure for optimizing the parameters of a function, and we have tried it out on a very boring function: a simple linear classifier. A linear classifier is very constrained in terms of what it can do. To make it a bit more complex (and able to handle more tasks), we need to add something nonlinear between two linear classifiers—this is what gives us a neural network.\nHere is the entire definition of a basic neural network:\n\ndef simple_net(xb): \n    res = xb@w1 + b1\n    res = res.max(tensor(0.0))\n    res = res@w2 + b2\n    return res\n\nThat’s it! All we have in simple_net is two linear classifiers with a max function between them.\nHere, w1 and w2 are weight tensors, and b1 and b2 are bias tensors; that is, parameters that are initially randomly initialized, just like we did in the previous section:\n\nw1 = init_params((28*28,30))\nb1 = init_params(30)\nw2 = init_params((30,1))\nb2 = init_params(1)\n\nThe key point about this is that w1 has 30 output activations (which means that w2 must have 30 input activations, so they match). That means that the first layer can construct 30 different features, each representing some different mix of pixels. You can change that 30 to anything you like, to make the model more or less complex.\nThat little function res.max(tensor(0.0)) is called a rectified linear unit, also known as ReLU. We think we can all agree that rectified linear unit sounds pretty fancy and complicated… But actually, there’s nothing more to it than res.max(tensor(0.0))—in other words, replace every negative number with a zero. This tiny function is also available in PyTorch as F.relu:\n\nplot_function(F.relu)\n\n\n\n\n\n\n\n\n\nJ: There is an enormous amount of jargon in deep learning, including terms like rectified linear unit. The vast vast majority of this jargon is no more complicated than can be implemented in a short line of code, as we saw in this example. The reality is that for academics to get their papers published they need to make them sound as impressive and sophisticated as possible. One of the ways that they do that is to introduce jargon. Unfortunately, this has the result that the field ends up becoming far more intimidating and difficult to get into than it should be. You do have to learn the jargon, because otherwise papers and tutorials are not going to mean much to you. But that doesn’t mean you have to find the jargon intimidating. Just remember, when you come across a word or phrase that you haven’t seen before, it will almost certainly turn out to be referring to a very simple concept.\n\nThe basic idea is that by using more linear layers, we can have our model do more computation, and therefore model more complex functions. But there’s no point just putting one linear layer directly after another one, because when we multiply things together and then add them up multiple times, that could be replaced by multiplying different things together and adding them up just once! That is to say, a series of any number of linear layers in a row can be replaced with a single linear layer with a different set of parameters.\nBut if we put a nonlinear function between them, such as max, then this is no longer true. Now each linear layer is actually somewhat decoupled from the other ones, and can do its own useful work. The max function is particularly interesting, because it operates as a simple if statement.\n\nS: Mathematically, we say the composition of two linear functions is another linear function. So, we can stack as many linear classifiers as we want on top of each other, and without nonlinear functions between them, it will just be the same as one linear classifier.\n\nAmazingly enough, it can be mathematically proven that this little function can solve any computable problem to an arbitrarily high level of accuracy, if you can find the right parameters for w1 and w2 and if you make these matrices big enough. For any arbitrarily wiggly function, we can approximate it as a bunch of lines joined together; to make it closer to the wiggly function, we just have to use shorter lines. This is known as the universal approximation theorem. The three lines of code that we have here are known as layers. The first and third are known as linear layers, and the second line of code is known variously as a nonlinearity, or activation function.\nJust like in the previous section, we can replace this code with something a bit simpler, by taking advantage of PyTorch:\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28,30),\n    nn.ReLU(),\n    nn.Linear(30,1)\n)\n\nnn.Sequential creates a module that will call each of the listed layers or functions in turn.\nnn.ReLU is a PyTorch module that does exactly the same thing as the F.relu function. Most functions that can appear in a model also have identical forms that are modules. Generally, it’s just a case of replacing F with nn and changing the capitalization. When using nn.Sequential, PyTorch requires us to use the module version. Since modules are classes, we have to instantiate them, which is why you see nn.ReLU() in this example.\nBecause nn.Sequential is a module, we can get its parameters, which will return a list of all the parameters of all the modules it contains. Let’s try it out! As this is a deeper model, we’ll use a lower learning rate and a few more epochs.\n\nlearn = Learner(dls, simple_net, opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\n\n#hide_output\nlearn.fit(40, 0.1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.305828\n0.399663\n0.508341\n00:00\n\n\n1\n0.142960\n0.225702\n0.807655\n00:00\n\n\n2\n0.079516\n0.113519\n0.919529\n00:00\n\n\n3\n0.052391\n0.076792\n0.943081\n00:00\n\n\n4\n0.039796\n0.060083\n0.956330\n00:00\n\n\n5\n0.033368\n0.050713\n0.963690\n00:00\n\n\n6\n0.029680\n0.044797\n0.965653\n00:00\n\n\n7\n0.027290\n0.040729\n0.968106\n00:00\n\n\n8\n0.025568\n0.037771\n0.968597\n00:00\n\n\n9\n0.024233\n0.035508\n0.970559\n00:00\n\n\n10\n0.023149\n0.033714\n0.972031\n00:00\n\n\n11\n0.022242\n0.032243\n0.972522\n00:00\n\n\n12\n0.021468\n0.031006\n0.973503\n00:00\n\n\n13\n0.020796\n0.029944\n0.974485\n00:00\n\n\n14\n0.020207\n0.029016\n0.975466\n00:00\n\n\n15\n0.019683\n0.028196\n0.976448\n00:00\n\n\n16\n0.019215\n0.027463\n0.976448\n00:00\n\n\n17\n0.018791\n0.026806\n0.976938\n00:00\n\n\n18\n0.018405\n0.026212\n0.977920\n00:00\n\n\n19\n0.018051\n0.025671\n0.977920\n00:00\n\n\n20\n0.017725\n0.025179\n0.977920\n00:00\n\n\n21\n0.017422\n0.024728\n0.978410\n00:00\n\n\n22\n0.017141\n0.024313\n0.978901\n00:00\n\n\n23\n0.016878\n0.023932\n0.979392\n00:00\n\n\n24\n0.016632\n0.023580\n0.979882\n00:00\n\n\n25\n0.016400\n0.023254\n0.979882\n00:00\n\n\n26\n0.016181\n0.022952\n0.979882\n00:00\n\n\n27\n0.015975\n0.022672\n0.980864\n00:00\n\n\n28\n0.015779\n0.022411\n0.980864\n00:00\n\n\n29\n0.015593\n0.022168\n0.981845\n00:00\n\n\n30\n0.015417\n0.021941\n0.981845\n00:00\n\n\n31\n0.015249\n0.021728\n0.981845\n00:00\n\n\n32\n0.015088\n0.021529\n0.981845\n00:00\n\n\n33\n0.014935\n0.021341\n0.981845\n00:00\n\n\n34\n0.014788\n0.021164\n0.981845\n00:00\n\n\n35\n0.014647\n0.020998\n0.982336\n00:00\n\n\n36\n0.014512\n0.020840\n0.982826\n00:00\n\n\n37\n0.014382\n0.020691\n0.982826\n00:00\n\n\n38\n0.014257\n0.020550\n0.982826\n00:00\n\n\n39\n0.014136\n0.020415\n0.982826\n00:00\n\n\n\n\n\nWe’re not showing the 40 lines of output here to save room; the training process is recorded in learn.recorder, with the table of output stored in the values attribute, so we can plot the accuracy over training as:\n\nplt.plot(L(learn.recorder.values).itemgot(2));\n\n\n\n\n\n\n\n\nAnd we can view the final accuracy:\n\nlearn.recorder.values[-1][2]\n\n0.982826292514801\n\n\nAt this point we have something that is rather magical:\n\nA function that can solve any problem to any level of accuracy (the neural network) given the correct set of parameters\nA way to find the best set of parameters for any function (stochastic gradient descent)\n\nThis is why deep learning can do things which seem rather magical, such fantastic things. Believing that this combination of simple techniques can really solve any problem is one of the biggest steps that we find many students have to take. It seems too good to be true—surely things should be more difficult and complicated than this? Our recommendation: try it out! We just tried it on the MNIST dataset and you have seen the results. And since we are doing everything from scratch ourselves (except for calculating the gradients) you know that there is no special magic hiding behind the scenes.\n\nGoing Deeper\nThere is no need to stop at just two linear layers. We can add as many as we want, as long as we add a nonlinearity between each pair of linear layers. As you will learn, however, the deeper the model gets, the harder it is to optimize the parameters in practice. Later in this book you will learn about some simple but brilliantly effective techniques for training deeper models.\nWe already know that a single nonlinearity with two linear layers is enough to approximate any function. So why would we use deeper models? The reason is performance. With a deeper model (that is, one with more layers) we do not need to use as many parameters; it turns out that we can use smaller matrices with more layers, and get better results than we would get with larger matrices, and few layers.\nThat means that we can train the model more quickly, and it will take up less memory. In the 1990s researchers were so focused on the universal approximation theorem that very few were experimenting with more than one nonlinearity. This theoretical but not practical foundation held back the field for years. Some researchers, however, did experiment with deep models, and eventually were able to show that these models could perform much better in practice. Eventually, theoretical results were developed which showed why this happens. Today, it is extremely unusual to find anybody using a neural network with just one nonlinearity.\nHere is what happens when we train an 18-layer model using the same approach we saw in &lt;&gt;:\n\ndls = ImageDataLoaders.from_folder(path)\nlearn = vision_learner(dls, resnet18, pretrained=False,\n                    loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(1, 0.1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.082089\n0.009578\n0.997056\n00:11\n\n\n\n\n\nNearly 100% accuracy! That’s a big difference compared to our simple neural net. But as you’ll learn in the remainder of this book, there are just a few little tricks you need to use to get such great results from scratch yourself. You already know the key foundational pieces. (Of course, even once you know all the tricks, you’ll nearly always want to work with the pre-built classes provided by PyTorch and fastai, because they save you having to think about all the little details yourself.)"
  },
  {
    "objectID": "Fastbook/04_mnist_basics.html#jargon-recap",
    "href": "Fastbook/04_mnist_basics.html#jargon-recap",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "Jargon Recap",
    "text": "Jargon Recap\nCongratulations: you now know how to create and train a deep neural network from scratch! We’ve gone through quite a few steps to get to this point, but you might be surprised at how simple it really is.\nNow that we are at this point, it is a good opportunity to define, and review, some jargon and key concepts.\nA neural network contains a lot of numbers, but they are only of two types: numbers that are calculated, and the parameters that these numbers are calculated from. This gives us the two most important pieces of jargon to learn:\n\nActivations:: Numbers that are calculated (both by linear and nonlinear layers)\nParameters:: Numbers that are randomly initialized, and optimized (that is, the numbers that define the model)\n\nWe will often talk in this book about activations and parameters. Remember that they have very specific meanings. They are numbers. They are not abstract concepts, but they are actual specific numbers that are in your model. Part of becoming a good deep learning practitioner is getting used to the idea of actually looking at your activations and parameters, and plotting them and testing whether they are behaving correctly.\nOur activations and parameters are all contained in tensors. These are simply regularly shaped arrays—for example, a matrix. Matrices have rows and columns; we call these the axes or dimensions. The number of dimensions of a tensor is its rank. There are some special tensors:\n\nRank zero: scalar\nRank one: vector\nRank two: matrix\n\nA neural network contains a number of layers. Each layer is either linear or nonlinear. We generally alternate between these two kinds of layers in a neural network. Sometimes people refer to both a linear layer and its subsequent nonlinearity together as a single layer. Yes, this is confusing. Sometimes a nonlinearity is referred to as an activation function.\n&lt;&gt; summarizes the key concepts related to SGD.\n[[dljargon1]]\n.Deep learning vocabulary\n[options=\"header\"]\n|=====\n| Term | Meaning\n|ReLU | Function that returns 0 for negative numbers and doesn't change positive numbers.\n|Mini-batch | A small group of inputs and labels gathered together in two arrays. A gradient descent step is updated on this batch (rather than a whole epoch).\n|Forward pass | Applying the model to some input and computing the predictions.\n|Loss | A value that represents how well (or badly) our model is doing.\n|Gradient | The derivative of the loss with respect to some parameter of the model.\n|Backward pass | Computing the gradients of the loss with respect to all model parameters.\n|Gradient descent | Taking a step in the directions opposite to the gradients to make the model parameters a little bit better.\n|Learning rate | The size of the step we take when applying SGD to update the parameters of the model.\n|=====\n\nnote: Choose Your Own Adventure Reminder: Did you choose to skip over chapters 2 & 3, in your excitement to peek under the hood? Well, here’s your reminder to head back to chapter 2 now, because you’ll be needing to know that stuff very soon!"
  },
  {
    "objectID": "Fastbook/04_mnist_basics.html#questionnaire",
    "href": "Fastbook/04_mnist_basics.html#questionnaire",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nHow is a grayscale image represented on a computer? How about a color image?\nHow are the files and folders in the MNIST_SAMPLE dataset structured? Why?\nExplain how the “pixel similarity” approach to classifying digits works.\nWhat is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\nWhat is a “rank-3 tensor”?\nWhat is the difference between tensor rank and shape? How do you get the rank from the shape?\nWhat are RMSE and L1 norm?\nHow can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?\nCreate a 3×3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.\nWhat is broadcasting?\nAre metrics generally calculated using the training set, or the validation set? Why?\nWhat is SGD?\nWhy does SGD use mini-batches?\nWhat are the seven steps in SGD for machine learning?\nHow do we initialize the weights in a model?\nWhat is “loss”?\nWhy can’t we always use a high learning rate?\nWhat is a “gradient”?\nDo you need to know how to calculate gradients yourself?\nWhy can’t we use accuracy as a loss function?\nDraw the sigmoid function. What is special about its shape?\nWhat is the difference between a loss function and a metric?\nWhat is the function to calculate new weights using a learning rate?\nWhat does the DataLoader class do?\nWrite pseudocode showing the basic steps taken in each epoch for SGD.\nCreate a function that, if passed two arguments [1,2,3,4] and 'abcd', returns [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]. What is special about that output data structure?\nWhat does view do in PyTorch?\nWhat are the “bias” parameters in a neural network? Why do we need them?\nWhat does the @ operator do in Python?\nWhat does the backward method do?\nWhy do we have to zero the gradients?\nWhat information do we have to pass to Learner?\nShow Python or pseudocode for the basic steps of a training loop.\nWhat is “ReLU”? Draw a plot of it for values from -2 to +2.\nWhat is an “activation function”?\nWhat’s the difference between F.relu and nn.ReLU?\nThe universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?\n\n\nFurther Research\n\nCreate your own implementation of Learner from scratch, based on the training loop shown in this chapter.\nComplete all the steps in this chapter using the full MNIST datasets (that is, for all digits, not just 3s and 7s). This is a significant project and will take you quite a bit of time to complete! You’ll need to do some of your own research to figure out how to overcome some obstacles you’ll meet on the way."
  },
  {
    "objectID": "Fastbook/README_ja.html",
    "href": "Fastbook/README_ja.html",
    "title": "The fastai book",
    "section": "",
    "text": "English / Spanish / Korean / Chinese / Bengali / Indonesian / Italian / Portuguese / Vietnamese / Japanese"
  },
  {
    "objectID": "Fastbook/README_ja.html#colab",
    "href": "Fastbook/README_ja.html#colab",
    "title": "The fastai book",
    "section": "Colab",
    "text": "Colab\nこのレポジトリをクローンして自分のマシンで開く代わりに、Google Colabを使ってノートブックを読んだり、作業することができます。この方法はPythonの開発環境をセットアップする必要がなく、ブラウザ上ですぐに作業を始められるので、勉強を始められたばかりの方には特におすすめです。\nこの本の任意の章を次のリンクから開くことができます:: Introduction to Jupyter | Chapter 1, Intro | Chapter 2, Production | Chapter 3, Ethics | Chapter 4, MNIST Basics | Chapter 5, Pet Breeds | Chapter 6, Multi-Category | Chapter 7, Sizing and TTA | Chapter 8, Collab | Chapter 9, Tabular | Chapter 10, NLP | Chapter 11, Mid-Level API | Chapter 12, NLP Deep-Dive | Chapter 13, Convolutions | Chapter 14, Resnet | Chapter 15, Arch Details | Chapter 16, Optimizers and Callbacks | Chapter 17, Foundations | Chapter 18, GradCAM | Chapter 19, Learner | Chapter 20, conclusion"
  },
  {
    "objectID": "Fastbook/README_ja.html#コントリビューション",
    "href": "Fastbook/README_ja.html#コントリビューション",
    "title": "The fastai book",
    "section": "コントリビューション",
    "text": "コントリビューション\nこのレポジトリに対してプルリクエストをした場合、その著作権はすべてJeremy HowardとSylvain Guggerに譲渡されます。（また、テキストやスペルの修正などの小さな変更を加える場合は、修正をするファイル名と修正事項に関する簡潔な説明文を含めてください。でないとレビュアーにはどこが修正されているか判断するのが困難です。よろしくおねがいします。）"
  },
  {
    "objectID": "Fastbook/README_ja.html#引用",
    "href": "Fastbook/README_ja.html#引用",
    "title": "The fastai book",
    "section": "引用",
    "text": "引用\nこの本を引用する場合は下記の内容をお使いください:\n@book{howard2020deep,\ntitle={Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD},\nauthor={Howard, J. and Gugger, S.},\nisbn={9781492045526},\nurl={https://books.google.no/books?id=xd6LxgEACAAJ},\nyear={2020},\npublisher={O'Reilly Media, Incorporated}\n}"
  },
  {
    "objectID": "Fastbook/README_ko.html",
    "href": "Fastbook/README_ko.html",
    "title": "The fastai book - draft",
    "section": "",
    "text": "English / Spanish / Korean / Chinese / Bengali / Indonesian / Italian / Portuguese / Vietnamese / Japanese\n\nThe fastai book - draft\n이 draft는 fastai, PyTorch 에 대한 소개를 다루고 있습니다. fastai는 딥러닝을 위한 레이어드 API입니다. fastai에 대한 자세한 정보는 the fastai paper 를 참고하실 수 있습니다. 이 저장소는 2020년부터 Jeremy Howard와 Sylvain Gugger가 저작권을 갖고 있습니다.\n이 노트북들은 2020년 3월부터 샌프란시스코에서 강의하는 과정 에 사용되며, 2020년 7월경부터는 MOOC 강좌로 이용할 수 있을 것입니다. 우리의 계획은 이 노트북들이 이 책 의 기본이 되어 선주문할 수 있게 하는 것입니다. 그 책에는 이 초안에 있는 것과 같은 GPL 규제를 가지고 있지 않을 것입니다.\n노트북에 포함된 코드와 python .py 파일의 코드는 GPL v3 라이센스로 처리됩니다. 자세한 내용은 License 파일을 참조 부탁드립니다.\n나머지(노트북 및 기타 모든 마크다운 셀 포함)는 노트북의 복사본을 만들거나 개인적으로 이 리포(repo)를 요청하는 것 외에는, 형식이나 매체의 재배포 또는 변경에 대해 허가되지 않습니다. 또한, 상업적으로 사용하거나 방송용으로의 이용은 허용되지 않습니다. 딥러닝을 배우는 데 도움이 될 수 있도록 이러한 자료를 무료로 제공하고 있으니, 당사의 저작권 및 제한 조치를 따라 주세요.\n다른 곳에서 이러한 자료의 사본을 호스팅하는 사람을 보게 되면, 불법 행위이며, 법적 조치로 이어질 수 있음을 알려주기 바랍니다. 또한 만약 사람들이 이 제한을 무시한다면, 더 이상 이런 식으로 추가 자료를 발표하지 않을 것이기 때문에 꼭 지켜주시길 바랍니다.\n이것은 가장 초안입니다. 노트북을 실행할 때 오류가 발생한다면 fastai-v2 포럼 에서 답변을 검색하고, 도움을 요청하십시오. GitHub issue 에서는 이러한 문제를 다루지 않을 것이니 올려주시지 않기를 부탁드립니다.\n만약 이 repo에 PR을 보낸다면, 해당 저작물의 저작권을 Jeremy Howard와 Sylvain Gugger에게 할당하는 것입니다. (추가적으로, 철자법이나 텍스트에 대해 편집을 한다면, 파일의 이름과 당신이 고치고 있는 것에 대한 간단한 설명을 함께 명시해주십시오. 검토자들이 어떤 수정이 이루어졌는지 확인하는 것이 어렵기 때문입니다. 감사합니다.)"
  },
  {
    "objectID": "Fastbook/07_sizing_and_tta.html",
    "href": "Fastbook/07_sizing_and_tta.html",
    "title": "Training a State-of-the-Art Model",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\n[[chapter_sizing_and_tta]]\nThis chapter introduces more advanced techniques for training an image classification model and getting state-of-the-art results. You can skip it if you want to learn more about other applications of deep learning and come back to it later—knowledge of this material will not be assumed in later chapters.\nWe will look at what normalization is, a powerful data augmentation technique called mixup, the progressive resizing approach and test time augmentation. To show all of this, we are going to train a model from scratch (not using transfer learning) using a subset of ImageNet called Imagenette. It contains a subset of 10 very different categories from the original ImageNet dataset, making for quicker training when we want to experiment.\nThis is going to be much harder to do well than with our previous datasets because we’re using full-size, full-color images, which are photos of objects of different sizes, in different orientations, in different lighting, and so forth. So, in this chapter we’re going to introduce some important techniques for getting the most out of your dataset, especially when you’re training from scratch, or using transfer learning to train a model on a very different kind of dataset than the pretrained model used."
  },
  {
    "objectID": "Fastbook/07_sizing_and_tta.html#imagenette",
    "href": "Fastbook/07_sizing_and_tta.html#imagenette",
    "title": "Training a State-of-the-Art Model",
    "section": "Imagenette",
    "text": "Imagenette\nWhen fast.ai first started there were three main datasets that people used for building and testing computer vision models:\n\nImageNet:: 1.3 million images of various sizes around 500 pixels across, in 1,000 categories, which took a few days to train\nMNIST:: 50,000 28×28-pixel grayscale handwritten digits\nCIFAR10:: 60,000 32×32-pixel color images in 10 classes\n\nThe problem was that the smaller datasets didn’t actually generalize effectively to the large ImageNet dataset. The approaches that worked well on ImageNet generally had to be developed and trained on ImageNet. This led to many people believing that only researchers with access to giant computing resources could effectively contribute to developing image classification algorithms.\nWe thought that seemed very unlikely to be true. We had never actually seen a study that showed that ImageNet happen to be exactly the right size, and that other datasets could not be developed which would provide useful insights. So we thought we would try to create a new dataset that researchers could test their algorithms on quickly and cheaply, but which would also provide insights likely to work on the full ImageNet dataset.\nAbout three hours later we had created Imagenette. We selected 10 classes from the full ImageNet that looked very different from one another. As we had hoped, we were able to quickly and cheaply create a classifier capable of recognizing these classes. We then tried out a few algorithmic tweaks to see how they impacted Imagenette. We found some that worked pretty well, and tested them on ImageNet as well—and we were very pleased to find that our tweaks worked well on ImageNet too!\nThere is an important message here: the dataset you get given is not necessarily the dataset you want. It’s particularly unlikely to be the dataset that you want to do your development and prototyping in. You should aim to have an iteration speed of no more than a couple of minutes—that is, when you come up with a new idea you want to try out, you should be able to train a model and see how it goes within a couple of minutes. If it’s taking longer to do an experiment, think about how you could cut down your dataset, or simplify your model, to improve your experimentation speed. The more experiments you can do, the better!\nLet’s get started with this dataset:\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.IMAGENETTE)\n\nFirst we’ll get our dataset into a DataLoaders object, using the presizing trick introduced in &lt;&gt;:\n\ndblock = DataBlock(blocks=(ImageBlock(), CategoryBlock()),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=Resize(460),\n                   batch_tfms=aug_transforms(size=224, min_scale=0.75))\ndls = dblock.dataloaders(path, bs=64)\n\nand do a training run that will serve as a baseline:\n\nmodel = xresnet50(n_out=dls.c)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.583403\n2.064317\n0.401792\n01:03\n\n\n1\n1.208877\n1.260106\n0.601568\n01:02\n\n\n2\n0.925265\n1.036154\n0.664302\n01:03\n\n\n3\n0.730190\n0.700906\n0.777819\n01:03\n\n\n4\n0.585707\n0.541810\n0.825243\n01:03\n\n\n\n\n\nThat’s a good baseline, since we are not using a pretrained model, but we can do better. When working with models that are being trained from scratch, or fine-tuned to a very different dataset than the one used for the pretraining, there are some additional techniques that are really important. In the rest of the chapter we’ll consider some of the key approaches you’ll want to be familiar with. The first one is normalizing your data."
  },
  {
    "objectID": "Fastbook/07_sizing_and_tta.html#normalization",
    "href": "Fastbook/07_sizing_and_tta.html#normalization",
    "title": "Training a State-of-the-Art Model",
    "section": "Normalization",
    "text": "Normalization\nWhen training a model, it helps if your input data is normalized—that is, has a mean of 0 and a standard deviation of 1. But most images and computer vision libraries use values between 0 and 255 for pixels, or between 0 and 1; in either case, your data is not going to have a mean of 0 and a standard deviation of 1.\nLet’s grab a batch of our data and look at those values, by averaging over all axes except for the channel axis, which is axis 1:\n\nx,y = dls.one_batch()\nx.mean(dim=[0,2,3]),x.std(dim=[0,2,3])\n\n(TensorImage([0.4842, 0.4711, 0.4511], device='cuda:5'),\n TensorImage([0.2873, 0.2893, 0.3110], device='cuda:5'))\n\n\nAs we expected, the mean and standard deviation are not very close to the desired values. Fortunately, normalizing the data is easy to do in fastai by adding the Normalize transform. This acts on a whole mini-batch at once, so you can add it to the batch_tfms section of your data block. You need to pass to this transform the mean and standard deviation that you want to use; fastai comes with the standard ImageNet mean and standard deviation already defined. (If you do not pass any statistics to the Normalize transform, fastai will automatically calculate them from a single batch of your data.)\nLet’s add this transform (using imagenet_stats as Imagenette is a subset of ImageNet) and take a look at one batch now:\n\ndef get_dls(bs, size):\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=Resize(460),\n                   batch_tfms=[*aug_transforms(size=size, min_scale=0.75),\n                               Normalize.from_stats(*imagenet_stats)])\n    return dblock.dataloaders(path, bs=bs)\n\n\ndls = get_dls(64, 224)\n\n\nx,y = dls.one_batch()\nx.mean(dim=[0,2,3]),x.std(dim=[0,2,3])\n\n(TensorImage([-0.0787,  0.0525,  0.2136], device='cuda:5'),\n TensorImage([1.2330, 1.2112, 1.3031], device='cuda:5'))\n\n\nLet’s check what effect this had on training our model:\n\nmodel = xresnet50(n_out=dls.c)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.632865\n2.250024\n0.391337\n01:02\n\n\n1\n1.294041\n1.579932\n0.517177\n01:02\n\n\n2\n0.960535\n1.069164\n0.657207\n01:04\n\n\n3\n0.730220\n0.767433\n0.771845\n01:05\n\n\n4\n0.577889\n0.550673\n0.824496\n01:06\n\n\n\n\n\nAlthough it only helped a little here, normalization becomes especially important when using pretrained models. The pretrained model only knows how to work with data of the type that it has seen before. If the average pixel value was 0 in the data it was trained with, but your data has 0 as the minimum possible value of a pixel, then the model is going to be seeing something very different to what is intended!\nThis means that when you distribute a model, you need to also distribute the statistics used for normalization, since anyone using it for inference, or transfer learning, will need to use the same statistics. By the same token, if you’re using a model that someone else has trained, make sure you find out what normalization statistics they used, and match them.\nWe didn’t have to handle normalization in previous chapters because when using a pretrained model through vision_learner, the fastai library automatically adds the proper Normalize transform; the model has been pretrained with certain statistics in Normalize (usually coming from the ImageNet dataset), so the library can fill those in for you. Note that this only applies with pretrained models, which is why we need to add this information manually here, when training from scratch.\nAll our training up until now has been done at size 224. We could have begun training at a smaller size before going to that. This is called progressive resizing."
  },
  {
    "objectID": "Fastbook/07_sizing_and_tta.html#progressive-resizing",
    "href": "Fastbook/07_sizing_and_tta.html#progressive-resizing",
    "title": "Training a State-of-the-Art Model",
    "section": "Progressive Resizing",
    "text": "Progressive Resizing\nWhen fast.ai and its team of students won the DAWNBench competition in 2018, one of the most important innovations was something very simple: start training using small images, and end training using large images. Spending most of the epochs training with small images, helps training complete much faster. Completing training using large images makes the final accuracy much higher. We call this approach progressive resizing.\n\njargon: progressive resizing: Gradually using larger and larger images as you train.\n\nAs we have seen, the kinds of features that are learned by convolutional neural networks are not in any way specific to the size of the image—early layers find things like edges and gradients, and later layers may find things like noses and sunsets. So, when we change image size in the middle of training, it doesn’t mean that we have to find totally different parameters for our model.\nBut clearly there are some differences between small images and big ones, so we shouldn’t expect our model to continue working exactly as well, with no changes at all. Does this remind you of something? When we developed this idea, it reminded us of transfer learning! We are trying to get our model to learn to do something a little bit different from what it has learned to do before. Therefore, we should be able to use the fine_tune method after we resize our images.\nThere is an additional benefit to progressive resizing: it is another form of data augmentation. Therefore, you should expect to see better generalization of your models that are trained with progressive resizing.\nTo implement progressive resizing it is most convenient if you first create a get_dls function which takes an image size and a batch size as we did in the section before, and returns your DataLoaders:\nNow you can create your DataLoaders with a small size and use fit_one_cycle in the usual way, training for a few less epochs than you might otherwise do:\n\ndls = get_dls(128, 128)\nlearn = Learner(dls, xresnet50(n_out=dls.c), loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy)\nlearn.fit_one_cycle(4, 3e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.902943\n2.447006\n0.401419\n00:30\n\n\n1\n1.315203\n1.572992\n0.525765\n00:30\n\n\n2\n1.001199\n0.767886\n0.759149\n00:30\n\n\n3\n0.765864\n0.665562\n0.797984\n00:30\n\n\n\n\n\nThen you can replace the DataLoaders inside the Learner, and fine-tune:\n\nlearn.dls = get_dls(64, 224)\nlearn.fine_tune(5, 1e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.985213\n1.654063\n0.565721\n01:06\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.706869\n0.689622\n0.784541\n01:07\n\n\n1\n0.739217\n0.928541\n0.712472\n01:07\n\n\n2\n0.629462\n0.788906\n0.764003\n01:07\n\n\n3\n0.491912\n0.502622\n0.836445\n01:06\n\n\n4\n0.414880\n0.431332\n0.863331\n01:06\n\n\n\n\n\nAs you can see, we’re getting much better performance, and the initial training on small images was much faster on each epoch.\nYou can repeat the process of increasing size and training more epochs as many times as you like, for as big an image as you wish—but of course, you will not get any benefit by using an image size larger than the size of your images on disk.\nNote that for transfer learning, progressive resizing may actually hurt performance. This is most likely to happen if your pretrained model was quite similar to your transfer learning task and dataset and was trained on similar-sized images, so the weights don’t need to be changed much. In that case, training on smaller images may damage the pretrained weights.\nOn the other hand, if the transfer learning task is going to use images that are of different sizes, shapes, or styles than those used in the pretraining task, progressive resizing will probably help. As always, the answer to “Will it help?” is “Try it!”\nAnother thing we could try is applying data augmentation to the validation set. Up until now, we have only applied it on the training set; the validation set always gets the same images. But maybe we could try to make predictions for a few augmented versions of the validation set and average them. We’ll consider this approach next."
  },
  {
    "objectID": "Fastbook/07_sizing_and_tta.html#test-time-augmentation",
    "href": "Fastbook/07_sizing_and_tta.html#test-time-augmentation",
    "title": "Training a State-of-the-Art Model",
    "section": "Test Time Augmentation",
    "text": "Test Time Augmentation\nWe have been using random cropping as a way to get some useful data augmentation, which leads to better generalization, and results in a need for less training data. When we use random cropping, fastai will automatically use center cropping for the validation set—that is, it will select the largest square area it can in the center of the image, without going past the image’s edges.\nThis can often be problematic. For instance, in a multi-label dataset sometimes there are small objects toward the edges of an image; these could be entirely cropped out by center cropping. Even for problems such as our pet breed classification example, it’s possible that some critical feature necessary for identifying the correct breed, such as the color of the nose, could be cropped out.\nOne solution to this problem is to avoid random cropping entirely. Instead, we could simply squish or stretch the rectangular images to fit into a square space. But then we miss out on a very useful data augmentation, and we also make the image recognition more difficult for our model, because it has to learn how to recognize squished and squeezed images, rather than just correctly proportioned images.\nAnother solution is to not just center crop for validation, but instead to select a number of areas to crop from the original rectangular image, pass each of them through our model, and take the maximum or average of the predictions. In fact, we could do this not just for different crops, but for different values across all of our test time augmentation parameters. This is known as test time augmentation (TTA).\n\njargon: test time augmentation (TTA): During inference or validation, creating multiple versions of each image, using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image.\n\nDepending on the dataset, test time augmentation can result in dramatic improvements in accuracy. It does not change the time required to train at all, but will increase the amount of time required for validation or inference by the number of test-time-augmented images requested. By default, fastai will use the unaugmented center crop image plus four randomly augmented images.\nYou can pass any DataLoader to fastai’s tta method; by default, it will use your validation set:\n\npreds,targs = learn.tta()\naccuracy(preds, targs).item()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.8737863898277283\n\n\nAs we can see, using TTA gives us good a boost in performance, with no additional training required. However, it does make inference slower—if you’re averaging five images for TTA, inference will be five times slower.\nWe’ve seen examples of how data augmentation helps train better models. Let’s now focus on a new data augmentation technique called Mixup."
  },
  {
    "objectID": "Fastbook/07_sizing_and_tta.html#mixup",
    "href": "Fastbook/07_sizing_and_tta.html#mixup",
    "title": "Training a State-of-the-Art Model",
    "section": "Mixup",
    "text": "Mixup\nMixup, introduced in the 2017 paper “mixup: Beyond Empirical Risk Minimization” by Hongyi Zhang et al., is a very powerful data augmentation technique that can provide dramatically higher accuracy, especially when you don’t have much data and don’t have a pretrained model that was trained on data similar to your dataset. The paper explains: “While data augmentation consistently leads to improved generalization, the procedure is dataset-dependent, and thus requires the use of expert knowledge.” For instance, it’s common to flip images as part of data augmentation, but should you flip only horizontally, or also vertically? The answer is that it depends on your dataset. In addition, if flipping (for instance) doesn’t provide enough data augmentation for you, you can’t “flip more.” It’s helpful to have data augmentation techniques where you can “dial up” or “dial down” the amount of change, to see what works best for you.\nMixup works as follows, for each image:\n\nSelect another image from your dataset at random.\nPick a weight at random.\nTake a weighted average (using the weight from step 2) of the selected image with your image; this will be your independent variable.\nTake a weighted average (with the same weight) of this image’s labels with your image’s labels; this will be your dependent variable.\n\nIn pseudocode, we’re doing this (where t is the weight for our weighted average):\nimage2,target2 = dataset[randint(0,len(dataset)]\nt = random_float(0.5,1.0)\nnew_image = t * image1 + (1-t) * image2\nnew_target = t * target1 + (1-t) * target2\nFor this to work, our targets need to be one-hot encoded. The paper describes this using the equations shown in &lt;&gt; where \\(\\lambda\\) is the same as t in our pseudocode:\n\n\nSidebar: Papers and Math\nWe’re going to be looking at more and more research papers from here on in the book. Now that you have the basic jargon, you might be surprised to discover how much of them you can understand, with a little practice! One issue you’ll notice is that Greek letters, such as \\(\\lambda\\), appear in most papers. It’s a very good idea to learn the names of all the Greek letters, since otherwise it’s very hard to read the papers to yourself, and remember them (or to read code based on them, since code often uses the names of the Greek letters spelled out, such as lambda).\nThe bigger issue with papers is that they use math, instead of code, to explain what’s going on. If you don’t have much of a math background, this will likely be intimidating and confusing at first. But remember: what is being shown in the math, is something that will be implemented in code. It’s just another way of talking about the same thing! After reading a few papers, you’ll pick up more and more of the notation. If you don’t know what a symbol is, try looking it up in Wikipedia’s list of mathematical symbols or drawing it in Detexify, which (using machine learning!) will find the name of your hand-drawn symbol. Then you can search online for that name to find out what it’s for.\n\n\nEnd sidebar\n&lt;&gt; shows what it looks like when we take a linear combination of images, as done in Mixup.\n\n#hide_input\n#id mixup_example\n#caption Mixing a church and a gas station\n#alt An image of a church, a gas station and the two mixed up.\nchurch = PILImage.create(get_image_files_sorted(path/'train'/'n03028079')[0])\ngas = PILImage.create(get_image_files_sorted(path/'train'/'n03425413')[0])\nchurch = church.resize((256,256))\ngas = gas.resize((256,256))\ntchurch = tensor(church).float() / 255.\ntgas = tensor(gas).float() / 255.\n\n_,axs = plt.subplots(1, 3, figsize=(12,4))\nshow_image(tchurch, ax=axs[0]);\nshow_image(tgas, ax=axs[1]);\nshow_image((0.3*tchurch + 0.7*tgas), ax=axs[2]);\n\n\n\n\n\n\n\n\nThe third image is built by adding 0.3 times the first one and 0.7 times the second. In this example, should the model predict “church” or “gas station”? The right answer is 30% church and 70% gas station, since that’s what we’ll get if we take the linear combination of the one-hot-encoded targets. For instance, suppose we have 10 classes and “church” is represented by the index 2 and “gas station” is represented by the index 7, the one-hot-encoded representations are:\n[0, 0, 1, 0, 0, 0, 0, 0, 0, 0] and [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\nso our final target is:\n[0, 0, 0.3, 0, 0, 0, 0, 0.7, 0, 0]\nThis all done for us inside fastai by adding a callback to our Learner. Callbacks are what is used inside fastai to inject custom behavior in the training loop (like a learning rate schedule, or training in mixed precision). We’ll be learning all about callbacks, including how to make your own, in &lt;&gt;. For now, all you need to know is that you use the cbs parameter to Learner to pass callbacks.\nHere is how we train a model with Mixup:\nmodel = xresnet50(n_out=dls.c)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy, cbs=MixUp())\nlearn.fit_one_cycle(5, 3e-3)\nWhat happens when we train a model with data that’s “mixed up” in this way? Clearly, it’s going to be harder to train, because it’s harder to see what’s in each image. And the model has to predict two labels per image, rather than just one, as well as figuring out how much each one is weighted. Overfitting seems less likely to be a problem, however, because we’re not showing the same image in each epoch, but are instead showing a random combination of two images.\nMixup requires far more epochs to train to get better accuracy, compared to other augmentation approaches we’ve seen. You can try training Imagenette with and without Mixup by using the examples/train_imagenette.py script in the fastai repo. At the time of writing, the leaderboard in the Imagenette repo is showing that Mixup is used for all leading results for trainings of &gt;80 epochs, and for fewer epochs Mixup is not being used. This is in line with our experience of using Mixup too.\nOne of the reasons that Mixup is so exciting is that it can be applied to types of data other than photos. In fact, some people have even shown good results by using Mixup on activations inside their models, not just on inputs—this allows Mixup to be used for NLP and other data types too.\nThere’s another subtle issue that Mixup deals with for us, which is that it’s not actually possible with the models we’ve seen before for our loss to ever be perfect. The problem is that our labels are 1s and 0s, but the outputs of softmax and sigmoid can never equal 1 or 0. This means training our model pushes our activations ever closer to those values, such that the more epochs we do, the more extreme our activations become.\nWith Mixup we no longer have that problem, because our labels will only be exactly 1 or 0 if we happen to “mix” with another image of the same class. The rest of the time our labels will be a linear combination, such as the 0.7 and 0.3 we got in the church and gas station example earlier.\nOne issue with this, however, is that Mixup is “accidentally” making the labels bigger than 0, or smaller than 1. That is to say, we’re not explicitly telling our model that we want to change the labels in this way. So, if we want to make the labels closer to, or further away from 0 and 1, we have to change the amount of Mixup—which also changes the amount of data augmentation, which might not be what we want. There is, however, a way to handle this more directly, which is to use label smoothing."
  },
  {
    "objectID": "Fastbook/07_sizing_and_tta.html#label-smoothing",
    "href": "Fastbook/07_sizing_and_tta.html#label-smoothing",
    "title": "Training a State-of-the-Art Model",
    "section": "Label Smoothing",
    "text": "Label Smoothing\nIn the theoretical expression of loss, in classification problems, our targets are one-hot encoded (in practice we tend to avoid doing this to save memory, but what we compute is the same loss as if we had used one-hot encoding). That means the model is trained to return 0 for all categories but one, for which it is trained to return 1. Even 0.999 is not “good enough”, the model will get gradients and learn to predict activations with even higher confidence. This encourages overfitting and gives you at inference time a model that is not going to give meaningful probabilities: it will always say 1 for the predicted category even if it’s not too sure, just because it was trained this way.\nThis can become very harmful if your data is not perfectly labeled. In the bear classifier we studied in &lt;&gt;, we saw that some of the images were mislabeled, or contained two different kinds of bears. In general, your data will never be perfect. Even if the labels were manually produced by humans, they could make mistakes, or have differences of opinions on images that are harder to label.\nInstead, we could replace all our 1s with a number a bit less than 1, and our 0s by a number a bit more than 0, and then train. This is called label smoothing. By encouraging your model to be less confident, label smoothing will make your training more robust, even if there is mislabeled data. The result will be a model that generalizes better.\nThis is how label smoothing works in practice: we start with one-hot-encoded labels, then replace all 0s with \\(\\frac{\\epsilon}{N}\\) (that’s the Greek letter epsilon, which is what was used in the paper that introduced label smoothing and is used in the fastai code), where \\(N\\) is the number of classes and \\(\\epsilon\\) is a parameter (usually 0.1, which would mean we are 10% unsure of our labels). Since we want the labels to add up to 1, replace the 1 by \\(1-\\epsilon + \\frac{\\epsilon}{N}\\). This way, we don’t encourage the model to predict something overconfidently. In our Imagenette example where we have 10 classes, the targets become something like (here for a target that corresponds to the index 3):\n[0.01, 0.01, 0.01, 0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\nIn practice, we don’t want to one-hot encode the labels, and fortunately we won’t need to (the one-hot encoding is just good to explain what label smoothing is and visualize it).\n\nSidebar: Label Smoothing, the Paper\nHere is how the reasoning behind label smoothing was explained in the paper by Christian Szegedy et al.:\n\n: This maximum is not achievable for finite \\(z_k\\) but is approached if \\(z_y\\gg z_k\\) for all \\(k\\neq y\\)—that is, if the logit corresponding to the ground-truth label is much great than all other logits. This, however, can cause two problems. First, it may result in over-fitting: if the model learns to assign full probability to the ground-truth label for each training example, it is not guaranteed to generalize. Second, it encourages the differences between the largest logit and all others to become large, and this, combined with the bounded gradient \\(\\frac{\\partial\\ell}{\\partial z_k}\\), reduces the ability of the model to adapt. Intuitively, this happens because the model becomes too confident about its predictions.\n\nLet’s practice our paper-reading skills to try to interpret this. “This maximum” is refering to the previous part of the paragraph, which talked about the fact that 1 is the value of the label for the positive class. So it’s not possible for any value (except infinity) to result in 1 after sigmoid or softmax. In a paper, you won’t normally see “any value” written; instead it will get a symbol, which in this case is \\(z_k\\). This shorthand is helpful in a paper, because it can be referred to again later and the reader will know what value is being discussed.\nThen it says “if \\(z_y\\gg z_k\\) for all \\(k\\neq y\\).” In this case, the paper immediately follows the math with an English description, which is handy because you can just read that. In the math, the \\(y\\) is refering to the target (\\(y\\) is defined earlier in the paper; sometimes it’s hard to find where symbols are defined, but nearly all papers will define all their symbols somewhere), and \\(z_y\\) is the activation corresponding to the target. So to get close to 1, this activation needs to be much higher than all the others for that prediction.\nNext, consider the statement “if the model learns to assign full probability to the ground-truth label for each training example, it is not guaranteed to generalize.” This is saying that making \\(z_y\\) really big means we’ll need large weights and large activations throughout our model. Large weights lead to “bumpy” functions, where a small change in input results in a big change to predictions. This is really bad for generalization, because it means just one pixel changing a bit could change our prediction entirely!\nFinally, we have “it encourages the differences between the largest logit and all others to become large, and this, combined with the bounded gradient \\(\\frac{\\partial\\ell}{\\partial z_k}\\), reduces the ability of the model to adapt.” The gradient of cross-entropy, remember, is basically output - target. Both output and target are between 0 and 1, so the difference is between -1 and 1, which is why the paper says the gradient is “bounded” (it can’t be infinite). Therefore our SGD steps are bounded too. “Reduces the ability of the model to adapt” means that it is hard for it to be updated in a transfer learning setting. This follows because the difference in loss due to incorrect predictions is unbounded, but we can only take a limited step each time.\n\n\nEnd sidebar\nTo use this in practice, we just have to change the loss function in our call to Learner:\nmodel = xresnet50(n_out=dls.c)\nlearn = Learner(dls, model, loss_func=LabelSmoothingCrossEntropy(), \n                metrics=accuracy)\nlearn.fit_one_cycle(5, 3e-3)\nLike with Mixup, you won’t generally see significant improvements from label smoothing until you train more epochs. Try it yourself and see: how many epochs do you have to train before label smoothing shows an improvement?"
  },
  {
    "objectID": "Fastbook/07_sizing_and_tta.html#conclusion",
    "href": "Fastbook/07_sizing_and_tta.html#conclusion",
    "title": "Training a State-of-the-Art Model",
    "section": "Conclusion",
    "text": "Conclusion\nYou have now seen everything you need to train a state-of-the-art model in computer vision, whether from scratch or using transfer learning. Now all you have to do is experiment on your own problems! See if training longer with Mixup and/or label smoothing avoids overfitting and gives you better results. Try progressive resizing, and test time augmentation.\nMost importantly, remember that if your dataset is big, there is no point prototyping on the whole thing. Find a small subset that is representative of the whole, like we did with Imagenette, and experiment on it.\nIn the next three chapters, we will look at the other applications directly supported by fastai: collaborative filtering, tabular modeling and working with text. We will go back to computer vision in the next section of the book, with a deep dive into convolutional neural networks in &lt;&gt;."
  },
  {
    "objectID": "Fastbook/07_sizing_and_tta.html#questionnaire",
    "href": "Fastbook/07_sizing_and_tta.html#questionnaire",
    "title": "Training a State-of-the-Art Model",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhat is the difference between ImageNet and Imagenette? When is it better to experiment on one versus the other?\nWhat is normalization?\nWhy didn’t we have to care about normalization when using a pretrained model?\nWhat is progressive resizing?\nImplement progressive resizing in your own project. Did it help?\nWhat is test time augmentation? How do you use it in fastai?\nIs using TTA at inference slower or faster than regular inference? Why?\nWhat is Mixup? How do you use it in fastai?\nWhy does Mixup prevent the model from being too confident?\nWhy does training with Mixup for five epochs end up worse than training without Mixup?\nWhat is the idea behind label smoothing?\nWhat problems in your data can label smoothing help with?\nWhen using label smoothing with five categories, what is the target associated with the index 1?\nWhat is the first step to take when you want to prototype quick experiments on a new dataset?\n\n\nFurther Research\n\nUse the fastai documentation to build a function that crops an image to a square in each of the four corners, then implement a TTA method that averages the predictions on a center crop and those four crops. Did it help? Is it better than the TTA method of fastai?\nFind the Mixup paper on arXiv and read it. Pick one or two more recent articles introducing variants of Mixup and read them, then try to implement them on your problem.\nFind the script training Imagenette using Mixup and use it as an example to build a script for a long training on your own project. Execute it and see if it helps.\nRead the sidebar “Label Smoothing, the Paper”, look at the relevant section of the original paper and see if you can follow it. Don’t be afraid to ask for help!"
  },
  {
    "objectID": "Fastbook/app_blog.html",
    "href": "Fastbook/app_blog.html",
    "title": "Creating a Blog",
    "section": "",
    "text": "[[appendix_blog]] [appendix] [role=“Creating a blog”]\nUnfortunately, when it comes to blogging, it seems like you have to make a difficult decision: either use a platform that makes it easy but subjects you and your readers to advertisements, paywalls, and fees, or spend hours setting up your own hosting service and weeks learning about all kinds of intricate details. Perhaps the biggest benefit to the “do-it-yourself” approach is that you really own your own posts, rather than being at the whim of a service provider and their decisions about how to monetize your content in the future.\nIt turns out, however, that you can have the best of both worlds!"
  },
  {
    "objectID": "Fastbook/app_blog.html#blogging-with-github-pages",
    "href": "Fastbook/app_blog.html#blogging-with-github-pages",
    "title": "Creating a Blog",
    "section": "Blogging with GitHub Pages",
    "text": "Blogging with GitHub Pages\nA great solution is to host your blog on a platform called GitHub Pages, which is free, has no ads or pay wall, and makes your data available in a standard way such that you can at any time move your blog to another host. But all the approaches I’ve seen to using GitHub Pages have required knowledge of the command line and arcane tools that only software developers are likely to be familiar with. For instance, GitHub’s own documentation on setting up a blog includes a long list of instructions that involve installing the Ruby programming language, using the git command-line tool, copying over version numbers, and more—17 steps in total!\nTo cut down the hassle, we’ve created an easy approach that allows you to use an entirely browser-based interface for all your blogging needs. You will be up and running with your new blog within about five minutes. It doesn’t cost anything, and you can easily add your own custom domain to it if you wish to. In this section, we’ll explain how to do it, using a template we’ve created called fast_template. (NB: be sure to check the book’s website for the latest blog recommendations, since new tools are always coming out).\n\nCreating the Repository\nYou’ll need an account on GitHub, so head over there now and create an account if you don’t have one already. Make sure that you are logged in. Normally, GitHub is used by software developers for writing code, and they use a sophisticated command-line tool to work with it—but we’re going to show you an approach that doesn’t use the command line at all!\nTo get started, point your browser to https://github.com/fastai/fast_template/generate (you need to be logged in to GitHub for the link to work). This will allow you to create a place to store your blog, called a repository. You will a screen like the one in &lt;&gt;. Note that you have to enter your repository name using the exact format shown here—that is, your GitHub username followed by .github.io.\n\nOnce you’ve entered that, and any description you like, click “Create repository from template.” You have the choice to make the repository “private,” but since you are creating a blog that you want other people to read, having the underlying files publicly available hopefully won’t be a problem for you.\nNow, let’s set up your home page!\n\n\nSetting Up Your Home Page\nWhen readers arrive at your blog the first thing that they will see is the content of a file called index.md. This is a markdown file. Markdown is a powerful yet simple way of creating formatted text, such as bullet points, italics, hyperlinks, and so forth. It is very widely used, including for all the formatting in Jupyter notebooks, nearly every part of the GitHub site, and many other places all over the internet. To create markdown text, you can just type in plain English, then add some special characters to add special behavior. For instance, if you type a * character before and after a word or phrase, that will put it in italics. Let’s try it now.\nTo open the file, click its filename in GitHub. To edit it, click on the pencil icon at the far right hand side of the screen as shown in &lt;&gt;.\n\nYou can add to, edit, or replace the texts that you see. Click “Preview changes” (&lt;&gt;) to see what your markdown text will look like in your blog. Lines that you have added or changed will appear with a green bar on the lefthand side.\n\nTo save your changes, scroll to the bottom of the page and click “Commit changes,” as shown in &lt;&gt;. On GitHub, to “commit” something means to save it to the GitHub server.\n\nNext, you should configure your blog’s settings. To do so, click on the file called _config.yml, then click the edit button like you did for the index file. Change the title, description, and GitHub username values (see &lt;&gt;. You need to leave the names before the colons in place, and type your new values in after the colon (and a space) on each line. You can also add to your email address and Twitter username if you wish, but note that these will appear on your public blog if you fill them in here.\n\nAfter you’re done, commit your changes just like you did with the index file, then wait a minute or so while GitHub processes your new blog. Point your web browser to &lt;username&gt; .github.io (replacing &lt;username&gt; with your GitHub username). You should see your blog, which will look something like &lt;&gt;.\n\n\n\nCreating Posts\nNow you’re ready to create your first post. All your posts will go in the _posts folder. Click on that now, and then click the “Create file” button. You need to be careful to name your file using the format &lt;year&gt;-&lt;month&gt;-&lt;day&gt;-&lt;name&gt;.md, as shown in &lt;&gt;, where &lt;year&gt; is a four-digit number, and &lt;month&gt; and &lt;day&gt; are two-digit numbers. &lt;name&gt; can be anything you want that will help you remember what this post was about. The .md extension is for markdown documents.\n\nYou can then type the contents of your first post. The only rule is that the first line of your post must be a markdown heading. This is created by putting # at the start of a line, as shown in &lt;&gt; (that creates a level-1 heading, which you should just use once at the start of your document; you can create level-2 headings using ##, level 3 with ###, and so forth).\n\nAs before, you can click the “Preview” button to see how your markdown formatting will look (&lt;&gt;).\n\nAnd you will need to click the “Commit new file” button to save it to GitHub, as shown in &lt;&gt;.\n\nHave a look at your blog home page again, and you will see that this post has now appeared–&lt;&gt; shows the result with the sample pose we just added. (Remember that you will need to wait a minute or so for GitHub to process the request before the file shows up.)\n\nYou may have noticed that we provided a sample blog post, which you can go ahead and delete now. Go to your _posts folder, as before, and click on 2020-01-14-welcome.md. Then click the trash icon on the far right, as shown in &lt;&gt;.\n\nIn GitHub, nothing actually changes until you commit—including when you delete a file! So, after you click the trash icon, scroll down to the bottom of the page and commit your changes.\nYou can include images in your posts by adding a line of markdown like the following:\n![Image description](images/filename.jpg)\nFor this to work, you will need to put the image inside your images folder. To do this, click the images folder, them click “Upload files” button (&lt;&gt;).\n\nNow let’s see how to do all of this directly from your computer.\n\n\nSynchronizing GitHub and Your Computer\nThere are lots of reasons you might want to copy your blog content from GitHub to your computer–you might want to be able to read or edit your posts offline, or maybe you’d like a backup in case something happens to your GitHub repository.\nGitHub does more than just let you copy your repository to your computer; it lets you synchronize it with your computer. That means you can make changes on GitHub, and they’ll copy over to your computer, and you can make changes on your computer, and they’ll copy over to GitHub. You can even let other people access and modify your blog, and their changes and your changes will be automatically combined together the next time you sync.\nTo make this work, you have to install an application called GitHub Desktop on your computer. It runs on Mac, Windows, and Linux. Follow the directions to install it, and when you run it it’ll ask you to log in to GitHub and select the repository to sync. Click “Clone a repository from the Internet,” as shown in &lt;&gt;.\n\nOnce GitHub has finished syncing your repo, you’ll be able to click “View the files of your repository in Explorer” (or Finder), as shown in &lt;&gt; and you’ll see the local copy of your blog! Try editing one of the files on your computer. Then return to GitHub Desktop, and you’ll see the “Sync” button is waiting for you to press it. When you click it your changes will be copied over to GitHub, where you’ll see them reflected on the website.\n\nIf you haven’t used git before, GitHub Desktop is a great way to get started. As you’ll discover, it’s a fundamental tool used by most data scientists. Another tool that we hope you now love is Jupyter Notebooks–and there’s a way to write your blog directly with that too!"
  },
  {
    "objectID": "Fastbook/app_blog.html#jupyter-for-blogging",
    "href": "Fastbook/app_blog.html#jupyter-for-blogging",
    "title": "Creating a Blog",
    "section": "Jupyter for Blogging",
    "text": "Jupyter for Blogging\nYou can also write blog posts using Jupyter notebooks. Your markdown cells, code cells, and all the outputs will appear in your exported blog post. The best way to do this may have changed by the time you are reading this book, so be sure to check out the book’s website for the latest information. As we write this, the easiest way to create a blog from notebooks is to use fastpages, which is a more advanced version of fast_template.\nTo blog with a notebook, just pop it in the _notebooks folder in your blog repo, and it will appear in your list of blog posts. When you write your notebook, write whatever you want your audience to see. Since most writing platforms make it hard to include code and outputs, many of us are in the habit of including fewer real examples than we should. This is a great way to instead get into the habit of including lots of examples as you write.\nOften, you’ll want to hide boilerplate such as import statements. You can add #hide to the top of any cell to make it not show up in output. Jupyter displays the result of the last line of a cell, so there’s no need to include print(). (Including extra code that isn’t needed means there’s more cognitive overhead for the reader; so don’t include code that you don’t really need!)"
  },
  {
    "objectID": "Fastbook/clean/03_ethics.html",
    "href": "Fastbook/clean/03_ethics.html",
    "title": "Data Ethics",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()"
  },
  {
    "objectID": "Fastbook/clean/03_ethics.html#key-examples-for-data-ethics",
    "href": "Fastbook/clean/03_ethics.html#key-examples-for-data-ethics",
    "title": "Data Ethics",
    "section": "Key Examples for Data Ethics",
    "text": "Key Examples for Data Ethics\n\nBugs and Recourse: Buggy Algorithm Used for Healthcare Benefits\n\n\nFeedback Loops: YouTube’s Recommendation System\n\n\nBias: Professor Latanya Sweeney “Arrested”\n\n\nWhy Does This Matter?"
  },
  {
    "objectID": "Fastbook/clean/03_ethics.html#integrating-machine-learning-with-product-design",
    "href": "Fastbook/clean/03_ethics.html#integrating-machine-learning-with-product-design",
    "title": "Data Ethics",
    "section": "Integrating Machine Learning with Product Design",
    "text": "Integrating Machine Learning with Product Design"
  },
  {
    "objectID": "Fastbook/clean/03_ethics.html#topics-in-data-ethics",
    "href": "Fastbook/clean/03_ethics.html#topics-in-data-ethics",
    "title": "Data Ethics",
    "section": "Topics in Data Ethics",
    "text": "Topics in Data Ethics\n\nRecourse and Accountability\n\n\nFeedback Loops\n\n\nBias\n\nHistorical bias\n\n\nMeasurement bias\n\n\nAggregation bias\n\n\nRepresentation bias\n\n\n\nAddressing different types of bias\n\n\nDisinformation"
  },
  {
    "objectID": "Fastbook/clean/03_ethics.html#identifying-and-addressing-ethical-issues",
    "href": "Fastbook/clean/03_ethics.html#identifying-and-addressing-ethical-issues",
    "title": "Data Ethics",
    "section": "Identifying and Addressing Ethical Issues",
    "text": "Identifying and Addressing Ethical Issues\n\nAnalyze a Project You Are Working On\n\n\nProcesses to Implement\n\nEthical lenses\n\n\n\nThe Power of Diversity\n\n\nFairness, Accountability, and Transparency"
  },
  {
    "objectID": "Fastbook/clean/03_ethics.html#role-of-policy",
    "href": "Fastbook/clean/03_ethics.html#role-of-policy",
    "title": "Data Ethics",
    "section": "Role of Policy",
    "text": "Role of Policy\n\nThe Effectiveness of Regulation\n\n\nRights and Policy\n\n\nCars: A Historical Precedent"
  },
  {
    "objectID": "Fastbook/clean/03_ethics.html#conclusion",
    "href": "Fastbook/clean/03_ethics.html#conclusion",
    "title": "Data Ethics",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "Fastbook/clean/03_ethics.html#questionnaire",
    "href": "Fastbook/clean/03_ethics.html#questionnaire",
    "title": "Data Ethics",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nDoes ethics provide a list of “right answers”?\nHow can working with people of different backgrounds help when considering ethical questions?\nWhat was the role of IBM in Nazi Germany? Why did the company participate as it did? Why did the workers participate?\nWhat was the role of the first person jailed in the Volkswagen diesel scandal?\nWhat was the problem with a database of suspected gang members maintained by California law enforcement officials?\nWhy did YouTube’s recommendation algorithm recommend videos of partially clothed children to pedophiles, even though no employee at Google had programmed this feature?\nWhat are the problems with the centrality of metrics?\nWhy did Meetup.com not include gender in its recommendation system for tech meetups?\nWhat are the six types of bias in machine learning, according to Suresh and Guttag?\nGive two examples of historical race bias in the US.\nWhere are most images in ImageNet from?\nIn the paper “Does Machine Learning Automate Moral Hazard and Error” why is sinusitis found to be predictive of a stroke?\nWhat is representation bias?\nHow are machines and people different, in terms of their use for making decisions?\nIs disinformation the same as “fake news”?\nWhy is disinformation through auto-generated text a particularly significant issue?\nWhat are the five ethical lenses described by the Markkula Center?\nWhere is policy an appropriate tool for addressing data ethics issues?\n\n\nFurther Research:\n\nRead the article “What Happens When an Algorithm Cuts Your Healthcare”. How could problems like this be avoided in the future?\nResearch to find out more about YouTube’s recommendation system and its societal impacts. Do you think recommendation systems must always have feedback loops with negative results? What approaches could Google take to avoid them? What about the government?\nRead the paper “Discrimination in Online Ad Delivery”. Do you think Google should be considered responsible for what happened to Dr. Sweeney? What would be an appropriate response?\nHow can a cross-disciplinary team help avoid negative consequences?\nRead the paper “Does Machine Learning Automate Moral Hazard and Error”. What actions do you think should be taken to deal with the issues identified in this paper?\nRead the article “How Will We Prevent AI-Based Forgery?” Do you think Etzioni’s proposed approach could work? Why?\nComplete the section “Analyze a Project You Are Working On” in this chapter.\nConsider whether your team could be more diverse. If so, what approaches might help?"
  },
  {
    "objectID": "Fastbook/clean/03_ethics.html#deep-learning-in-practice-thats-a-wrap",
    "href": "Fastbook/clean/03_ethics.html#deep-learning-in-practice-thats-a-wrap",
    "title": "Data Ethics",
    "section": "Deep Learning in Practice: That’s a Wrap!",
    "text": "Deep Learning in Practice: That’s a Wrap!\nCongratulations! You’ve made it to the end of the first section of the book. In this section we’ve tried to show you what deep learning can do, and how you can use it to create real applications and products. At this point, you will get a lot more out of the book if you spend some time trying out what you’ve learned. Perhaps you have already been doing this as you go along—in which case, great! If not, that’s no problem either… Now is a great time to start experimenting yourself.\nIf you haven’t been to the book’s website yet, head over there now. It’s really important that you get yourself set up to run the notebooks. Becoming an effective deep learning practitioner is all about practice, so you need to be training models. So, please go get the notebooks running now if you haven’t already! And also have a look on the website for any important updates or notices; deep learning changes fast, and we can’t change the words that are printed in this book, so the website is where you need to look to ensure you have the most up-to-date information.\nMake sure that you have completed the following steps:\n\nConnect to one of the GPU Jupyter servers recommended on the book’s website.\nRun the first notebook yourself.\nUpload an image that you find in the first notebook; then try a few different images of different kinds to see what happens.\nRun the second notebook, collecting your own dataset based on image search queries that you come up with.\nThink about how you can use deep learning to help you with your own projects, including what kinds of data you could use, what kinds of problems may come up, and how you might be able to mitigate these issues in practice.\n\nIn the next section of the book you will learn about how and why deep learning works, instead of just seeing how you can use it in practice. Understanding the how and why is important for both practitioners and researchers, because in this fairly new field nearly every project requires some level of customization and debugging. The better you understand the foundations of deep learning, the better your models will be. These foundations are less important for executives, product managers, and so forth (although still useful, so feel free to keep reading!), but they are critical for anybody who is actually training and deploying models themselves."
  },
  {
    "objectID": "Fastbook/clean/18_CAM.html",
    "href": "Fastbook/clean/18_CAM.html",
    "title": "CNN Interpretation with CAM",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *"
  },
  {
    "objectID": "Fastbook/clean/18_CAM.html#cam-and-hooks",
    "href": "Fastbook/clean/18_CAM.html#cam-and-hooks",
    "title": "CNN Interpretation with CAM",
    "section": "CAM and Hooks",
    "text": "CAM and Hooks\n\npath = untar_data(URLs.PETS)/'images'\ndef is_cat(x): return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=21,\n    label_func=is_cat, item_tfms=Resize(224))\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n\n\nimg = PILImage.create(image_cat())\nx, = first(dls.test_dl([img]))\n\n\nclass Hook():\n    def hook_func(self, m, i, o): self.stored = o.detach().clone()\n\n\nhook_output = Hook()\nhook = learn.model[0].register_forward_hook(hook_output.hook_func)\n\n\nwith torch.no_grad(): output = learn.model.eval()(x)\n\n\nact = hook_output.stored[0]\n\n\nF.softmax(output, dim=-1)\n\n\ndls.vocab\n\n\nx.shape\n\n\ncam_map = torch.einsum('ck,kij-&gt;cij', learn.model[1][-1].weight, act)\ncam_map.shape\n\n\nx_dec = TensorImage(dls.train.decode((x,))[0][0])\n_,ax = plt.subplots()\nx_dec.show(ctx=ax)\nax.imshow(cam_map[1].detach().cpu(), alpha=0.6, extent=(0,224,224,0),\n              interpolation='bilinear', cmap='magma');\n\n\nhook.remove()\n\n\nclass Hook():\n    def __init__(self, m):\n        self.hook = m.register_forward_hook(self.hook_func)   \n    def hook_func(self, m, i, o): self.stored = o.detach().clone()\n    def __enter__(self, *args): return self\n    def __exit__(self, *args): self.hook.remove()\n\n\nwith Hook(learn.model[0]) as hook:\n    with torch.no_grad(): output = learn.model.eval()(x.cuda())\n    act = hook.stored"
  },
  {
    "objectID": "Fastbook/clean/18_CAM.html#gradient-cam",
    "href": "Fastbook/clean/18_CAM.html#gradient-cam",
    "title": "CNN Interpretation with CAM",
    "section": "Gradient CAM",
    "text": "Gradient CAM\n\nclass HookBwd():\n    def __init__(self, m):\n        self.hook = m.register_backward_hook(self.hook_func)   \n    def hook_func(self, m, gi, go): self.stored = go[0].detach().clone()\n    def __enter__(self, *args): return self\n    def __exit__(self, *args): self.hook.remove()\n\n\ncls = 1\nwith HookBwd(learn.model[0]) as hookg:\n    with Hook(learn.model[0]) as hook:\n        output = learn.model.eval()(x.cuda())\n        act = hook.stored\n    output[0,cls].backward()\n    grad = hookg.stored\n\n\nw = grad[0].mean(dim=[1,2], keepdim=True)\ncam_map = (w * act[0]).sum(0)\n\n\n_,ax = plt.subplots()\nx_dec.show(ctx=ax)\nax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0),\n              interpolation='bilinear', cmap='magma');\n\n\nwith HookBwd(learn.model[0][-2]) as hookg:\n    with Hook(learn.model[0][-2]) as hook:\n        output = learn.model.eval()(x.cuda())\n        act = hook.stored\n    output[0,cls].backward()\n    grad = hookg.stored\n\n\nw = grad[0].mean(dim=[1,2], keepdim=True)\ncam_map = (w * act[0]).sum(0)\n\n\n_,ax = plt.subplots()\nx_dec.show(ctx=ax)\nax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0),\n              interpolation='bilinear', cmap='magma');"
  },
  {
    "objectID": "Fastbook/clean/18_CAM.html#conclusion",
    "href": "Fastbook/clean/18_CAM.html#conclusion",
    "title": "CNN Interpretation with CAM",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "Fastbook/clean/18_CAM.html#questionnaire",
    "href": "Fastbook/clean/18_CAM.html#questionnaire",
    "title": "CNN Interpretation with CAM",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhat is a “hook” in PyTorch?\nWhich layer does CAM use the outputs of?\nWhy does CAM require a hook?\nLook at the source code of the ActivationStats class and see how it uses hooks.\nWrite a hook that stores the activations of a given layer in a model (without peeking, if possible).\nWhy do we call eval before getting the activations? Why do we use no_grad?\nUse torch.einsum to compute the “dog” or “cat” score of each of the locations in the last activation of the body of the model.\nHow do you check which order the categories are in (i.e., the correspondence of index-&gt;category)?\nWhy are we using decode when displaying the input image?\nWhat is a “context manager”? What special methods need to be defined to create one?\nWhy can’t we use plain CAM for the inner layers of a network?\nWhy do we need to register a hook on the backward pass in order to do Grad-CAM?\nWhy can’t we call output.backward() when output is a rank-2 tensor of output activations per image per class?\n\n\nFurther Research\n\nTry removing keepdim and see what happens. Look up this parameter in the PyTorch docs. Why do we need it in this notebook?\nCreate a notebook like this one, but for NLP, and use it to find which words in a movie review are most significant in assessing the sentiment of a particular movie review."
  },
  {
    "objectID": "Fastbook/clean/20_conclusion.html",
    "href": "Fastbook/clean/20_conclusion.html",
    "title": "Concluding Thoughts",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()"
  },
  {
    "objectID": "Fastbook/clean/11_midlevel_data.html",
    "href": "Fastbook/clean/11_midlevel_data.html",
    "title": "Data Munging with fastai’s Mid-Level API",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\nfrom IPython.display import display,HTML"
  },
  {
    "objectID": "Fastbook/clean/11_midlevel_data.html#going-deeper-into-fastais-layered-api",
    "href": "Fastbook/clean/11_midlevel_data.html#going-deeper-into-fastais-layered-api",
    "title": "Data Munging with fastai’s Mid-Level API",
    "section": "Going Deeper into fastai’s Layered API",
    "text": "Going Deeper into fastai’s Layered API\n\nfrom fastai.text.all import *\n\ndls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\n\n\npath = untar_data(URLs.IMDB)\ndls = DataBlock(\n    blocks=(TextBlock.from_folder(path),CategoryBlock),\n    get_y = parent_label,\n    get_items=partial(get_text_files, folders=['train', 'test']),\n    splitter=GrandparentSplitter(valid_name='test')\n).dataloaders(path)\n\n\nTransforms\n\nfiles = get_text_files(path, folders = ['train', 'test'])\ntxts = L(o.open().read() for o in files[:2000])\n\n\ntok = Tokenizer.from_folder(path)\ntok.setup(txts)\ntoks = txts.map(tok)\ntoks[0]\n\n\nnum = Numericalize()\nnum.setup(toks)\nnums = toks.map(num)\nnums[0][:10]\n\n\nnums_dec = num.decode(nums[0][:10]); nums_dec\n\n\ntok.decode(nums_dec)\n\n\ntok((txts[0], txts[1]))\n\n\n\nWriting Your Own Transform\n\ndef f(x:int): return x+1\ntfm = Transform(f)\ntfm(2),tfm(2.0)\n\n\n@Transform\ndef f(x:int): return x+1\nf(2),f(2.0)\n\n\nclass NormalizeMean(Transform):\n    def setups(self, items): self.mean = sum(items)/len(items)\n    def encodes(self, x): return x-self.mean\n    def decodes(self, x): return x+self.mean\n\n\ntfm = NormalizeMean()\ntfm.setup([1,2,3,4,5])\nstart = 2\ny = tfm(start)\nz = tfm.decode(y)\ntfm.mean,y,z\n\n\n\nPipeline\n\ntfms = Pipeline([tok, num])\nt = tfms(txts[0]); t[:20]\n\n\ntfms.decode(t)[:100]"
  },
  {
    "objectID": "Fastbook/clean/11_midlevel_data.html#tfmdlists-and-datasets-transformed-collections",
    "href": "Fastbook/clean/11_midlevel_data.html#tfmdlists-and-datasets-transformed-collections",
    "title": "Data Munging with fastai’s Mid-Level API",
    "section": "TfmdLists and Datasets: Transformed Collections",
    "text": "TfmdLists and Datasets: Transformed Collections\n\nTfmdLists\n\ntls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize])\n\n\nt = tls[0]; t[:20]\n\n\ntls.decode(t)[:100]\n\n\ntls.show(t)\n\n\ncut = int(len(files)*0.8)\nsplits = [list(range(cut)), list(range(cut,len(files)))]\ntls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize], \n                splits=splits)\n\n\ntls.valid[0][:20]\n\n\nlbls = files.map(parent_label)\nlbls\n\n\ncat = Categorize()\ncat.setup(lbls)\ncat.vocab, cat(lbls[0])\n\n\ntls_y = TfmdLists(files, [parent_label, Categorize()])\ntls_y[0]\n\n\n\nDatasets\n\nx_tfms = [Tokenizer.from_folder(path), Numericalize]\ny_tfms = [parent_label, Categorize()]\ndsets = Datasets(files, [x_tfms, y_tfms])\nx,y = dsets[0]\nx[:20],y\n\n\nx_tfms = [Tokenizer.from_folder(path), Numericalize]\ny_tfms = [parent_label, Categorize()]\ndsets = Datasets(files, [x_tfms, y_tfms], splits=splits)\nx,y = dsets.valid[0]\nx[:20],y\n\n\nt = dsets.valid[0]\ndsets.decode(t)\n\n\ndls = dsets.dataloaders(bs=64, before_batch=pad_input)\n\n\ntfms = [[Tokenizer.from_folder(path), Numericalize], [parent_label, Categorize]]\nfiles = get_text_files(path, folders = ['train', 'test'])\nsplits = GrandparentSplitter(valid_name='test')(files)\ndsets = Datasets(files, tfms, splits=splits)\ndls = dsets.dataloaders(dl_type=SortedDL, before_batch=pad_input)\n\n\npath = untar_data(URLs.IMDB)\ndls = DataBlock(\n    blocks=(TextBlock.from_folder(path),CategoryBlock),\n    get_y = parent_label,\n    get_items=partial(get_text_files, folders=['train', 'test']),\n    splitter=GrandparentSplitter(valid_name='test')\n).dataloaders(path)"
  },
  {
    "objectID": "Fastbook/clean/11_midlevel_data.html#applying-the-mid-level-data-api-siamesepair",
    "href": "Fastbook/clean/11_midlevel_data.html#applying-the-mid-level-data-api-siamesepair",
    "title": "Data Munging with fastai’s Mid-Level API",
    "section": "Applying the Mid-Level Data API: SiamesePair",
    "text": "Applying the Mid-Level Data API: SiamesePair\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\n\nclass SiameseImage(fastuple):\n    def show(self, ctx=None, **kwargs): \n        img1,img2,same_breed = self\n        if not isinstance(img1, Tensor):\n            if img2.size != img1.size: img2 = img2.resize(img1.size)\n            t1,t2 = tensor(img1),tensor(img2)\n            t1,t2 = t1.permute(2,0,1),t2.permute(2,0,1)\n        else: t1,t2 = img1,img2\n        line = t1.new_zeros(t1.shape[0], t1.shape[1], 10)\n        return show_image(torch.cat([t1,line,t2], dim=2), \n                          title=same_breed, ctx=ctx)\n\n\nimg = PILImage.create(files[0])\ns = SiameseImage(img, img, True)\ns.show();\n\n\nimg1 = PILImage.create(files[1])\ns1 = SiameseImage(img, img1, False)\ns1.show();\n\n\ns2 = Resize(224)(s1)\ns2.show();\n\n\ndef label_func(fname):\n    return re.match(r'^(.*)_\\d+.jpg$', fname.name).groups()[0]\n\n\nclass SiameseTransform(Transform):\n    def __init__(self, files, label_func, splits):\n        self.labels = files.map(label_func).unique()\n        self.lbl2files = {l: L(f for f in files if label_func(f) == l) \n                          for l in self.labels}\n        self.label_func = label_func\n        self.valid = {f: self._draw(f) for f in files[splits[1]]}\n        \n    def encodes(self, f):\n        f2,t = self.valid.get(f, self._draw(f))\n        img1,img2 = PILImage.create(f),PILImage.create(f2)\n        return SiameseImage(img1, img2, t)\n    \n    def _draw(self, f):\n        same = random.random() &lt; 0.5\n        cls = self.label_func(f)\n        if not same: \n            cls = random.choice(L(l for l in self.labels if l != cls)) \n        return random.choice(self.lbl2files[cls]),same\n\n\nsplits = RandomSplitter()(files)\ntfm = SiameseTransform(files, label_func, splits)\ntfm(files[0]).show();\n\n\ntls = TfmdLists(files, tfm, splits=splits)\nshow_at(tls.valid, 0);\n\n\ndls = tls.dataloaders(after_item=[Resize(224), ToTensor], \n    after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)])"
  },
  {
    "objectID": "Fastbook/clean/11_midlevel_data.html#conclusion",
    "href": "Fastbook/clean/11_midlevel_data.html#conclusion",
    "title": "Data Munging with fastai’s Mid-Level API",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "Fastbook/clean/11_midlevel_data.html#questionnaire",
    "href": "Fastbook/clean/11_midlevel_data.html#questionnaire",
    "title": "Data Munging with fastai’s Mid-Level API",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhy do we say that fastai has a “layered” API? What does it mean?\nWhy does a Transform have a decode method? What does it do?\nWhy does a Transform have a setup method? What does it do?\nHow does a Transform work when called on a tuple?\nWhich methods do you need to implement when writing your own Transform?\nWrite a Normalize transform that fully normalizes items (subtract the mean and divide by the standard deviation of the dataset), and that can decode that behavior. Try not to peek!\nWrite a Transform that does the numericalization of tokenized texts (it should set its vocab automatically from the dataset seen and have a decode method). Look at the source code of fastai if you need help.\nWhat is a Pipeline?\nWhat is a TfmdLists?\nWhat is a Datasets? How is it different from a TfmdLists?\nWhy are TfmdLists and Datasets named with an “s”?\nHow can you build a DataLoaders from a TfmdLists or a Datasets?\nHow do you pass item_tfms and batch_tfms when building a DataLoaders from a TfmdLists or a Datasets?\nWhat do you need to do when you want to have your custom items work with methods like show_batch or show_results?\nWhy can we easily apply fastai data augmentation transforms to the SiamesePair we built?\n\n\nFurther Research\n\nUse the mid-level API to prepare the data in DataLoaders on your own datasets. Try this with the Pet dataset and the Adult dataset from Chapter 1.\nLook at the Siamese tutorial in the fastai documentation to learn how to customize the behavior of show_batch and show_results for new type of items. Implement it in your own project."
  },
  {
    "objectID": "Fastbook/clean/11_midlevel_data.html#understanding-fastais-applications-wrap-up",
    "href": "Fastbook/clean/11_midlevel_data.html#understanding-fastais-applications-wrap-up",
    "title": "Data Munging with fastai’s Mid-Level API",
    "section": "Understanding fastai’s Applications: Wrap Up",
    "text": "Understanding fastai’s Applications: Wrap Up\nCongratulations—you’ve completed all of the chapters in this book that cover the key practical parts of training models and using deep learning! You know how to use all of fastai’s built-in applications, and how to customize them using the data block API and loss functions. You even know how to create a neural network from scratch, and train it! (And hopefully you now know some of the questions to ask to make sure your creations help improve society too.)\nThe knowledge you already have is enough to create full working prototypes of many types of neural network applications. More importantly, it will help you understand the capabilities and limitations of deep learning models, and how to design a system that’s well adapted to them.\nIn the rest of this book we will be pulling apart those applications, piece by piece, to understand the foundations they are built on. This is important knowledge for a deep learning practitioner, because it is what allows you to inspect and debug models that you build and create new applications that are customized for your particular projects."
  },
  {
    "objectID": "Fastbook/clean/12_nlp_dive.html",
    "href": "Fastbook/clean/12_nlp_dive.html",
    "title": "A Language Model from Scratch",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *"
  },
  {
    "objectID": "Fastbook/clean/12_nlp_dive.html#the-data",
    "href": "Fastbook/clean/12_nlp_dive.html#the-data",
    "title": "A Language Model from Scratch",
    "section": "The Data",
    "text": "The Data\n\nfrom fastai.text.all import *\npath = untar_data(URLs.HUMAN_NUMBERS)\n\n\n#hide\nPath.BASE_PATH = path\n\n\npath.ls()\n\n\nlines = L()\nwith open(path/'train.txt') as f: lines += L(*f.readlines())\nwith open(path/'valid.txt') as f: lines += L(*f.readlines())\nlines\n\n\ntext = ' . '.join([l.strip() for l in lines])\ntext[:100]\n\n\ntokens = text.split(' ')\ntokens[:10]\n\n\nvocab = L(*tokens).unique()\nvocab\n\n\nword2idx = {w:i for i,w in enumerate(vocab)}\nnums = L(word2idx[i] for i in tokens)\nnums"
  },
  {
    "objectID": "Fastbook/clean/12_nlp_dive.html#our-first-language-model-from-scratch",
    "href": "Fastbook/clean/12_nlp_dive.html#our-first-language-model-from-scratch",
    "title": "A Language Model from Scratch",
    "section": "Our First Language Model from Scratch",
    "text": "Our First Language Model from Scratch\n\nL((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3))\n\n\nseqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3))\nseqs\n\n\nbs = 64\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False)\n\n\nOur Language Model in PyTorch\n\nclass LMModel1(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        \n    def forward(self, x):\n        h = F.relu(self.h_h(self.i_h(x[:,0])))\n        h = h + self.i_h(x[:,1])\n        h = F.relu(self.h_h(h))\n        h = h + self.i_h(x[:,2])\n        h = F.relu(self.h_h(h))\n        return self.h_o(h)\n\n\nlearn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, \n                metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)\n\n\nn,counts = 0,torch.zeros(len(vocab))\nfor x,y in dls.valid:\n    n += y.shape[0]\n    for i in range_of(vocab): counts[i] += (y==i).long().sum()\nidx = torch.argmax(counts)\nidx, vocab[idx.item()], counts[idx].item()/n\n\n\n\nOur First Recurrent Neural Network\n\nclass LMModel2(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        \n    def forward(self, x):\n        h = 0\n        for i in range(3):\n            h = h + self.i_h(x[:,i])\n            h = F.relu(self.h_h(h))\n        return self.h_o(h)\n\n\nlearn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, \n                metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)"
  },
  {
    "objectID": "Fastbook/clean/12_nlp_dive.html#improving-the-rnn",
    "href": "Fastbook/clean/12_nlp_dive.html#improving-the-rnn",
    "title": "A Language Model from Scratch",
    "section": "Improving the RNN",
    "text": "Improving the RNN\n\nMaintaining the State of an RNN\n\nclass LMModel3(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        self.h = 0\n        \n    def forward(self, x):\n        for i in range(3):\n            self.h = self.h + self.i_h(x[:,i])\n            self.h = F.relu(self.h_h(self.h))\n        out = self.h_o(self.h)\n        self.h = self.h.detach()\n        return out\n    \n    def reset(self): self.h = 0\n\n\nm = len(seqs)//bs\nm,bs,len(seqs)\n\n\ndef group_chunks(ds, bs):\n    m = len(ds) // bs\n    new_ds = L()\n    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n    return new_ds\n\n\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(\n    group_chunks(seqs[:cut], bs), \n    group_chunks(seqs[cut:], bs), \n    bs=bs, drop_last=True, shuffle=False)\n\n\nlearn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy,\n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(10, 3e-3)\n\n\n\nCreating More Signal\n\nsl = 16\nseqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n         for i in range(0,len(nums)-sl-1,sl))\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n                             group_chunks(seqs[cut:], bs),\n                             bs=bs, drop_last=True, shuffle=False)\n\n\n[L(vocab[o] for o in s) for s in seqs[0]]\n\n\nclass LMModel4(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        self.h = 0\n        \n    def forward(self, x):\n        outs = []\n        for i in range(sl):\n            self.h = self.h + self.i_h(x[:,i])\n            self.h = F.relu(self.h_h(self.h))\n            outs.append(self.h_o(self.h))\n        self.h = self.h.detach()\n        return torch.stack(outs, dim=1)\n    \n    def reset(self): self.h = 0\n\n\ndef loss_func(inp, targ):\n    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))\n\n\nlearn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func,\n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)"
  },
  {
    "objectID": "Fastbook/clean/12_nlp_dive.html#multilayer-rnns",
    "href": "Fastbook/clean/12_nlp_dive.html#multilayer-rnns",
    "title": "A Language Model from Scratch",
    "section": "Multilayer RNNs",
    "text": "Multilayer RNNs\n\nThe Model\n\nclass LMModel5(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h = torch.zeros(n_layers, bs, n_hidden)\n        \n    def forward(self, x):\n        res,h = self.rnn(self.i_h(x), self.h)\n        self.h = h.detach()\n        return self.h_o(res)\n    \n    def reset(self): self.h.zero_()\n\n\nlearn = Learner(dls, LMModel5(len(vocab), 64, 2), \n                loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)\n\n\n\nExploding or Disappearing Activations"
  },
  {
    "objectID": "Fastbook/clean/12_nlp_dive.html#lstm",
    "href": "Fastbook/clean/12_nlp_dive.html#lstm",
    "title": "A Language Model from Scratch",
    "section": "LSTM",
    "text": "LSTM\n\nBuilding an LSTM from Scratch\n\nclass LSTMCell(Module):\n    def __init__(self, ni, nh):\n        self.forget_gate = nn.Linear(ni + nh, nh)\n        self.input_gate  = nn.Linear(ni + nh, nh)\n        self.cell_gate   = nn.Linear(ni + nh, nh)\n        self.output_gate = nn.Linear(ni + nh, nh)\n\n    def forward(self, input, state):\n        h,c = state\n        h = torch.cat([h, input], dim=1)\n        forget = torch.sigmoid(self.forget_gate(h))\n        c = c * forget\n        inp = torch.sigmoid(self.input_gate(h))\n        cell = torch.tanh(self.cell_gate(h))\n        c = c + inp * cell\n        out = torch.sigmoid(self.output_gate(h))\n        h = out * torch.tanh(c)\n        return h, (h,c)\n\n\nclass LSTMCell(Module):\n    def __init__(self, ni, nh):\n        self.ih = nn.Linear(ni,4*nh)\n        self.hh = nn.Linear(nh,4*nh)\n\n    def forward(self, input, state):\n        h,c = state\n        # One big multiplication for all the gates is better than 4 smaller ones\n        gates = (self.ih(input) + self.hh(h)).chunk(4, 1)\n        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n        cellgate = gates[3].tanh()\n\n        c = (forgetgate*c) + (ingate*cellgate)\n        h = outgate * c.tanh()\n        return h, (h,c)\n\n\nt = torch.arange(0,10); t\n\n\nt.chunk(2)\n\n\n\nTraining a Language Model Using LSTMs\n\nclass LMModel6(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n        \n    def forward(self, x):\n        res,h = self.rnn(self.i_h(x), self.h)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(res)\n    \n    def reset(self): \n        for h in self.h: h.zero_()\n\n\nlearn = Learner(dls, LMModel6(len(vocab), 64, 2), \n                loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 1e-2)"
  },
  {
    "objectID": "Fastbook/clean/12_nlp_dive.html#regularizing-an-lstm",
    "href": "Fastbook/clean/12_nlp_dive.html#regularizing-an-lstm",
    "title": "A Language Model from Scratch",
    "section": "Regularizing an LSTM",
    "text": "Regularizing an LSTM\n\nDropout\n\nclass Dropout(Module):\n    def __init__(self, p): self.p = p\n    def forward(self, x):\n        if not self.training: return x\n        mask = x.new(*x.shape).bernoulli_(1-p)\n        return x * mask.div_(1-p)\n\n\n\nActivation Regularization and Temporal Activation Regularization\n\n\nTraining a Weight-Tied Regularized LSTM\n\nclass LMModel7(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.drop = nn.Dropout(p)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h_o.weight = self.i_h.weight\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n        \n    def forward(self, x):\n        raw,h = self.rnn(self.i_h(x), self.h)\n        out = self.drop(raw)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(out),raw,out\n    \n    def reset(self): \n        for h in self.h: h.zero_()\n\n\nlearn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5),\n                loss_func=CrossEntropyLossFlat(), metrics=accuracy,\n                cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)])\n\n\nlearn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),\n                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n\n\nlearn.fit_one_cycle(15, 1e-2, wd=0.1)"
  },
  {
    "objectID": "Fastbook/clean/12_nlp_dive.html#conclusion",
    "href": "Fastbook/clean/12_nlp_dive.html#conclusion",
    "title": "A Language Model from Scratch",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "Fastbook/clean/12_nlp_dive.html#questionnaire",
    "href": "Fastbook/clean/12_nlp_dive.html#questionnaire",
    "title": "A Language Model from Scratch",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nIf the dataset for your project is so big and complicated that working with it takes a significant amount of time, what should you do?\nWhy do we concatenate the documents in our dataset before creating a language model?\nTo use a standard fully connected network to predict the fourth word given the previous three words, what two tweaks do we need to make to our model?\nHow can we share a weight matrix across multiple layers in PyTorch?\nWrite a module that predicts the third word given the previous two words of a sentence, without peeking.\nWhat is a recurrent neural network?\nWhat is “hidden state”?\nWhat is the equivalent of hidden state in LMModel1?\nTo maintain the state in an RNN, why is it important to pass the text to the model in order?\nWhat is an “unrolled” representation of an RNN?\nWhy can maintaining the hidden state in an RNN lead to memory and performance problems? How do we fix this problem?\nWhat is “BPTT”?\nWrite code to print out the first few batches of the validation set, including converting the token IDs back into English strings, as we showed for batches of IMDb data in &lt;&gt;.\nWhat does the ModelResetter callback do? Why do we need it?\nWhat are the downsides of predicting just one output word for each three input words?\nWhy do we need a custom loss function for LMModel4?\nWhy is the training of LMModel4 unstable?\nIn the unrolled representation, we can see that a recurrent neural network actually has many layers. So why do we need to stack RNNs to get better results?\nDraw a representation of a stacked (multilayer) RNN.\nWhy should we get better results in an RNN if we call detach less often? Why might this not happen in practice with a simple RNN?\nWhy can a deep network result in very large or very small activations? Why does this matter?\nIn a computer’s floating-point representation of numbers, which numbers are the most precise?\nWhy do vanishing gradients prevent training?\nWhy does it help to have two hidden states in the LSTM architecture? What is the purpose of each one?\nWhat are these two states called in an LSTM?\nWhat is tanh, and how is it related to sigmoid?\nWhat is the purpose of this code in LSTMCell: h = torch.cat([h, input], dim=1)\nWhat does chunk do in PyTorch?\nStudy the refactored version of LSTMCell carefully to ensure you understand how and why it does the same thing as the non-refactored version.\nWhy can we use a higher learning rate for LMModel6?\nWhat are the three regularization techniques used in an AWD-LSTM model?\nWhat is “dropout”?\nWhy do we scale the acitvations with dropout? Is this applied during training, inference, or both?\nWhat is the purpose of this line from Dropout: if not self.training: return x\nExperiment with bernoulli_ to understand how it works.\nHow do you set your model in training mode in PyTorch? In evaluation mode?\nWrite the equation for activation regularization (in math or code, as you prefer). How is it different from weight decay?\nWrite the equation for temporal activation regularization (in math or code, as you prefer). Why wouldn’t we use this for computer vision problems?\nWhat is “weight tying” in a language model?\n\n\nFurther Research\n\nIn LMModel2, why can forward start with h=0? Why don’t we need to say h=torch.zeros(...)?\nWrite the code for an LSTM from scratch (you may refer to &lt;&gt;).\nSearch the internet for the GRU architecture and implement it from scratch, and try training a model. See if you can get results similar to those we saw in this chapter. Compare your results to the results of PyTorch’s built in GRU module.\nTake a look at the source code for AWD-LSTM in fastai, and try to map each of the lines of code to the concepts shown in this chapter."
  },
  {
    "objectID": "Fastbook/clean/19_learner.html",
    "href": "Fastbook/clean/19_learner.html",
    "title": "A fastai Learner from Scratch",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *"
  },
  {
    "objectID": "Fastbook/clean/19_learner.html#data",
    "href": "Fastbook/clean/19_learner.html#data",
    "title": "A fastai Learner from Scratch",
    "section": "Data",
    "text": "Data\n\npath = untar_data(URLs.IMAGENETTE_160)\n\n\nt = get_image_files(path)\nt[0]\n\n\nfrom glob import glob\nfiles = L(glob(f'{path}/**/*.JPEG', recursive=True)).map(Path)\nfiles[0]\n\n\nim = Image.open(files[0])\nim\n\n\nim_t = tensor(im)\nim_t.shape\n\n\nlbls = files.map(Self.parent.name()).unique(); lbls\n\n\nv2i = lbls.val2idx(); v2i\n\n\nDataset\n\nclass Dataset:\n    def __init__(self, fns): self.fns=fns\n    def __len__(self): return len(self.fns)\n    def __getitem__(self, i):\n        im = Image.open(self.fns[i]).resize((64,64)).convert('RGB')\n        y = v2i[self.fns[i].parent.name]\n        return tensor(im).float()/255, tensor(y)\n\n\ntrain_filt = L(o.parent.parent.name=='train' for o in files)\ntrain,valid = files[train_filt],files[~train_filt]\nlen(train),len(valid)\n\n\ntrain_ds,valid_ds = Dataset(train),Dataset(valid)\nx,y = train_ds[0]\nx.shape,y\n\n\nshow_image(x, title=lbls[y]);\n\n\ndef collate(idxs, ds): \n    xb,yb = zip(*[ds[i] for i in idxs])\n    return torch.stack(xb),torch.stack(yb)\n\n\nx,y = collate([1,2], train_ds)\nx.shape,y\n\n\nclass DataLoader:\n    def __init__(self, ds, bs=128, shuffle=False, n_workers=1):\n        self.ds,self.bs,self.shuffle,self.n_workers = ds,bs,shuffle,n_workers\n\n    def __len__(self): return (len(self.ds)-1)//self.bs+1\n\n    def __iter__(self):\n        idxs = L.range(self.ds)\n        if self.shuffle: idxs = idxs.shuffle()\n        chunks = [idxs[n:n+self.bs] for n in range(0, len(self.ds), self.bs)]\n        with ProcessPoolExecutor(self.n_workers) as ex:\n            yield from ex.map(collate, chunks, ds=self.ds)\n\n\nn_workers = min(16, defaults.cpus)\ntrain_dl = DataLoader(train_ds, bs=128, shuffle=True, n_workers=n_workers)\nvalid_dl = DataLoader(valid_ds, bs=256, shuffle=False, n_workers=n_workers)\nxb,yb = first(train_dl)\nxb.shape,yb.shape,len(train_dl)\n\n\nstats = [xb.mean((0,1,2)),xb.std((0,1,2))]\nstats\n\n\nclass Normalize:\n    def __init__(self, stats): self.stats=stats\n    def __call__(self, x):\n        if x.device != self.stats[0].device:\n            self.stats = to_device(self.stats, x.device)\n        return (x-self.stats[0])/self.stats[1]\n\n\nnorm = Normalize(stats)\ndef tfm_x(x): return norm(x).permute((0,3,1,2))\n\n\nt = tfm_x(x)\nt.mean((0,2,3)),t.std((0,2,3))"
  },
  {
    "objectID": "Fastbook/clean/19_learner.html#module-and-parameter",
    "href": "Fastbook/clean/19_learner.html#module-and-parameter",
    "title": "A fastai Learner from Scratch",
    "section": "Module and Parameter",
    "text": "Module and Parameter\n\nclass Parameter(Tensor):\n    def __new__(self, x): return Tensor._make_subclass(Parameter, x, True)\n    def __init__(self, *args, **kwargs): self.requires_grad_()\n\n\nParameter(tensor(3.))\n\n\nclass Module:\n    def __init__(self):\n        self.hook,self.params,self.children,self._training = None,[],[],False\n        \n    def register_parameters(self, *ps): self.params += ps\n    def register_modules   (self, *ms): self.children += ms\n        \n    @property\n    def training(self): return self._training\n    @training.setter\n    def training(self,v):\n        self._training = v\n        for m in self.children: m.training=v\n            \n    def parameters(self):\n        return self.params + sum([m.parameters() for m in self.children], [])\n\n    def __setattr__(self,k,v):\n        super().__setattr__(k,v)\n        if isinstance(v,Parameter): self.register_parameters(v)\n        if isinstance(v,Module):    self.register_modules(v)\n        \n    def __call__(self, *args, **kwargs):\n        res = self.forward(*args, **kwargs)\n        if self.hook is not None: self.hook(res, args)\n        return res\n    \n    def cuda(self):\n        for p in self.parameters(): p.data = p.data.cuda()\n\n\nclass ConvLayer(Module):\n    def __init__(self, ni, nf, stride=1, bias=True, act=True):\n        super().__init__()\n        self.w = Parameter(torch.zeros(nf,ni,3,3))\n        self.b = Parameter(torch.zeros(nf)) if bias else None\n        self.act,self.stride = act,stride\n        init = nn.init.kaiming_normal_ if act else nn.init.xavier_normal_\n        init(self.w)\n    \n    def forward(self, x):\n        x = F.conv2d(x, self.w, self.b, stride=self.stride, padding=1)\n        if self.act: x = F.relu(x)\n        return x\n\n\nl = ConvLayer(3, 4)\nlen(l.parameters())\n\n\nxbt = tfm_x(xb)\nr = l(xbt)\nr.shape\n\n\nclass Linear(Module):\n    def __init__(self, ni, nf):\n        super().__init__()\n        self.w = Parameter(torch.zeros(nf,ni))\n        self.b = Parameter(torch.zeros(nf))\n        nn.init.xavier_normal_(self.w)\n    \n    def forward(self, x): return x@self.w.t() + self.b\n\n\nl = Linear(4,2)\nr = l(torch.ones(3,4))\nr.shape\n\n\nclass T(Module):\n    def __init__(self):\n        super().__init__()\n        self.c,self.l = ConvLayer(3,4),Linear(4,2)\n\n\nt = T()\nlen(t.parameters())\n\n\nt.cuda()\nt.l.w.device\n\n\nSimple CNN\n\nclass Sequential(Module):\n    def __init__(self, *layers):\n        super().__init__()\n        self.layers = layers\n        self.register_modules(*layers)\n\n    def forward(self, x):\n        for l in self.layers: x = l(x)\n        return x\n\n\nclass AdaptivePool(Module):\n    def forward(self, x): return x.mean((2,3))\n\n\ndef simple_cnn():\n    return Sequential(\n        ConvLayer(3 ,16 ,stride=2), #32\n        ConvLayer(16,32 ,stride=2), #16\n        ConvLayer(32,64 ,stride=2), # 8\n        ConvLayer(64,128,stride=2), # 4\n        AdaptivePool(),\n        Linear(128, 10)\n    )\n\n\nm = simple_cnn()\nlen(m.parameters())\n\n\ndef print_stats(outp, inp): print (outp.mean().item(),outp.std().item())\nfor i in range(4): m.layers[i].hook = print_stats\n\nr = m(xbt)\nr.shape"
  },
  {
    "objectID": "Fastbook/clean/19_learner.html#loss",
    "href": "Fastbook/clean/19_learner.html#loss",
    "title": "A fastai Learner from Scratch",
    "section": "Loss",
    "text": "Loss\n\ndef nll(input, target): return -input[range(target.shape[0]), target].mean()\n\n\ndef log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\n\nsm = log_softmax(r); sm[0][0]\n\n\nloss = nll(sm, yb)\nloss\n\n\ndef log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()\nsm = log_softmax(r); sm[0][0]\n\n\nx = torch.rand(5)\na = x.max()\nx.exp().sum().log() == a + (x-a).exp().sum().log()\n\n\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\n\nlogsumexp(r)[0]\n\n\ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n\n\nsm = log_softmax(r); sm[0][0]\n\n\ndef cross_entropy(preds, yb): return nll(log_softmax(preds), yb).mean()"
  },
  {
    "objectID": "Fastbook/clean/19_learner.html#learner",
    "href": "Fastbook/clean/19_learner.html#learner",
    "title": "A fastai Learner from Scratch",
    "section": "Learner",
    "text": "Learner\n\nclass SGD:\n    def __init__(self, params, lr, wd=0.): store_attr()\n    def step(self):\n        for p in self.params:\n            p.data -= (p.grad.data + p.data*self.wd) * self.lr\n            p.grad.data.zero_()\n\n\nclass DataLoaders:\n    def __init__(self, *dls): self.train,self.valid = dls\n\ndls = DataLoaders(train_dl,valid_dl)\n\n\nclass Learner:\n    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=SGD):\n        store_attr()\n        for cb in cbs: cb.learner = self\n\n    def one_batch(self):\n        self('before_batch')\n        xb,yb = self.batch\n        self.preds = self.model(xb)\n        self.loss = self.loss_func(self.preds, yb)\n        if self.model.training:\n            self.loss.backward()\n            self.opt.step()\n        self('after_batch')\n\n    def one_epoch(self, train):\n        self.model.training = train\n        self('before_epoch')\n        dl = self.dls.train if train else self.dls.valid\n        for self.num,self.batch in enumerate(progress_bar(dl, leave=False)):\n            self.one_batch()\n        self('after_epoch')\n    \n    def fit(self, n_epochs):\n        self('before_fit')\n        self.opt = self.opt_func(self.model.parameters(), self.lr)\n        self.n_epochs = n_epochs\n        try:\n            for self.epoch in range(n_epochs):\n                self.one_epoch(True)\n                self.one_epoch(False)\n        except CancelFitException: pass\n        self('after_fit')\n        \n    def __call__(self,name):\n        for cb in self.cbs: getattr(cb,name,noop)()\n\n\nCallbacks\n\nclass Callback(GetAttr): _default='learner'\n\n\nclass SetupLearnerCB(Callback):\n    def before_batch(self):\n        xb,yb = to_device(self.batch)\n        self.learner.batch = tfm_x(xb),yb\n\n    def before_fit(self): self.model.cuda()\n\n\nclass TrackResults(Callback):\n    def before_epoch(self): self.accs,self.losses,self.ns = [],[],[]\n        \n    def after_epoch(self):\n        n = sum(self.ns)\n        print(self.epoch, self.model.training,\n              sum(self.losses).item()/n, sum(self.accs).item()/n)\n        \n    def after_batch(self):\n        xb,yb = self.batch\n        acc = (self.preds.argmax(dim=1)==yb).float().sum()\n        self.accs.append(acc)\n        n = len(xb)\n        self.losses.append(self.loss*n)\n        self.ns.append(n)\n\n\ncbs = [SetupLearnerCB(),TrackResults()]\nlearn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs)\nlearn.fit(1)\n\n\n\nScheduling the Learning Rate\n\nclass LRFinder(Callback):\n    def before_fit(self):\n        self.losses,self.lrs = [],[]\n        self.learner.lr = 1e-6\n        \n    def before_batch(self):\n        if not self.model.training: return\n        self.opt.lr *= 1.2\n\n    def after_batch(self):\n        if not self.model.training: return\n        if self.opt.lr&gt;10 or torch.isnan(self.loss): raise CancelFitException\n        self.losses.append(self.loss.item())\n        self.lrs.append(self.opt.lr)\n\n\nlrfind = LRFinder()\nlearn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs+[lrfind])\nlearn.fit(2)\n\n\nplt.plot(lrfind.lrs[:-2],lrfind.losses[:-2])\nplt.xscale('log')\n\n\nclass OneCycle(Callback):\n    def __init__(self, base_lr): self.base_lr = base_lr\n    def before_fit(self): self.lrs = []\n\n    def before_batch(self):\n        if not self.model.training: return\n        n = len(self.dls.train)\n        bn = self.epoch*n + self.num\n        mn = self.n_epochs*n\n        pct = bn/mn\n        pct_start,div_start = 0.25,10\n        if pct&lt;pct_start:\n            pct /= pct_start\n            lr = (1-pct)*self.base_lr/div_start + pct*self.base_lr\n        else:\n            pct = (pct-pct_start)/(1-pct_start)\n            lr = (1-pct)*self.base_lr\n        self.opt.lr = lr\n        self.lrs.append(lr)\n\n\nonecyc = OneCycle(0.1)\nlearn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs+[onecyc])\n\n\nlearn.fit(8)\n\n\nplt.plot(onecyc.lrs);"
  },
  {
    "objectID": "Fastbook/clean/19_learner.html#conclusion",
    "href": "Fastbook/clean/19_learner.html#conclusion",
    "title": "A fastai Learner from Scratch",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "Fastbook/clean/19_learner.html#questionnaire",
    "href": "Fastbook/clean/19_learner.html#questionnaire",
    "title": "A fastai Learner from Scratch",
    "section": "Questionnaire",
    "text": "Questionnaire\n\ntip: Experiments: For the questions here that ask you to explain what some function or class is, you should also complete your own code experiments.\n\n\nWhat is glob?\nHow do you open an image with the Python imaging library?\nWhat does L.map do?\nWhat does Self do?\nWhat is L.val2idx?\nWhat methods do you need to implement to create your own Dataset?\nWhy do we call convert when we open an image from Imagenette?\nWhat does ~ do? How is it useful for splitting training and validation sets?\nDoes ~ work with the L or Tensor classes? What about NumPy arrays, Python lists, or pandas DataFrames?\nWhat is ProcessPoolExecutor?\nHow does L.range(self.ds) work?\nWhat is __iter__?\nWhat is first?\nWhat is permute? Why is it needed?\nWhat is a recursive function? How does it help us define the parameters method?\nWrite a recursive function that returns the first 20 items of the Fibonacci sequence.\nWhat is super?\nWhy do subclasses of Module need to override forward instead of defining __call__?\nIn ConvLayer, why does init depend on act?\nWhy does Sequential need to call register_modules?\nWrite a hook that prints the shape of every layer’s activations.\nWhat is “LogSumExp”?\nWhy is log_softmax useful?\nWhat is GetAttr? How is it helpful for callbacks?\nReimplement one of the callbacks in this chapter without inheriting from Callback or GetAttr.\nWhat does Learner.__call__ do?\nWhat is getattr? (Note the case difference to GetAttr!)\nWhy is there a try block in fit?\nWhy do we check for model.training in one_batch?\nWhat is store_attr?\nWhat is the purpose of TrackResults.before_epoch?\nWhat does model.cuda do? How does it work?\nWhy do we need to check model.training in LRFinder and OneCycle?\nUse cosine annealing in OneCycle.\n\n\nFurther Research\n\nWrite resnet18 from scratch (refer to &lt;&gt; as needed), and train it with the Learner in this chapter.\nImplement a batchnorm layer from scratch and use it in your resnet18.\nWrite a Mixup callback for use in this chapter.\nAdd momentum to SGD.\nPick a few features that you’re interested in from fastai (or any other library) and implement them in this chapter.\nPick a research paper that’s not yet implemented in fastai or PyTorch and implement it in this chapter.\n\n\nPort it over to fastai.\nSubmit a pull request to fastai, or create your own extension module and release it.\nHint: you may find it helpful to use nbdev to create and deploy your package."
  },
  {
    "objectID": "Fastbook/clean/app_jupyter.html",
    "href": "Fastbook/clean/app_jupyter.html",
    "title": "Appendix: Jupyter Notebook 101",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\nfrom fastbook import *"
  },
  {
    "objectID": "Fastbook/clean/app_jupyter.html#introduction",
    "href": "Fastbook/clean/app_jupyter.html#introduction",
    "title": "Appendix: Jupyter Notebook 101",
    "section": "Introduction",
    "text": "Introduction\n\n1+1"
  },
  {
    "objectID": "Fastbook/clean/app_jupyter.html#writing",
    "href": "Fastbook/clean/app_jupyter.html#writing",
    "title": "Appendix: Jupyter Notebook 101",
    "section": "Writing",
    "text": "Writing\n\n3/2"
  },
  {
    "objectID": "Fastbook/clean/app_jupyter.html#modes",
    "href": "Fastbook/clean/app_jupyter.html#modes",
    "title": "Appendix: Jupyter Notebook 101",
    "section": "Modes",
    "text": "Modes"
  },
  {
    "objectID": "Fastbook/clean/app_jupyter.html#other-important-considerations",
    "href": "Fastbook/clean/app_jupyter.html#other-important-considerations",
    "title": "Appendix: Jupyter Notebook 101",
    "section": "Other Important Considerations",
    "text": "Other Important Considerations"
  },
  {
    "objectID": "Fastbook/clean/app_jupyter.html#markdown-formatting",
    "href": "Fastbook/clean/app_jupyter.html#markdown-formatting",
    "title": "Appendix: Jupyter Notebook 101",
    "section": "Markdown Formatting",
    "text": "Markdown Formatting\n\nItalics, Bold, Strikethrough, Inline, Blockquotes and Links\n\n\nHeadings\n\n\nLists"
  },
  {
    "objectID": "Fastbook/clean/app_jupyter.html#code-capabilities",
    "href": "Fastbook/clean/app_jupyter.html#code-capabilities",
    "title": "Appendix: Jupyter Notebook 101",
    "section": "Code Capabilities",
    "text": "Code Capabilities\n\n# Import necessary libraries\nfrom fastai.vision.all import * \nimport matplotlib.pyplot as plt\n\n\nfrom PIL import Image\n\n\na = 1\nb = a + 1\nc = b + a + 1\nd = c + b + a + 1\na, b, c ,d\n\n\nplt.plot([a,b,c,d])\nplt.show()\n\n\nImage.open(image_cat())"
  },
  {
    "objectID": "Fastbook/clean/app_jupyter.html#running-the-app-locally",
    "href": "Fastbook/clean/app_jupyter.html#running-the-app-locally",
    "title": "Appendix: Jupyter Notebook 101",
    "section": "Running the App Locally",
    "text": "Running the App Locally"
  },
  {
    "objectID": "Fastbook/clean/app_jupyter.html#creating-a-notebook",
    "href": "Fastbook/clean/app_jupyter.html#creating-a-notebook",
    "title": "Appendix: Jupyter Notebook 101",
    "section": "Creating a Notebook",
    "text": "Creating a Notebook"
  },
  {
    "objectID": "Fastbook/clean/app_jupyter.html#shortcuts-and-tricks",
    "href": "Fastbook/clean/app_jupyter.html#shortcuts-and-tricks",
    "title": "Appendix: Jupyter Notebook 101",
    "section": "Shortcuts and Tricks",
    "text": "Shortcuts and Tricks\n\nCommand Mode Shortcuts\n\n\nCell Tricks\n\n\nLine Magics\n\n%timeit [i+1 for i in range(1000)]"
  },
  {
    "objectID": "Fastbook/clean/16_accel_sgd.html",
    "href": "Fastbook/clean/16_accel_sgd.html",
    "title": "The Training Process",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *"
  },
  {
    "objectID": "Fastbook/clean/16_accel_sgd.html#establishing-a-baseline",
    "href": "Fastbook/clean/16_accel_sgd.html#establishing-a-baseline",
    "title": "The Training Process",
    "section": "Establishing a Baseline",
    "text": "Establishing a Baseline\n\ndef get_data(url, presize, resize):\n    path = untar_data(url)\n    return DataBlock(\n        blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, \n        splitter=GrandparentSplitter(valid_name='val'),\n        get_y=parent_label, item_tfms=Resize(presize),\n        batch_tfms=[*aug_transforms(min_scale=0.5, size=resize),\n                    Normalize.from_stats(*imagenet_stats)],\n    ).dataloaders(path, bs=128)\n\n\ndls = get_data(URLs.IMAGENETTE_160, 160, 128)\n\n\ndef get_learner(**kwargs):\n    return vision_learner(dls, resnet34, pretrained=False,\n                    metrics=accuracy, **kwargs).to_fp16()\n\n\nlearn = get_learner()\nlearn.fit_one_cycle(3, 0.003)\n\n\nlearn = get_learner(opt_func=SGD)\n\n\nlearn.lr_find()\n\n\nlearn.fit_one_cycle(3, 0.03, moms=(0,0,0))"
  },
  {
    "objectID": "Fastbook/clean/16_accel_sgd.html#a-generic-optimizer",
    "href": "Fastbook/clean/16_accel_sgd.html#a-generic-optimizer",
    "title": "The Training Process",
    "section": "A Generic Optimizer",
    "text": "A Generic Optimizer\n\ndef sgd_cb(p, lr, **kwargs): p.data.add_(-lr, p.grad.data)\n\n\nopt_func = partial(Optimizer, cbs=[sgd_cb])\n\n\nlearn = get_learner(opt_func=opt_func)\nlearn.fit(3, 0.03)"
  },
  {
    "objectID": "Fastbook/clean/16_accel_sgd.html#momentum",
    "href": "Fastbook/clean/16_accel_sgd.html#momentum",
    "title": "The Training Process",
    "section": "Momentum",
    "text": "Momentum\n\nx = np.linspace(-4, 4, 100)\ny = 1 - (x/3) ** 2\nx1 = x + np.random.randn(100) * 0.1\ny1 = y + np.random.randn(100) * 0.1\nplt.scatter(x1,y1)\nidx = x1.argsort()\nbeta,avg,res = 0.7,0,[]\nfor i in idx:\n    avg = beta * avg + (1-beta) * y1[i]\n    res.append(avg/(1-beta**(i+1)))\nplt.plot(x1[idx],np.array(res), color='red');\n\n\nx = np.linspace(-4, 4, 100)\ny = 1 - (x/3) ** 2\nx1 = x + np.random.randn(100) * 0.1\ny1 = y + np.random.randn(100) * 0.1\n_,axs = plt.subplots(2,2, figsize=(12,8))\nbetas = [0.5,0.7,0.9,0.99]\nidx = x1.argsort()\nfor beta,ax in zip(betas, axs.flatten()):\n    ax.scatter(x1,y1)\n    avg,res = 0,[]\n    for i in idx:\n        avg = beta * avg + (1-beta) * y1[i]\n        res.append(avg)#/(1-beta**(i+1)))\n    ax.plot(x1[idx],np.array(res), color='red');\n    ax.set_title(f'beta={beta}')\n\n\ndef average_grad(p, mom, grad_avg=None, **kwargs):\n    if grad_avg is None: grad_avg = torch.zeros_like(p.grad.data)\n    return {'grad_avg': grad_avg*mom + p.grad.data}\n\n\ndef momentum_step(p, lr, grad_avg, **kwargs): p.data.add_(-lr, grad_avg)\n\n\nopt_func = partial(Optimizer, cbs=[average_grad,momentum_step], mom=0.9)\n\n\nlearn = get_learner(opt_func=opt_func)\nlearn.fit_one_cycle(3, 0.03)\n\n\nlearn.recorder.plot_sched()"
  },
  {
    "objectID": "Fastbook/clean/16_accel_sgd.html#rmsprop",
    "href": "Fastbook/clean/16_accel_sgd.html#rmsprop",
    "title": "The Training Process",
    "section": "RMSProp",
    "text": "RMSProp\n\ndef average_sqr_grad(p, sqr_mom, sqr_avg=None, **kwargs):\n    if sqr_avg is None: sqr_avg = torch.zeros_like(p.grad.data)\n    return {'sqr_avg': sqr_mom*sqr_avg + (1-sqr_mom)*p.grad.data**2}\n\n\ndef rms_prop_step(p, lr, sqr_avg, eps, grad_avg=None, **kwargs):\n    denom = sqr_avg.sqrt().add_(eps)\n    p.data.addcdiv_(-lr, p.grad, denom)\n\nopt_func = partial(Optimizer, cbs=[average_sqr_grad,rms_prop_step],\n                   sqr_mom=0.99, eps=1e-7)\n\n\nlearn = get_learner(opt_func=opt_func)\nlearn.fit_one_cycle(3, 0.003)"
  },
  {
    "objectID": "Fastbook/clean/16_accel_sgd.html#adam",
    "href": "Fastbook/clean/16_accel_sgd.html#adam",
    "title": "The Training Process",
    "section": "Adam",
    "text": "Adam"
  },
  {
    "objectID": "Fastbook/clean/16_accel_sgd.html#decoupled-weight-decay",
    "href": "Fastbook/clean/16_accel_sgd.html#decoupled-weight-decay",
    "title": "The Training Process",
    "section": "Decoupled Weight Decay",
    "text": "Decoupled Weight Decay"
  },
  {
    "objectID": "Fastbook/clean/16_accel_sgd.html#callbacks",
    "href": "Fastbook/clean/16_accel_sgd.html#callbacks",
    "title": "The Training Process",
    "section": "Callbacks",
    "text": "Callbacks\n\nCreating a Callback\n\nclass ModelResetter(Callback):\n    def before_train(self):    self.model.reset()\n    def before_validate(self): self.model.reset()\n\n\nclass RNNRegularizer(Callback):\n    def __init__(self, alpha=0., beta=0.): self.alpha,self.beta = alpha,beta\n\n    def after_pred(self):\n        self.raw_out,self.out = self.pred[1],self.pred[2]\n        self.learn.pred = self.pred[0]\n\n    def after_loss(self):\n        if not self.training: return\n        if self.alpha != 0.:\n            self.learn.loss += self.alpha * self.out[-1].float().pow(2).mean()\n        if self.beta != 0.:\n            h = self.raw_out[-1]\n            if len(h)&gt;1:\n                self.learn.loss += self.beta * (h[:,1:] - h[:,:-1]\n                                               ).float().pow(2).mean()\n\n\n\nCallback Ordering and Exceptions\n\nclass TerminateOnNaNCallback(Callback):\n    run_before=Recorder\n    def after_batch(self):\n        if torch.isinf(self.loss) or torch.isnan(self.loss):\n            raise CancelFitException"
  },
  {
    "objectID": "Fastbook/clean/16_accel_sgd.html#conclusion",
    "href": "Fastbook/clean/16_accel_sgd.html#conclusion",
    "title": "The Training Process",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "Fastbook/clean/16_accel_sgd.html#questionnaire",
    "href": "Fastbook/clean/16_accel_sgd.html#questionnaire",
    "title": "The Training Process",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhat is the equation for a step of SGD, in math or code (as you prefer)?\nWhat do we pass to vision_learner to use a non-default optimizer?\nWhat are optimizer callbacks?\nWhat does zero_grad do in an optimizer?\nWhat does step do in an optimizer? How is it implemented in the general optimizer?\nRewrite sgd_cb to use the += operator, instead of add_.\nWhat is “momentum”? Write out the equation.\nWhat’s a physical analogy for momentum? How does it apply in our model training settings?\nWhat does a bigger value for momentum do to the gradients?\nWhat are the default values of momentum for 1cycle training?\nWhat is RMSProp? Write out the equation.\nWhat do the squared values of the gradients indicate?\nHow does Adam differ from momentum and RMSProp?\nWrite out the equation for Adam.\nCalculate the values of unbias_avg and w.avg for a few batches of dummy values.\nWhat’s the impact of having a high eps in Adam?\nRead through the optimizer notebook in fastai’s repo, and execute it.\nIn what situations do dynamic learning rate methods like Adam change the behavior of weight decay?\nWhat are the four steps of a training loop?\nWhy is using callbacks better than writing a new training loop for each tweak you want to add?\nWhat aspects of the design of fastai’s callback system make it as flexible as copying and pasting bits of code?\nHow can you get the list of events available to you when writing a callback?\nWrite the ModelResetter callback (without peeking).\nHow can you access the necessary attributes of the training loop inside a callback? When can you use or not use the shortcuts that go with them?\nHow can a callback influence the control flow of the training loop?\nWrite the TerminateOnNaN callback (without peeking, if possible).\nHow do you make sure your callback runs after or before another callback?\n\n\nFurther Research\n\nLook up the “Rectified Adam” paper, implement it using the general optimizer framework, and try it out. Search for other recent optimizers that work well in practice, and pick one to implement.\nLook at the mixed-precision callback with the documentation. Try to understand what each event and line of code does.\nImplement your own version of the learning rate finder from scratch. Compare it with fastai’s version.\nLook at the source code of the callbacks that ship with fastai. See if you can find one that’s similar to what you’re looking to do, to get some inspiration."
  },
  {
    "objectID": "Fastbook/clean/16_accel_sgd.html#foundations-of-deep-learning-wrap-up",
    "href": "Fastbook/clean/16_accel_sgd.html#foundations-of-deep-learning-wrap-up",
    "title": "The Training Process",
    "section": "Foundations of Deep Learning: Wrap up",
    "text": "Foundations of Deep Learning: Wrap up\nCongratulations, you have made it to the end of the “foundations of deep learning” section of the book! You now understand how all of fastai’s applications and most important architectures are built, and the recommended ways to train them—and you have all the information you need to build these from scratch. While you probably won’t need to create your own training loop, or batchnorm layer, for instance, knowing what is going on behind the scenes is very helpful for debugging, profiling, and deploying your solutions.\nSince you understand the foundations of fastai’s applications now, be sure to spend some time digging through the source notebooks and running and experimenting with parts of them. This will give you a better idea of how everything in fastai is developed.\nIn the next section, we will be looking even further under the covers: we’ll explore how the actual forward and backward passes of a neural network are done, and we will see what tools are at our disposal to get better performance. We will then continue with a project that brings together all the material in the book, which we will use to build a tool for interpreting convolutional neural networks. Last but not least, we’ll finish by building fastai’s Learner class from scratch."
  },
  {
    "objectID": "Fastbook/clean/14_resnet.html",
    "href": "Fastbook/clean/14_resnet.html",
    "title": "ResNets",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *"
  },
  {
    "objectID": "Fastbook/clean/14_resnet.html#going-back-to-imagenette",
    "href": "Fastbook/clean/14_resnet.html#going-back-to-imagenette",
    "title": "ResNets",
    "section": "Going Back to Imagenette",
    "text": "Going Back to Imagenette\n\ndef get_data(url, presize, resize):\n    path = untar_data(url)\n    return DataBlock(\n        blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, \n        splitter=GrandparentSplitter(valid_name='val'),\n        get_y=parent_label, item_tfms=Resize(presize),\n        batch_tfms=[*aug_transforms(min_scale=0.5, size=resize),\n                    Normalize.from_stats(*imagenet_stats)],\n    ).dataloaders(path, bs=128)\n\n\ndls = get_data(URLs.IMAGENETTE_160, 160, 128)\n\n\ndls.show_batch(max_n=4)\n\n\ndef avg_pool(x): return x.mean((2,3))\n\n\ndef block(ni, nf): return ConvLayer(ni, nf, stride=2)\ndef get_model():\n    return nn.Sequential(\n        block(3, 16),\n        block(16, 32),\n        block(32, 64),\n        block(64, 128),\n        block(128, 256),\n        nn.AdaptiveAvgPool2d(1),\n        Flatten(),\n        nn.Linear(256, dls.c))\n\n\ndef get_learner(m):\n    return Learner(dls, m, loss_func=nn.CrossEntropyLoss(), metrics=accuracy\n                  ).to_fp16()\n\nlearn = get_learner(get_model())\n\n\nlearn.lr_find()\n\n\nlearn.fit_one_cycle(5, 3e-3)"
  },
  {
    "objectID": "Fastbook/clean/14_resnet.html#building-a-modern-cnn-resnet",
    "href": "Fastbook/clean/14_resnet.html#building-a-modern-cnn-resnet",
    "title": "ResNets",
    "section": "Building a Modern CNN: ResNet",
    "text": "Building a Modern CNN: ResNet\n\nSkip Connections\n\nclass ResBlock(Module):\n    def __init__(self, ni, nf):\n        self.convs = nn.Sequential(\n            ConvLayer(ni,nf),\n            ConvLayer(nf,nf, norm_type=NormType.BatchZero))\n        \n    def forward(self, x): return x + self.convs(x)\n\n\ndef _conv_block(ni,nf,stride):\n    return nn.Sequential(\n        ConvLayer(ni, nf, stride=stride),\n        ConvLayer(nf, nf, act_cls=None, norm_type=NormType.BatchZero))\n\n\nclass ResBlock(Module):\n    def __init__(self, ni, nf, stride=1):\n        self.convs = _conv_block(ni,nf,stride)\n        self.idconv = noop if ni==nf else ConvLayer(ni, nf, 1, act_cls=None)\n        self.pool = noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n\n    def forward(self, x):\n        return F.relu(self.convs(x) + self.idconv(self.pool(x)))\n\n\ndef block(ni,nf): return ResBlock(ni, nf, stride=2)\nlearn = get_learner(get_model())\n\n\nlearn.fit_one_cycle(5, 3e-3)\n\n\ndef block(ni, nf):\n    return nn.Sequential(ResBlock(ni, nf, stride=2), ResBlock(nf, nf))\n\n\nlearn = get_learner(get_model())\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\nA State-of-the-Art ResNet\n\ndef _resnet_stem(*sizes):\n    return [\n        ConvLayer(sizes[i], sizes[i+1], 3, stride = 2 if i==0 else 1)\n            for i in range(len(sizes)-1)\n    ] + [nn.MaxPool2d(kernel_size=3, stride=2, padding=1)]\n\n\n_resnet_stem(3,32,32,64)\n\n\nclass ResNet(nn.Sequential):\n    def __init__(self, n_out, layers, expansion=1):\n        stem = _resnet_stem(3,32,32,64)\n        self.block_szs = [64, 64, 128, 256, 512]\n        for i in range(1,5): self.block_szs[i] *= expansion\n        blocks = [self._make_layer(*o) for o in enumerate(layers)]\n        super().__init__(*stem, *blocks,\n                         nn.AdaptiveAvgPool2d(1), Flatten(),\n                         nn.Linear(self.block_szs[-1], n_out))\n    \n    def _make_layer(self, idx, n_layers):\n        stride = 1 if idx==0 else 2\n        ch_in,ch_out = self.block_szs[idx:idx+2]\n        return nn.Sequential(*[\n            ResBlock(ch_in if i==0 else ch_out, ch_out, stride if i==0 else 1)\n            for i in range(n_layers)\n        ])\n\n\nrn = ResNet(dls.c, [2,2,2,2])\n\n\nlearn = get_learner(rn)\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\nBottleneck Layers\n\ndef _conv_block(ni,nf,stride):\n    return nn.Sequential(\n        ConvLayer(ni, nf//4, 1),\n        ConvLayer(nf//4, nf//4, stride=stride), \n        ConvLayer(nf//4, nf, 1, act_cls=None, norm_type=NormType.BatchZero))\n\n\ndls = get_data(URLs.IMAGENETTE_320, presize=320, resize=224)\n\n\nrn = ResNet(dls.c, [3,4,6,3], 4)\n\n\nlearn = get_learner(rn)\nlearn.fit_one_cycle(20, 3e-3)"
  },
  {
    "objectID": "Fastbook/clean/14_resnet.html#conclusion",
    "href": "Fastbook/clean/14_resnet.html#conclusion",
    "title": "ResNets",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "Fastbook/clean/14_resnet.html#questionnaire",
    "href": "Fastbook/clean/14_resnet.html#questionnaire",
    "title": "ResNets",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nHow did we get to a single vector of activations in the CNNs used for MNIST in previous chapters? Why isn’t that suitable for Imagenette?\nWhat do we do for Imagenette instead?\nWhat is “adaptive pooling”?\nWhat is “average pooling”?\nWhy do we need Flatten after an adaptive average pooling layer?\nWhat is a “skip connection”?\nWhy do skip connections allow us to train deeper models?\nWhat does &lt;&gt; show? How did that lead to the idea of skip connections?\nWhat is “identity mapping”?\nWhat is the basic equation for a ResNet block (ignoring batchnorm and ReLU layers)?\nWhat do ResNets have to do with residuals?\nHow do we deal with the skip connection when there is a stride-2 convolution? How about when the number of filters changes?\nHow can we express a 1×1 convolution in terms of a vector dot product?\nCreate a 1x1 convolution with F.conv2d or nn.Conv2d and apply it to an image. What happens to the shape of the image?\nWhat does the noop function return?\nExplain what is shown in &lt;&gt;.\nWhen is top-5 accuracy a better metric than top-1 accuracy?\nWhat is the “stem” of a CNN?\nWhy do we use plain convolutions in the CNN stem, instead of ResNet blocks?\nHow does a bottleneck block differ from a plain ResNet block?\nWhy is a bottleneck block faster?\nHow do fully convolutional nets (and nets with adaptive pooling in general) allow for progressive resizing?\n\n\nFurther Research\n\nTry creating a fully convolutional net with adaptive average pooling for MNIST (note that you’ll need fewer stride-2 layers). How does it compare to a network without such a pooling layer?\nIn &lt;&gt; we introduce Einstein summation notation. Skip ahead to see how this works, and then write an implementation of the 1×1 convolution operation using torch.einsum. Compare it to the same operation using torch.conv2d.\nWrite a “top-5 accuracy” function using plain PyTorch or plain Python.\nTrain a model on Imagenette for more epochs, with and without label smoothing. Take a look at the Imagenette leaderboards and see how close you can get to the best results shown. Read the linked pages describing the leading approaches."
  },
  {
    "objectID": "Fastbook/clean/15_arch_details.html",
    "href": "Fastbook/clean/15_arch_details.html",
    "title": "Application Architectures Deep Dive",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *"
  },
  {
    "objectID": "Fastbook/clean/15_arch_details.html#computer-vision",
    "href": "Fastbook/clean/15_arch_details.html#computer-vision",
    "title": "Application Architectures Deep Dive",
    "section": "Computer Vision",
    "text": "Computer Vision\n\nvision_learner\n\nmodel_meta[resnet50]\n\n\ncreate_head(20,2)\n\n\n\nunet_learner\n\n\nA Siamese Network\n\n#hide\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\nclass SiameseImage(fastuple):\n    def show(self, ctx=None, **kwargs): \n        img1,img2,same_breed = self\n        if not isinstance(img1, Tensor):\n            if img2.size != img1.size: img2 = img2.resize(img1.size)\n            t1,t2 = tensor(img1),tensor(img2)\n            t1,t2 = t1.permute(2,0,1),t2.permute(2,0,1)\n        else: t1,t2 = img1,img2\n        line = t1.new_zeros(t1.shape[0], t1.shape[1], 10)\n        return show_image(torch.cat([t1,line,t2], dim=2), \n                          title=same_breed, ctx=ctx)\n    \ndef label_func(fname):\n    return re.match(r'^(.*)_\\d+.jpg$', fname.name).groups()[0]\n\nclass SiameseTransform(Transform):\n    def __init__(self, files, label_func, splits):\n        self.labels = files.map(label_func).unique()\n        self.lbl2files = {l: L(f for f in files if label_func(f) == l) for l in self.labels}\n        self.label_func = label_func\n        self.valid = {f: self._draw(f) for f in files[splits[1]]}\n        \n    def encodes(self, f):\n        f2,t = self.valid.get(f, self._draw(f))\n        img1,img2 = PILImage.create(f),PILImage.create(f2)\n        return SiameseImage(img1, img2, t)\n    \n    def _draw(self, f):\n        same = random.random() &lt; 0.5\n        cls = self.label_func(f)\n        if not same: cls = random.choice(L(l for l in self.labels if l != cls)) \n        return random.choice(self.lbl2files[cls]),same\n    \nsplits = RandomSplitter()(files)\ntfm = SiameseTransform(files, label_func, splits)\ntls = TfmdLists(files, tfm, splits=splits)\ndls = tls.dataloaders(after_item=[Resize(224), ToTensor], \n    after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)])\n\n\nclass SiameseModel(Module):\n    def __init__(self, encoder, head):\n        self.encoder,self.head = encoder,head\n    \n    def forward(self, x1, x2):\n        ftrs = torch.cat([self.encoder(x1), self.encoder(x2)], dim=1)\n        return self.head(ftrs)\n\n\nencoder = create_body(resnet34, cut=-2)\n\n\nhead = create_head(512*2, 2, ps=0.5)\n\n\nmodel = SiameseModel(encoder, head)\n\n\ndef loss_func(out, targ):\n    return nn.CrossEntropyLoss()(out, targ.long())\n\n\ndef siamese_splitter(model):\n    return [params(model.encoder), params(model.head)]\n\n\nlearn = Learner(dls, model, loss_func=loss_func, \n                splitter=siamese_splitter, metrics=accuracy)\nlearn.freeze()\n\n\nlearn.fit_one_cycle(4, 3e-3)\n\n\nlearn.unfreeze()\nlearn.fit_one_cycle(4, slice(1e-6,1e-4))"
  },
  {
    "objectID": "Fastbook/clean/15_arch_details.html#natural-language-processing",
    "href": "Fastbook/clean/15_arch_details.html#natural-language-processing",
    "title": "Application Architectures Deep Dive",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing"
  },
  {
    "objectID": "Fastbook/clean/15_arch_details.html#tabular",
    "href": "Fastbook/clean/15_arch_details.html#tabular",
    "title": "Application Architectures Deep Dive",
    "section": "Tabular",
    "text": "Tabular"
  },
  {
    "objectID": "Fastbook/clean/15_arch_details.html#wrapping-up-architectures",
    "href": "Fastbook/clean/15_arch_details.html#wrapping-up-architectures",
    "title": "Application Architectures Deep Dive",
    "section": "Wrapping Up Architectures",
    "text": "Wrapping Up Architectures"
  },
  {
    "objectID": "Fastbook/clean/15_arch_details.html#questionnaire",
    "href": "Fastbook/clean/15_arch_details.html#questionnaire",
    "title": "Application Architectures Deep Dive",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhat is the “head” of a neural net?\nWhat is the “body” of a neural net?\nWhat is “cutting” a neural net? Why do we need to do this for transfer learning?\nWhat is model_meta? Try printing it to see what’s inside.\nRead the source code for create_head and make sure you understand what each line does.\nLook at the output of create_head and make sure you understand why each layer is there, and how the create_head source created it.\nFigure out how to change the dropout, layer size, and number of layers created by vision_learner, and see if you can find values that result in better accuracy from the pet recognizer.\nWhat does AdaptiveConcatPool2d do?\nWhat is “nearest neighbor interpolation”? How can it be used to upsample convolutional activations?\nWhat is a “transposed convolution”? What is another name for it?\nCreate a conv layer with transpose=True and apply it to an image. Check the output shape.\nDraw the U-Net architecture.\nWhat is “BPTT for Text Classification” (BPT3C)?\nHow do we handle different length sequences in BPT3C?\nTry to run each line of TabularModel.forward separately, one line per cell, in a notebook, and look at the input and output shapes at each step.\nHow is self.layers defined in TabularModel?\nWhat are the five steps for preventing over-fitting?\nWhy don’t we reduce architecture complexity before trying other approaches to preventing overfitting?\n\n\nFurther Research\n\nWrite your own custom head and try training the pet recognizer with it. See if you can get a better result than fastai’s default.\nTry switching between AdaptiveConcatPool2d and AdaptiveAvgPool2d in a CNN head and see what difference it makes.\nWrite your own custom splitter to create a separate parameter group for every ResNet block, and a separate group for the stem. Try training with it, and see if it improves the pet recognizer.\nRead the online chapter about generative image models, and create your own colorizer, super-resolution model, or style transfer model.\nCreate a custom head using nearest neighbor interpolation and use it to do segmentation on CamVid."
  },
  {
    "objectID": "Fastbook/clean/17_foundations.html",
    "href": "Fastbook/clean/17_foundations.html",
    "title": "A Neural Net from the Foundations",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()"
  },
  {
    "objectID": "Fastbook/clean/17_foundations.html#building-a-neural-net-layer-from-scratch",
    "href": "Fastbook/clean/17_foundations.html#building-a-neural-net-layer-from-scratch",
    "title": "A Neural Net from the Foundations",
    "section": "Building a Neural Net Layer from Scratch",
    "text": "Building a Neural Net Layer from Scratch\n\nModeling a Neuron\n\n\nMatrix Multiplication from Scratch\n\nimport torch\nfrom torch import tensor\n\n\ndef matmul(a,b):\n    ar,ac = a.shape # n_rows * n_cols\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n    return c\n\n\nm1 = torch.randn(5,28*28)\nm2 = torch.randn(784,10)\n\n\n%time t1=matmul(m1, m2)\n\n\n%timeit -n 20 t2=m1@m2\n\n\n\nElementwise Arithmetic\n\na = tensor([10., 6, -4])\nb = tensor([2., 8, 7])\na + b\n\n\na &lt; b\n\n\n(a &lt; b).all(), (a==b).all()\n\n\n(a + b).mean().item()\n\n\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\nm*m\n\n\nn = tensor([[1., 2, 3], [4,5,6]])\nm*n\n\n\ndef matmul(a,b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = (a[i] * b[:,j]).sum()\n    return c\n\n\n%timeit -n 20 t3 = matmul(m1,m2)\n\n\n\nBroadcasting\n\nBroadcasting with a scalar\n\na = tensor([10., 6, -4])\na &gt; 0\n\n\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\n(m - 5) / 2.73\n\n\n\nBroadcasting a vector to a matrix\n\nc = tensor([10.,20,30])\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\nm.shape,c.shape\n\n\nm + c\n\n\nc.expand_as(m)\n\n\nt = c.expand_as(m)\nt.storage()\n\n\nt.stride(), t.shape\n\n\nc + m\n\n\nc = tensor([10.,20,30])\nm = tensor([[1., 2, 3], [4,5,6]])\nc+m\n\n\nc = tensor([10.,20])\nm = tensor([[1., 2, 3], [4,5,6]])\nc+m\n\n\nc = tensor([10.,20,30])\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\nc = c.unsqueeze(1)\nm.shape,c.shape\n\n\nc+m\n\n\nt = c.expand_as(m)\nt.storage()\n\n\nt.stride(), t.shape\n\n\nc = tensor([10.,20,30])\nc.shape, c.unsqueeze(0).shape,c.unsqueeze(1).shape\n\n\nc.shape, c[None,:].shape,c[:,None].shape\n\n\nc[None].shape,c[...,None].shape\n\n\ndef matmul(a,b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n#       c[i,j] = (a[i,:]          * b[:,j]).sum() # previous\n        c[i]   = (a[i  ].unsqueeze(-1) * b).sum(dim=0)\n    return c\n\n\n%timeit -n 20 t4 = matmul(m1,m2)\n\n\n\nBroadcasting rules\n\n\n\nEinstein Summation\n\ndef matmul(a,b): return torch.einsum('ik,kj-&gt;ij', a, b)\n\n\n%timeit -n 20 t5 = matmul(m1,m2)"
  },
  {
    "objectID": "Fastbook/clean/17_foundations.html#the-forward-and-backward-passes",
    "href": "Fastbook/clean/17_foundations.html#the-forward-and-backward-passes",
    "title": "A Neural Net from the Foundations",
    "section": "The Forward and Backward Passes",
    "text": "The Forward and Backward Passes\n\nDefining and Initializing a Layer\n\ndef lin(x, w, b): return x @ w + b\n\n\nx = torch.randn(200, 100)\ny = torch.randn(200)\n\n\nw1 = torch.randn(100,50)\nb1 = torch.zeros(50)\nw2 = torch.randn(50,1)\nb2 = torch.zeros(1)\n\n\nl1 = lin(x, w1, b1)\nl1.shape\n\n\nl1.mean(), l1.std()\n\n\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ torch.randn(100,100)\nx[0:5,0:5]\n\n\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ (torch.randn(100,100) * 0.01)\nx[0:5,0:5]\n\n\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ (torch.randn(100,100) * 0.1)\nx[0:5,0:5]\n\n\nx.std()\n\n\nx = torch.randn(200, 100)\ny = torch.randn(200)\n\n\nfrom math import sqrt\nw1 = torch.randn(100,50) / sqrt(100)\nb1 = torch.zeros(50)\nw2 = torch.randn(50,1) / sqrt(50)\nb2 = torch.zeros(1)\n\n\nl1 = lin(x, w1, b1)\nl1.mean(),l1.std()\n\n\ndef relu(x): return x.clamp_min(0.)\n\n\nl2 = relu(l1)\nl2.mean(),l2.std()\n\n\nx = torch.randn(200, 100)\nfor i in range(50): x = relu(x @ (torch.randn(100,100) * 0.1))\nx[0:5,0:5]\n\n\nx = torch.randn(200, 100)\nfor i in range(50): x = relu(x @ (torch.randn(100,100) * sqrt(2/100)))\nx[0:5,0:5]\n\n\nx = torch.randn(200, 100)\ny = torch.randn(200)\n\n\nw1 = torch.randn(100,50) * sqrt(2 / 100)\nb1 = torch.zeros(50)\nw2 = torch.randn(50,1) * sqrt(2 / 50)\nb2 = torch.zeros(1)\n\n\nl1 = lin(x, w1, b1)\nl2 = relu(l1)\nl2.mean(), l2.std()\n\n\ndef model(x):\n    l1 = lin(x, w1, b1)\n    l2 = relu(l1)\n    l3 = lin(l2, w2, b2)\n    return l3\n\n\nout = model(x)\nout.shape\n\n\ndef mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()\n\n\nloss = mse(out, y)\n\n\n\nGradients and the Backward Pass\n\ndef mse_grad(inp, targ): \n    # grad of loss with respect to output of previous layer\n    inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]\n\n\ndef relu_grad(inp, out):\n    # grad of relu with respect to input activations\n    inp.g = (inp&gt;0).float() * out.g\n\n\ndef lin_grad(inp, out, w, b):\n    # grad of matmul with respect to input\n    inp.g = out.g @ w.t()\n    w.g = inp.t() @ out.g\n    b.g = out.g.sum(0)\n\n\n\nSidebar: SymPy\n\nfrom sympy import symbols,diff\nsx,sy = symbols('sx sy')\ndiff(sx**2, sx)\n\n\n\nEnd sidebar\n\ndef forward_and_backward(inp, targ):\n    # forward pass:\n    l1 = inp @ w1 + b1\n    l2 = relu(l1)\n    out = l2 @ w2 + b2\n    # we don't actually need the loss in backward!\n    loss = mse(out, targ)\n    \n    # backward pass:\n    mse_grad(out, targ)\n    lin_grad(l2, out, w2, b2)\n    relu_grad(l1, l2)\n    lin_grad(inp, l1, w1, b1)\n\n\n\nRefactoring the Model\n\nclass Relu():\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp.clamp_min(0.)\n        return self.out\n    \n    def backward(self): self.inp.g = (self.inp&gt;0).float() * self.out.g\n\n\nclass Lin():\n    def __init__(self, w, b): self.w,self.b = w,b\n        \n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp@self.w + self.b\n        return self.out\n    \n    def backward(self):\n        self.inp.g = self.out.g @ self.w.t()\n        self.w.g = self.inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n\n\nclass Mse():\n    def __call__(self, inp, targ):\n        self.inp = inp\n        self.targ = targ\n        self.out = (inp.squeeze() - targ).pow(2).mean()\n        return self.out\n    \n    def backward(self):\n        x = (self.inp.squeeze()-self.targ).unsqueeze(-1)\n        self.inp.g = 2.*x/self.targ.shape[0]\n\n\nclass Model():\n    def __init__(self, w1, b1, w2, b2):\n        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n        self.loss = Mse()\n        \n    def __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return self.loss(x, targ)\n    \n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\n\n\nmodel = Model(w1, b1, w2, b2)\n\n\nloss = model(x, y)\n\n\nmodel.backward()\n\n\n\nGoing to PyTorch\n\nclass LayerFunction():\n    def __call__(self, *args):\n        self.args = args\n        self.out = self.forward(*args)\n        return self.out\n    \n    def forward(self):  raise Exception('not implemented')\n    def bwd(self):      raise Exception('not implemented')\n    def backward(self): self.bwd(self.out, *self.args)\n\n\nclass Relu(LayerFunction):\n    def forward(self, inp): return inp.clamp_min(0.)\n    def bwd(self, out, inp): inp.g = (inp&gt;0).float() * out.g\n\n\nclass Lin(LayerFunction):\n    def __init__(self, w, b): self.w,self.b = w,b\n        \n    def forward(self, inp): return inp@self.w + self.b\n    \n    def bwd(self, out, inp):\n        inp.g = out.g @ self.w.t()\n        self.w.g = inp.t() @ self.out.g\n        self.b.g = out.g.sum(0)\n\n\nclass Mse(LayerFunction):\n    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n    def bwd(self, out, inp, targ): \n        inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]\n\n\nfrom torch.autograd import Function\n\nclass MyRelu(Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i.clamp_min(0.)\n        ctx.save_for_backward(i)\n        return result\n    \n    @staticmethod\n    def backward(ctx, grad_output):\n        i, = ctx.saved_tensors\n        return grad_output * (i&gt;0).float()\n\n\nimport torch.nn as nn\n\nclass LinearLayer(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(n_out, n_in) * sqrt(2/n_in))\n        self.bias = nn.Parameter(torch.zeros(n_out))\n    \n    def forward(self, x): return x @ self.weight.t() + self.bias\n\n\nlin = LinearLayer(10,2)\np1,p2 = lin.parameters()\np1.shape,p2.shape\n\n\nclass Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out))\n        self.loss = mse\n        \n    def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ)\n\n\nclass Model(Module):\n    def __init__(self, n_in, nh, n_out):\n        self.layers = nn.Sequential(\n            nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out))\n        self.loss = mse\n        \n    def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ)"
  },
  {
    "objectID": "Fastbook/clean/17_foundations.html#conclusion",
    "href": "Fastbook/clean/17_foundations.html#conclusion",
    "title": "A Neural Net from the Foundations",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "Fastbook/clean/17_foundations.html#questionnaire",
    "href": "Fastbook/clean/17_foundations.html#questionnaire",
    "title": "A Neural Net from the Foundations",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWrite the Python code to implement a single neuron.\nWrite the Python code to implement ReLU.\nWrite the Python code for a dense layer in terms of matrix multiplication.\nWrite the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python).\nWhat is the “hidden size” of a layer?\nWhat does the t method do in PyTorch?\nWhy is matrix multiplication written in plain Python very slow?\nIn matmul, why is ac==br?\nIn Jupyter Notebook, how do you measure the time taken for a single cell to execute?\nWhat is “elementwise arithmetic”?\nWrite the PyTorch code to test whether every element of a is greater than the corresponding element of b.\nWhat is a rank-0 tensor? How do you convert it to a plain Python data type?\nWhat does this return, and why? tensor([1,2]) + tensor([1])\nWhat does this return, and why? tensor([1,2]) + tensor([1,2,3])\nHow does elementwise arithmetic help us speed up matmul?\nWhat are the broadcasting rules?\nWhat is expand_as? Show an example of how it can be used to match the results of broadcasting.\nHow does unsqueeze help us to solve certain broadcasting problems?\nHow can we use indexing to do the same operation as unsqueeze?\nHow do we show the actual contents of the memory used for a tensor?\nWhen adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)\nDo broadcasting and expand_as result in increased memory use? Why or why not?\nImplement matmul using Einstein summation.\nWhat does a repeated index letter represent on the left-hand side of einsum?\nWhat are the three rules of Einstein summation notation? Why?\nWhat are the forward pass and backward pass of a neural network?\nWhy do we need to store some of the activations calculated for intermediate layers in the forward pass?\nWhat is the downside of having activations with a standard deviation too far away from 1?\nHow can weight initialization help avoid this problem?\nWhat is the formula to initialize weights such that we get a standard deviation of 1 for a plain linear layer, and for a linear layer followed by ReLU?\nWhy do we sometimes have to use the squeeze method in loss functions?\nWhat does the argument to the squeeze method do? Why might it be important to include this argument, even though PyTorch does not require it?\nWhat is the “chain rule”? Show the equation in either of the two forms presented in this chapter.\nShow how to calculate the gradients of mse(lin(l2, w2, b2), y) using the chain rule.\nWhat is the gradient of ReLU? Show it in math or code. (You shouldn’t need to commit this to memory—try to figure it using your knowledge of the shape of the function.)\nIn what order do we need to call the *_grad functions in the backward pass? Why?\nWhat is __call__?\nWhat methods must we implement when writing a torch.autograd.Function?\nWrite nn.Linear from scratch, and test it works.\nWhat is the difference between nn.Module and fastai’s Module?\n\n\nFurther Research\n\nImplement ReLU as a torch.autograd.Function and train a model with it.\nIf you are mathematically inclined, find out what the gradients of a linear layer are in mathematical notation. Map that to the implementation we saw in this chapter.\nLearn about the unfold method in PyTorch, and use it along with matrix multiplication to implement your own 2D convolution function. Then train a CNN that uses it.\nImplement everything in this chapter using NumPy instead of PyTorch."
  },
  {
    "objectID": "Fastbook/README_es.html",
    "href": "Fastbook/README_es.html",
    "title": "The fastai book",
    "section": "",
    "text": "English / Spanish / Korean / Chinese / Bengali / Indonesian / Italian / Portuguese / Vietnamese / Japanese\n\nThe fastai book\nUna colección de Jupyter Notebooks que cubren una introducción al Deep Learning, fastai y PyTorch. fastai es una API en capas para Deep Learning; para mayor información lea la publicación. Todos los documentos incluídos en este repositorio tienen derechos de autor para Jeremy Howard y Sylvain Gugger, desde 2020 en adelante.\nEstos Notebooks son usados para el MOOC (Massive Open Online Courses por sus siglas en ingles, cursos abiertos masivos en línea) y forman la base del libro, el cual se encuentra actualmente disponible para su compra. El libro no tiene las mismas restricciones de GPL que se incluyen en este borrador.\nEl código en los Notebooks y en los archivos de python .py está cubierto por la licencia GPL v3; consulte el archivo de LICENSE para obtener más detalles.\nEl resto de textos (incluidas todas las celdas de Markdown en los Notebooks) no tienen licencia para su redistribución o cambio de formato o medio, más allá del uso de copias de los Notebooks o bifurcar el repositorio para uso privado. No se permite el uso comercial o de difusión. Ponemos estos materiales a su disposición de forma gratuita para ayudarle a aprender sobre Deep Learning, así que por favor respete las restricciones y nuestros derechos de autor.\nSi observa a alguien usando una copia de estos materiales incumpliendo la licencia, por favor infórmele que sus acciones no están permitidas y pueden dar lugar a acciones legales. Además, estarían perjudicando a la comunidad porque es probable que dejemos de publicar materiales adicionales si la gente ignora nuestros derechos de autor.\nEste es un borrador inicial. Si encuentra problemas ejecutando los Notebooks, podrá buscar ayuda y respuestas en el foro fastai-dev. Por favor, no utilice GitHub issues para problemas relacionados con la ejecución de los Notebooks.\nTodo pull request realizado a este repositorio queda asignado bajo derechos de autor a Jeremy Howard y Sylvain Gugger. (Además, si realiza pequeñas modificaciones en la ortografía o el texto, por favor indique el nombre del archivo y una descripción breve de lo que está corrigiendo. A los revisores les resulta cada vez más difícil saber qué correcciones ya se han realizado. Gracias)."
  },
  {
    "objectID": "posts/2025-09-25-1.html",
    "href": "posts/2025-09-25-1.html",
    "title": "Почему я завел этот блог",
    "section": "",
    "text": "Я нашел классный курс по генеративному ии. Его можно проходить бесплатно.\nЗавел блог по рекомендации Jeremy Howard. Он является автором еще одного крутого бесплатного курса по глубокому обучению. Я благодарен ему за одну очень ценную рекомендацию по ораганизации процесса обучения: после прохождения урока стирать код в ноутбуке (под ноутбуками в этом блоге я буду подразумевать Jupyter notebook files) и писать его самому. Выполнение этой рекомендации невероятно обогащает процесс обучения, потому что когда просто читаешь код: “А, все понятно”, а когда пишешь задаешься вопросами: “почему нужно вызывать вызывать эту функцию и как ее найти, а вот у нее еще есть параметр, а что будет если запускать ее без него?”\nЕще во мне дремлет учительская душа. Мне так хочется обучать (и общаться с юношами), когда я нахожу какой-то классный курс. Может и пойду проведу бесплатный курс в колледже.\n\nЕще один плюс ведения блога. Изначально я написал что это желание разбивается о невозможности прилично на этом заработать, но пока я писал эти строки, у меня появились идея что можно обкатать свой курс на живой аудитории, а потом разместить его на udemy.\n\nЕще одна причина завести блог кроется в моем желании накидать костяк для этого курса."
  },
  {
    "objectID": "posts/2025-10-22-1.html",
    "href": "posts/2025-10-22-1.html",
    "title": "Collaborative filtering",
    "section": "",
    "text": "Я учусь по книге Jeremy Howard. Она сделана в виде jupyter notebooks. Начну с ноутбука про collaborative filtering. Оно работает следующим образом. Мы смотрим какие продукты пользователь лайкал, а затем смотрим на лайки пользователей, которые лайкали такие же продукты. Продукты, которые понравились этим пользователям, вероятно понравятся и нашему пользователю.\nЭто не обязательно продукты. Мы также можем работать с треками, статьями, фильмами, медицинскими диагнозами, поставленным врачами определённым пациентам.\nКлючевая идея заключается в том, что признаки, по которым модель предлагает вам рекомендации скрыты. Например, вам нравится фантастика и современный российский кинематограф. У нас есть база из миллионов пользователей, которые ставили оценки фильмам. Мы не указываем напрямую, какие фильмы относятся к категории фантастики, но модель ведет себя так как будто знает, что такая категория существует. Она использует ее, не зная ее названия, а лишь через числовые представления (скрытые признаки).\nВ этом ноутбуке мы работаем в сабсетом MovieLens на 100к строк. Далее я с чистого листа применю изученный подход к датасету с русскими фильмами."
  },
  {
    "objectID": "posts/2025-10-23.html",
    "href": "posts/2025-10-23.html",
    "title": "Улучшения генома homo sapiens",
    "section": "",
    "text": "Что можно делать с техническим бэкграундом\nЯ хочу улучшать геном человека. У меня есть техническое образование, но нет биологического.\nБиоинформатика / анализ геномных данных — самый естественный путь для инженера: работа с данными, ассоциации, ML-анализ.\n\n\nЧто такое ассоциации (association studies)\nКогда у тебя есть много людей и их варианты ДНК + фенотипы (например, “Есть диабет / нет диабета”, “рост”, “IQ”, “ответ на лекарство”), ты можешь искать ассоциации между генетикой и признаком.\nЭто и есть GWAS - Genome-Wide Association Study.\n\n\nML-анализ\nДля сложных черт (например, интеллект, депрессия, метаболизм) тысячи генов вносят микровклад. ML позволяет объединить всё это в предсказательную систему.\n\n\nЧто нужно изучить для работы с генетическими ассоциациями\nНачать можно с уже полюбившегося мне курса MITx 7.00x: Introduction to Biology - The Secret of Life. Жаль что через 4 дня его контент будет заархивирован и более не активен. Сохранить что ль лекции…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Добро пожаловать в мой блог!",
    "section": "",
    "text": "Конфигурация молекулы ДНК у прокариотов\n\n\n\n\n\n\n\n\nOct 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nКонфигурация молекулы ДНК у прокариотов\n\n\n\n\n\n\n\n\nOct 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nЧто такое теломера и почему она укорачивается?\n\n\n\n\n\n\n\n\nOct 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nУлучшения генома homo sapiens\n\n\n\n\n\n\n\n\nOct 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCollaborative filtering\n\n\n\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nВозвращение к курсу ML от fast.ai\n\n\n\n\n\n\n\n\nOct 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nПочему я завел этот блог\n\n\n\n\n\n\n\n\nSep 25, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-10-30.html",
    "href": "posts/2025-10-30.html",
    "title": "Конфигурация молекулы ДНК у прокариотов",
    "section": "",
    "text": "Курс Introduction to Biology - The Secret of Life был перенесен в архив, но у меня остался доступ и к лекциям, и к заданиям. Так что я могу спойоно его закончить. На этой неделе мы перешли к молекулярной биологии. Теперь я знаю секрет жизни (ДНК) и как ее читать.\nСтарение обусловлено двумя основными причинами: - накопление ошибок при копировании ДНК - укорочение теломеры\n\nПочему теломера укорачивается?\nЧтобы понять что такое теломера и почему она укорачивается нам нужно разобраться с механизмом копирования ДНК. Это довольно сложная тема для неподготовленного читателя. Наверняка есть объяснения и попонятнее, но я в своей статье стремился не отбрасывать мелочи. Я даже не уверен, что кто-то кроменя когда-то будет ее читать, поэтому главная цель ее написания - это самообучение.\n\n\nИнтересное открытие сегодняшнего дня\nУ прокариотов (например батерий), молекула ДНК замкнута! То есть теломера им вообще не нужна. Но раз эволюция выбрала для нас линейную ДНК, видимо, она оказалась практичнее."
  },
  {
    "objectID": "posts/2025-10-31.html",
    "href": "posts/2025-10-31.html",
    "title": "Конфигурация молекулы ДНК у прокариотов",
    "section": "",
    "text": "Курс Introduction to Biology - The Secret of Life был перенесен в архив, но у меня остался доступ и к лекциям, и к заданиям. Так что я могу спойоно его закончить. На этой неделе мы перешли к молекулярной биологии. Теперь я знаю секрет жизни (ДНК) и как ее читать.\nСтарение обусловлено двумя основными причинами: - накопление ошибок при копировании ДНК - укорочение теломеры\n\nПочему теломера укорачивается?\nЧтобы понять что такое теломера и почему она укорачивается нам нужно разобраться с механизмом копирования ДНК. Это довольно сложная тема для неподготовленного читателя. Наверняка есть объяснения и попонятнее, но я в своей статье стремился не отбрасывать мелочи. Я даже не уверен, что кто-то кроменя когда-то будет ее читать, поэтому главная цель ее написания - это самообучение.\n\n\nИнтересное открытие сегодняшнего дня\nУ прокариотов (например батерий), молекула ДНК замкнута! То есть теломера им вообще не нужна. Но раз эволюция выбрала для нас линейную ДНК, видимо, она оказалась практичнее."
  },
  {
    "objectID": "posts/2025-10-30-1.html",
    "href": "posts/2025-10-30-1.html",
    "title": "Что такое теломера и почему она укорачивается?",
    "section": "",
    "text": "Фермент, который строит новую цепь ДНК, называется ДНК-полимераза. И у нее есть очень важное ограничение:\n\nДНК-полимераза не может начинать синтез с нуля. Она может только добавлять нуклеотиды к уже существующей цепи.\n\nЧтобы обойти это ограничесние, клетка использует другой фермент - праймазу. Она умеет синтезировать РНК, используя ДНК как шаблон. Когда начинается репликация, ДНК расплетается, и на одной из цепей праймаза:\n\nСмотрит на последовательность ДНК\nСтроит короткую РНК, комплиментарную этой последовательности\nЭта РНК - праймер - прикрепляется к ДНК-матрице (шаблон, одна из двух цепей двойной спирали, на основе которой строится новая комплиментарная цепь)\nДНК-полимераза садится на этот праймер и начинает строить ДНК\nКогда участок скопирован, РНК больше не нужна - она не является частью генома\nРНК-праймер удаляется\nДругая ДНК-полимераза вставляет вместо него ДНК\nФермент лигаза запечатывает шов\n\n\n\n\nДНК расплетается\n\n\nДНК-полимераза может прикреплять нуклеотиды ТОЛЬКО к концу цепи, который называется 3’-конец. Она двигается вдоль матрицы от 3’ к 5’, строя новую цепь в направлении 5’ → 3’.\n\n\n\nВедущая цепь\n\n\nЭта цепь называется ведущей, потому что репликационная вилка расширяется в том же направлении, в котором идет строительство новой цепи. Но нам нужно копировать и вторую цепь.\n\n\n\nОтстающая цепь\n\n\nРепликационная вилка расширяется в сторону строительства ведущей цепи. С ведущей цепью проблем нет, а вот чтобы продолжать строить отстающую цепь, нам нужно добавлять новые праймеры справа.\n\n\n\nНовый праймер\n\n\nРепликационная вилка движется вправо и доходит до конца хромосомы. Ведущая цепь скопирована без проблем.\n\n\n\nВедущая цепь скопирована до конца\n\n\nНа остающей цепи добавляется последний праймер.\n\n\n\nПоследний праймер\n\n\nНо что если последний праймер не попал на самый конец цепи? Тогда наша хромосома при копировании станет чуть короче. Есть греческое слово, означающее конец: telos. Так вот, концы цепей называются теломерами."
  },
  {
    "objectID": "posts/2025-10-21-1.html",
    "href": "posts/2025-10-21-1.html",
    "title": "Возвращение к курсу ML от fast.ai",
    "section": "",
    "text": "Я нашел курс по генеративному ии от IBM довольно плохо сделанным.\nЯ вернулся к прохождению курса по глубокому обучению от fast.ai. Это один из самых уважаемых курсов по ML, хоть его фокус направлен на компьютерное зрение."
  },
  {
    "objectID": "Fastbook/17_foundations.html",
    "href": "Fastbook/17_foundations.html",
    "title": "A Neural Net from the Foundations",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n[[chapter_foundations]]\nThis chapter begins a journey where we will dig deep into the internals of the models we used in the previous chapters. We will be covering many of the same things we’ve seen before, but this time around we’ll be looking much more closely at the implementation details, and much less closely at the practical issues of how and why things are as they are.\nWe will build everything from scratch, only using basic indexing into a tensor. We’ll write a neural net from the ground up, then implement backpropagation manually, so we know exactly what’s happening in PyTorch when we call loss.backward. We’ll also see how to extend PyTorch with custom autograd functions that allow us to specify our own forward and backward computations."
  },
  {
    "objectID": "Fastbook/17_foundations.html#building-a-neural-net-layer-from-scratch",
    "href": "Fastbook/17_foundations.html#building-a-neural-net-layer-from-scratch",
    "title": "A Neural Net from the Foundations",
    "section": "Building a Neural Net Layer from Scratch",
    "text": "Building a Neural Net Layer from Scratch\nLet’s start by refreshing our understanding of how matrix multiplication is used in a basic neural network. Since we’re building everything up from scratch, we’ll use nothing but plain Python initially (except for indexing into PyTorch tensors), and then replace the plain Python with PyTorch functionality once we’ve seen how to create it.\n\nModeling a Neuron\nA neuron receives a given number of inputs and has an internal weight for each of them. It sums those weighted inputs to produce an output and adds an inner bias. In math, this can be written as:\n\\[ out = \\sum_{i=1}^{n} x_{i} w_{i} + b\\]\nif we name our inputs \\((x_{1},\\dots,x_{n})\\), our weights \\((w_{1},\\dots,w_{n})\\), and our bias \\(b\\). In code this translates into:\noutput = sum([x*w for x,w in zip(inputs,weights)]) + bias\nThis output is then fed into a nonlinear function called an activation function before being sent to another neuron. In deep learning the most common of these is the rectified Linear unit, or ReLU, which, as we’ve seen, is a fancy way of saying:\ndef relu(x): return x if x &gt;= 0 else 0\nA deep learning model is then built by stacking a lot of those neurons in successive layers. We create a first layer with a certain number of neurons (known as hidden size) and link all the inputs to each of those neurons. Such a layer is often called a fully connected layer or a dense layer (for densely connected), or a linear layer.\nIt requires to compute, for each input in our batch and each neuron with a give weight, the dot product:\nsum([x*w for x,w in zip(input,weight)])\nIf you have done a little bit of linear algebra, you may remember that having a lot of those dot products happens when you do a matrix multiplication. More precisely, if our inputs are in a matrix x with a size of batch_size by n_inputs, and if we have grouped the weights of our neurons in a matrix w of size n_neurons by n_inputs (each neuron must have the same number of weights as it has inputs) and all the biases in a vector b of size n_neurons, then the output of this fully connected layer is:\ny = x @ w.t() + b\nwhere @ represents the matrix product and w.t() is the transpose matrix of w. The output y is then of size batch_size by n_neurons, and in position (i,j) we have (for the mathy folks out there):\n\\[y_{i,j} = \\sum_{k=1}^{n} x_{i,k} w_{k,j} + b_{j}\\]\nOr in code:\ny[i,j] = sum([a * b for a,b in zip(x[i,:],w[j,:])]) + b[j]\nThe transpose is necessary because in the mathematical definition of the matrix product m @ n, the coefficient (i,j) is:\nsum([a * b for a,b in zip(m[i,:],n[:,j])])\nSo the very basic operation we need is a matrix multiplication, as it’s what is hidden in the core of a neural net.\n\n\nMatrix Multiplication from Scratch\nLet’s write a function that computes the matrix product of two tensors, before we allow ourselves to use the PyTorch version of it. We will only use the indexing in PyTorch tensors:\n\nimport torch\nfrom torch import tensor\n\nWe’ll need three nested for loops: one for the row indices, one for the column indices, and one for the inner sum. ac and ar stand for number of columns of a and number of rows of a, respectively (the same convention is followed for b), and we make sure calculating the matrix product is possible by checking that a has as many columns as b has rows:\n\ndef matmul(a,b):\n    ar,ac = a.shape # n_rows * n_cols\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n    return c\n\nTo test this out, we’ll pretend (using random matrices) that we’re working with a small batch of 5 MNIST images, flattened into 28×28 vectors, with linear model to turn them into 10 activations:\n\nm1 = torch.randn(5,28*28)\nm2 = torch.randn(784,10)\n\nLet’s time our function, using the Jupyter “magic” command %time:\n\n%time t1=matmul(m1, m2)\n\nCPU times: user 1.15 s, sys: 4.09 ms, total: 1.15 s\nWall time: 1.15 s\n\n\nAnd see how that compares to PyTorch’s built-in @:\n\n%timeit -n 20 t2=m1@m2\n\n14 µs ± 8.95 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\n\n\nAs we can see, in Python three nested loops is a very bad idea! Python is a slow language, and this isn’t going to be very efficient. We see here that PyTorch is around 100,000 times faster than Python—and that’s before we even start using the GPU!\nWhere does this difference come from? PyTorch didn’t write its matrix multiplication in Python, but rather in C++ to make it fast. In general, whenever we do computations on tensors we will need to vectorize them so that we can take advantage of the speed of PyTorch, usually by using two techniques: elementwise arithmetic and broadcasting.\n\n\nElementwise Arithmetic\nAll the basic operators (+, -, *, /, &gt;, &lt;, ==) can be applied elementwise. That means if we write a+b for two tensors a and b that have the same shape, we will get a tensor composed of the sums the elements of a and b:\n\na = tensor([10., 6, -4])\nb = tensor([2., 8, 7])\na + b\n\ntensor([12., 14.,  3.])\n\n\nThe Booleans operators will return an array of Booleans:\n\na &lt; b\n\ntensor([False,  True,  True])\n\n\nIf we want to know if every element of a is less than the corresponding element in b, or if two tensors are equal, we need to combine those elementwise operations with torch.all:\n\n(a &lt; b).all(), (a==b).all()\n\n(tensor(False), tensor(False))\n\n\nReduction operations like all(), sum() and mean() return tensors with only one element, called rank-0 tensors. If you want to convert this to a plain Python Boolean or number, you need to call .item():\n\n(a + b).mean().item()\n\n9.666666984558105\n\n\nThe elementwise operations work on tensors of any rank, as long as they have the same shape:\n\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\nm*m\n\ntensor([[ 1.,  4.,  9.],\n        [16., 25., 36.],\n        [49., 64., 81.]])\n\n\nHowever you can’t perform elementwise operations on tensors that don’t have the same shape (unless they are broadcastable, as discussed in the next section):\n\nn = tensor([[1., 2, 3], [4,5,6]])\nm*n\n\n\n------------------------------------------------------------------------\nRuntimeError                           Traceback (most recent call last)\n&lt;ipython-input-12-add73c4f74e0&gt; in &lt;module&gt;\n      1 n = tensor([[1., 2, 3], [4,5,6]])\n----&gt; 2 m*n\n\nRuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0\n\n\n\nWith elementwise arithmetic, we can remove one of our three nested loops: we can multiply the tensors that correspond to the i-th row of a and the j-th column of b before summing all the elements, which will speed things up because the inner loop will now be executed by PyTorch at C speed.\nTo access one column or row, we can simply write a[i,:] or b[:,j]. The : means take everything in that dimension. We could restrict this and take only a slice of that particular dimension by passing a range, like 1:5, instead of just :. In that case, we would take the elements in columns or rows 1 to 4 (the second number is noninclusive).\nOne simplification is that we can always omit a trailing colon, so a[i,:] can be abbreviated to a[i]. With all of that in mind, we can write a new version of our matrix multiplication:\n\ndef matmul(a,b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = (a[i] * b[:,j]).sum()\n    return c\n\n\n%timeit -n 20 t3 = matmul(m1,m2)\n\n1.7 ms ± 88.1 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\n\n\nWe’re already ~700 times faster, just by removing that inner for loop! And that’s just the beginning—with broadcasting we can remove another loop and get an even more important speed up.\n\n\nBroadcasting\nAs we discussed in &lt;&gt;, broadcasting is a term introduced by the NumPy library that describes how tensors of different ranks are treated during arithmetic operations. For instance, it’s obvious there is no way to add a 3×3 matrix with a 4×5 matrix, but what if we want to add one scalar (which can be represented as a 1×1 tensor) with a matrix? Or a vector of size 3 with a 3×4 matrix? In both cases, we can find a way to make sense of this operation.\nBroadcasting gives specific rules to codify when shapes are compatible when trying to do an elementwise operation, and how the tensor of the smaller shape is expanded to match the tensor of the bigger shape. It’s essential to master those rules if you want to be able to write code that executes quickly. In this section, we’ll expand our previous treatment of broadcasting to understand these rules.\n\nBroadcasting with a scalar\nBroadcasting with a scalar is the easiest type of broadcasting. When we have a tensor a and a scalar, we just imagine a tensor of the same shape as a filled with that scalar and perform the operation:\n\na = tensor([10., 6, -4])\na &gt; 0\n\ntensor([ True,  True, False])\n\n\nHow are we able to do this comparison? 0 is being broadcast to have the same dimensions as a. Note that this is done without creating a tensor full of zeros in memory (that would be very inefficient).\nThis is very useful if you want to normalize your dataset by subtracting the mean (a scalar) from the entire data set (a matrix) and dividing by the standard deviation (another scalar):\n\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\n(m - 5) / 2.73\n\ntensor([[-1.4652, -1.0989, -0.7326],\n        [-0.3663,  0.0000,  0.3663],\n        [ 0.7326,  1.0989,  1.4652]])\n\n\nWhat if have different means for each row of the matrix? in that case you will need to broadcast a vector to a matrix.\n\n\nBroadcasting a vector to a matrix\nWe can broadcast a vector to a matrix as follows:\n\nc = tensor([10.,20,30])\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\nm.shape,c.shape\n\n(torch.Size([3, 3]), torch.Size([3]))\n\n\n\nm + c\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\nHere the elements of c are expanded to make three rows that match, making the operation possible. Again, PyTorch doesn’t actually create three copies of c in memory. This is done by the expand_as method behind the scenes:\n\nc.expand_as(m)\n\ntensor([[10., 20., 30.],\n        [10., 20., 30.],\n        [10., 20., 30.]])\n\n\nIf we look at the corresponding tensor, we can ask for its storage property (which shows the actual contents of the memory used for the tensor) to check there is no useless data stored:\n\nt = c.expand_as(m)\nt.storage()\n\n 10.0\n 20.0\n 30.0\n[torch.FloatStorage of size 3]\n\n\nEven though the tensor officially has nine elements, only three scalars are stored in memory. This is possible thanks to the clever trick of giving that dimension a stride of 0 (which means that when PyTorch looks for the next row by adding the stride, it doesn’t move):\n\nt.stride(), t.shape\n\n((0, 1), torch.Size([3, 3]))\n\n\nSince m is of size 3×3, there are two ways to do broadcasting. The fact it was done on the last dimension is a convention that comes from the rules of broadcasting and has nothing to do with the way we ordered our tensors. If instead we do this, we get the same result:\n\nc + m\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\nIn fact, it’s only possible to broadcast a vector of size n with a matrix of size m by n:\n\nc = tensor([10.,20,30])\nm = tensor([[1., 2, 3], [4,5,6]])\nc+m\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.]])\n\n\nThis won’t work:\n\nc = tensor([10.,20])\nm = tensor([[1., 2, 3], [4,5,6]])\nc+m\n\n\n------------------------------------------------------------------------\nRuntimeError                           Traceback (most recent call last)\n&lt;ipython-input-25-64bbbad4d99c&gt; in &lt;module&gt;\n      1 c = tensor([10.,20])\n      2 m = tensor([[1., 2, 3], [4,5,6]])\n----&gt; 3 c+m\n\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\n\n\nIf we want to broadcast in the other dimension, we have to change the shape of our vector to make it a 3×1 matrix. This is done with the unsqueeze method in PyTorch:\n\nc = tensor([10.,20,30])\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\nc = c.unsqueeze(1)\nm.shape,c.shape\n\n(torch.Size([3, 3]), torch.Size([3, 1]))\n\n\nThis time, c is expanded on the column side:\n\nc+m\n\ntensor([[11., 12., 13.],\n        [24., 25., 26.],\n        [37., 38., 39.]])\n\n\nLike before, only three scalars are stored in memory:\n\nt = c.expand_as(m)\nt.storage()\n\n 10.0\n 20.0\n 30.0\n[torch.FloatStorage of size 3]\n\n\nAnd the expanded tensor has the right shape because the column dimension has a stride of 0:\n\nt.stride(), t.shape\n\n((1, 0), torch.Size([3, 3]))\n\n\nWith broadcasting, by default if we need to add dimensions, they are added at the beginning. When we were broadcasting before, Pytorch was doing c.unsqueeze(0) behind the scenes:\n\nc = tensor([10.,20,30])\nc.shape, c.unsqueeze(0).shape,c.unsqueeze(1).shape\n\n(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))\n\n\nThe unsqueeze command can be replaced by None indexing:\n\nc.shape, c[None,:].shape,c[:,None].shape\n\n(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))\n\n\nYou can always omit trailing colons, and ... means all preceding dimensions:\n\nc[None].shape,c[...,None].shape\n\n(torch.Size([1, 3]), torch.Size([3, 1]))\n\n\nWith this, we can remove another for loop in our matrix multiplication function. Now, instead of multiplying a[i] with b[:,j], we can multiply a[i] with the whole matrix b using broadcasting, then sum the results:\n\ndef matmul(a,b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n#       c[i,j] = (a[i,:]          * b[:,j]).sum() # previous\n        c[i]   = (a[i  ].unsqueeze(-1) * b).sum(dim=0)\n    return c\n\n\n%timeit -n 20 t4 = matmul(m1,m2)\n\n357 µs ± 7.2 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\n\n\nWe’re now 3,700 times faster than our first implementation! Before we move on, let’s discuss the rules of broadcasting in a little more detail.\n\n\nBroadcasting rules\nWhen operating on two tensors, PyTorch compares their shapes elementwise. It starts with the trailing dimensions and works its way backward, adding 1 when it meets empty dimensions. Two dimensions are compatible when one of the following is true:\n\nThey are equal.\nOne of them is 1, in which case that dimension is broadcast to make it the same as the other.\n\nArrays do not need to have the same number of dimensions. For example, if you have a 256×256×3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with three values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:\nImage  (3d tensor): 256 x 256 x 3\nScale  (1d tensor):  (1)   (1)  3\nResult (3d tensor): 256 x 256 x 3\nHowever, a 2D tensor of size 256×256 isn’t compatible with our image:\nImage  (3d tensor): 256 x 256 x   3\nScale  (2d tensor):  (1)  256 x 256\nError\nIn our earlier examples we had with a 3×3 matrix and a vector of size 3, broadcasting was done on the rows:\nMatrix (2d tensor):   3 x 3\nVector (1d tensor): (1)   3\nResult (2d tensor):   3 x 3\nAs an exercise, try to determine what dimensions to add (and where) when you need to normalize a batch of images of size 64 x 3 x 256 x 256 with vectors of three elements (one for the mean and one for the standard deviation).\nAnother useful way of simplifying tensor manipulations is the use of Einstein summations convention.\n\n\n\nEinstein Summation\nBefore using the PyTorch operation @ or torch.matmul, there is one last way we can implement matrix multiplication: Einstein summation (einsum). This is a compact representation for combining products and sums in a general way. We write an equation like this:\nik,kj -&gt; ij\nThe lefthand side represents the operands dimensions, separated by commas. Here we have two tensors that each have two dimensions (i,k and k,j). The righthand side represents the result dimensions, so here we have a tensor with two dimensions i,j.\nThe rules of Einstein summation notation are as follows:\n\nRepeated indices on the left side are implicitly summed over if they are not on the right side.\nEach index can appear at most twice on the left side.\nThe unrepeated indices on the left side must appear on the right side.\n\nSo in our example, since k is repeated, we sum over that index. In the end the formula represents the matrix obtained when we put in (i,j) the sum of all the coefficients (i,k) in the first tensor multiplied by the coefficients (k,j) in the second tensor… which is the matrix product! Here is how we can code this in PyTorch:\n\ndef matmul(a,b): return torch.einsum('ik,kj-&gt;ij', a, b)\n\nEinstein summation is a very practical way of expressing operations involving indexing and sum of products. Note that you can have just one member on the lefthand side. For instance, this:\ntorch.einsum('ij-&gt;ji', a)\nreturns the transpose of the matrix a. You can also have three or more members. This:\ntorch.einsum('bi,ij,bj-&gt;b', a, b, c)\nwill return a vector of size b where the k-th coordinate is the sum of a[k,i] b[i,j] c[k,j]. This notation is particularly convenient when you have more dimensions because of batches. For example, if you have two batches of matrices and want to compute the matrix product per batch, you would could this:\ntorch.einsum('bik,bkj-&gt;bij', a, b)\nLet’s go back to our new matmul implementation using einsum and look at its speed:\n\n%timeit -n 20 t5 = matmul(m1,m2)\n\n68.7 µs ± 4.06 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\n\n\nAs you can see, not only is it practical, but it’s very fast. einsum is often the fastest way to do custom operations in PyTorch, without diving into C++ and CUDA. (But it’s generally not as fast as carefully optimized CUDA code, as you see from the results in “Matrix Multiplication from Scratch”.)\nNow that we know how to implement a matrix multiplication from scratch, we are ready to build our neural net—specifically its forward and backward passes—using just matrix multiplications."
  },
  {
    "objectID": "Fastbook/17_foundations.html#the-forward-and-backward-passes",
    "href": "Fastbook/17_foundations.html#the-forward-and-backward-passes",
    "title": "A Neural Net from the Foundations",
    "section": "The Forward and Backward Passes",
    "text": "The Forward and Backward Passes\nAs we saw in &lt;&gt;, to train a model, we will need to compute all the gradients of a given loss with respect to its parameters, which is known as the backward pass. The forward pass is where we compute the output of the model on a given input, based on the matrix products. As we define our first neural net, we will also delve into the problem of properly initializing the weights, which is crucial for making training start properly.\n\nDefining and Initializing a Layer\nWe will take the example of a two-layer neural net first. As we’ve seen, one layer can be expressed as y = x @ w + b, with x our inputs, y our outputs, w the weights of the layer (which is of size number of inputs by number of neurons if we don’t transpose like before), and b is the bias vector:\n\ndef lin(x, w, b): return x @ w + b\n\nWe can stack the second layer on top of the first, but since mathematically the composition of two linear operations is another linear operation, this only makes sense if we put something nonlinear in the middle, called an activation function. As mentioned at the beginning of the chapter, in deep learning applications the activation function most commonly used is a ReLU, which returns the maximum of x and 0.\nWe won’t actually train our model in this chapter, so we’ll use random tensors for our inputs and targets. Let’s say our inputs are 200 vectors of size 100, which we group into one batch, and our targets are 200 random floats:\n\nx = torch.randn(200, 100)\ny = torch.randn(200)\n\nFor our two-layer model we will need two weight matrices and two bias vectors. Let’s say we have a hidden size of 50 and the output size is 1 (for one of our inputs, the corresponding output is one float in this toy example). We initialize the weights randomly and the bias at zero:\n\nw1 = torch.randn(100,50)\nb1 = torch.zeros(50)\nw2 = torch.randn(50,1)\nb2 = torch.zeros(1)\n\nThen the result of our first layer is simply:\n\nl1 = lin(x, w1, b1)\nl1.shape\n\ntorch.Size([200, 50])\n\n\nNote that this formula works with our batch of inputs, and returns a batch of hidden state: l1 is a matrix of size 200 (our batch size) by 50 (our hidden size).\nThere is a problem with the way our model was initialized, however. To understand it, we need to look at the mean and standard deviation (std) of l1:\n\nl1.mean(), l1.std()\n\n(tensor(0.0019), tensor(10.1058))\n\n\nThe mean is close to zero, which is understandable since both our input and weight matrices have means close to zero. But the standard deviation, which represents how far away our activations go from the mean, went from 1 to 10. This is a really big problem because that’s with just one layer. Modern neural nets can have hundred of layers, so if each of them multiplies the scale of our activations by 10, by the end of the last layer we won’t have numbers representable by a computer.\nIndeed, if we make just 50 multiplications between x and random matrices of size 100×100, we’ll have:\n\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ torch.randn(100,100)\nx[0:5,0:5]\n\ntensor([[nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan]])\n\n\nThe result is nans everywhere. So maybe the scale of our matrix was too big, and we need to have smaller weights? But if we use too small weights, we will have the opposite problem—the scale of our activations will go from 1 to 0.1, and after 50 layers we’ll be left with zeros everywhere:\n\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ (torch.randn(100,100) * 0.01)\nx[0:5,0:5]\n\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\nSo we have to scale our weight matrices exactly right so that the standard deviation of our activations stays at 1. We can compute the exact value to use mathematically, as illustrated by Xavier Glorot and Yoshua Bengio in “Understanding the Difficulty of Training Deep Feedforward Neural Networks”. The right scale for a given layer is \\(1/\\sqrt{n_{in}}\\), where \\(n_{in}\\) represents the number of inputs.\nIn our case, if we have 100 inputs, we should scale our weight matrices by 0.1:\n\nx = torch.randn(200, 100)\nfor i in range(50): x = x @ (torch.randn(100,100) * 0.1)\nx[0:5,0:5]\n\ntensor([[ 0.7554,  0.6167, -0.1757, -1.5662,  0.5644],\n        [-0.1987,  0.6292,  0.3283, -1.1538,  0.5416],\n        [ 0.6106,  0.2556, -0.0618, -0.9463,  0.4445],\n        [ 0.4484,  0.7144,  0.1164, -0.8626,  0.4413],\n        [ 0.3463,  0.5930,  0.3375, -0.9486,  0.5643]])\n\n\nFinally some numbers that are neither zeros nor nans! Notice how stable the scale of our activations is, even after those 50 fake layers:\n\nx.std()\n\ntensor(0.7042)\n\n\nIf you play a little bit with the value for scale you’ll notice that even a slight variation from 0.1 will get you either to very small or very large numbers, so initializing the weights properly is extremely important.\nLet’s go back to our neural net. Since we messed a bit with our inputs, we need to redefine them:\n\nx = torch.randn(200, 100)\ny = torch.randn(200)\n\nAnd for our weights, we’ll use the right scale, which is known as Xavier initialization (or Glorot initialization):\n\nfrom math import sqrt\nw1 = torch.randn(100,50) / sqrt(100)\nb1 = torch.zeros(50)\nw2 = torch.randn(50,1) / sqrt(50)\nb2 = torch.zeros(1)\n\nNow if we compute the result of the first layer, we can check that the mean and standard deviation are under control:\n\nl1 = lin(x, w1, b1)\nl1.mean(),l1.std()\n\n(tensor(-0.0050), tensor(1.0000))\n\n\nVery good. Now we need to go through a ReLU, so let’s define one. A ReLU removes the negatives and replaces them with zeros, which is another way of saying it clamps our tensor at zero:\n\ndef relu(x): return x.clamp_min(0.)\n\nWe pass our activations through this:\n\nl2 = relu(l1)\nl2.mean(),l2.std()\n\n(tensor(0.3961), tensor(0.5783))\n\n\nAnd we’re back to square one: the mean of our activations has gone to 0.4 (which is understandable since we removed the negatives) and the std went down to 0.58. So like before, after a few layers we will probably wind up with zeros:\n\nx = torch.randn(200, 100)\nfor i in range(50): x = relu(x @ (torch.randn(100,100) * 0.1))\nx[0:5,0:5]\n\ntensor([[0.0000e+00, 1.9689e-08, 4.2820e-08, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 1.6701e-08, 4.3501e-08, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 1.0976e-08, 3.0411e-08, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 1.8457e-08, 4.9469e-08, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 1.9949e-08, 4.1643e-08, 0.0000e+00, 0.0000e+00]])\n\n\nThis means our initialization wasn’t right. Why? At the time Glorot and Bengio wrote their article, the popular activation in a neural net was the hyperbolic tangent (tanh, which is the one they used), and that initialization doesn’t account for our ReLU. Fortunately, someone else has done the math for us and computed the right scale for us to use. In “Delving Deep into Rectifiers: Surpassing Human-Level Performance” (which we’ve seen before—it’s the article that introduced the ResNet), Kaiming He et al. show that we should use the following scale instead: \\(\\sqrt{2 / n_{in}}\\), where \\(n_{in}\\) is the number of inputs of our model. Let’s see what this gives us:\n\nx = torch.randn(200, 100)\nfor i in range(50): x = relu(x @ (torch.randn(100,100) * sqrt(2/100)))\nx[0:5,0:5]\n\ntensor([[0.2871, 0.0000, 0.0000, 0.0000, 0.0026],\n        [0.4546, 0.0000, 0.0000, 0.0000, 0.0015],\n        [0.6178, 0.0000, 0.0000, 0.0180, 0.0079],\n        [0.3333, 0.0000, 0.0000, 0.0545, 0.0000],\n        [0.1940, 0.0000, 0.0000, 0.0000, 0.0096]])\n\n\nThat’s better: our numbers aren’t all zeroed this time. So let’s go back to the definition of our neural net and use this initialization (which is named Kaiming initialization or He initialization):\n\nx = torch.randn(200, 100)\ny = torch.randn(200)\n\n\nw1 = torch.randn(100,50) * sqrt(2 / 100)\nb1 = torch.zeros(50)\nw2 = torch.randn(50,1) * sqrt(2 / 50)\nb2 = torch.zeros(1)\n\nLet’s look at the scale of our activations after going through the first linear layer and ReLU:\n\nl1 = lin(x, w1, b1)\nl2 = relu(l1)\nl2.mean(), l2.std()\n\n(tensor(0.5661), tensor(0.8339))\n\n\nMuch better! Now that our weights are properly initialized, we can define our whole model:\n\ndef model(x):\n    l1 = lin(x, w1, b1)\n    l2 = relu(l1)\n    l3 = lin(l2, w2, b2)\n    return l3\n\nThis is the forward pass. Now all that’s left to do is to compare our output to the labels we have (random numbers, in this example) with a loss function. In this case, we will use the mean squared error. (It’s a toy problem, and this is the easiest loss function to use for what is next, computing the gradients.)\nThe only subtlety is that our outputs and targets don’t have exactly the same shape—after going though the model, we get an output like this:\n\nout = model(x)\nout.shape\n\ntorch.Size([200, 1])\n\n\nTo get rid of this trailing 1 dimension, we use the squeeze function:\n\ndef mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()\n\nAnd now we are ready to compute our loss:\n\nloss = mse(out, y)\n\nThat’s all for the forward pass—let’s now look at the gradients.\n\n\nGradients and the Backward Pass\nWe’ve seen that PyTorch computes all the gradients we need with a magic call to loss.backward, but let’s explore what’s happening behind the scenes.\nNow comes the part where we need to compute the gradients of the loss with respect to all the weights of our model, so all the floats in w1, b1, w2, and b2. For this, we will need a bit of math—specifically the chain rule. This is the rule of calculus that guides how we can compute the derivative of a composed function:\n\\[(g \\circ f)'(x) = g'(f(x)) f'(x)\\]\n\nj: I find this notation very hard to wrap my head around, so instead I like to think of it as: if y = g(u) and u=f(x); then dy/dx = dy/du * du/dx. The two notations mean the same thing, so use whatever works for you.\n\nOur loss is a big composition of different functions: mean squared error (which is in turn the composition of a mean and a power of two), the second linear layer, a ReLU and the first linear layer. For instance, if we want the gradients of the loss with respect to b2 and our loss is defined by:\nloss = mse(out,y) = mse(lin(l2, w2, b2), y)\nThe chain rule tells us that we have: \\[\\frac{\\text{d} loss}{\\text{d} b_{2}} = \\frac{\\text{d} loss}{\\text{d} out} \\times \\frac{\\text{d} out}{\\text{d} b_{2}} = \\frac{\\text{d}}{\\text{d} out} mse(out, y) \\times \\frac{\\text{d}}{\\text{d} b_{2}} lin(l_{2}, w_{2}, b_{2})\\]\nTo compute the gradients of the loss with respect to \\(b_{2}\\), we first need the gradients of the loss with respect to our output \\(out\\). It’s the same if we want the gradients of the loss with respect to \\(w_{2}\\). Then, to get the gradients of the loss with respect to \\(b_{1}\\) or \\(w_{1}\\), we will need the gradients of the loss with respect to \\(l_{1}\\), which in turn requires the gradients of the loss with respect to \\(l_{2}\\), which will need the gradients of the loss with respect to \\(out\\).\nSo to compute all the gradients we need for the update, we need to begin from the output of the model and work our way backward, one layer after the other—which is why this step is known as backpropagation. We can automate it by having each function we implemented (relu, mse, lin) provide its backward step: that is, how to derive the gradients of the loss with respect to the input(s) from the gradients of the loss with respect to the output.\nHere we populate those gradients in an attribute of each tensor, a bit like PyTorch does with .grad.\nThe first are the gradients of the loss with respect to the output of our model (which is the input of the loss function). We undo the squeeze we did in mse, then we use the formula that gives us the derivative of \\(x^{2}\\): \\(2x\\). The derivative of the mean is just \\(1/n\\) where \\(n\\) is the number of elements in our input:\n\ndef mse_grad(inp, targ): \n    # grad of loss with respect to output of previous layer\n    inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]\n\nFor the gradients of the ReLU and our linear layer, we use the gradients of the loss with respect to the output (in out.g) and apply the chain rule to compute the gradients of the loss with respect to the input (in inp.g). The chain rule tells us that inp.g = relu'(inp) * out.g. The derivative of relu is either 0 (when inputs are negative) or 1 (when inputs are positive), so this gives us:\n\ndef relu_grad(inp, out):\n    # grad of relu with respect to input activations\n    inp.g = (inp&gt;0).float() * out.g\n\nThe scheme is the same to compute the gradients of the loss with respect to the inputs, weights, and bias in the linear layer:\n\ndef lin_grad(inp, out, w, b):\n    # grad of matmul with respect to input\n    inp.g = out.g @ w.t()\n    w.g = inp.t() @ out.g\n    b.g = out.g.sum(0)\n\nWe won’t linger on the mathematical formulas that define them since they’re not important for our purposes, but do check out Khan Academy’s excellent calculus lessons if you’re interested in this topic.\n\n\nSidebar: SymPy\nSymPy is a library for symbolic computation that is extremely useful library when working with calculus. Per the documentation:\n\n: Symbolic computation deals with the computation of mathematical objects symbolically. This means that the mathematical objects are represented exactly, not approximately, and mathematical expressions with unevaluated variables are left in symbolic form.\n\nTo do symbolic computation, we first define a symbol, and then do a computation, like so:\n\nfrom sympy import symbols,diff\nsx,sy = symbols('sx sy')\ndiff(sx**2, sx)\n\n\\(\\displaystyle 2 sx\\)\n\n\nHere, SymPy has taken the derivative of x**2 for us! It can take the derivative of complicated compound expressions, simplify and factor equations, and much more. There’s really not much reason for anyone to do calculus manually nowadays—for calculating gradients, PyTorch does it for us, and for showing the equations, SymPy does it for us!\n\n\nEnd sidebar\nOnce we have have defined those functions, we can use them to write the backward pass. Since each gradient is automatically populated in the right tensor, we don’t need to store the results of those _grad functions anywhere—we just need to execute them in the reverse order of the forward pass, to make sure that in each function out.g exists:\n\ndef forward_and_backward(inp, targ):\n    # forward pass:\n    l1 = inp @ w1 + b1\n    l2 = relu(l1)\n    out = l2 @ w2 + b2\n    # we don't actually need the loss in backward!\n    loss = mse(out, targ)\n    \n    # backward pass:\n    mse_grad(out, targ)\n    lin_grad(l2, out, w2, b2)\n    relu_grad(l1, l2)\n    lin_grad(inp, l1, w1, b1)\n\nAnd now we can access the gradients of our model parameters in w1.g, b1.g, w2.g, and b2.g.\nWe have successfully defined our model—now let’s make it a bit more like a PyTorch module.\n\n\nRefactoring the Model\nThe three functions we used have two associated functions: a forward pass and a backward pass. Instead of writing them separately, we can create a class to wrap them together. That class can also store the inputs and outputs for the backward pass. This way, we will just have to call backward:\n\nclass Relu():\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp.clamp_min(0.)\n        return self.out\n    \n    def backward(self): self.inp.g = (self.inp&gt;0).float() * self.out.g\n\n__call__ is a magic name in Python that will make our class callable. This is what will be executed when we type y = Relu()(x). We can do the same for our linear layer and the MSE loss:\n\nclass Lin():\n    def __init__(self, w, b): self.w,self.b = w,b\n        \n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp@self.w + self.b\n        return self.out\n    \n    def backward(self):\n        self.inp.g = self.out.g @ self.w.t()\n        self.w.g = self.inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n\n\nclass Mse():\n    def __call__(self, inp, targ):\n        self.inp = inp\n        self.targ = targ\n        self.out = (inp.squeeze() - targ).pow(2).mean()\n        return self.out\n    \n    def backward(self):\n        x = (self.inp.squeeze()-self.targ).unsqueeze(-1)\n        self.inp.g = 2.*x/self.targ.shape[0]\n\nThen we can put everything in a model that we initiate with our tensors w1, b1, w2, b2:\n\nclass Model():\n    def __init__(self, w1, b1, w2, b2):\n        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n        self.loss = Mse()\n        \n    def __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return self.loss(x, targ)\n    \n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\n\nWhat is really nice about this refactoring and registering things as layers of our model is that the forward and backward passes are now really easy to write. If we want to instantiate our model, we just need to write:\n\nmodel = Model(w1, b1, w2, b2)\n\nThe forward pass can then be executed with:\n\nloss = model(x, y)\n\nAnd the backward pass with:\n\nmodel.backward()\n\n\n\nGoing to PyTorch\nThe Lin, Mse and Relu classes we wrote have a lot in common, so we could make them all inherit from the same base class:\n\nclass LayerFunction():\n    def __call__(self, *args):\n        self.args = args\n        self.out = self.forward(*args)\n        return self.out\n    \n    def forward(self):  raise Exception('not implemented')\n    def bwd(self):      raise Exception('not implemented')\n    def backward(self): self.bwd(self.out, *self.args)\n\nThen we just need to implement forward and bwd in each of our subclasses:\n\nclass Relu(LayerFunction):\n    def forward(self, inp): return inp.clamp_min(0.)\n    def bwd(self, out, inp): inp.g = (inp&gt;0).float() * out.g\n\n\nclass Lin(LayerFunction):\n    def __init__(self, w, b): self.w,self.b = w,b\n        \n    def forward(self, inp): return inp@self.w + self.b\n    \n    def bwd(self, out, inp):\n        inp.g = out.g @ self.w.t()\n        self.w.g = inp.t() @ self.out.g\n        self.b.g = out.g.sum(0)\n\n\nclass Mse(LayerFunction):\n    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n    def bwd(self, out, inp, targ): \n        inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]\n\nThe rest of our model can be the same as before. This is getting closer and closer to what PyTorch does. Each basic function we need to differentiate is written as a torch.autograd.Function object that has a forward and a backward method. PyTorch will then keep trace of any computation we do to be able to properly run the backward pass, unless we set the requires_grad attribute of our tensors to False.\nWriting one of these is (almost) as easy as writing our original classes. The difference is that we choose what to save and what to put in a context variable (so that we make sure we don’t save anything we don’t need), and we return the gradients in the backward pass. It’s very rare to have to write your own Function but if you ever need something exotic or want to mess with the gradients of a regular function, here is how to write one:\n\nfrom torch.autograd import Function\n\nclass MyRelu(Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i.clamp_min(0.)\n        ctx.save_for_backward(i)\n        return result\n    \n    @staticmethod\n    def backward(ctx, grad_output):\n        i, = ctx.saved_tensors\n        return grad_output * (i&gt;0).float()\n\nThe structure used to build a more complex model that takes advantage of those Functions is a torch.nn.Module. This is the base structure for all models, and all the neural nets you have seen up until now inherited from that class. It mostly helps to register all the trainable parameters, which as we’ve seen can be used in the training loop.\nTo implement an nn.Module you just need to:\n\nMake sure the superclass __init__ is called first when you initialize it.\nDefine any parameters of the model as attributes with nn.Parameter.\nDefine a forward function that returns the output of your model.\n\nAs an example, here is the linear layer from scratch:\n\nimport torch.nn as nn\n\nclass LinearLayer(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(n_out, n_in) * sqrt(2/n_in))\n        self.bias = nn.Parameter(torch.zeros(n_out))\n    \n    def forward(self, x): return x @ self.weight.t() + self.bias\n\nAs you see, this class automatically keeps track of what parameters have been defined:\n\nlin = LinearLayer(10,2)\np1,p2 = lin.parameters()\np1.shape,p2.shape\n\n(torch.Size([2, 10]), torch.Size([2]))\n\n\nIt is thanks to this feature of nn.Module that we can just say opt.step() and have an optimizer loop through the parameters and update each one.\nNote that in PyTorch, the weights are stored as an n_out x n_in matrix, which is why we have the transpose in the forward pass.\nBy using the linear layer from PyTorch (which uses the Kaiming initialization as well), the model we have been building up during this chapter can be written like this:\n\nclass Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out))\n        self.loss = mse\n        \n    def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ)\n\nfastai provides its own variant of Module that is identical to nn.Module, but doesn’t require you to call super().__init__() (it does that for you automatically):\n\nclass Model(Module):\n    def __init__(self, n_in, nh, n_out):\n        self.layers = nn.Sequential(\n            nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out))\n        self.loss = mse\n        \n    def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ)\n\nIn the last chapter, we will start from such a model and see how to build a training loop from scratch and refactor it to what we’ve been using in previous chapters."
  },
  {
    "objectID": "Fastbook/17_foundations.html#conclusion",
    "href": "Fastbook/17_foundations.html#conclusion",
    "title": "A Neural Net from the Foundations",
    "section": "Conclusion",
    "text": "Conclusion\nIn this chapter we explored the foundations of deep learning, beginning with matrix multiplication and moving on to implementing the forward and backward passes of a neural net from scratch. We then refactored our code to show how PyTorch works beneath the hood.\nHere are a few things to remember:\n\nA neural net is basically a bunch of matrix multiplications with nonlinearities in between.\nPython is slow, so to write fast code we have to vectorize it and take advantage of techniques such as elementwise arithmetic and broadcasting.\nTwo tensors are broadcastable if the dimensions starting from the end and going backward match (if they are the same, or one of them is 1). To make tensors broadcastable, we may need to add dimensions of size 1 with unsqueeze or a None index.\nProperly initializing a neural net is crucial to get training started. Kaiming initialization should be used when we have ReLU nonlinearities.\nThe backward pass is the chain rule applied multiple times, computing the gradients from the output of our model and going back, one layer at a time.\nWhen subclassing nn.Module (if not using fastai’s Module) we have to call the superclass __init__ method in our __init__ method and we have to define a forward function that takes an input and returns the desired result."
  },
  {
    "objectID": "Fastbook/17_foundations.html#questionnaire",
    "href": "Fastbook/17_foundations.html#questionnaire",
    "title": "A Neural Net from the Foundations",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWrite the Python code to implement a single neuron.\nWrite the Python code to implement ReLU.\nWrite the Python code for a dense layer in terms of matrix multiplication.\nWrite the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python).\nWhat is the “hidden size” of a layer?\nWhat does the t method do in PyTorch?\nWhy is matrix multiplication written in plain Python very slow?\nIn matmul, why is ac==br?\nIn Jupyter Notebook, how do you measure the time taken for a single cell to execute?\nWhat is “elementwise arithmetic”?\nWrite the PyTorch code to test whether every element of a is greater than the corresponding element of b.\nWhat is a rank-0 tensor? How do you convert it to a plain Python data type?\nWhat does this return, and why? tensor([1,2]) + tensor([1])\nWhat does this return, and why? tensor([1,2]) + tensor([1,2,3])\nHow does elementwise arithmetic help us speed up matmul?\nWhat are the broadcasting rules?\nWhat is expand_as? Show an example of how it can be used to match the results of broadcasting.\nHow does unsqueeze help us to solve certain broadcasting problems?\nHow can we use indexing to do the same operation as unsqueeze?\nHow do we show the actual contents of the memory used for a tensor?\nWhen adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)\nDo broadcasting and expand_as result in increased memory use? Why or why not?\nImplement matmul using Einstein summation.\nWhat does a repeated index letter represent on the left-hand side of einsum?\nWhat are the three rules of Einstein summation notation? Why?\nWhat are the forward pass and backward pass of a neural network?\nWhy do we need to store some of the activations calculated for intermediate layers in the forward pass?\nWhat is the downside of having activations with a standard deviation too far away from 1?\nHow can weight initialization help avoid this problem?\nWhat is the formula to initialize weights such that we get a standard deviation of 1 for a plain linear layer, and for a linear layer followed by ReLU?\nWhy do we sometimes have to use the squeeze method in loss functions?\nWhat does the argument to the squeeze method do? Why might it be important to include this argument, even though PyTorch does not require it?\nWhat is the “chain rule”? Show the equation in either of the two forms presented in this chapter.\nShow how to calculate the gradients of mse(lin(l2, w2, b2), y) using the chain rule.\nWhat is the gradient of ReLU? Show it in math or code. (You shouldn’t need to commit this to memory—try to figure it using your knowledge of the shape of the function.)\nIn what order do we need to call the *_grad functions in the backward pass? Why?\nWhat is __call__?\nWhat methods must we implement when writing a torch.autograd.Function?\nWrite nn.Linear from scratch, and test it works.\nWhat is the difference between nn.Module and fastai’s Module?\n\n\nFurther Research\n\nImplement ReLU as a torch.autograd.Function and train a model with it.\nIf you are mathematically inclined, find out what the gradients of a linear layer are in mathematical notation. Map that to the implementation we saw in this chapter.\nLearn about the unfold method in PyTorch, and use it along with matrix multiplication to implement your own 2D convolution function. Then train a CNN that uses it.\nImplement everything in this chapter using NumPy instead of PyTorch."
  },
  {
    "objectID": "Fastbook/clean/app_blog.html#jupyter-for-blogging",
    "href": "Fastbook/clean/app_blog.html#jupyter-for-blogging",
    "title": "Creating a Blog",
    "section": "Jupyter for Blogging",
    "text": "Jupyter for Blogging"
  },
  {
    "objectID": "Fastbook/clean/07_sizing_and_tta.html",
    "href": "Fastbook/clean/07_sizing_and_tta.html",
    "title": "Training a State-of-the-Art Model",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *"
  },
  {
    "objectID": "Fastbook/clean/07_sizing_and_tta.html#imagenette",
    "href": "Fastbook/clean/07_sizing_and_tta.html#imagenette",
    "title": "Training a State-of-the-Art Model",
    "section": "Imagenette",
    "text": "Imagenette\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.IMAGENETTE)\n\n\ndblock = DataBlock(blocks=(ImageBlock(), CategoryBlock()),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=Resize(460),\n                   batch_tfms=aug_transforms(size=224, min_scale=0.75))\ndls = dblock.dataloaders(path, bs=64)\n\n\nmodel = xresnet50(n_out=dls.c)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn.fit_one_cycle(5, 3e-3)"
  },
  {
    "objectID": "Fastbook/clean/07_sizing_and_tta.html#normalization",
    "href": "Fastbook/clean/07_sizing_and_tta.html#normalization",
    "title": "Training a State-of-the-Art Model",
    "section": "Normalization",
    "text": "Normalization\n\nx,y = dls.one_batch()\nx.mean(dim=[0,2,3]),x.std(dim=[0,2,3])\n\n\ndef get_dls(bs, size):\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=Resize(460),\n                   batch_tfms=[*aug_transforms(size=size, min_scale=0.75),\n                               Normalize.from_stats(*imagenet_stats)])\n    return dblock.dataloaders(path, bs=bs)\n\n\ndls = get_dls(64, 224)\n\n\nx,y = dls.one_batch()\nx.mean(dim=[0,2,3]),x.std(dim=[0,2,3])\n\n\nmodel = xresnet50(n_out=dls.c)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn.fit_one_cycle(5, 3e-3)"
  },
  {
    "objectID": "Fastbook/clean/07_sizing_and_tta.html#progressive-resizing",
    "href": "Fastbook/clean/07_sizing_and_tta.html#progressive-resizing",
    "title": "Training a State-of-the-Art Model",
    "section": "Progressive Resizing",
    "text": "Progressive Resizing\n\ndls = get_dls(128, 128)\nlearn = Learner(dls, xresnet50(n_out=dls.c), loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy)\nlearn.fit_one_cycle(4, 3e-3)\n\n\nlearn.dls = get_dls(64, 224)\nlearn.fine_tune(5, 1e-3)"
  },
  {
    "objectID": "Fastbook/clean/07_sizing_and_tta.html#test-time-augmentation",
    "href": "Fastbook/clean/07_sizing_and_tta.html#test-time-augmentation",
    "title": "Training a State-of-the-Art Model",
    "section": "Test Time Augmentation",
    "text": "Test Time Augmentation\n\npreds,targs = learn.tta()\naccuracy(preds, targs).item()"
  },
  {
    "objectID": "Fastbook/clean/07_sizing_and_tta.html#mixup",
    "href": "Fastbook/clean/07_sizing_and_tta.html#mixup",
    "title": "Training a State-of-the-Art Model",
    "section": "Mixup",
    "text": "Mixup\n\nSidebar: Papers and Math\n\n\nEnd sidebar\n\nchurch = PILImage.create(get_image_files_sorted(path/'train'/'n03028079')[0])\ngas = PILImage.create(get_image_files_sorted(path/'train'/'n03425413')[0])\nchurch = church.resize((256,256))\ngas = gas.resize((256,256))\ntchurch = tensor(church).float() / 255.\ntgas = tensor(gas).float() / 255.\n\n_,axs = plt.subplots(1, 3, figsize=(12,4))\nshow_image(tchurch, ax=axs[0]);\nshow_image(tgas, ax=axs[1]);\nshow_image((0.3*tchurch + 0.7*tgas), ax=axs[2]);"
  },
  {
    "objectID": "Fastbook/clean/07_sizing_and_tta.html#label-smoothing",
    "href": "Fastbook/clean/07_sizing_and_tta.html#label-smoothing",
    "title": "Training a State-of-the-Art Model",
    "section": "Label Smoothing",
    "text": "Label Smoothing\n\nSidebar: Label Smoothing, the Paper\n\n\nEnd sidebar"
  },
  {
    "objectID": "Fastbook/clean/07_sizing_and_tta.html#conclusion",
    "href": "Fastbook/clean/07_sizing_and_tta.html#conclusion",
    "title": "Training a State-of-the-Art Model",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "Fastbook/clean/07_sizing_and_tta.html#questionnaire",
    "href": "Fastbook/clean/07_sizing_and_tta.html#questionnaire",
    "title": "Training a State-of-the-Art Model",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhat is the difference between ImageNet and Imagenette? When is it better to experiment on one versus the other?\nWhat is normalization?\nWhy didn’t we have to care about normalization when using a pretrained model?\nWhat is progressive resizing?\nImplement progressive resizing in your own project. Did it help?\nWhat is test time augmentation? How do you use it in fastai?\nIs using TTA at inference slower or faster than regular inference? Why?\nWhat is Mixup? How do you use it in fastai?\nWhy does Mixup prevent the model from being too confident?\nWhy does training with Mixup for five epochs end up worse than training without Mixup?\nWhat is the idea behind label smoothing?\nWhat problems in your data can label smoothing help with?\nWhen using label smoothing with five categories, what is the target associated with the index 1?\nWhat is the first step to take when you want to prototype quick experiments on a new dataset?\n\n\nFurther Research\n\nUse the fastai documentation to build a function that crops an image to a square in each of the four corners, then implement a TTA method that averages the predictions on a center crop and those four crops. Did it help? Is it better than the TTA method of fastai?\nFind the Mixup paper on arXiv and read it. Pick one or two more recent articles introducing variants of Mixup and read them, then try to implement them on your problem.\nFind the script training Imagenette using Mixup and use it as an example to build a script for a long training on your own project. Execute it and see if it helps.\nRead the sidebar “Label Smoothing, the Paper”, look at the relevant section of the original paper and see if you can follow it. Don’t be afraid to ask for help!"
  },
  {
    "objectID": "Fastbook/clean/01_intro.html",
    "href": "Fastbook/clean/01_intro.html",
    "title": "Your Deep Learning Journey",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *"
  },
  {
    "objectID": "Fastbook/clean/01_intro.html#deep-learning-is-for-everyone",
    "href": "Fastbook/clean/01_intro.html#deep-learning-is-for-everyone",
    "title": "Your Deep Learning Journey",
    "section": "Deep Learning Is for Everyone",
    "text": "Deep Learning Is for Everyone"
  },
  {
    "objectID": "Fastbook/clean/01_intro.html#neural-networks-a-brief-history",
    "href": "Fastbook/clean/01_intro.html#neural-networks-a-brief-history",
    "title": "Your Deep Learning Journey",
    "section": "Neural Networks: A Brief History",
    "text": "Neural Networks: A Brief History"
  },
  {
    "objectID": "Fastbook/clean/01_intro.html#who-we-are",
    "href": "Fastbook/clean/01_intro.html#who-we-are",
    "title": "Your Deep Learning Journey",
    "section": "Who We Are",
    "text": "Who We Are"
  },
  {
    "objectID": "Fastbook/clean/01_intro.html#how-to-learn-deep-learning",
    "href": "Fastbook/clean/01_intro.html#how-to-learn-deep-learning",
    "title": "Your Deep Learning Journey",
    "section": "How to Learn Deep Learning",
    "text": "How to Learn Deep Learning\n\nYour Projects and Your Mindset"
  },
  {
    "objectID": "Fastbook/clean/01_intro.html#the-software-pytorch-fastai-and-jupyter",
    "href": "Fastbook/clean/01_intro.html#the-software-pytorch-fastai-and-jupyter",
    "title": "Your Deep Learning Journey",
    "section": "The Software: PyTorch, fastai, and Jupyter",
    "text": "The Software: PyTorch, fastai, and Jupyter"
  },
  {
    "objectID": "Fastbook/clean/01_intro.html#your-first-model",
    "href": "Fastbook/clean/01_intro.html#your-first-model",
    "title": "Your Deep Learning Journey",
    "section": "Your First Model",
    "text": "Your First Model\n\nGetting a GPU Deep Learning Server\n\n\nRunning Your First Notebook\n\n# CLICK ME\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)/'images'\n\ndef is_cat(x): return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=42,\n    label_func=is_cat, item_tfms=Resize(224))\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n\n\n\nSidebar: This Book Was Written in Jupyter Notebooks\n\n1+1\n\n\nimg = PILImage.create(image_cat())\nimg.to_thumb(192)\n\n\n\nEnd sidebar\n\nuploader = widgets.FileUpload()\nuploader\n\n\n#hide\n# For the book, we can't actually click an upload button, so we fake it\nuploader = SimpleNamespace(data = ['images/chapter1_cat_example.jpg'])\n\n\nimg = PILImage.create(uploader.data[0])\nis_cat,_,probs = learn.predict(img)\nprint(f\"Is this a cat?: {is_cat}.\")\nprint(f\"Probability it's a cat: {probs[1].item():.6f}\")\n\n\n\nWhat Is Machine Learning?\n\ngv('''program[shape=box3d width=1 height=0.7]\ninputs-&gt;program-&gt;results''')\n\n\ngv('''model[shape=box3d width=1 height=0.7]\ninputs-&gt;model-&gt;results; weights-&gt;model''')\n\n\ngv('''ordering=in\nmodel[shape=box3d width=1 height=0.7]\ninputs-&gt;model-&gt;results; weights-&gt;model; results-&gt;performance\nperformance-&gt;weights[constraint=false label=update]''')\n\n\ngv('''model[shape=box3d width=1 height=0.7]\ninputs-&gt;model-&gt;results''')\n\n\n\nWhat Is a Neural Network?\n\n\nA Bit of Deep Learning Jargon\n\ngv('''ordering=in\nmodel[shape=box3d width=1 height=0.7 label=architecture]\ninputs-&gt;model-&gt;predictions; parameters-&gt;model; labels-&gt;loss; predictions-&gt;loss\nloss-&gt;parameters[constraint=false label=update]''')\n\n\n\nLimitations Inherent To Machine Learning\nFrom this picture we can now see some fundamental things about training a deep learning model:\n\nA model cannot be created without data.\nA model can only learn to operate on the patterns seen in the input data used to train it.\nThis learning approach only creates predictions, not recommended actions.\nIt’s not enough to just have examples of input data; we need labels for that data too (e.g., pictures of dogs and cats aren’t enough to train a model; we need a label for each one, saying which ones are dogs, and which are cats).\n\nGenerally speaking, we’ve seen that most organizations that say they don’t have enough data, actually mean they don’t have enough labeled data. If any organization is interested in doing something in practice with a model, then presumably they have some inputs they plan to run their model against. And presumably they’ve been doing that some other way for a while (e.g., manually, or with some heuristic program), so they have data from those processes! For instance, a radiology practice will almost certainly have an archive of medical scans (since they need to be able to check how their patients are progressing over time), but those scans may not have structured labels containing a list of diagnoses or interventions (since radiologists generally create free-text natural language reports, not structured data). We’ll be discussing labeling approaches a lot in this book, because it’s such an important issue in practice.\nSince these kinds of machine learning models can only make predictions (i.e., attempt to replicate labels), this can result in a significant gap between organizational goals and model capabilities. For instance, in this book you’ll learn how to create a recommendation system that can predict what products a user might purchase. This is often used in e-commerce, such as to customize products shown on a home page by showing the highest-ranked items. But such a model is generally created by looking at a user and their buying history (inputs) and what they went on to buy or look at (labels), which means that the model is likely to tell you about products the user already has or already knows about, rather than new products that they are most likely to be interested in hearing about. That’s very different to what, say, an expert at your local bookseller might do, where they ask questions to figure out your taste, and then tell you about authors or series that you’ve never heard of before.\n\n\nHow Our Image Recognizer Works\n\n\nWhat Our Image Recognizer Learned\n\n\nImage Recognizers Can Tackle Non-Image Tasks\n\n\nJargon Recap"
  },
  {
    "objectID": "Fastbook/clean/01_intro.html#deep-learning-is-not-just-for-image-classification",
    "href": "Fastbook/clean/01_intro.html#deep-learning-is-not-just-for-image-classification",
    "title": "Your Deep Learning Journey",
    "section": "Deep Learning Is Not Just for Image Classification",
    "text": "Deep Learning Is Not Just for Image Classification\n\npath = untar_data(URLs.CAMVID_TINY)\ndls = SegmentationDataLoaders.from_label_func(\n    path, bs=8, fnames = get_image_files(path/\"images\"),\n    label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n    codes = np.loadtxt(path/'codes.txt', dtype=str)\n)\n\nlearn = unet_learner(dls, resnet34)\nlearn.fine_tune(8)\n\n\nlearn.show_results(max_n=6, figsize=(7,8))\n\n\nfrom fastai.text.all import *\n\ndls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\nlearn.fine_tune(4, 1e-2)\n\nIf you hit a “CUDA out of memory error” after running this cell, click on the menu Kernel, then restart. Instead of executing the cell above, copy and paste the following code in it:\nfrom fastai.text.all import *\n\ndls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test', bs=32)\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\nlearn.fine_tune(4, 1e-2)\nThis reduces the batch size to 32 (we will explain this later). If you keep hitting the same error, change 32 to 16.\n\nlearn.predict(\"I really liked that movie!\")\n\n\nSidebar: The Order Matters\n\n\nEnd sidebar\n\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ADULT_SAMPLE)\n\ndls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names=\"salary\",\n    cat_names = ['workclass', 'education', 'marital-status', 'occupation',\n                 'relationship', 'race'],\n    cont_names = ['age', 'fnlwgt', 'education-num'],\n    procs = [Categorify, FillMissing, Normalize])\n\nlearn = tabular_learner(dls, metrics=accuracy)\n\n\nlearn.fit_one_cycle(3)\n\n\nfrom fastai.collab import *\npath = untar_data(URLs.ML_SAMPLE)\ndls = CollabDataLoaders.from_csv(path/'ratings.csv')\nlearn = collab_learner(dls, y_range=(0.5,5.5))\nlearn.fine_tune(10)\n\n\nlearn.show_results()\n\n\n\nSidebar: Datasets: Food for Models\n\n\nEnd sidebar"
  },
  {
    "objectID": "Fastbook/clean/01_intro.html#validation-sets-and-test-sets",
    "href": "Fastbook/clean/01_intro.html#validation-sets-and-test-sets",
    "title": "Your Deep Learning Journey",
    "section": "Validation Sets and Test Sets",
    "text": "Validation Sets and Test Sets\n\nUse Judgment in Defining Test Sets"
  },
  {
    "objectID": "Fastbook/clean/01_intro.html#a-choose-your-own-adventure-moment",
    "href": "Fastbook/clean/01_intro.html#a-choose-your-own-adventure-moment",
    "title": "Your Deep Learning Journey",
    "section": "A Choose Your Own Adventure moment",
    "text": "A Choose Your Own Adventure moment"
  },
  {
    "objectID": "Fastbook/clean/01_intro.html#questionnaire",
    "href": "Fastbook/clean/01_intro.html#questionnaire",
    "title": "Your Deep Learning Journey",
    "section": "Questionnaire",
    "text": "Questionnaire\nIt can be hard to know in pages and pages of prose what the key things are that you really need to focus on and remember. So, we’ve prepared a list of questions and suggested steps to complete at the end of each chapter. All the answers are in the text of the chapter, so if you’re not sure about anything here, reread that part of the text and make sure you understand it. Answers to all these questions are also available on the book’s website. You can also visit the forums if you get stuck to get help from other folks studying this material.\nFor more questions, including detailed answers and links to the video timeline, have a look at Radek Osmulski’s aiquizzes.\n\nDo you need these for deep learning?\n\nLots of math T / F\nLots of data T / F\nLots of expensive computers T / F\nA PhD T / F\n\nName five areas where deep learning is now the best in the world.\nWhat was the name of the first device that was based on the principle of the artificial neuron?\nBased on the book of the same name, what are the requirements for parallel distributed processing (PDP)?\nWhat were the two theoretical misunderstandings that held back the field of neural networks?\nWhat is a GPU?\nOpen a notebook and execute a cell containing: 1+1. What happens?\nFollow through each cell of the stripped version of the notebook for this chapter. Before executing each cell, guess what will happen.\nComplete the Jupyter Notebook online appendix.\nWhy is it hard to use a traditional computer program to recognize images in a photo?\nWhat did Samuel mean by “weight assignment”?\nWhat term do we normally use in deep learning for what Samuel called “weights”?\nDraw a picture that summarizes Samuel’s view of a machine learning model.\nWhy is it hard to understand why a deep learning model makes a particular prediction?\nWhat is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy?\nWhat do you need in order to train a model?\nHow could a feedback loop impact the rollout of a predictive policing model?\nDo we always have to use 224×224-pixel images with the cat recognition model?\nWhat is the difference between classification and regression?\nWhat is a validation set? What is a test set? Why do we need them?\nWhat will fastai do if you don’t provide a validation set?\nCan we always use a random sample for a validation set? Why or why not?\nWhat is overfitting? Provide an example.\nWhat is a metric? How does it differ from “loss”?\nHow can pretrained models help?\nWhat is the “head” of a model?\nWhat kinds of features do the early layers of a CNN find? How about the later layers?\nAre image models only useful for photos?\nWhat is an “architecture”?\nWhat is segmentation?\nWhat is y_range used for? When do we need it?\nWhat are “hyperparameters”?\nWhat’s the best way to avoid failures when using AI in an organization?\n\n\nFurther Research\nEach chapter also has a “Further Research” section that poses questions that aren’t fully answered in the text, or gives more advanced assignments. Answers to these questions aren’t on the book’s website; you’ll need to do your own research!\n\nWhy is a GPU useful for deep learning? How is a CPU different, and why is it less effective for deep learning?\nTry to think of three areas where feedback loops might impact the use of machine learning. See if you can find documented examples of that happening in practice."
  },
  {
    "objectID": "Fastbook/clean/04_mnist_basics.html",
    "href": "Fastbook/clean/04_mnist_basics.html",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastai.vision.all import *\nfrom fastbook import *\n\nmatplotlib.rc('image', cmap='Greys')"
  },
  {
    "objectID": "Fastbook/clean/04_mnist_basics.html#pixels-the-foundations-of-computer-vision",
    "href": "Fastbook/clean/04_mnist_basics.html#pixels-the-foundations-of-computer-vision",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "Pixels: The Foundations of Computer Vision",
    "text": "Pixels: The Foundations of Computer Vision"
  },
  {
    "objectID": "Fastbook/clean/04_mnist_basics.html#sidebar-tenacity-and-deep-learning",
    "href": "Fastbook/clean/04_mnist_basics.html#sidebar-tenacity-and-deep-learning",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "Sidebar: Tenacity and Deep Learning",
    "text": "Sidebar: Tenacity and Deep Learning"
  },
  {
    "objectID": "Fastbook/clean/04_mnist_basics.html#end-sidebar",
    "href": "Fastbook/clean/04_mnist_basics.html#end-sidebar",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "End sidebar",
    "text": "End sidebar\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\n#hide\nPath.BASE_PATH = path\n\n\npath.ls()\n\n\n(path/'train').ls()\n\n\nthrees = (path/'train'/'3').ls().sorted()\nsevens = (path/'train'/'7').ls().sorted()\nthrees\n\n\nim3_path = threes[1]\nim3 = Image.open(im3_path)\nim3\n\n\narray(im3)[4:10,4:10]\n\n\ntensor(im3)[4:10,4:10]\n\n\nim3_t = tensor(im3)\ndf = pd.DataFrame(im3_t[4:15,4:22])\ndf.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')"
  },
  {
    "objectID": "Fastbook/clean/04_mnist_basics.html#first-try-pixel-similarity",
    "href": "Fastbook/clean/04_mnist_basics.html#first-try-pixel-similarity",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "First Try: Pixel Similarity",
    "text": "First Try: Pixel Similarity\n\nseven_tensors = [tensor(Image.open(o)) for o in sevens]\nthree_tensors = [tensor(Image.open(o)) for o in threes]\nlen(three_tensors),len(seven_tensors)\n\n\nshow_image(three_tensors[1]);\n\n\nstacked_sevens = torch.stack(seven_tensors).float()/255\nstacked_threes = torch.stack(three_tensors).float()/255\nstacked_threes.shape\n\n\nlen(stacked_threes.shape)\n\n\nstacked_threes.ndim\n\n\nmean3 = stacked_threes.mean(0)\nshow_image(mean3);\n\n\nmean7 = stacked_sevens.mean(0)\nshow_image(mean7);\n\n\na_3 = stacked_threes[1]\nshow_image(a_3);\n\n\ndist_3_abs = (a_3 - mean3).abs().mean()\ndist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\ndist_3_abs,dist_3_sqr\n\n\ndist_7_abs = (a_3 - mean7).abs().mean()\ndist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\ndist_7_abs,dist_7_sqr\n\n\nF.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt()\n\n\nNumPy Arrays and PyTorch Tensors\n\ndata = [[1,2,3],[4,5,6]]\narr = array (data)\ntns = tensor(data)\n\n\narr  # numpy\n\n\ntns  # pytorch\n\n\ntns[1]\n\n\ntns[:,1]\n\n\ntns[1,1:3]\n\n\ntns+1\n\n\ntns.type()\n\n\ntns*1.5"
  },
  {
    "objectID": "Fastbook/clean/04_mnist_basics.html#computing-metrics-using-broadcasting",
    "href": "Fastbook/clean/04_mnist_basics.html#computing-metrics-using-broadcasting",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "Computing Metrics Using Broadcasting",
    "text": "Computing Metrics Using Broadcasting\n\nvalid_3_tens = torch.stack([tensor(Image.open(o)) \n                            for o in (path/'valid'/'3').ls()])\nvalid_3_tens = valid_3_tens.float()/255\nvalid_7_tens = torch.stack([tensor(Image.open(o)) \n                            for o in (path/'valid'/'7').ls()])\nvalid_7_tens = valid_7_tens.float()/255\nvalid_3_tens.shape,valid_7_tens.shape\n\n\ndef mnist_distance(a,b): return (a-b).abs().mean((-1,-2))\nmnist_distance(a_3, mean3)\n\n\nvalid_3_dist = mnist_distance(valid_3_tens, mean3)\nvalid_3_dist, valid_3_dist.shape\n\n\ntensor([1,2,3]) + tensor(1)\n\n\n(valid_3_tens-mean3).shape\n\n\ndef is_3(x): return mnist_distance(x,mean3) &lt; mnist_distance(x,mean7)\n\n\nis_3(a_3), is_3(a_3).float()\n\n\nis_3(valid_3_tens)\n\n\naccuracy_3s =      is_3(valid_3_tens).float() .mean()\naccuracy_7s = (1 - is_3(valid_7_tens).float()).mean()\n\naccuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2"
  },
  {
    "objectID": "Fastbook/clean/04_mnist_basics.html#stochastic-gradient-descent-sgd",
    "href": "Fastbook/clean/04_mnist_basics.html#stochastic-gradient-descent-sgd",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "Stochastic Gradient Descent (SGD)",
    "text": "Stochastic Gradient Descent (SGD)\n\ngv('''\ninit-&gt;predict-&gt;loss-&gt;gradient-&gt;step-&gt;stop\nstep-&gt;predict[label=repeat]\n''')\n\n\ndef f(x): return x**2\n\n\nplot_function(f, 'x', 'x**2')\n\n\nplot_function(f, 'x', 'x**2')\nplt.scatter(-1.5, f(-1.5), color='red');\n\n\nCalculating Gradients\n\nxt = tensor(3.).requires_grad_()\n\n\nyt = f(xt)\nyt\n\n\nyt.backward()\n\n\nxt.grad\n\n\nxt = tensor([3.,4.,10.]).requires_grad_()\nxt\n\n\ndef f(x): return (x**2).sum()\n\nyt = f(xt)\nyt\n\n\nyt.backward()\nxt.grad\n\n\n\nStepping With a Learning Rate\n\n\nAn End-to-End SGD Example\n\ntime = torch.arange(0,20).float(); time\n\n\nspeed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1\nplt.scatter(time,speed);\n\n\ndef f(t, params):\n    a,b,c = params\n    return a*(t**2) + (b*t) + c\n\n\ndef mse(preds, targets): return ((preds-targets)**2).mean()\n\n\nStep 1: Initialize the parameters\n\nparams = torch.randn(3).requires_grad_()\n\n\n#hide\norig_params = params.clone()\n\n\n\nStep 2: Calculate the predictions\n\npreds = f(time, params)\n\n\ndef show_preds(preds, ax=None):\n    if ax is None: ax=plt.subplots()[1]\n    ax.scatter(time, speed)\n    ax.scatter(time, to_np(preds), color='red')\n    ax.set_ylim(-300,100)\n\n\nshow_preds(preds)\n\n\n\nStep 3: Calculate the loss\n\nloss = mse(preds, speed)\nloss\n\n\n\nStep 4: Calculate the gradients\n\nloss.backward()\nparams.grad\n\n\nparams.grad * 1e-5\n\n\nparams\n\n\n\nStep 5: Step the weights.\n\nlr = 1e-5\nparams.data -= lr * params.grad.data\nparams.grad = None\n\n\npreds = f(time,params)\nmse(preds, speed)\n\n\nshow_preds(preds)\n\n\ndef apply_step(params, prn=True):\n    preds = f(time, params)\n    loss = mse(preds, speed)\n    loss.backward()\n    params.data -= lr * params.grad.data\n    params.grad = None\n    if prn: print(loss.item())\n    return preds\n\n\n\nStep 6: Repeat the process\n\nfor i in range(10): apply_step(params)\n\n\n#hide\nparams = orig_params.detach().requires_grad_()\n\n\n_,axs = plt.subplots(1,4,figsize=(12,3))\nfor ax in axs: show_preds(apply_step(params, False), ax)\nplt.tight_layout()\n\n\n\nStep 7: stop\n\n\n\nSummarizing Gradient Descent\n\ngv('''\ninit-&gt;predict-&gt;loss-&gt;gradient-&gt;step-&gt;stop\nstep-&gt;predict[label=repeat]\n''')"
  },
  {
    "objectID": "Fastbook/clean/04_mnist_basics.html#the-mnist-loss-function",
    "href": "Fastbook/clean/04_mnist_basics.html#the-mnist-loss-function",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "The MNIST Loss Function",
    "text": "The MNIST Loss Function\n\ntrain_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n\n\ntrain_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\ntrain_x.shape,train_y.shape\n\n\ndset = list(zip(train_x,train_y))\nx,y = dset[0]\nx.shape,y\n\n\nvalid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\nvalid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\nvalid_dset = list(zip(valid_x,valid_y))\n\n\ndef init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n\n\nweights = init_params((28*28,1))\n\n\nbias = init_params(1)\n\n\n(train_x[0]*weights.T).sum() + bias\n\n\ndef linear1(xb): return xb@weights + bias\npreds = linear1(train_x)\npreds\n\n\ncorrects = (preds&gt;0.0).float() == train_y\ncorrects\n\n\ncorrects.float().mean().item()\n\n\nwith torch.no_grad(): weights[0] *= 1.0001\n\n\npreds = linear1(train_x)\n((preds&gt;0.0).float() == train_y).float().mean().item()\n\n\ntrgts  = tensor([1,0,1])\nprds   = tensor([0.9, 0.4, 0.2])\n\n\ndef mnist_loss(predictions, targets):\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\n\ntorch.where(trgts==1, 1-prds, prds)\n\n\nmnist_loss(prds,trgts)\n\n\nmnist_loss(tensor([0.9, 0.4, 0.8]),trgts)\n\n\nSigmoid\n\ndef sigmoid(x): return 1/(1+torch.exp(-x))\n\n\nplot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)\n\n\ndef mnist_loss(predictions, targets):\n    predictions = predictions.sigmoid()\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\n\n\nSGD and Mini-Batches\n\ncoll = range(15)\ndl = DataLoader(coll, batch_size=5, shuffle=True)\nlist(dl)\n\n\nds = L(enumerate(string.ascii_lowercase))\nds\n\n\ndl = DataLoader(ds, batch_size=6, shuffle=True)\nlist(dl)"
  },
  {
    "objectID": "Fastbook/clean/04_mnist_basics.html#putting-it-all-together",
    "href": "Fastbook/clean/04_mnist_basics.html#putting-it-all-together",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "Putting It All Together",
    "text": "Putting It All Together\n\nweights = init_params((28*28,1))\nbias = init_params(1)\n\n\ndl = DataLoader(dset, batch_size=256)\nxb,yb = first(dl)\nxb.shape,yb.shape\n\n\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n\n\nbatch = train_x[:4]\nbatch.shape\n\n\npreds = linear1(batch)\npreds\n\n\nloss = mnist_loss(preds, train_y[:4])\nloss\n\n\nloss.backward()\nweights.grad.shape,weights.grad.mean(),bias.grad\n\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(),bias.grad\n\n\ncalc_grad(batch, train_y[:4], linear1)\nweights.grad.mean(),bias.grad\n\n\nweights.grad.zero_()\nbias.grad.zero_();\n\n\ndef train_epoch(model, lr, params):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\n\n(preds&gt;0.0).float() == train_y[:4]\n\n\ndef batch_accuracy(xb, yb):\n    preds = xb.sigmoid()\n    correct = (preds&gt;0.5) == yb\n    return correct.float().mean()\n\n\nbatch_accuracy(linear1(batch), train_y[:4])\n\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\n\nvalidate_epoch(linear1)\n\n\nlr = 1.\nparams = weights,bias\ntrain_epoch(linear1, lr, params)\nvalidate_epoch(linear1)\n\n\nfor i in range(20):\n    train_epoch(linear1, lr, params)\n    print(validate_epoch(linear1), end=' ')\n\n\nCreating an Optimizer\n\nlinear_model = nn.Linear(28*28,1)\n\n\nw,b = linear_model.parameters()\nw.shape,b.shape\n\n\nclass BasicOptim:\n    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n    def step(self, *args, **kwargs):\n        for p in self.params: p.data -= p.grad.data * self.lr\n\n    def zero_grad(self, *args, **kwargs):\n        for p in self.params: p.grad = None\n\n\nopt = BasicOptim(linear_model.parameters(), lr)\n\n\ndef train_epoch(model):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n\n\nvalidate_epoch(linear_model)\n\n\ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=' ')\n\n\ntrain_model(linear_model, 20)\n\n\nlinear_model = nn.Linear(28*28,1)\nopt = SGD(linear_model.parameters(), lr)\ntrain_model(linear_model, 20)\n\n\ndls = DataLoaders(dl, valid_dl)\n\n\nlearn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\n\nlearn.fit(10, lr=lr)"
  },
  {
    "objectID": "Fastbook/clean/04_mnist_basics.html#adding-a-nonlinearity",
    "href": "Fastbook/clean/04_mnist_basics.html#adding-a-nonlinearity",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "Adding a Nonlinearity",
    "text": "Adding a Nonlinearity\n\ndef simple_net(xb): \n    res = xb@w1 + b1\n    res = res.max(tensor(0.0))\n    res = res@w2 + b2\n    return res\n\n\nw1 = init_params((28*28,30))\nb1 = init_params(30)\nw2 = init_params((30,1))\nb2 = init_params(1)\n\n\nplot_function(F.relu)\n\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28,30),\n    nn.ReLU(),\n    nn.Linear(30,1)\n)\n\n\nlearn = Learner(dls, simple_net, opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n\n\nlearn.fit(40, 0.1)\n\n\nplt.plot(L(learn.recorder.values).itemgot(2));\n\n\nlearn.recorder.values[-1][2]\n\n\nGoing Deeper\n\ndls = ImageDataLoaders.from_folder(path)\nlearn = vision_learner(dls, resnet18, pretrained=False,\n                    loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(1, 0.1)"
  },
  {
    "objectID": "Fastbook/clean/04_mnist_basics.html#jargon-recap",
    "href": "Fastbook/clean/04_mnist_basics.html#jargon-recap",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "Jargon Recap",
    "text": "Jargon Recap"
  },
  {
    "objectID": "Fastbook/clean/04_mnist_basics.html#questionnaire",
    "href": "Fastbook/clean/04_mnist_basics.html#questionnaire",
    "title": "Under the Hood: Training a Digit Classifier",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nHow is a grayscale image represented on a computer? How about a color image?\nHow are the files and folders in the MNIST_SAMPLE dataset structured? Why?\nExplain how the “pixel similarity” approach to classifying digits works.\nWhat is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\nWhat is a “rank-3 tensor”?\nWhat is the difference between tensor rank and shape? How do you get the rank from the shape?\nWhat are RMSE and L1 norm?\nHow can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?\nCreate a 3×3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.\nWhat is broadcasting?\nAre metrics generally calculated using the training set, or the validation set? Why?\nWhat is SGD?\nWhy does SGD use mini-batches?\nWhat are the seven steps in SGD for machine learning?\nHow do we initialize the weights in a model?\nWhat is “loss”?\nWhy can’t we always use a high learning rate?\nWhat is a “gradient”?\nDo you need to know how to calculate gradients yourself?\nWhy can’t we use accuracy as a loss function?\nDraw the sigmoid function. What is special about its shape?\nWhat is the difference between a loss function and a metric?\nWhat is the function to calculate new weights using a learning rate?\nWhat does the DataLoader class do?\nWrite pseudocode showing the basic steps taken in each epoch for SGD.\nCreate a function that, if passed two arguments [1,2,3,4] and 'abcd', returns [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]. What is special about that output data structure?\nWhat does view do in PyTorch?\nWhat are the “bias” parameters in a neural network? Why do we need them?\nWhat does the @ operator do in Python?\nWhat does the backward method do?\nWhy do we have to zero the gradients?\nWhat information do we have to pass to Learner?\nShow Python or pseudocode for the basic steps of a training loop.\nWhat is “ReLU”? Draw a plot of it for values from -2 to +2.\nWhat is an “activation function”?\nWhat’s the difference between F.relu and nn.ReLU?\nThe universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?\n\n\nFurther Research\n\nCreate your own implementation of Learner from scratch, based on the training loop shown in this chapter.\nComplete all the steps in this chapter using the full MNIST datasets (that is, for all digits, not just 3s and 7s). This is a significant project and will take you quite a bit of time to complete! You’ll need to do some of your own research to figure out how to overcome some obstacles you’ll meet on the way."
  },
  {
    "objectID": "Fastbook/clean/06_multicat.html",
    "href": "Fastbook/clean/06_multicat.html",
    "title": "Other Computer Vision Problems",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *"
  },
  {
    "objectID": "Fastbook/clean/06_multicat.html#multi-label-classification",
    "href": "Fastbook/clean/06_multicat.html#multi-label-classification",
    "title": "Other Computer Vision Problems",
    "section": "Multi-Label Classification",
    "text": "Multi-Label Classification\n\nThe Data\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.PASCAL_2007)\n\n\ndf = pd.read_csv(path/'train.csv')\ndf.head()\n\n\n\nSidebar: Pandas and DataFrames\n\ndf.iloc[:,0]\n\n\ndf.iloc[0,:]\n# Trailing :s are always optional (in numpy, pytorch, pandas, etc.),\n#   so this is equivalent:\ndf.iloc[0]\n\n\ndf['fname']\n\n\ntmp_df = pd.DataFrame({'a':[1,2], 'b':[3,4]})\ntmp_df\n\n\ntmp_df['c'] = tmp_df['a']+tmp_df['b']\ntmp_df\n\n\n\nEnd sidebar\n\n\nConstructing a DataBlock\n\ndblock = DataBlock()\n\n\ndsets = dblock.datasets(df)\n\n\nlen(dsets.train),len(dsets.valid)\n\n\nx,y = dsets.train[0]\nx,y\n\n\nx['fname']\n\n\ndblock = DataBlock(get_x = lambda r: r['fname'], get_y = lambda r: r['labels'])\ndsets = dblock.datasets(df)\ndsets.train[0]\n\n\ndef get_x(r): return r['fname']\ndef get_y(r): return r['labels']\ndblock = DataBlock(get_x = get_x, get_y = get_y)\ndsets = dblock.datasets(df)\ndsets.train[0]\n\n\ndef get_x(r): return path/'train'/r['fname']\ndef get_y(r): return r['labels'].split(' ')\ndblock = DataBlock(get_x = get_x, get_y = get_y)\ndsets = dblock.datasets(df)\ndsets.train[0]\n\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   get_x = get_x, get_y = get_y)\ndsets = dblock.datasets(df)\ndsets.train[0]\n\n\nidxs = torch.where(dsets.train[0][1]==1.)[0]\ndsets.train.vocab[idxs]\n\n\ndef splitter(df):\n    train = df.index[~df['is_valid']].tolist()\n    valid = df.index[df['is_valid']].tolist()\n    return train,valid\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y)\n\ndsets = dblock.datasets(df)\ndsets.train[0]\n\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y,\n                   item_tfms = RandomResizedCrop(128, min_scale=0.35))\ndls = dblock.dataloaders(df)\n\n\ndls.show_batch(nrows=1, ncols=3)\n\n\n\nBinary Cross-Entropy\n\nlearn = vision_learner(dls, resnet18)\n\n\nx,y = to_cpu(dls.train.one_batch())\nactivs = learn.model(x)\nactivs.shape\n\n\nactivs[0]\n\n\ndef binary_cross_entropy(inputs, targets):\n    inputs = inputs.sigmoid()\n    return -torch.where(targets==1, inputs, 1-inputs).log().mean()\n\n\nloss_func = nn.BCEWithLogitsLoss()\nloss = loss_func(activs, y)\nloss\n\n\ndef say_hello(name, say_what=\"Hello\"): return f\"{say_what} {name}.\"\nsay_hello('Jeremy'),say_hello('Jeremy', 'Ahoy!')\n\n\nf = partial(say_hello, say_what=\"Bonjour\")\nf(\"Jeremy\"),f(\"Sylvain\")\n\n\nlearn = vision_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)\n\n\nlearn.metrics = partial(accuracy_multi, thresh=0.1)\nlearn.validate()\n\n\nlearn.metrics = partial(accuracy_multi, thresh=0.99)\nlearn.validate()\n\n\npreds,targs = learn.get_preds()\n\n\naccuracy_multi(preds, targs, thresh=0.9, sigmoid=False)\n\n\nxs = torch.linspace(0.05,0.95,29)\naccs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs]\nplt.plot(xs,accs);"
  },
  {
    "objectID": "Fastbook/clean/06_multicat.html#regression",
    "href": "Fastbook/clean/06_multicat.html#regression",
    "title": "Other Computer Vision Problems",
    "section": "Regression",
    "text": "Regression\n\nAssemble the Data\n\npath = untar_data(URLs.BIWI_HEAD_POSE)\n\n\n#hide\nPath.BASE_PATH = path\n\n\npath.ls().sorted()\n\n\n(path/'01').ls().sorted()\n\n\nimg_files = get_image_files(path)\ndef img2pose(x): return Path(f'{str(x)[:-7]}pose.txt')\nimg2pose(img_files[0])\n\n\nim = PILImage.create(img_files[0])\nim.shape\n\n\nim.to_thumb(160)\n\n\ncal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)\ndef get_ctr(f):\n    ctr = np.genfromtxt(img2pose(f), skip_header=3)\n    c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n    c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n    return tensor([c1,c2])\n\n\nget_ctr(img_files[0])\n\n\nbiwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    get_y=get_ctr,\n    splitter=FuncSplitter(lambda o: o.parent.name=='13'),\n    batch_tfms=aug_transforms(size=(240,320)), \n)\n\n\ndls = biwi.dataloaders(path)\ndls.show_batch(max_n=9, figsize=(8,6))\n\n\nxb,yb = dls.one_batch()\nxb.shape,yb.shape\n\n\nyb[0]\n\n\n\nTraining a Model\n\nlearn = vision_learner(dls, resnet18, y_range=(-1,1))\n\n\ndef sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo\n\n\nplot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4)\n\n\ndls.loss_func\n\n\nlearn.lr_find()\n\n\nlr = 1e-2\nlearn.fine_tune(3, lr)\n\n\nmath.sqrt(0.0001)\n\n\nlearn.show_results(ds_idx=1, nrows=3, figsize=(6,8))"
  },
  {
    "objectID": "Fastbook/clean/06_multicat.html#conclusion",
    "href": "Fastbook/clean/06_multicat.html#conclusion",
    "title": "Other Computer Vision Problems",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "Fastbook/clean/06_multicat.html#questionnaire",
    "href": "Fastbook/clean/06_multicat.html#questionnaire",
    "title": "Other Computer Vision Problems",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nHow could multi-label classification improve the usability of the bear classifier?\nHow do we encode the dependent variable in a multi-label classification problem?\nHow do you access the rows and columns of a DataFrame as if it was a matrix?\nHow do you get a column by name from a DataFrame?\nWhat is the difference between a Dataset and DataLoader?\nWhat does a Datasets object normally contain?\nWhat does a DataLoaders object normally contain?\nWhat does lambda do in Python?\nWhat are the methods to customize how the independent and dependent variables are created with the data block API?\nWhy is softmax not an appropriate output activation function when using a one hot encoded target?\nWhy is nll_loss not an appropriate loss function when using a one-hot-encoded target?\nWhat is the difference between nn.BCELoss and nn.BCEWithLogitsLoss?\nWhy can’t we use regular accuracy in a multi-label problem?\nWhen is it okay to tune a hyperparameter on the validation set?\nHow is y_range implemented in fastai? (See if you can implement it yourself and test it without peeking!)\nWhat is a regression problem? What loss function should you use for such a problem?\nWhat do you need to do to make sure the fastai library applies the same data augmentation to your input images and your target point coordinates?\n\n\nFurther Research\n\nRead a tutorial about Pandas DataFrames and experiment with a few methods that look interesting to you. See the book’s website for recommended tutorials.\nRetrain the bear classifier using multi-label classification. See if you can make it work effectively with images that don’t contain any bears, including showing that information in the web application. Try an image with two different kinds of bears. Check whether the accuracy on the single-label dataset is impacted using multi-label classification."
  },
  {
    "objectID": "Fastbook/clean/09_tabular.html",
    "href": "Fastbook/clean/09_tabular.html",
    "title": "Tabular Modeling Deep Dive",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook kaggle waterfallcharts treeinterpreter dtreeviz==1.4.1\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG\n\npd.options.display.max_rows = 20\npd.options.display.max_columns = 8"
  },
  {
    "objectID": "Fastbook/clean/09_tabular.html#categorical-embeddings",
    "href": "Fastbook/clean/09_tabular.html#categorical-embeddings",
    "title": "Tabular Modeling Deep Dive",
    "section": "Categorical Embeddings",
    "text": "Categorical Embeddings"
  },
  {
    "objectID": "Fastbook/clean/09_tabular.html#beyond-deep-learning",
    "href": "Fastbook/clean/09_tabular.html#beyond-deep-learning",
    "title": "Tabular Modeling Deep Dive",
    "section": "Beyond Deep Learning",
    "text": "Beyond Deep Learning"
  },
  {
    "objectID": "Fastbook/clean/09_tabular.html#the-dataset",
    "href": "Fastbook/clean/09_tabular.html#the-dataset",
    "title": "Tabular Modeling Deep Dive",
    "section": "The Dataset",
    "text": "The Dataset\n\nKaggle Competitions\n\ncreds = ''\n\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\n\ncomp = 'bluebook-for-bulldozers'\npath = URLs.path(comp)\npath\n\n\n#hide\nPath.BASE_PATH = path\n\n\nfrom kaggle import api\n\nif not path.exists():\n    path.mkdir(parents=true)\n    api.competition_download_cli(comp, path=path)\n    shutil.unpack_archive(str(path/f'{comp}.zip'), str(path))\n\npath.ls(file_type='text')\n\n\n\nLook at the Data\n\ndf = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\n\n\ndf.columns\n\n\ndf['ProductSize'].unique()\n\n\nsizes = 'Large','Large / Medium','Medium','Small','Mini','Compact'\n\n\ndf['ProductSize'] = df['ProductSize'].astype('category')\ndf['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)\n\n\ndep_var = 'SalePrice'\n\n\ndf[dep_var] = np.log(df[dep_var])"
  },
  {
    "objectID": "Fastbook/clean/09_tabular.html#decision-trees",
    "href": "Fastbook/clean/09_tabular.html#decision-trees",
    "title": "Tabular Modeling Deep Dive",
    "section": "Decision Trees",
    "text": "Decision Trees\n\nHandling Dates\n\ndf = add_datepart(df, 'saledate')\n\n\ndf_test = pd.read_csv(path/'Test.csv', low_memory=False)\ndf_test = add_datepart(df_test, 'saledate')\n\n\n' '.join(o for o in df.columns if o.startswith('sale'))\n\n\n\nUsing TabularPandas and TabularProc\n\nprocs = [Categorify, FillMissing]\n\n\ncond = (df.saleYear&lt;2011) | (df.saleMonth&lt;10)\ntrain_idx = np.where( cond)[0]\nvalid_idx = np.where(~cond)[0]\n\nsplits = (list(train_idx),list(valid_idx))\n\n\ncont,cat = cont_cat_split(df, 1, dep_var=dep_var)\n\n\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)\n\n\nlen(to.train),len(to.valid)\n\n\nto.show(3)\n\n\nto1 = TabularPandas(df, procs, ['state', 'ProductGroup', 'Drive_System', 'Enclosure'], [], y_names=dep_var, splits=splits)\nto1.show(3)\n\n\nto.items.head(3)\n\n\nto1.items[['state', 'ProductGroup', 'Drive_System', 'Enclosure']].head(3)\n\n\nto.classes['ProductSize']\n\n\nsave_pickle(path/'to.pkl',to)\n\n\n\nCreating the Decision Tree\n\n#hide\nto = load_pickle(path/'to.pkl')\n\n\nxs,y = to.train.xs,to.train.y\nvalid_xs,valid_y = to.valid.xs,to.valid.y\n\n\nm = DecisionTreeRegressor(max_leaf_nodes=4)\nm.fit(xs, y);\n\n\ndraw_tree(m, xs, size=10, leaves_parallel=True, precision=2)\n\n\nsamp_idx = np.random.permutation(len(y))[:500]\ndtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n        fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n        orientation='LR')\n\n\nxs.loc[xs['YearMade']&lt;1900, 'YearMade'] = 1950\nvalid_xs.loc[valid_xs['YearMade']&lt;1900, 'YearMade'] = 1950\n\n\nm = DecisionTreeRegressor(max_leaf_nodes=4).fit(xs, y)\n\ndtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n        fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n        orientation='LR')\n\n\nm = DecisionTreeRegressor()\nm.fit(xs, y);\n\n\ndef r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6)\ndef m_rmse(m, xs, y): return r_mse(m.predict(xs), y)\n\n\nm_rmse(m, xs, y)\n\n\nm_rmse(m, valid_xs, valid_y)\n\n\nm.get_n_leaves(), len(xs)\n\n\nm = DecisionTreeRegressor(min_samples_leaf=25)\nm.fit(to.train.xs, to.train.y)\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n\n\nm.get_n_leaves()\n\n\n\nCategorical Variables"
  },
  {
    "objectID": "Fastbook/clean/09_tabular.html#random-forests",
    "href": "Fastbook/clean/09_tabular.html#random-forests",
    "title": "Tabular Modeling Deep Dive",
    "section": "Random Forests",
    "text": "Random Forests\n\n#hide\n# pip install —pre -f https://sklearn-nightly.scdn8.secure.raxcdn.com scikit-learn —U\n\n\nCreating a Random Forest\n\ndef rf(xs, y, n_estimators=40, max_samples=200_000,\n       max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,\n        max_samples=max_samples, max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\n\n\nm = rf(xs, y);\n\n\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n\n\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_])\n\n\nr_mse(preds.mean(0), valid_y)\n\n\nplt.plot([r_mse(preds[:i+1].mean(0), valid_y) for i in range(40)]);\n\n\n\nOut-of-Bag Error\n\nr_mse(m.oob_prediction_, y)"
  },
  {
    "objectID": "Fastbook/clean/09_tabular.html#model-interpretation",
    "href": "Fastbook/clean/09_tabular.html#model-interpretation",
    "title": "Tabular Modeling Deep Dive",
    "section": "Model Interpretation",
    "text": "Model Interpretation\n\nTree Variance for Prediction Confidence\n\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_])\n\n\npreds.shape\n\n\npreds_std = preds.std(0)\n\n\npreds_std[:5]\n\n\n\nFeature Importance\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n\n\nfi = rf_feat_importance(m, xs)\nfi[:10]\n\n\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30]);\n\n\n\nRemoving Low-Importance Variables\n\nto_keep = fi[fi.imp&gt;0.005].cols\nlen(to_keep)\n\n\nxs_imp = xs[to_keep]\nvalid_xs_imp = valid_xs[to_keep]\n\n\nm = rf(xs_imp, y)\n\n\nm_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y)\n\n\nlen(xs.columns), len(xs_imp.columns)\n\n\nplot_fi(rf_feat_importance(m, xs_imp));\n\n\n\nRemoving Redundant Features\n\ncluster_columns(xs_imp)\n\n\ndef get_oob(df):\n    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,\n        max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(df, y)\n    return m.oob_score_\n\n\nget_oob(xs_imp)\n\n\n{c:get_oob(xs_imp.drop(c, axis=1)) for c in (\n    'saleYear', 'saleElapsed', 'ProductGroupDesc','ProductGroup',\n    'fiModelDesc', 'fiBaseModel',\n    'Hydraulics_Flow','Grouser_Tracks', 'Coupler_System')}\n\n\nto_drop = ['saleYear', 'ProductGroupDesc', 'fiBaseModel', 'Grouser_Tracks']\nget_oob(xs_imp.drop(to_drop, axis=1))\n\n\nxs_final = xs_imp.drop(to_drop, axis=1)\nvalid_xs_final = valid_xs_imp.drop(to_drop, axis=1)\n\n\nsave_pickle(path/'xs_final.pkl', xs_final)\nsave_pickle(path/'valid_xs_final.pkl', valid_xs_final)\n\n\nxs_final = load_pickle(path/'xs_final.pkl')\nvalid_xs_final = load_pickle(path/'valid_xs_final.pkl')\n\n\nm = rf(xs_final, y)\nm_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y)\n\n\n\nPartial Dependence\n\np = valid_xs_final['ProductSize'].value_counts(sort=False).plot.barh()\nc = to.classes['ProductSize']\nplt.yticks(range(len(c)), c);\n\n\nax = valid_xs_final['YearMade'].hist()\n\n\nfrom sklearn.inspection import plot_partial_dependence\n\nfig,ax = plt.subplots(figsize=(12, 4))\nplot_partial_dependence(m, valid_xs_final, ['YearMade','ProductSize'],\n                        grid_resolution=20, ax=ax);\n\n\n\nData Leakage\n\n\nTree Interpreter\n\n#hide\nimport warnings\nwarnings.simplefilter('ignore', FutureWarning)\n\nfrom treeinterpreter import treeinterpreter\nfrom waterfall_chart import plot as waterfall\n\n\nrow = valid_xs_final.iloc[:5]\n\n\nprediction,bias,contributions = treeinterpreter.predict(m, row.values)\n\n\nprediction[0], bias[0], contributions[0].sum()\n\n\nwaterfall(valid_xs_final.columns, contributions[0], threshold=0.08, \n          rotation_value=45,formatting='{:,.3f}');"
  },
  {
    "objectID": "Fastbook/clean/09_tabular.html#extrapolation-and-neural-networks",
    "href": "Fastbook/clean/09_tabular.html#extrapolation-and-neural-networks",
    "title": "Tabular Modeling Deep Dive",
    "section": "Extrapolation and Neural Networks",
    "text": "Extrapolation and Neural Networks\n\nThe Extrapolation Problem\n\n#hide\nnp.random.seed(42)\n\n\nx_lin = torch.linspace(0,20, steps=40)\ny_lin = x_lin + torch.randn_like(x_lin)\nplt.scatter(x_lin, y_lin);\n\n\nxs_lin = x_lin.unsqueeze(1)\nx_lin.shape,xs_lin.shape\n\n\nx_lin[:,None].shape\n\n\nm_lin = RandomForestRegressor().fit(xs_lin[:30],y_lin[:30])\n\n\nplt.scatter(x_lin, y_lin, 20)\nplt.scatter(x_lin, m_lin.predict(xs_lin), color='red', alpha=0.5);\n\n\n\nFinding Out-of-Domain Data\n\ndf_dom = pd.concat([xs_final, valid_xs_final])\nis_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final))\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:6]\n\n\nm = rf(xs_final, y)\nprint('orig', m_rmse(m, valid_xs_final, valid_y))\n\nfor c in ('SalesID','saleElapsed','MachineID'):\n    m = rf(xs_final.drop(c,axis=1), y)\n    print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y))\n\n\ntime_vars = ['SalesID','MachineID']\nxs_final_time = xs_final.drop(time_vars, axis=1)\nvalid_xs_time = valid_xs_final.drop(time_vars, axis=1)\n\nm = rf(xs_final_time, y)\nm_rmse(m, valid_xs_time, valid_y)\n\n\nxs['saleYear'].hist();\n\n\nfilt = xs['saleYear']&gt;2004\nxs_filt = xs_final_time[filt]\ny_filt = y[filt]\n\n\nm = rf(xs_filt, y_filt)\nm_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y)\n\n\n\nUsing a Neural Network\n\ndf_nn = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\ndf_nn['ProductSize'] = df_nn['ProductSize'].astype('category')\ndf_nn['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)\ndf_nn[dep_var] = np.log(df_nn[dep_var])\ndf_nn = add_datepart(df_nn, 'saledate')\n\n\ndf_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]]\n\n\ncont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)\n\n\ncont_nn\n\n\ndf_nn_final[cat_nn].nunique()\n\n\nxs_filt2 = xs_filt.drop('fiModelDescriptor', axis=1)\nvalid_xs_time2 = valid_xs_time.drop('fiModelDescriptor', axis=1)\nm2 = rf(xs_filt2, y_filt)\nm_rmse(m2, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y)\n\n\ncat_nn.remove('fiModelDescriptor')\n\n\nprocs_nn = [Categorify, FillMissing, Normalize]\nto_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn,\n                      splits=splits, y_names=dep_var)\n\n\ndls = to_nn.dataloaders(1024)\n\n\ny = to_nn.train.y\ny.min(),y.max()\n\n\nlearn = tabular_learner(dls, y_range=(8,12), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\n\n\nlearn.lr_find()\n\n\nlearn.fit_one_cycle(5, 1e-2)\n\n\npreds,targs = learn.get_preds()\nr_mse(preds,targs)\n\n\nlearn.save('nn')\n\n\n\nSidebar: fastai’s Tabular Classes\n\n\nEnd sidebar"
  },
  {
    "objectID": "Fastbook/clean/09_tabular.html#ensembling",
    "href": "Fastbook/clean/09_tabular.html#ensembling",
    "title": "Tabular Modeling Deep Dive",
    "section": "Ensembling",
    "text": "Ensembling\n\nrf_preds = m.predict(valid_xs_time)\nens_preds = (to_np(preds.squeeze()) + rf_preds) /2\n\n\nr_mse(ens_preds,valid_y)\n\n\nBoosting\n\n\nCombining Embeddings with Other Methods"
  },
  {
    "objectID": "Fastbook/clean/09_tabular.html#conclusion-our-advice-for-tabular-modeling",
    "href": "Fastbook/clean/09_tabular.html#conclusion-our-advice-for-tabular-modeling",
    "title": "Tabular Modeling Deep Dive",
    "section": "Conclusion: Our Advice for Tabular Modeling",
    "text": "Conclusion: Our Advice for Tabular Modeling"
  },
  {
    "objectID": "Fastbook/clean/09_tabular.html#questionnaire",
    "href": "Fastbook/clean/09_tabular.html#questionnaire",
    "title": "Tabular Modeling Deep Dive",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhat is a continuous variable?\nWhat is a categorical variable?\nProvide two of the words that are used for the possible values of a categorical variable.\nWhat is a “dense layer”?\nHow do entity embeddings reduce memory usage and speed up neural networks?\nWhat kinds of datasets are entity embeddings especially useful for?\nWhat are the two main families of machine learning algorithms?\nWhy do some categorical columns need a special ordering in their classes? How do you do this in Pandas?\nSummarize what a decision tree algorithm does.\nWhy is a date different from a regular categorical or continuous variable, and how can you preprocess it to allow it to be used in a model?\nShould you pick a random validation set in the bulldozer competition? If no, what kind of validation set should you pick?\nWhat is pickle and what is it useful for?\nHow are mse, samples, and values calculated in the decision tree drawn in this chapter?\nHow do we deal with outliers, before building a decision tree?\nHow do we handle categorical variables in a decision tree?\nWhat is bagging?\nWhat is the difference between max_samples and max_features when creating a random forest?\nIf you increase n_estimators to a very high value, can that lead to overfitting? Why or why not?\nIn the section “Creating a Random Forest”, just after &lt;&gt;, why did preds.mean(0) give the same result as our random forest?\nWhat is “out-of-bag-error”?\nMake a list of reasons why a model’s validation set error might be worse than the OOB error. How could you test your hypotheses?\nExplain why random forests are well suited to answering each of the following question:\n\nHow confident are we in our predictions using a particular row of data?\nFor predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?\nWhich columns are the strongest predictors?\nHow do predictions vary as we vary these columns?\n\nWhat’s the purpose of removing unimportant variables?\nWhat’s a good type of plot for showing tree interpreter results?\nWhat is the “extrapolation problem”?\nHow can you tell if your test or validation set is distributed in a different way than your training set?\nWhy do we ensure saleElapsed is a continuous variable, even although it has less than 9,000 distinct values?\nWhat is “boosting”?\nHow could we use embeddings with a random forest? Would we expect this to help?\nWhy might we not always use a neural net for tabular modeling?\n\n\nFurther Research\n\nPick a competition on Kaggle with tabular data (current or past) and try to adapt the techniques seen in this chapter to get the best possible results. Compare your results to the private leaderboard.\nImplement the decision tree algorithm in this chapter from scratch yourself, and try it on the dataset you used in the first exercise.\nUse the embeddings from the neural net in this chapter in a random forest, and see if you can improve on the random forest results we saw.\nExplain what each line of the source of TabularModel does (with the exception of the BatchNorm1d and Dropout layers)."
  },
  {
    "objectID": "Fastbook/clean/02_production.html",
    "href": "Fastbook/clean/02_production.html",
    "title": "From Model to Production",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\nfrom fastai.vision.widgets import *"
  },
  {
    "objectID": "Fastbook/clean/02_production.html#the-practice-of-deep-learning",
    "href": "Fastbook/clean/02_production.html#the-practice-of-deep-learning",
    "title": "From Model to Production",
    "section": "The Practice of Deep Learning",
    "text": "The Practice of Deep Learning\n\nStarting Your Project\n\n\nThe State of Deep Learning\n\nComputer vision\n\n\nText (natural language processing)\n\n\nCombining text and images\n\n\nTabular data\n\n\nRecommendation systems\n\n\nOther data types\n\n\n\nThe Drivetrain Approach"
  },
  {
    "objectID": "Fastbook/clean/02_production.html#gathering-data",
    "href": "Fastbook/clean/02_production.html#gathering-data",
    "title": "From Model to Production",
    "section": "Gathering Data",
    "text": "Gathering Data"
  },
  {
    "objectID": "Fastbook/clean/02_production.html#from-data-to-dataloaders",
    "href": "Fastbook/clean/02_production.html#from-data-to-dataloaders",
    "title": "From Model to Production",
    "section": "From Data to DataLoaders",
    "text": "From Data to DataLoaders\n\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\n\n\ndls = bears.dataloaders(path)\n\n\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros'))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\nbears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=4, nrows=1, unique=True)\n\n\nData Augmentation\n\nbears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=8, nrows=2, unique=True)"
  },
  {
    "objectID": "Fastbook/clean/02_production.html#training-your-model-and-using-it-to-clean-your-data",
    "href": "Fastbook/clean/02_production.html#training-your-model-and-using-it-to-clean-your-data",
    "title": "From Model to Production",
    "section": "Training Your Model, and Using It to Clean Your Data",
    "text": "Training Your Model, and Using It to Clean Your Data\n\nbears = bears.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\ndls = bears.dataloaders(path)\n\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\ninterp.plot_top_losses(5, nrows=1)\n\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n#hide\n# for idx in cleaner.delete(): cleaner.fns[idx].unlink()\n# for idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)"
  },
  {
    "objectID": "Fastbook/clean/02_production.html#turning-your-model-into-an-online-application",
    "href": "Fastbook/clean/02_production.html#turning-your-model-into-an-online-application",
    "title": "From Model to Production",
    "section": "Turning Your Model into an Online Application",
    "text": "Turning Your Model into an Online Application\n\nUsing the Model for Inference\n\nlearn.export()\n\n\npath = Path()\npath.ls(file_exts='.pkl')\n\n\nlearn_inf = load_learner(path/'export.pkl')\n\n\nlearn_inf.predict('images/grizzly.jpg')\n\n\nlearn_inf.dls.vocab\n\n\n\nCreating a Notebook App from the Model\n\nbtn_upload = widgets.FileUpload()\nbtn_upload\n\n\n#hide\n# For the book, we can't actually click an upload button, so we fake it\nbtn_upload = SimpleNamespace(data = ['images/grizzly.jpg'])\n\n\nimg = PILImage.create(btn_upload.data[-1])\n\n\nout_pl = widgets.Output()\nout_pl.clear_output()\nwith out_pl: display(img.to_thumb(128,128))\nout_pl\n\n\npred,pred_idx,probs = learn_inf.predict(img)\n\n\nlbl_pred = widgets.Label()\nlbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\nlbl_pred\n\n\nbtn_run = widgets.Button(description='Classify')\nbtn_run\n\n\ndef on_click_classify(change):\n    img = PILImage.create(btn_upload.data[-1])\n    out_pl.clear_output()\n    with out_pl: display(img.to_thumb(128,128))\n    pred,pred_idx,probs = learn_inf.predict(img)\n    lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\n\nbtn_run.on_click(on_click_classify)\n\n\n#hide\n#Putting back btn_upload to a widget for next cell\nbtn_upload = widgets.FileUpload()\n\n\nVBox([widgets.Label('Select your bear!'), \n      btn_upload, btn_run, out_pl, lbl_pred])\n\n\n\nTurning Your Notebook into a Real App\n\n#hide\n# !pip install voila\n# !jupyter serverextension enable --sys-prefix voila \n\n\n\nDeploying your app"
  },
  {
    "objectID": "Fastbook/clean/02_production.html#how-to-avoid-disaster",
    "href": "Fastbook/clean/02_production.html#how-to-avoid-disaster",
    "title": "From Model to Production",
    "section": "How to Avoid Disaster",
    "text": "How to Avoid Disaster\n\nUnforeseen Consequences and Feedback Loops"
  },
  {
    "objectID": "Fastbook/clean/02_production.html#get-writing",
    "href": "Fastbook/clean/02_production.html#get-writing",
    "title": "From Model to Production",
    "section": "Get Writing!",
    "text": "Get Writing!"
  },
  {
    "objectID": "Fastbook/clean/02_production.html#questionnaire",
    "href": "Fastbook/clean/02_production.html#questionnaire",
    "title": "From Model to Production",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nProvide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.\nWhere do text models currently have a major deficiency?\nWhat are possible negative societal implications of text generation models?\nIn situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?\nWhat kind of tabular data is deep learning particularly good at?\nWhat’s a key downside of directly using a deep learning model for recommendation systems?\nWhat are the steps of the Drivetrain Approach?\nHow do the steps of the Drivetrain Approach map to a recommendation system?\nCreate an image recognition model using data you curate, and deploy it on the web.\nWhat is DataLoaders?\nWhat four things do we need to tell fastai to create DataLoaders?\nWhat does the splitter parameter to DataBlock do?\nHow do we ensure a random split always gives the same validation set?\nWhat letters are often used to signify the independent and dependent variables?\nWhat’s the difference between the crop, pad, and squish resize approaches? When might you choose one over the others?\nWhat is data augmentation? Why is it needed?\nWhat is the difference between item_tfms and batch_tfms?\nWhat is a confusion matrix?\nWhat does export save?\nWhat is it called when we use a model for getting predictions, instead of training?\nWhat are IPython widgets?\nWhen might you want to use CPU for deployment? When might GPU be better?\nWhat are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC?\nWhat are three examples of problems that could occur when rolling out a bear warning system in practice?\nWhat is “out-of-domain data”?\nWhat is “domain shift”?\nWhat are the three steps in the deployment process?\n\n\nFurther Research\n\nConsider how the Drivetrain Approach maps to a project or problem you’re interested in.\nWhen might it be best to avoid certain types of data augmentation?\nFor a project you’re interested in applying deep learning to, consider the thought experiment “What would happen if it went really, really well?”\nStart a blog, and write your first blog post. For instance, write about what you think deep learning might be useful for in a domain you’re interested in."
  },
  {
    "objectID": "Fastbook/clean/13_convolutions.html",
    "href": "Fastbook/clean/13_convolutions.html",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastai.vision.all import *\nfrom fastbook import *\n\nmatplotlib.rc('image', cmap='Greys')"
  },
  {
    "objectID": "Fastbook/clean/13_convolutions.html#the-magic-of-convolutions",
    "href": "Fastbook/clean/13_convolutions.html#the-magic-of-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "The Magic of Convolutions",
    "text": "The Magic of Convolutions\n\ntop_edge = tensor([[-1,-1,-1],\n                   [ 0, 0, 0],\n                   [ 1, 1, 1]]).float()\n\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\n#hide\nPath.BASE_PATH = path\n\n\nim3 = Image.open(path/'train'/'3'/'12.png')\nshow_image(im3);\n\n\nim3_t = tensor(im3)\nim3_t[0:3,0:3] * top_edge\n\n\n(im3_t[0:3,0:3] * top_edge).sum()\n\n\ndf = pd.DataFrame(im3_t[:10,:20])\ndf.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')\n\n\n(im3_t[4:7,6:9] * top_edge).sum()\n\n\n(im3_t[7:10,17:20] * top_edge).sum()\n\n\ndef apply_kernel(row, col, kernel):\n    return (im3_t[row-1:row+2,col-1:col+2] * kernel).sum()\n\n\napply_kernel(5,7,top_edge)\n\n\nMapping a Convolution Kernel\n\n[[(i,j) for j in range(1,5)] for i in range(1,5)]\n\n\nrng = range(1,27)\ntop_edge3 = tensor([[apply_kernel(i,j,top_edge) for j in rng] for i in rng])\n\nshow_image(top_edge3);\n\n\nleft_edge = tensor([[-1,1,0],\n                    [-1,1,0],\n                    [-1,1,0]]).float()\n\nleft_edge3 = tensor([[apply_kernel(i,j,left_edge) for j in rng] for i in rng])\n\nshow_image(left_edge3);\n\n\n\nConvolutions in PyTorch\n\ndiag1_edge = tensor([[ 0,-1, 1],\n                     [-1, 1, 0],\n                     [ 1, 0, 0]]).float()\ndiag2_edge = tensor([[ 1,-1, 0],\n                     [ 0, 1,-1],\n                     [ 0, 0, 1]]).float()\n\nedge_kernels = torch.stack([left_edge, top_edge, diag1_edge, diag2_edge])\nedge_kernels.shape\n\n\nmnist = DataBlock((ImageBlock(cls=PILImageBW), CategoryBlock), \n                  get_items=get_image_files, \n                  splitter=GrandparentSplitter(),\n                  get_y=parent_label)\n\ndls = mnist.dataloaders(path)\nxb,yb = first(dls.valid)\nxb.shape\n\n\nxb,yb = to_cpu(xb),to_cpu(yb)\n\n\nedge_kernels.shape,edge_kernels.unsqueeze(1).shape\n\n\nedge_kernels = edge_kernels.unsqueeze(1)\n\n\nbatch_features = F.conv2d(xb, edge_kernels)\nbatch_features.shape\n\n\nshow_image(batch_features[0,0]);\n\n\n\nStrides and Padding\n\n\nUnderstanding the Convolution Equations"
  },
  {
    "objectID": "Fastbook/clean/13_convolutions.html#our-first-convolutional-neural-network",
    "href": "Fastbook/clean/13_convolutions.html#our-first-convolutional-neural-network",
    "title": "Convolutional Neural Networks",
    "section": "Our First Convolutional Neural Network",
    "text": "Our First Convolutional Neural Network\n\nCreating the CNN\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28,30),\n    nn.ReLU(),\n    nn.Linear(30,1)\n)\n\n\nsimple_net\n\n\nbroken_cnn = sequential(\n    nn.Conv2d(1,30, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(30,1, kernel_size=3, padding=1)\n)\n\n\nbroken_cnn(xb).shape\n\n\ndef conv(ni, nf, ks=3, act=True):\n    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n\n\nsimple_cnn = sequential(\n    conv(1 ,4),            #14x14\n    conv(4 ,8),            #7x7\n    conv(8 ,16),           #4x4\n    conv(16,32),           #2x2\n    conv(32,2, act=False), #1x1\n    Flatten(),\n)\n\n\nsimple_cnn(xb).shape\n\n\nlearn = Learner(dls, simple_cnn, loss_func=F.cross_entropy, metrics=accuracy)\n\n\nlearn.summary()\n\n\nlearn.fit_one_cycle(2, 0.01)\n\n\n\nUnderstanding Convolution Arithmetic\n\nm = learn.model[0]\nm\n\n\nm[0].weight.shape\n\n\nm[0].bias.shape\n\n\n\nReceptive Fields\n\n\nA Note About Twitter"
  },
  {
    "objectID": "Fastbook/clean/13_convolutions.html#color-images",
    "href": "Fastbook/clean/13_convolutions.html#color-images",
    "title": "Convolutional Neural Networks",
    "section": "Color Images",
    "text": "Color Images\n\nim = image2tensor(Image.open(image_bear()))\nim.shape\n\n\nshow_image(im);\n\n\n_,axs = subplots(1,3)\nfor bear,ax,color in zip(im,axs,('Reds','Greens','Blues')):\n    show_image(255-bear, ax=ax, cmap=color)"
  },
  {
    "objectID": "Fastbook/clean/13_convolutions.html#improving-training-stability",
    "href": "Fastbook/clean/13_convolutions.html#improving-training-stability",
    "title": "Convolutional Neural Networks",
    "section": "Improving Training Stability",
    "text": "Improving Training Stability\n\npath = untar_data(URLs.MNIST)\n\n\n#hide\nPath.BASE_PATH = path\n\n\npath.ls()\n\n\ndef get_dls(bs=64):\n    return DataBlock(\n        blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), \n        get_items=get_image_files, \n        splitter=GrandparentSplitter('training','testing'),\n        get_y=parent_label,\n        batch_tfms=Normalize()\n    ).dataloaders(path, bs=bs)\n\ndls = get_dls()\n\n\ndls.show_batch(max_n=9, figsize=(4,4))\n\n\nA Simple Baseline\n\ndef conv(ni, nf, ks=3, act=True):\n    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n\n\ndef simple_cnn():\n    return sequential(\n        conv(1 ,8, ks=5),        #14x14\n        conv(8 ,16),             #7x7\n        conv(16,32),             #4x4\n        conv(32,64),             #2x2\n        conv(64,10, act=False),  #1x1\n        Flatten(),\n    )\n\n\nfrom fastai.callback.hook import *\n\n\ndef fit(epochs=1):\n    learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy,\n                    metrics=accuracy, cbs=ActivationStats(with_hist=True))\n    learn.fit(epochs, 0.06)\n    return learn\n\n\nlearn = fit()\n\n\nlearn.activation_stats.plot_layer_stats(0)\n\n\nlearn.activation_stats.plot_layer_stats(-2)\n\n\n\nIncrease Batch Size\n\ndls = get_dls(512)\n\n\nlearn = fit()\n\n\nlearn.activation_stats.plot_layer_stats(-2)\n\n\n\n1cycle Training\n\ndef fit(epochs=1, lr=0.06):\n    learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy,\n                    metrics=accuracy, cbs=ActivationStats(with_hist=True))\n    learn.fit_one_cycle(epochs, lr)\n    return learn\n\n\nlearn = fit()\n\n\nlearn.recorder.plot_sched()\n\n\nlearn.activation_stats.plot_layer_stats(-2)\n\n\nlearn.activation_stats.color_dim(-2)\n\n\nlearn.activation_stats.color_dim(-2)\n\n\n\nBatch Normalization\n\ndef conv(ni, nf, ks=3, act=True):\n    layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)]\n    if act: layers.append(nn.ReLU())\n    layers.append(nn.BatchNorm2d(nf))\n    return nn.Sequential(*layers)\n\n\nlearn = fit()\n\n\nlearn.activation_stats.color_dim(-4)\n\n\nlearn = fit(5, lr=0.1)"
  },
  {
    "objectID": "Fastbook/clean/13_convolutions.html#conclusions",
    "href": "Fastbook/clean/13_convolutions.html#conclusions",
    "title": "Convolutional Neural Networks",
    "section": "Conclusions",
    "text": "Conclusions"
  },
  {
    "objectID": "Fastbook/clean/13_convolutions.html#questionnaire",
    "href": "Fastbook/clean/13_convolutions.html#questionnaire",
    "title": "Convolutional Neural Networks",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhat is a “feature”?\nWrite out the convolutional kernel matrix for a top edge detector.\nWrite out the mathematical operation applied by a 3×3 kernel to a single pixel in an image.\nWhat is the value of a convolutional kernel apply to a 3×3 matrix of zeros?\nWhat is “padding”?\nWhat is “stride”?\nCreate a nested list comprehension to complete any task that you choose.\nWhat are the shapes of the input and weight parameters to PyTorch’s 2D convolution?\nWhat is a “channel”?\nWhat is the relationship between a convolution and a matrix multiplication?\nWhat is a “convolutional neural network”?\nWhat is the benefit of refactoring parts of your neural network definition?\nWhat is Flatten? Where does it need to be included in the MNIST CNN? Why?\nWhat does “NCHW” mean?\nWhy does the third layer of the MNIST CNN have 7*7*(1168-16) multiplications?\nWhat is a “receptive field”?\nWhat is the size of the receptive field of an activation after two stride 2 convolutions? Why?\nRun conv-example.xlsx yourself and experiment with trace precedents.\nHave a look at Jeremy or Sylvain’s list of recent Twitter “like”s, and see if you find any interesting resources or ideas there.\nHow is a color image represented as a tensor?\nHow does a convolution work with a color input?\nWhat method can we use to see that data in DataLoaders?\nWhy do we double the number of filters after each stride-2 conv?\nWhy do we use a larger kernel in the first conv with MNIST (with simple_cnn)?\nWhat information does ActivationStats save for each layer?\nHow can we access a learner’s callback after training?\nWhat are the three statistics plotted by plot_layer_stats? What does the x-axis represent?\nWhy are activations near zero problematic?\nWhat are the upsides and downsides of training with a larger batch size?\nWhy should we avoid using a high learning rate at the start of training?\nWhat is 1cycle training?\nWhat are the benefits of training with a high learning rate?\nWhy do we want to use a low learning rate at the end of training?\nWhat is “cyclical momentum”?\nWhat callback tracks hyperparameter values during training (along with other information)?\nWhat does one column of pixels in the color_dim plot represent?\nWhat does “bad training” look like in color_dim? Why?\nWhat trainable parameters does a batch normalization layer contain?\nWhat statistics are used to normalize in batch normalization during training? How about during validation?\nWhy do models with batch normalization layers generalize better?\n\n\nFurther Research\n\nWhat features other than edge detectors have been used in computer vision (especially before deep learning became popular)?\nThere are other normalization layers available in PyTorch. Try them out and see what works best. Learn about why other normalization layers have been developed, and how they differ from batch normalization.\nTry moving the activation function after the batch normalization layer in conv. Does it make a difference? See what you can find out about what order is recommended, and why."
  },
  {
    "objectID": "Fastbook/clean/08_collab.html",
    "href": "Fastbook/clean/08_collab.html",
    "title": "Collaborative Filtering Deep Dive",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *"
  },
  {
    "objectID": "Fastbook/clean/08_collab.html#a-first-look-at-the-data",
    "href": "Fastbook/clean/08_collab.html#a-first-look-at-the-data",
    "title": "Collaborative Filtering Deep Dive",
    "section": "A First Look at the Data",
    "text": "A First Look at the Data\n\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ML_100k)\n\n\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None,\n                      names=['user','movie','rating','timestamp'])\nratings.head()\n\n\nlast_skywalker = np.array([0.98,0.9,-0.9])\n\n\nuser1 = np.array([0.9,0.8,-0.6])\n\n\n(user1*last_skywalker).sum()\n\n\ncasablanca = np.array([-0.99,-0.3,0.8])\n\n\n(user1*casablanca).sum()"
  },
  {
    "objectID": "Fastbook/clean/08_collab.html#learning-the-latent-factors",
    "href": "Fastbook/clean/08_collab.html#learning-the-latent-factors",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Learning the Latent Factors",
    "text": "Learning the Latent Factors"
  },
  {
    "objectID": "Fastbook/clean/08_collab.html#creating-the-dataloaders",
    "href": "Fastbook/clean/08_collab.html#creating-the-dataloaders",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Creating the DataLoaders",
    "text": "Creating the DataLoaders\n\nmovies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1',\n                     usecols=(0,1), names=('movie','title'), header=None)\nmovies.head()\n\n\nratings = ratings.merge(movies)\nratings.head()\n\n\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\ndls.classes\n\n\nn_users  = len(dls.classes['user'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\n\nuser_factors = torch.randn(n_users, n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\n\n\none_hot_3 = one_hot(3, n_users).float()\n\n\nuser_factors.t() @ one_hot_3\n\n\nuser_factors[3]"
  },
  {
    "objectID": "Fastbook/clean/08_collab.html#collaborative-filtering-from-scratch",
    "href": "Fastbook/clean/08_collab.html#collaborative-filtering-from-scratch",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Collaborative Filtering from Scratch",
    "text": "Collaborative Filtering from Scratch\n\nclass Example:\n    def __init__(self, a): self.a = a\n    def say(self,x): return f'Hello {self.a}, {x}.'\n\n\nex = Example('Sylvain')\nex.say('nice to meet you')\n\n\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        return (users * movies).sum(dim=1)\n\n\nx,y = dls.one_batch()\nx.shape\n\n\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\n\nlearn.fit_one_cycle(5, 5e-3)\n\n\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\n\n\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3)\n\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.user_bias = Embedding(n_users, 1)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.movie_bias = Embedding(n_movies, 1)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        res = (users * movies).sum(dim=1, keepdim=True)\n        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n        return sigmoid_range(res, *self.y_range)\n\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3)\n\n\nWeight Decay\n\nx = np.linspace(-2,2,100)\na_s = [1,2,5,10,50] \nys = [a * x**2 for a in a_s]\n_,ax = plt.subplots(figsize=(8,6))\nfor a,y in zip(a_s,ys): ax.plot(x,y, label=f'a={a}')\nax.set_ylim([0,5])\nax.legend();\n\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\nCreating Our Own Embedding Module\n\nclass T(Module):\n    def __init__(self): self.a = torch.ones(3)\n\nL(T().parameters())\n\n\nclass T(Module):\n    def __init__(self): self.a = nn.Parameter(torch.ones(3))\n\nL(T().parameters())\n\n\nclass T(Module):\n    def __init__(self): self.a = nn.Linear(1, 3, bias=False)\n\nt = T()\nL(t.parameters())\n\n\ntype(t.a.weight)\n\n\ndef create_params(size):\n    return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))\n\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = create_params([n_users, n_factors])\n        self.user_bias = create_params([n_users])\n        self.movie_factors = create_params([n_movies, n_factors])\n        self.movie_bias = create_params([n_movies])\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors[x[:,0]]\n        movies = self.movie_factors[x[:,1]]\n        res = (users*movies).sum(dim=1)\n        res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n        return sigmoid_range(res, *self.y_range)\n\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)"
  },
  {
    "objectID": "Fastbook/clean/08_collab.html#interpreting-embeddings-and-biases",
    "href": "Fastbook/clean/08_collab.html#interpreting-embeddings-and-biases",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Interpreting Embeddings and Biases",
    "text": "Interpreting Embeddings and Biases\n\nmovie_bias = learn.model.movie_bias.squeeze()\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs]\n\n\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n\ng = ratings.groupby('title')['rating'].count()\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\ntop_idxs = tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])\nmovie_w = learn.model.movie_factors[top_idxs].cpu().detach()\nmovie_pca = movie_w.pca(3)\nfac0,fac1,fac2 = movie_pca.t()\nidxs = list(range(50))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n\n\nUsing fastai.collab\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\n\n\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\nlearn.model\n\n\nmovie_bias = learn.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n\n\nEmbedding Distance\n\nmovie_factors = learn.model.i_weight.weight\nidx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']\ndistances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])\nidx = distances.argsort(descending=True)[1]\ndls.classes['title'][idx]"
  },
  {
    "objectID": "Fastbook/clean/08_collab.html#bootstrapping-a-collaborative-filtering-model",
    "href": "Fastbook/clean/08_collab.html#bootstrapping-a-collaborative-filtering-model",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Bootstrapping a Collaborative Filtering Model",
    "text": "Bootstrapping a Collaborative Filtering Model"
  },
  {
    "objectID": "Fastbook/clean/08_collab.html#deep-learning-for-collaborative-filtering",
    "href": "Fastbook/clean/08_collab.html#deep-learning-for-collaborative-filtering",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Deep Learning for Collaborative Filtering",
    "text": "Deep Learning for Collaborative Filtering\n\nembs = get_emb_sz(dls)\nembs\n\n\nclass CollabNN(Module):\n    def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):\n        self.user_factors = Embedding(*user_sz)\n        self.item_factors = Embedding(*item_sz)\n        self.layers = nn.Sequential(\n            nn.Linear(user_sz[1]+item_sz[1], n_act),\n            nn.ReLU(),\n            nn.Linear(n_act, 1))\n        self.y_range = y_range\n        \n    def forward(self, x):\n        embs = self.user_factors(x[:,0]),self.item_factors(x[:,1])\n        x = self.layers(torch.cat(embs, dim=1))\n        return sigmoid_range(x, *self.y_range)\n\n\nmodel = CollabNN(*embs)\n\n\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.01)\n\n\nlearn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50])\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n@delegates(TabularModel)\nclass EmbeddingNN(TabularModel):\n    def __init__(self, emb_szs, layers, **kwargs):\n        super().__init__(emb_szs, layers=layers, n_cont=0, out_sz=1, **kwargs)\n\n\nSidebar: kwargs and Delegates\n\n\nEnd sidebar"
  },
  {
    "objectID": "Fastbook/clean/08_collab.html#conclusion",
    "href": "Fastbook/clean/08_collab.html#conclusion",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "Fastbook/clean/08_collab.html#questionnaire",
    "href": "Fastbook/clean/08_collab.html#questionnaire",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhat problem does collaborative filtering solve?\nHow does it solve it?\nWhy might a collaborative filtering predictive model fail to be a very useful recommendation system?\nWhat does a crosstab representation of collaborative filtering data look like?\nWrite the code to create a crosstab representation of the MovieLens data (you might need to do some web searching!).\nWhat is a latent factor? Why is it “latent”?\nWhat is a dot product? Calculate a dot product manually using pure Python with lists.\nWhat does pandas.DataFrame.merge do?\nWhat is an embedding matrix?\nWhat is the relationship between an embedding and a matrix of one-hot-encoded vectors?\nWhy do we need Embedding if we could use one-hot-encoded vectors for the same thing?\nWhat does an embedding contain before we start training (assuming we’re not using a pretained model)?\nCreate a class (without peeking, if possible!) and use it.\nWhat does x[:,0] return?\nRewrite the DotProduct class (without peeking, if possible!) and train a model with it.\nWhat is a good loss function to use for MovieLens? Why?\nWhat would happen if we used cross-entropy loss with MovieLens? How would we need to change the model?\nWhat is the use of bias in a dot product model?\nWhat is another name for weight decay?\nWrite the equation for weight decay (without peeking!).\nWrite the equation for the gradient of weight decay. Why does it help reduce weights?\nWhy does reducing weights lead to better generalization?\nWhat does argsort do in PyTorch?\nDoes sorting the movie biases give the same result as averaging overall movie ratings by movie? Why/why not?\nHow do you print the names and details of the layers in a model?\nWhat is the “bootstrapping problem” in collaborative filtering?\nHow could you deal with the bootstrapping problem for new users? For new movies?\nHow can feedback loops impact collaborative filtering systems?\nWhen using a neural network in collaborative filtering, why can we have different numbers of factors for movies and users?\nWhy is there an nn.Sequential in the CollabNN model?\nWhat kind of model should we use if we want to add metadata about users and items, or information such as date and time, to a collaborative filtering model?\n\n\nFurther Research\n\nTake a look at all the differences between the Embedding version of DotProductBias and the create_params version, and try to understand why each of those changes is required. If you’re not sure, try reverting each change to see what happens. (NB: even the type of brackets used in forward has changed!)\nFind three other areas where collaborative filtering is being used, and find out what the pros and cons of this approach are in those areas.\nComplete this notebook using the full MovieLens dataset, and compare your results to online benchmarks. See if you can improve your accuracy. Look on the book’s website and the fast.ai forum for ideas. Note that there are more columns in the full dataset—see if you can use those too (the next chapter might give you ideas).\nCreate a model for MovieLens that works with cross-entropy loss, and compare it to the model in this chapter."
  },
  {
    "objectID": "Fastbook/clean/05_pet_breeds.html",
    "href": "Fastbook/clean/05_pet_breeds.html",
    "title": "Image Classification",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *"
  },
  {
    "objectID": "Fastbook/clean/05_pet_breeds.html#from-dogs-and-cats-to-pet-breeds",
    "href": "Fastbook/clean/05_pet_breeds.html#from-dogs-and-cats-to-pet-breeds",
    "title": "Image Classification",
    "section": "From Dogs and Cats to Pet Breeds",
    "text": "From Dogs and Cats to Pet Breeds\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)\n\n\n#hide\nPath.BASE_PATH = path\n\n\npath.ls()\n\n\n(path/\"images\").ls()\n\n\nfname = (path/\"images\").ls()[0]\n\n\nre.findall(r'(.+)_\\d+.jpg$', fname.name)\n\n\npets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n                 item_tfms=Resize(460),\n                 batch_tfms=aug_transforms(size=224, min_scale=0.75))\ndls = pets.dataloaders(path/\"images\")"
  },
  {
    "objectID": "Fastbook/clean/05_pet_breeds.html#presizing",
    "href": "Fastbook/clean/05_pet_breeds.html#presizing",
    "title": "Image Classification",
    "section": "Presizing",
    "text": "Presizing\n\ndblock1 = DataBlock(blocks=(ImageBlock(), CategoryBlock()),\n                   get_y=parent_label,\n                   item_tfms=Resize(460))\n# Place an image in the 'images/grizzly.jpg' subfolder where this notebook is located before running this\ndls1 = dblock1.dataloaders([(Path.cwd()/'images'/'grizzly.jpg')]*100, bs=8)\ndls1.train.get_idxs = lambda: Inf.ones\nx,y = dls1.valid.one_batch()\n_,axs = subplots(1, 2)\n\nx1 = TensorImage(x.clone())\nx1 = x1.affine_coord(sz=224)\nx1 = x1.rotate(draw=30, p=1.)\nx1 = x1.zoom(draw=1.2, p=1.)\nx1 = x1.warp(draw_x=-0.2, draw_y=0.2, p=1.)\n\ntfms = setup_aug_tfms([Rotate(draw=30, p=1, size=224), Zoom(draw=1.2, p=1., size=224),\n                       Warp(draw_x=-0.2, draw_y=0.2, p=1., size=224)])\nx = Pipeline(tfms)(x)\n#x.affine_coord(coord_tfm=coord_tfm, sz=size, mode=mode, pad_mode=pad_mode)\nTensorImage(x[0]).show(ctx=axs[0])\nTensorImage(x1[0]).show(ctx=axs[1]);\n\n\nChecking and Debugging a DataBlock\n\ndls.show_batch(nrows=1, ncols=3)\n\n\npets1 = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'))\npets1.summary(path/\"images\")\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2)"
  },
  {
    "objectID": "Fastbook/clean/05_pet_breeds.html#cross-entropy-loss",
    "href": "Fastbook/clean/05_pet_breeds.html#cross-entropy-loss",
    "title": "Image Classification",
    "section": "Cross-Entropy Loss",
    "text": "Cross-Entropy Loss\n\nViewing Activations and Labels\n\nx,y = dls.one_batch()\n\n\ny\n\n\npreds,_ = learn.get_preds(dl=[(x,y)])\npreds[0]\n\n\nlen(preds[0]),preds[0].sum()\n\n\n\nSoftmax\n\nplot_function(torch.sigmoid, min=-4,max=4)\n\n\n#hide\ntorch.random.manual_seed(42);\n\n\nacts = torch.randn((6,2))*2\nacts\n\n\nacts.sigmoid()\n\n\n(acts[:,0]-acts[:,1]).sigmoid()\n\n\nsm_acts = torch.softmax(acts, dim=1)\nsm_acts\n\n\n\nLog Likelihood\n\ntarg = tensor([0,1,0,1,1,0])\n\n\nsm_acts\n\n\nidx = range(6)\nsm_acts[idx, targ]\n\n\nfrom IPython.display import HTML\ndf = pd.DataFrame(sm_acts, columns=[\"3\",\"7\"])\ndf['targ'] = targ\ndf['idx'] = idx\ndf['result'] = sm_acts[range(6), targ]\nt = df.style.hide_index()\n#To have html code compatible with our script\nhtml = t._repr_html_().split('&lt;/style&gt;')[1]\nhtml = re.sub(r'&lt;table id=\"([^\"]+)\"\\s*&gt;', r'&lt;table &gt;', html)\ndisplay(HTML(html))\n\n\n-sm_acts[idx, targ]\n\n\nF.nll_loss(sm_acts, targ, reduction='none')\n\n\nTaking the Log\nRecall that cross entropy loss may involve the multiplication of many numbers. Multiplying lots of negative numbers together can cause problems like numerical underflow in computers. Therefore, we want to transform these probabilities to larger values so we can perform mathematical operations on them. There is a mathematical function that does exactly this: the logarithm (available as torch.log). It is not defined for numbers less than 0, and looks like this between 0 and 1:\n\nplot_function(torch.log, min=0,max=1, ty='log(x)', tx='x')\n\n\nplot_function(lambda x: -1*torch.log(x), min=0,max=1, tx='x', ty='- log(x)', title = 'Log Loss when true label = 1')\n\n\nfrom IPython.display import HTML\ndf['loss'] = -torch.log(tensor(df['result']))\nt = df.style.hide_index()\n#To have html code compatible with our script\nhtml = t._repr_html_().split('&lt;/style&gt;')[1]\nhtml = re.sub(r'&lt;table id=\"([^\"]+)\"\\s*&gt;', r'&lt;table &gt;', html)\ndisplay(HTML(html))\n\n\n\n\nNegative Log Likelihood\n\nloss_func = nn.CrossEntropyLoss()\n\n\nloss_func(acts, targ)\n\n\nF.cross_entropy(acts, targ)\n\n\nnn.CrossEntropyLoss(reduction='none')(acts, targ)"
  },
  {
    "objectID": "Fastbook/clean/05_pet_breeds.html#model-interpretation",
    "href": "Fastbook/clean/05_pet_breeds.html#model-interpretation",
    "title": "Image Classification",
    "section": "Model Interpretation",
    "text": "Model Interpretation\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(12,12), dpi=60)\n\n\ninterp.most_confused(min_val=5)"
  },
  {
    "objectID": "Fastbook/clean/05_pet_breeds.html#improving-our-model",
    "href": "Fastbook/clean/05_pet_breeds.html#improving-our-model",
    "title": "Image Classification",
    "section": "Improving Our Model",
    "text": "Improving Our Model\n\nThe Learning Rate Finder\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1, base_lr=0.1)\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlr_min,lr_steep = learn.lr_find(suggest_funcs=(minimum, steep))\n\n\nprint(f\"Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}\")\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2, base_lr=3e-3)\n\n\n\nUnfreezing and Transfer Learning\n\nlearn.fine_tune??\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3)\n\n\nlearn.unfreeze()\n\n\nlearn.lr_find()\n\n\nlearn.fit_one_cycle(6, lr_max=1e-5)\n\n\n\nDiscriminative Learning Rates\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3)\nlearn.unfreeze()\nlearn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4))\n\n\nlearn.recorder.plot_loss()\n\n\n\nSelecting the Number of Epochs\n\n\nDeeper Architectures\n\nfrom fastai.callback.fp16 import *\nlearn = vision_learner(dls, resnet50, metrics=error_rate).to_fp16()\nlearn.fine_tune(6, freeze_epochs=3)"
  },
  {
    "objectID": "Fastbook/clean/05_pet_breeds.html#conclusion",
    "href": "Fastbook/clean/05_pet_breeds.html#conclusion",
    "title": "Image Classification",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "Fastbook/clean/05_pet_breeds.html#questionnaire",
    "href": "Fastbook/clean/05_pet_breeds.html#questionnaire",
    "title": "Image Classification",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhy do we first resize to a large size on the CPU, and then to a smaller size on the GPU?\nIf you are not familiar with regular expressions, find a regular expression tutorial, and some problem sets, and complete them. Have a look on the book’s website for suggestions.\nWhat are the two ways in which data is most commonly provided, for most deep learning datasets?\nLook up the documentation for L and try using a few of the new methods that it adds.\nLook up the documentation for the Python pathlib module and try using a few methods of the Path class.\nGive two examples of ways that image transformations can degrade the quality of the data.\nWhat method does fastai provide to view the data in a DataLoaders?\nWhat method does fastai provide to help you debug a DataBlock?\nShould you hold off on training a model until you have thoroughly cleaned your data?\nWhat are the two pieces that are combined into cross-entropy loss in PyTorch?\nWhat are the two properties of activations that softmax ensures? Why is this important?\nWhen might you want your activations to not have these two properties?\nCalculate the exp and softmax columns of &lt;&gt; yourself (i.e., in a spreadsheet, with a calculator, or in a notebook).\nWhy can’t we use torch.where to create a loss function for datasets where our label can have more than two categories?\nWhat is the value of log(-2)? Why?\nWhat are two good rules of thumb for picking a learning rate from the learning rate finder?\nWhat two steps does the fine_tune method do?\nIn Jupyter Notebook, how do you get the source code for a method or function?\nWhat are discriminative learning rates?\nHow is a Python slice object interpreted when passed as a learning rate to fastai?\nWhy is early stopping a poor choice when using 1cycle training?\nWhat is the difference between resnet50 and resnet101?\nWhat does to_fp16 do?\n\n\nFurther Research\n\nFind the paper by Leslie Smith that introduced the learning rate finder, and read it.\nSee if you can improve the accuracy of the classifier in this chapter. What’s the best accuracy you can achieve? Look on the forums and the book’s website to see what other students have achieved with this dataset, and how they did it."
  },
  {
    "objectID": "Fastbook/clean/10_nlp.html",
    "href": "Fastbook/clean/10_nlp.html",
    "title": "NLP Deep Dive: RNNs",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\nfrom IPython.display import display,HTML"
  },
  {
    "objectID": "Fastbook/clean/10_nlp.html#text-preprocessing",
    "href": "Fastbook/clean/10_nlp.html#text-preprocessing",
    "title": "NLP Deep Dive: RNNs",
    "section": "Text Preprocessing",
    "text": "Text Preprocessing\n\nTokenization\n\n\nWord Tokenization with fastai\n\nfrom fastai.text.all import *\npath = untar_data(URLs.IMDB)\n\n\nfiles = get_text_files(path, folders = ['train', 'test', 'unsup'])\n\n\ntxt = files[0].open().read(); txt[:75]\n\n\nspacy = WordTokenizer()\ntoks = first(spacy([txt]))\nprint(coll_repr(toks, 30))\n\n\nfirst(spacy(['The U.S. dollar $1 is $1.00.']))\n\n\ntkn = Tokenizer(spacy)\nprint(coll_repr(tkn(txt), 31))\n\n\ndefaults.text_proc_rules\n\n\ncoll_repr(tkn('&copy;   Fast.ai www.fast.ai/INDEX'), 31)\n\n\n\nSubword Tokenization\n\ntxts = L(o.open().read() for o in files[:2000])\n\n\ndef subword(sz):\n    sp = SubwordTokenizer(vocab_sz=sz)\n    sp.setup(txts)\n    return ' '.join(first(sp([txt]))[:40])\n\n\nsubword(1000)\n\n\nsubword(200)\n\n\nsubword(10000)\n\n\n\nNumericalization with fastai\n\ntoks = tkn(txt)\nprint(coll_repr(tkn(txt), 31))\n\n\ntoks200 = txts[:200].map(tkn)\ntoks200[0]\n\n\nnum = Numericalize()\nnum.setup(toks200)\ncoll_repr(num.vocab,20)\n\n\nnums = num(toks)[:20]; nums\n\n\n' '.join(num.vocab[o] for o in nums)\n\n\n\nPutting Our Texts into Batches for a Language Model\n\nstream = \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\ntokens = tkn(stream)\nbs,seq_len = 6,15\nd_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])\ndf = pd.DataFrame(d_tokens)\ndisplay(HTML(df.to_html(index=False,header=None)))\n\n\nbs,seq_len = 6,5\nd_tokens = np.array([tokens[i*15:i*15+seq_len] for i in range(bs)])\ndf = pd.DataFrame(d_tokens)\ndisplay(HTML(df.to_html(index=False,header=None)))\n\n\nbs,seq_len = 6,5\nd_tokens = np.array([tokens[i*15+seq_len:i*15+2*seq_len] for i in range(bs)])\ndf = pd.DataFrame(d_tokens)\ndisplay(HTML(df.to_html(index=False,header=None)))\n\n\nbs,seq_len = 6,5\nd_tokens = np.array([tokens[i*15+10:i*15+15] for i in range(bs)])\ndf = pd.DataFrame(d_tokens)\ndisplay(HTML(df.to_html(index=False,header=None)))\n\n\nnums200 = toks200.map(num)\n\n\ndl = LMDataLoader(nums200)\n\n\nx,y = first(dl)\nx.shape,y.shape\n\n\n' '.join(num.vocab[o] for o in x[0][:20])\n\n\n' '.join(num.vocab[o] for o in y[0][:20])"
  },
  {
    "objectID": "Fastbook/clean/10_nlp.html#training-a-text-classifier",
    "href": "Fastbook/clean/10_nlp.html#training-a-text-classifier",
    "title": "NLP Deep Dive: RNNs",
    "section": "Training a Text Classifier",
    "text": "Training a Text Classifier\n\nLanguage Model Using DataBlock\n\nget_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n\ndls_lm = DataBlock(\n    blocks=TextBlock.from_folder(path, is_lm=True),\n    get_items=get_imdb, splitter=RandomSplitter(0.1)\n).dataloaders(path, path=path, bs=128, seq_len=80)\n\n\ndls_lm.show_batch(max_n=2)\n\n\n\nFine-Tuning the Language Model\n\nlearn = language_model_learner(\n    dls_lm, AWD_LSTM, drop_mult=0.3, \n    metrics=[accuracy, Perplexity()]).to_fp16()\n\n\nlearn.fit_one_cycle(1, 2e-2)\n\n\n\nSaving and Loading Models\n\nlearn.save('1epoch')\n\n\nlearn = learn.load('1epoch')\n\n\nlearn.unfreeze()\nlearn.fit_one_cycle(10, 2e-3)\n\n\nlearn.save_encoder('finetuned')\n\n\n\nText Generation\n\nTEXT = \"I liked this movie because\"\nN_WORDS = 40\nN_SENTENCES = 2\npreds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n         for _ in range(N_SENTENCES)]\n\n\nprint(\"\\n\".join(preds))\n\n\n\nCreating the Classifier DataLoaders\n\ndls_clas = DataBlock(\n    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n    get_y = parent_label,\n    get_items=partial(get_text_files, folders=['train', 'test']),\n    splitter=GrandparentSplitter(valid_name='test')\n).dataloaders(path, path=path, bs=128, seq_len=72)\n\n\ndls_clas.show_batch(max_n=3)\n\n\nnums_samp = toks200[:10].map(num)\n\n\nnums_samp.map(len)\n\n\nlearn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n                                metrics=accuracy).to_fp16()\n\n\nlearn = learn.load_encoder('finetuned')\n\n\n\nFine-Tuning the Classifier\n\nlearn.fit_one_cycle(1, 2e-2)\n\n\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n\n\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n\n\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
  },
  {
    "objectID": "Fastbook/clean/10_nlp.html#disinformation-and-language-models",
    "href": "Fastbook/clean/10_nlp.html#disinformation-and-language-models",
    "title": "NLP Deep Dive: RNNs",
    "section": "Disinformation and Language Models",
    "text": "Disinformation and Language Models"
  },
  {
    "objectID": "Fastbook/clean/10_nlp.html#conclusion",
    "href": "Fastbook/clean/10_nlp.html#conclusion",
    "title": "NLP Deep Dive: RNNs",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "Fastbook/clean/10_nlp.html#questionnaire",
    "href": "Fastbook/clean/10_nlp.html#questionnaire",
    "title": "NLP Deep Dive: RNNs",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhat is “self-supervised learning”?\nWhat is a “language model”?\nWhy is a language model considered self-supervised?\nWhat are self-supervised models usually used for?\nWhy do we fine-tune language models?\nWhat are the three steps to create a state-of-the-art text classifier?\nHow do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset?\nWhat are the three steps to prepare your data for a language model?\nWhat is “tokenization”? Why do we need it?\nName three different approaches to tokenization.\nWhat is xxbos?\nList four rules that fastai applies to text during tokenization.\nWhy are repeated characters replaced with a token showing the number of repetitions and the character that’s repeated?\nWhat is “numericalization”?\nWhy might there be words that are replaced with the “unknown word” token?\nWith a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful—students often get this one wrong! Be sure to check your answer on the book’s website.)\nWhy do we need padding for text classification? Why don’t we need it for language modeling?\nWhat does an embedding matrix for NLP contain? What is its shape?\nWhat is “perplexity”?\nWhy do we have to pass the vocabulary of the language model to the classifier data block?\nWhat is “gradual unfreezing”?\nWhy is text generation always likely to be ahead of automatic identification of machine-generated texts?\n\n\nFurther Research\n\nSee what you can learn about language models and disinformation. What are the best language models today? Take a look at some of their outputs. Do you find them convincing? How could a bad actor best use such a model to create conflict and uncertainty?\nGiven the limitation that models are unlikely to be able to consistently recognize machine-generated texts, what other approaches may be needed to handle large-scale disinformation campaigns that leverage deep learning?"
  },
  {
    "objectID": "Fastbook/15_arch_details.html",
    "href": "Fastbook/15_arch_details.html",
    "title": "Application Architectures Deep Dive",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\n[[chapter_arch_details]]\nWe are now in the exciting position that we can fully understand the architectures that we have been using for our state-of-the-art models for computer vision, natural language processing, and tabular analysis. In this chapter, we’re going to fill in all the missing details on how fastai’s application models work and show you how to build the models they use.\nWe will also go back to the custom data preprocessing pipeline we saw in &lt;&gt; for Siamese networks and show you how you can use the components in the fastai library to build custom pretrained models for new tasks.\nWe’ll start with computer vision."
  },
  {
    "objectID": "Fastbook/15_arch_details.html#computer-vision",
    "href": "Fastbook/15_arch_details.html#computer-vision",
    "title": "Application Architectures Deep Dive",
    "section": "Computer Vision",
    "text": "Computer Vision\nFor computer vision application we use the functions vision_learner and unet_learner to build our models, depending on the task. In this section we’ll explore how to build the Learner objects we used in Parts 1 and 2 of this book.\n\nvision_learner\nLet’s take a look at what happens when we use the vision_learner function. We begin by passing this function an architecture to use for the body of the network. Most of the time we use a ResNet, which you already know how to create, so we don’t need to delve into that any further. Pretrained weights are downloaded as required and loaded into the ResNet.\nThen, for transfer learning, the network needs to be cut. This refers to slicing off the final layer, which is only responsible for ImageNet-specific categorization. In fact, we do not slice off only this layer, but everything from the adaptive average pooling layer onwards. The reason for this will become clear in just a moment. Since different architectures might use different types of pooling layers, or even completely different kinds of heads, we don’t just search for the adaptive pooling layer to decide where to cut the pretrained model. Instead, we have a dictionary of information that is used for each model to determine where its body ends, and its head starts. We call this model_meta—here it is for resnet-50:\n\nmodel_meta[resnet50]\n\n{'cut': -2,\n 'split': &lt;function fastai.vision.learner._resnet_split(m)&gt;,\n 'stats': ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])}\n\n\n\njargon: Body and Head: The “head” of a neural net is the part that is specialized for a particular task. For a CNN, it’s generally the part after the adaptive average pooling layer. The “body” is everything else, and includes the “stem” (which we learned about in &lt;&gt;).\n\nIf we take all of the layers prior to the cut point of -2, we get the part of the model that fastai will keep for transfer learning. Now, we put on our new head. This is created using the function create_head:\n\n#hide_output\ncreate_head(20,2)\n\nSequential(\n  (0): AdaptiveConcatPool2d(\n    (ap): AdaptiveAvgPool2d(output_size=1)\n    (mp): AdaptiveMaxPool2d(output_size=1)\n  )\n  (1): full: False\n  (2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Dropout(p=0.25, inplace=False)\n  (4): Linear(in_features=20, out_features=512, bias=False)\n  (5): ReLU(inplace=True)\n  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (7): Dropout(p=0.5, inplace=False)\n  (8): Linear(in_features=512, out_features=2, bias=False)\n)\n\n\nSequential(\n  (0): AdaptiveConcatPool2d(\n    (ap): AdaptiveAvgPool2d(output_size=1)\n    (mp): AdaptiveMaxPool2d(output_size=1)\n  )\n  (1): Flatten()\n  (2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True)\n  (3): Dropout(p=0.25, inplace=False)\n  (4): Linear(in_features=20, out_features=512, bias=False)\n  (5): ReLU(inplace=True)\n  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)\n  (7): Dropout(p=0.5, inplace=False)\n  (8): Linear(in_features=512, out_features=2, bias=False)\n)\nWith this function you can choose how many additional linear layers are added to the end, how much dropout to use after each one, and what kind of pooling to use. By default, fastai will apply both average pooling, and max pooling, and will concatenate the two together (this is the AdaptiveConcatPool2d layer). This is not a particularly common approach, but it was developed independently at fastai and other research labs in recent years, and tends to provide some small improvement over using just average pooling.\nfastai is a bit different from most libraries in that by default it adds two linear layers, rather than one, in the CNN head. The reason for this is that transfer learning can still be useful even, as we have seen, when transferring the pretrained model to very different domains. However, just using a single linear layer is unlikely to be enough in these cases; we have found that using two linear layers can allow transfer learning to be used more quickly and easily, in more situations.\n\nnote: One Last Batchnorm?: One parameter to create_head that is worth looking at is bn_final. Setting this to true will cause a batchnorm layer to be added as your final layer. This can be useful in helping your model scale appropriately for your output activations. We haven’t seen this approach published anywhere as yet, but we have found that it works well in practice wherever we have used it.\n\nLet’s now take a look at what unet_learner did in the segmentation problem we showed in &lt;&gt;.\n\n\nunet_learner\nOne of the most interesting architectures in deep learning is the one that we used for segmentation in &lt;&gt;. Segmentation is a challenging task, because the output required is really an image, or a pixel grid, containing the predicted label for every pixel. There are other tasks that share a similar basic design, such as increasing the resolution of an image (super-resolution), adding color to a black-and-white image (colorization), or converting a photo into a synthetic painting (style transfer)—these tasks are covered by an online chapter of this book, so be sure to check it out after you’ve read this chapter. In each case, we are starting with an image and converting it to some other image of the same dimensions or aspect ratio, but with the pixels altered in some way. We refer to these as generative vision models.\nThe way we do this is to start with the exact same approach to developing a CNN head as we saw in the previous problem. We start with a ResNet, for instance, and cut off the adaptive pooling layer and everything after that. Then we replace those layers with our custom head, which does the generative task.\nThere was a lot of handwaving in that last sentence! How on earth do we create a CNN head that generates an image? If we start with, say, a 224-pixel input image, then at the end of the ResNet body we will have a 7×7 grid of convolutional activations. How can we convert that into a 224-pixel segmentation mask?\nNaturally, we do this with a neural network! So we need some kind of layer that can increase the grid size in a CNN. One very simple approach to this is to replace every pixel in the 7×7 grid with four pixels in a 2×2 square. Each of those four pixels will have the same value—this is known as nearest neighbor interpolation. PyTorch provides a layer that does this for us, so one option is to create a head that contains stride-1 convolutional layers (along with batchnorm and ReLU layers as usual) interspersed with 2×2 nearest neighbor interpolation layers. In fact, you can try this now! See if you can create a custom head designed like this, and try it on the CamVid segmentation task. You should find that you get some reasonable results, although they won’t be as good as our &lt;&gt; results.\nAnother approach is to replace the nearest neighbor and convolution combination with a transposed convolution, otherwise known as a stride half convolution. This is identical to a regular convolution, but first zero padding is inserted between all the pixels in the input. This is easiest to see with a picture—&lt;&gt; shows a diagram from the excellent convolutional arithmetic paper we discussed in &lt;&gt;, showing a 3×3 transposed convolution applied to a 3×3 image.\n\nAs you see, the result of this is to increase the size of the input. You can try this out now by using fastai’s ConvLayer class; pass the parameter transpose=True to create a transposed convolution, instead of a regular one, in your custom head.\nNeither of these approaches, however, works really well. The problem is that our 7×7 grid simply doesn’t have enough information to create a 224×224-pixel output. It’s asking an awful lot of the activations of each of those grid cells to have enough information to fully regenerate every pixel in the output. The solution to this problem is to use skip connections, like in a ResNet, but skipping from the activations in the body of the ResNet all the way over to the activations of the transposed convolution on the opposite side of the architecture. This approach, illustrated in &lt;&gt;, was developed by Olaf Ronneberger, Philipp Fischer, and Thomas Brox in the 2015 paper “U-Net: Convolutional Networks for Biomedical Image Segmentation”. Although the paper focused on medical applications, the U-Net has revolutionized all kinds of generative vision models.\n\nThis picture shows the CNN body on the left (in this case, it’s a regular CNN, not a ResNet, and they’re using 2×2 max pooling instead of stride-2 convolutions, since this paper was written before ResNets came along) and the transposed convolutional (“up-conv”) layers on the right. Then extra skip connections are shown as gray arrows crossing from left to right (these are sometimes called cross connections). You can see why it’s called a “U-Net!”\nWith this architecture, the input to the transposed convolutions is not just the lower-resolution grid in the preceding layer, but also the higher-resolution grid in the ResNet head. This allows the U-Net to use all of the information of the original image, as it is needed. One challenge with U-Nets is that the exact architecture depends on the image size. fastai has a unique DynamicUnet class that autogenerates an architecture of the right size based on the data provided.\nLet’s focus now on an example where we leverage the fastai library to write a custom model.\n\n\nA Siamese Network\n\n#hide\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\nclass SiameseImage(fastuple):\n    def show(self, ctx=None, **kwargs): \n        img1,img2,same_breed = self\n        if not isinstance(img1, Tensor):\n            if img2.size != img1.size: img2 = img2.resize(img1.size)\n            t1,t2 = tensor(img1),tensor(img2)\n            t1,t2 = t1.permute(2,0,1),t2.permute(2,0,1)\n        else: t1,t2 = img1,img2\n        line = t1.new_zeros(t1.shape[0], t1.shape[1], 10)\n        return show_image(torch.cat([t1,line,t2], dim=2), \n                          title=same_breed, ctx=ctx)\n    \ndef label_func(fname):\n    return re.match(r'^(.*)_\\d+.jpg$', fname.name).groups()[0]\n\nclass SiameseTransform(Transform):\n    def __init__(self, files, label_func, splits):\n        self.labels = files.map(label_func).unique()\n        self.lbl2files = {l: L(f for f in files if label_func(f) == l) for l in self.labels}\n        self.label_func = label_func\n        self.valid = {f: self._draw(f) for f in files[splits[1]]}\n        \n    def encodes(self, f):\n        f2,t = self.valid.get(f, self._draw(f))\n        img1,img2 = PILImage.create(f),PILImage.create(f2)\n        return SiameseImage(img1, img2, t)\n    \n    def _draw(self, f):\n        same = random.random() &lt; 0.5\n        cls = self.label_func(f)\n        if not same: cls = random.choice(L(l for l in self.labels if l != cls)) \n        return random.choice(self.lbl2files[cls]),same\n    \nsplits = RandomSplitter()(files)\ntfm = SiameseTransform(files, label_func, splits)\ntls = TfmdLists(files, tfm, splits=splits)\ndls = tls.dataloaders(after_item=[Resize(224), ToTensor], \n    after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)])\n\nLet’s go back to the input pipeline we set up in &lt;&gt; for a Siamese network. If you remember, it consisted of pair of images with the label being True or False, depending on if they were in the same class or not.\nUsing what we just saw, let’s build a custom model for this task and train it. How? We will use a pretrained architecture and pass our two images through it. Then we can concatenate the results and send them to a custom head that will return two predictions. In terms of modules, this looks like this:\n\nclass SiameseModel(Module):\n    def __init__(self, encoder, head):\n        self.encoder,self.head = encoder,head\n    \n    def forward(self, x1, x2):\n        ftrs = torch.cat([self.encoder(x1), self.encoder(x2)], dim=1)\n        return self.head(ftrs)\n\nTo create our encoder, we just need to take a pretrained model and cut it, as we explained before. The function create_body does that for us; we just have to pass it the place where we want to cut. As we saw earlier, per the dictionary of metadata for pretrained models, the cut value for a resnet is -2:\n\nencoder = create_body(resnet34, cut=-2)\n\nThen we can create our head. A look at the encoder tells us the last layer has 512 features, so this head will need to receive 512*2. Why 2? We have to multiply by 2 because we have two images. So we create the head as follows:\n\nhead = create_head(512*2, 2, ps=0.5)\n\nWith our encoder and head, we can now build our model:\n\nmodel = SiameseModel(encoder, head)\n\nBefore using Learner, we have two more things to define. First, we must define the loss function we want to use. It’s regular cross-entropy, but since our targets are Booleans, we need to convert them to integers or PyTorch will throw an error:\n\ndef loss_func(out, targ):\n    return nn.CrossEntropyLoss()(out, targ.long())\n\nMore importantly, to take full advantage of transfer learning, we have to define a custom splitter. A splitter is a function that tells the fastai library how to split the model into parameter groups. These are used behind the scenes to train only the head of a model when we do transfer learning.\nHere we want two parameter groups: one for the encoder and one for the head. We can thus define the following splitter (params is just a function that returns all parameters of a given module):\n\ndef siamese_splitter(model):\n    return [params(model.encoder), params(model.head)]\n\nThen we can define our Learner by passing the data, model, loss function, splitter, and any metric we want. Since we are not using a convenience function from fastai for transfer learning (like vision_learner), we have to call learn.freeze manually. This will make sure only the last parameter group (in this case, the head) is trained:\n\nlearn = Learner(dls, model, loss_func=loss_func, \n                splitter=siamese_splitter, metrics=accuracy)\nlearn.freeze()\n\nThen we can directly train our model with the usual methods:\n\nlearn.fit_one_cycle(4, 3e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.367015\n0.281242\n0.885656\n00:26\n\n\n1\n0.307688\n0.214721\n0.915426\n00:26\n\n\n2\n0.275221\n0.170615\n0.936401\n00:26\n\n\n3\n0.223771\n0.159633\n0.943843\n00:26\n\n\n\n\n\nBefore unfreezing and fine-tuning the whole model a bit more with discriminative learning rates (that is: a lower learning rate for the body and a higher one for the head):\n\nlearn.unfreeze()\nlearn.fit_one_cycle(4, slice(1e-6,1e-4))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.212744\n0.159033\n0.944520\n00:35\n\n\n1\n0.201893\n0.159615\n0.942490\n00:35\n\n\n2\n0.204606\n0.152338\n0.945196\n00:36\n\n\n3\n0.213203\n0.148346\n0.947903\n00:36\n\n\n\n\n\n94.8% is very good when we remember a classifier trained the same way (with no data augmentation) had an error rate of 7%.\nNow that we’ve seen how to create complete state-of-the-art computer vision models, let’s move on to NLP."
  },
  {
    "objectID": "Fastbook/15_arch_details.html#natural-language-processing",
    "href": "Fastbook/15_arch_details.html#natural-language-processing",
    "title": "Application Architectures Deep Dive",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing\nConverting an AWD-LSTM language model into a transfer learning classifier, as we did in &lt;&gt;, follows a very similar process to what we did with vision_learner in the first section of this chapter. We do not need a “meta” dictionary in this case, because we do not have such a variety of architectures to support in the body. All we need to do is select the stacked RNN for the encoder in the language model, which is a single PyTorch module. This encoder will provide an activation for every word of the input, because a language model needs to output a prediction for every next word.\nTo create a classifier from this we use an approach described in the ULMFiT paper as “BPTT for Text Classification (BPT3C)”:\n\n: We divide the document into fixed-length batches of size b. At the beginning of each batch, the model is initialized with the final state of the previous batch; we keep track of the hidden states for mean and max-pooling; gradients are back-propagated to the batches whose hidden states contributed to the final prediction. In practice, we use variable length backpropagation sequences.\n\nIn other words, the classifier contains a for loop, which loops over each batch of a sequence. The state is maintained across batches, and the activations of each batch are stored. At the end, we use the same average and max concatenated pooling trick that we use for computer vision models—but this time, we do not pool over CNN grid cells, but over RNN sequences.\nFor this for loop we need to gather our data in batches, but each text needs to be treated separately, as they each have their own labels. However, it’s very likely that those texts won’t all be of the same length, which means we won’t be able to put them all in the same array, like we did with the language model.\nThat’s where padding is going to help: when grabbing a bunch of texts, we determine the one with the greatest length, then we fill the ones that are shorter with a special token called xxpad. To avoid extreme cases where we have a text with 2,000 tokens in the same batch as a text with 10 tokens (so a lot of padding, and a lot of wasted computation), we alter the randomness by making sure texts of comparable size are put together. The texts will still be in a somewhat random order for the training set (for the validation set we can simply sort them by order of length), but not completely so.\nThis is done automatically behind the scenes by the fastai library when creating our DataLoaders."
  },
  {
    "objectID": "Fastbook/15_arch_details.html#tabular",
    "href": "Fastbook/15_arch_details.html#tabular",
    "title": "Application Architectures Deep Dive",
    "section": "Tabular",
    "text": "Tabular\nFinally, let’s take a look at fastai.tabular models. (We don’t need to look at collaborative filtering separately, since we’ve already seen that these models are just tabular models, or use the dot product approach, which we’ve implemented earlier from scratch.)\nHere is the forward method for TabularModel:\nif self.n_emb != 0:\n    x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n    x = torch.cat(x, 1)\n    x = self.emb_drop(x)\nif self.n_cont != 0:\n    x_cont = self.bn_cont(x_cont)\n    x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\nreturn self.layers(x)\nWe won’t show __init__ here, since it’s not that interesting, but we will look at each line of code in forward in turn. The first line:\nif self.n_emb != 0:\nis just testing whether there are any embeddings to deal with—we can skip this section if we only have continuous variables. self.embeds contains the embedding matrices, so this gets the activations of each:\n    x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\nand concatenates them into a single tensor:\n    x = torch.cat(x, 1)\nThen dropout is applied. You can pass embd_p to __init__ to change this value:\n    x = self.emb_drop(x)\nNow we test whether there are any continuous variables to deal with:\nif self.n_cont != 0:\nThey are passed through a batchnorm layer:\n    x_cont = self.bn_cont(x_cont)\nand concatenated with the embedding activations, if there were any:\n    x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\nFinally, this is passed through the linear layers (each of which includes batchnorm, if use_bn is True, and dropout, if ps is set to some value or list of values):\nreturn self.layers(x)\nCongratulations! Now you know every single piece of the architectures used in the fastai library!"
  },
  {
    "objectID": "Fastbook/15_arch_details.html#wrapping-up-architectures",
    "href": "Fastbook/15_arch_details.html#wrapping-up-architectures",
    "title": "Application Architectures Deep Dive",
    "section": "Wrapping Up Architectures",
    "text": "Wrapping Up Architectures\nAs you can see, the details of deep learning architectures need not scare you now. You can look inside the code of fastai and PyTorch and see just what is going on. More importantly, try to understand why it’s going on. Take a look at the papers that are being referenced in the code, and try to see how the code matches up to the algorithms that are described.\nNow that we have investigated all of the pieces of a model and the data that is passed into it, we can consider what this means for practical deep learning. If you have unlimited data, unlimited memory, and unlimited time, then the advice is easy: train a huge model on all of your data for a really long time. But the reason that deep learning is not straightforward is because your data, memory, and time are typically limited. If you are running out of memory or time, then the solution is to train a smaller model. If you are not able to train for long enough to overfit, then you are not taking advantage of the capacity of your model.\nSo, step one is to get to the point where you can overfit. Then the question is how to reduce that overfitting. &lt;&gt; shows how we recommend prioritizing the steps from there.\n\nMany practitioners, when faced with an overfitting model, start at exactly the wrong end of this diagram. Their starting point is to use a smaller model, or more regularization. Using a smaller model should be absolutely the last step you take, unless training your model is taking up too much time or memory. Reducing the size of your model reduces the ability of your model to learn subtle relationships in your data.\nInstead, your first step should be to seek to create more data. That could involve adding more labels to data that you already have, finding additional tasks that your model could be asked to solve (or, to think of it another way, identifying different kinds of labels that you could model), or creating additional synthetic data by using more or different data augmentation techniques. Thanks to the development of Mixup and similar approaches, effective data augmentation is now available for nearly all kinds of data.\nOnce you’ve got as much data as you think you can reasonably get hold of, and are using it as effectively as possible by taking advantage of all the labels that you can find and doing all the augmentation that makes sense, if you are still overfitting you should think about using more generalizable architectures. For instance, adding batch normalization may improve generalization.\nIf you are still overfitting after doing the best you can at using your data and tuning your architecture, then you can take a look at regularization. Generally speaking, adding dropout to the last layer or two will do a good job of regularizing your model. However, as we learned from the story of the development of AWD-LSTM, it is often the case that adding dropout of different types throughout your model can help even more. Generally speaking, a larger model with more regularization is more flexible, and can therefore be more accurate than a smaller model with less regularization.\nOnly after considering all of these options would we recommend that you try using a smaller version of your architecture."
  },
  {
    "objectID": "Fastbook/15_arch_details.html#questionnaire",
    "href": "Fastbook/15_arch_details.html#questionnaire",
    "title": "Application Architectures Deep Dive",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhat is the “head” of a neural net?\nWhat is the “body” of a neural net?\nWhat is “cutting” a neural net? Why do we need to do this for transfer learning?\nWhat is model_meta? Try printing it to see what’s inside.\nRead the source code for create_head and make sure you understand what each line does.\nLook at the output of create_head and make sure you understand why each layer is there, and how the create_head source created it.\nFigure out how to change the dropout, layer size, and number of layers created by vision_learner, and see if you can find values that result in better accuracy from the pet recognizer.\nWhat does AdaptiveConcatPool2d do?\nWhat is “nearest neighbor interpolation”? How can it be used to upsample convolutional activations?\nWhat is a “transposed convolution”? What is another name for it?\nCreate a conv layer with transpose=True and apply it to an image. Check the output shape.\nDraw the U-Net architecture.\nWhat is “BPTT for Text Classification” (BPT3C)?\nHow do we handle different length sequences in BPT3C?\nTry to run each line of TabularModel.forward separately, one line per cell, in a notebook, and look at the input and output shapes at each step.\nHow is self.layers defined in TabularModel?\nWhat are the five steps for preventing over-fitting?\nWhy don’t we reduce architecture complexity before trying other approaches to preventing overfitting?\n\n\nFurther Research\n\nWrite your own custom head and try training the pet recognizer with it. See if you can get a better result than fastai’s default.\nTry switching between AdaptiveConcatPool2d and AdaptiveAvgPool2d in a CNN head and see what difference it makes.\nWrite your own custom splitter to create a separate parameter group for every ResNet block, and a separate group for the stem. Try training with it, and see if it improves the pet recognizer.\nRead the online chapter about generative image models, and create your own colorizer, super-resolution model, or style transfer model.\nCreate a custom head using nearest neighbor interpolation and use it to do segmentation on CamVid."
  },
  {
    "objectID": "Fastbook/14_resnet.html",
    "href": "Fastbook/14_resnet.html",
    "title": "ResNets",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\n[[chapter_resnet]]\nIn this chapter, we will build on top of the CNNs introduced in the previous chapter and explain to you the ResNet (residual network) architecture. It was introduced in 2015 by Kaiming He et al. in the article “Deep Residual Learning for Image Recognition” and is by far the most used model architecture nowadays. More recent developments in image models almost always use the same trick of residual connections, and most of the time, they are just a tweak of the original ResNet.\nWe will first show you the basic ResNet as it was first designed, then explain to you what modern tweaks make it more performant. But first, we will need a problem a little bit more difficult than the MNIST dataset, since we are already close to 100% accuracy with a regular CNN on it."
  },
  {
    "objectID": "Fastbook/14_resnet.html#going-back-to-imagenette",
    "href": "Fastbook/14_resnet.html#going-back-to-imagenette",
    "title": "ResNets",
    "section": "Going Back to Imagenette",
    "text": "Going Back to Imagenette\nIt’s going to be tough to judge any improvements we make to our models when we are already at an accuracy that is as high as we saw on MNIST in the previous chapter, so we will tackle a tougher image classification problem by going back to Imagenette. We’ll stick with small images to keep things reasonably fast.\nLet’s grab the data—we’ll use the already-resized 160 px version to make things faster still, and will random crop to 128 px:\n\ndef get_data(url, presize, resize):\n    path = untar_data(url)\n    return DataBlock(\n        blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, \n        splitter=GrandparentSplitter(valid_name='val'),\n        get_y=parent_label, item_tfms=Resize(presize),\n        batch_tfms=[*aug_transforms(min_scale=0.5, size=resize),\n                    Normalize.from_stats(*imagenet_stats)],\n    ).dataloaders(path, bs=128)\n\n\ndls = get_data(URLs.IMAGENETTE_160, 160, 128)\n\n\ndls.show_batch(max_n=4)\n\n\n\n\n\n\n\n\nWhen we looked at MNIST we were dealing with 28×28-pixel images. For Imagenette we are going to be training with 128×128-pixel images. Later, we would like to be able to use larger images as well—at least as big as 224×224 pixels, the ImageNet standard. Do you recall how we managed to get a single vector of activations for each image out of the MNIST convolutional neural network?\nThe approach we used was to ensure that there were enough stride-2 convolutions such that the final layer would have a grid size of 1. Then we just flattened out the unit axes that we ended up with, to get a vector for each image (so, a matrix of activations for a mini-batch). We could do the same thing for Imagenette, but that would cause two problems:\n\nWe’d need lots of stride-2 layers to make our grid 1×1 at the end—perhaps more than we would otherwise choose.\nThe model would not work on images of any size other than the size we originally trained on.\n\nOne approach to dealing with the first of these issues would be to flatten the final convolutional layer in a way that handles a grid size other than 1×1. That is, we could simply flatten a matrix into a vector as we have done before, by laying out each row after the previous row. In fact, this is the approach that convolutional neural networks up until 2013 nearly always took. The most famous example is the 2013 ImageNet winner VGG, still sometimes used today. But there was another problem with this architecture: not only did it not work with images other than those of the same size used in the training set, but it required a lot of memory, because flattening out the convolutional layer resulted in many activations being fed into the final layers. Therefore, the weight matrices of the final layers were enormous.\nThis problem was solved through the creation of fully convolutional networks. The trick in fully convolutional networks is to take the average of activations across a convolutional grid. In other words, we can simply use this function:\n\ndef avg_pool(x): return x.mean((2,3))\n\nAs you see, it is taking the mean over the x- and y-axes. This function will always convert a grid of activations into a single activation per image. PyTorch provides a slightly more versatile module called nn.AdaptiveAvgPool2d, which averages a grid of activations into whatever sized destination you require (although we nearly always use a size of 1).\nA fully convolutional network, therefore, has a number of convolutional layers, some of which will be stride 2, at the end of which is an adaptive average pooling layer, a flatten layer to remove the unit axes, and finally a linear layer. Here is our first fully convolutional network:\n\ndef block(ni, nf): return ConvLayer(ni, nf, stride=2)\ndef get_model():\n    return nn.Sequential(\n        block(3, 16),\n        block(16, 32),\n        block(32, 64),\n        block(64, 128),\n        block(128, 256),\n        nn.AdaptiveAvgPool2d(1),\n        Flatten(),\n        nn.Linear(256, dls.c))\n\nWe’re going to be replacing the implementation of block in the network with other variants in a moment, which is why we’re not calling it conv any more. We’re also saving some time by taking advantage of fastai’s ConvLayer, which that already provides the functionality of conv from the last chapter (plus a lot more!).\n\nstop: Consider this question: would this approach makes sense for an optical character recognition (OCR) problem such as MNIST? The vast majority of practitioners tackling OCR and similar problems tend to use fully convolutional networks, because that’s what nearly everybody learns nowadays. But it really doesn’t make any sense! You can’t decide, for instance, whether a number is a 3 or an 8 by slicing it into small pieces, jumbling them up, and deciding whether on average each piece looks like a 3 or an 8. But that’s what adaptive average pooling effectively does! Fully convolutional networks are only really a good choice for objects that don’t have a single correct orientation or size (e.g., like most natural photos).\n\nOnce we are done with our convolutional layers, we will get activations of size bs x ch x h x w (batch size, a certain number of channels, height, and width). We want to convert this to a tensor of size bs x ch, so we take the average over the last two dimensions and flatten the trailing 1×1 dimension like we did in our previous model.\nThis is different from regular pooling in the sense that those layers will generally take the average (for average pooling) or the maximum (for max pooling) of a window of a given size. For instance, max pooling layers of size 2, which were very popular in older CNNs, reduce the size of our image by half on each dimension by taking the maximum of each 2×2 window (with a stride of 2).\nAs before, we can define a Learner with our custom model and then train it on the data we grabbed earlier:\n\ndef get_learner(m):\n    return Learner(dls, m, loss_func=nn.CrossEntropyLoss(), metrics=accuracy\n                  ).to_fp16()\n\nlearn = get_learner(get_model())\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\n\n\n\n3e-3 is often a good learning rate for CNNs, and that appears to be the case here too, so let’s try that:\n\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.901582\n2.155090\n0.325350\n00:07\n\n\n1\n1.559855\n1.586795\n0.507771\n00:07\n\n\n2\n1.296350\n1.295499\n0.571720\n00:07\n\n\n3\n1.144139\n1.139257\n0.639236\n00:07\n\n\n4\n1.049770\n1.092619\n0.659108\n00:07\n\n\n\n\n\nThat’s a pretty good start, considering we have to pick the correct one of 10 categories, and we’re training from scratch for just 5 epochs! We can do way better than this using a deeper mode, but just stacking new layers won’t really improve our results (you can try and see for yourself!). To work around this problem, ResNets introduce the idea of skip connections. We’ll explore those and other aspects of ResNets in the next section."
  },
  {
    "objectID": "Fastbook/14_resnet.html#building-a-modern-cnn-resnet",
    "href": "Fastbook/14_resnet.html#building-a-modern-cnn-resnet",
    "title": "ResNets",
    "section": "Building a Modern CNN: ResNet",
    "text": "Building a Modern CNN: ResNet\nWe now have all the pieces we need to build the models we have been using in our computer vision tasks since the beginning of this book: ResNets. We’ll introduce the main idea behind them and show how it improves accuracy on Imagenette compared to our previous model, before building a version with all the recent tweaks.\n\nSkip Connections\nIn 2015, the authors of the ResNet paper noticed something that they found curious. Even after using batchnorm, they saw that a network using more layers was doing less well than a network using fewer layers—and there were no other differences between the models. Most interestingly, the difference was observed not only in the validation set, but also in the training set; so, it wasn’t just a generalization issue, but a training issue. As the paper explains:\n\n: Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error, as [previously reported] and thoroughly verified by our experiments.\n\nThis phenomenon was illustrated by the graph in &lt;&gt;, with training error on the left and test error on the right.\n\nAs the authors mention here, they are not the first people to have noticed this curious fact. But they were the first to make a very important leap:\n\n: Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model.\n\nAs this is an academic paper this process is described in a rather inaccessible way, but the concept is actually very simple: start with a 20-layer neural network that is trained well, and add another 36 layers that do nothing at all (for instance, they could be linear layers with a single weight equal to 1, and bias equal to 0). The result will be a 56-layer network that does exactly the same thing as the 20-layer network, proving that there are always deep networks that should be at least as good as any shallow network. But for some reason, SGD does not seem able to find them.\n\njargon: Identity mapping: Returning the input without changing it at all. This process is performed by an identity function.\n\nActually, there is another way to create those extra 36 layers, which is much more interesting. What if we replaced every occurrence of conv(x) with x + conv(x), where conv is the function from the previous chapter that adds a second convolution, then a batchnorm layer, then a ReLU. Furthermore, recall that batchnorm does gamma*y + beta. What if we initialized gamma to zero for every one of those final batchnorm layers? Then our conv(x) for those extra 36 layers will always be equal to zero, which means x+conv(x) will always be equal to x.\nWhat has that gained us? The key thing is that those 36 extra layers, as they stand, are an identity mapping, but they have parameters, which means they are trainable. So, we can start with our best 20-layer model, add these 36 extra layers which initially do nothing at all, and then fine-tune the whole 56-layer model. Those extra 36 layers can then learn the parameters that make them most useful.\nThe ResNet paper actually proposed a variant of this, which is to instead “skip over” every second convolution, so effectively we get x+conv2(conv1(x)). This is shown by the diagram in &lt;&gt; (from the paper).\n\nThat arrow on the right is just the x part of x+conv2(conv1(x)), and is known as the identity branch or skip connection. The path on the left is the conv2(conv1(x)) part. You can think of the identity path as providing a direct route from the input to the output.\nIn a ResNet, we don’t actually proceed by first training a smaller number of layers, and then adding new layers on the end and fine-tuning. Instead, we use ResNet blocks like the one in &lt;&gt; throughout the CNN, initialized from scratch in the usual way, and trained with SGD in the usual way. We rely on the skip connections to make the network easier to train with SGD.\nThere’s another (largely equivalent) way to think of these ResNet blocks. This is how the paper describes it:\n\n: Instead of hoping each few stacked layers directly fit a desired underlying mapping, we explicitly let these layers fit a residual mapping. Formally, denoting the desired underlying mapping as H(x), we let the stacked nonlinear layers fit another mapping of F(x) := H(x)−x. The original mapping is recast into F(x)+x. We hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.\n\nAgain, this is rather inaccessible prose—so let’s try to restate it in plain English! If the outcome of a given layer is x, when using a ResNet block that returns y = x+block(x) we’re not asking the block to predict y, we are asking it to predict the difference between y and x. So the job of those blocks isn’t to predict certain features, but to minimize the error between x and the desired y. A ResNet is, therefore, good at learning about slight differences between doing nothing and passing though a block of two convolutional layers (with trainable weights). This is how these models got their name: they’re predicting residuals (reminder: “residual” is prediction minus target).\nOne key concept that both of these two ways of thinking about ResNets share is the idea of ease of learning. This is an important theme. Recall the universal approximation theorem, which states that a sufficiently large network can learn anything. This is still true, but there turns out to be a very important difference between what a network can learn in principle, and what it is easy for it to learn with realistic data and training regimes. Many of the advances in neural networks over the last decade have been like the ResNet block: the result of realizing how to make something that was always possible actually feasible.\n\nnote: True Identity Path: The original paper didn’t actually do the trick of using zero for the initial value of gamma in the last batchnorm layer of each block; that came a couple of years later. So, the original version of ResNet didn’t quite begin training with a truly identity path through the ResNet blocks, but nonetheless having the ability to “navigate through” the skip connections did indeed make it train better. Adding the batchnorm gamma init trick made the models train at even higher learning rates.\n\nHere’s the definition of a simple ResNet block (where norm_type=NormType.BatchZero causes fastai to init the gamma weights of the last batchnorm layer to zero):\n\nclass ResBlock(Module):\n    def __init__(self, ni, nf):\n        self.convs = nn.Sequential(\n            ConvLayer(ni,nf),\n            ConvLayer(nf,nf, norm_type=NormType.BatchZero))\n        \n    def forward(self, x): return x + self.convs(x)\n\nThere are two problems with this, however: it can’t handle a stride other than 1, and it requires that ni==nf. Stop for a moment to think carefully about why this is.\nThe issue is that with a stride of, say, 2 on one of the convolutions, the grid size of the output activations will be half the size on each axis of the input. So then we can’t add that back to x in forward because x and the output activations have different dimensions. The same basic issue occurs if ni!=nf: the shapes of the input and output connections won’t allow us to add them together.\nTo fix this, we need a way to change the shape of x to match the result of self.convs. Halving the grid size can be done using an average pooling layer with a stride of 2: that is, a layer that takes 2×2 patches from the input and replaces them with their average.\nChanging the number of channels can be done by using a convolution. We want this skip connection to be as close to an identity map as possible, however, which means making this convolution as simple as possible. The simplest possible convolution is one where the kernel size is 1. That means that the kernel is size ni*nf*1*1, so it’s only doing a dot product over the channels of each input pixel—it’s not combining across pixels at all. This kind of 1x1 convolution is very widely used in modern CNNs, so take a moment to think about how it works.\n\njargon: 1x1 convolution: A convolution with a kernel size of 1.\n\nHere’s a ResBlock using these tricks to handle changing shape in the skip connection:\n\ndef _conv_block(ni,nf,stride):\n    return nn.Sequential(\n        ConvLayer(ni, nf, stride=stride),\n        ConvLayer(nf, nf, act_cls=None, norm_type=NormType.BatchZero))\n\n\nclass ResBlock(Module):\n    def __init__(self, ni, nf, stride=1):\n        self.convs = _conv_block(ni,nf,stride)\n        self.idconv = noop if ni==nf else ConvLayer(ni, nf, 1, act_cls=None)\n        self.pool = noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n\n    def forward(self, x):\n        return F.relu(self.convs(x) + self.idconv(self.pool(x)))\n\nNote that we’re using the noop function here, which simply returns its input unchanged (noop is a computer science term that stands for “no operation”). In this case, idconv does nothing at all if ni==nf, and pool does nothing if stride==1, which is what we wanted in our skip connection.\nAlso, you’ll see that we’ve removed the ReLU (act_cls=None) from the final convolution in convs and from idconv, and moved it to after we add the skip connection. The thinking behind this is that the whole ResNet block is like a layer, and you want your activation to be after your layer.\nLet’s replace our block with ResBlock, and try it out:\n\ndef block(ni,nf): return ResBlock(ni, nf, stride=2)\nlearn = get_learner(get_model())\n\n\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.973174\n1.845491\n0.373248\n00:08\n\n\n1\n1.678627\n1.778713\n0.439236\n00:08\n\n\n2\n1.386163\n1.596503\n0.507261\n00:08\n\n\n3\n1.177839\n1.102993\n0.644841\n00:09\n\n\n4\n1.052435\n1.038013\n0.667771\n00:09\n\n\n\n\n\nIt’s not much better. But the whole point of this was to allow us to train deeper models, and we’re not really taking advantage of that yet. To create a model that’s, say, twice as deep, all we need to do is replace our block with two ResBlocks in a row:\n\ndef block(ni, nf):\n    return nn.Sequential(ResBlock(ni, nf, stride=2), ResBlock(nf, nf))\n\n\nlearn = get_learner(get_model())\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.964076\n1.864578\n0.355159\n00:12\n\n\n1\n1.636880\n1.596789\n0.502675\n00:12\n\n\n2\n1.335378\n1.304472\n0.588535\n00:12\n\n\n3\n1.089160\n1.065063\n0.663185\n00:12\n\n\n4\n0.942904\n0.963589\n0.692739\n00:12\n\n\n\n\n\nNow we’re making good progress!\nThe authors of the ResNet paper went on to win the 2015 ImageNet challenge. At the time, this was by far the most important annual event in computer vision. We have already seen another ImageNet winner: the 2013 winners, Zeiler and Fergus. It is interesting to note that in both cases the starting points for the breakthroughs were experimental observations: observations about what layers actually learn, in the case of Zeiler and Fergus, and observations about which kinds of networks can be trained, in the case of the ResNet authors. This ability to design and analyze thoughtful experiments, or even just to see an unexpected result, say “Hmmm, that’s interesting,” and then, most importantly, set about figuring out what on earth is going on, with great tenacity, is at the heart of many scientific discoveries. Deep learning is not like pure mathematics. It is a heavily experimental field, so it’s important to be a strong practitioner, not just a theoretician.\nSince the ResNet was introduced, it’s been widely studied and applied to many domains. One of the most interesting papers, published in 2018, is Hao Li et al.’s “Visualizing the Loss Landscape of Neural Nets”. It shows that using skip connections helps smooth the loss function, which makes training easier as it avoids falling into a very sharp area. &lt;&gt; shows a stunning picture from the paper, illustrating the difference between the bumpy terrain that SGD has to navigate to optimize a regular CNN (left) versus the smooth surface of a ResNet (right).\n\nOur first model is already good, but further research has discovered more tricks we can apply to make it better. We’ll look at those next.\n\n\nA State-of-the-Art ResNet\nIn “Bag of Tricks for Image Classification with Convolutional Neural Networks”, Tong He et al. study different variations of the ResNet architecture that come at almost no additional cost in terms of number of parameters or computation. By using a tweaked ResNet-50 architecture and Mixup they achieved 94.6% top-5 accuracy on ImageNet, in comparison to 92.2% with a regular ResNet-50 without Mixup. This result is better than that achieved by regular ResNet models that are twice as deep (and twice as slow, and much more likely to overfit).\n\njargon: top-5 accuracy: A metric testing how often the label we want is in the top 5 predictions of our model. It was used in the ImageNet competition because many of the images contained multiple objects, or contained objects that could be easily confused or may even have been mislabeled with a similar label. In these situations, looking at top-1 accuracy may be inappropriate. However, recently CNNs have been getting so good that top-5 accuracy is nearly 100%, so some researchers are using top-1 accuracy for ImageNet too now.\n\nWe’ll use this tweaked version as we scale up to the full ResNet, because it’s substantially better. It differs a little bit from our previous implementation, in that instead of just starting with ResNet blocks, it begins with a few convolutional layers followed by a max pooling layer. This is what the first layers, called the stem of the network, look like:\n\ndef _resnet_stem(*sizes):\n    return [\n        ConvLayer(sizes[i], sizes[i+1], 3, stride = 2 if i==0 else 1)\n            for i in range(len(sizes)-1)\n    ] + [nn.MaxPool2d(kernel_size=3, stride=2, padding=1)]\n\n\n#hide_output\n_resnet_stem(3,32,32,64)\n\n[ConvLayer(\n   (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n   (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n   (2): ReLU()\n ), ConvLayer(\n   (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n   (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n   (2): ReLU()\n ), ConvLayer(\n   (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n   (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n   (2): ReLU()\n ), MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)]\n\n\n[ConvLayer(\n   (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n   (1): BatchNorm2d(32, eps=1e-05, momentum=0.1)\n   (2): ReLU()\n ), ConvLayer(\n   (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n   (1): BatchNorm2d(32, eps=1e-05, momentum=0.1)\n   (2): ReLU()\n ), ConvLayer(\n   (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n   (1): BatchNorm2d(64, eps=1e-05, momentum=0.1)\n   (2): ReLU()\n ), MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=False)]\n\njargon: Stem: The first few layers of a CNN. Generally, the stem has a different structure than the main body of the CNN.\n\nThe reason that we have a stem of plain convolutional layers, instead of ResNet blocks, is based on a very important insight about all deep convolutional neural networks: the vast majority of the computation occurs in the early layers. Therefore, we should keep the early layers as fast and simple as possible.\nTo see why so much computation occurs in the early layers, consider the very first convolution on a 128-pixel input image. If it is a stride-1 convolution, then it will apply the kernel to every one of the 128×128 pixels. That’s a lot of work! In the later layers, however, the grid size could be as small as 4×4 or even 2×2, so there are far fewer kernel applications to do.\nOn the other hand, the first-layer convolution only has 3 input features and 32 output features. Since it is a 3×3 kernel, this is 3×32×3×3 = 864 parameters in the weights. But the last convolution will have 256 input features and 512 output features, resulting in 1,179,648 weights! So the first layers contain the vast majority of the computation, but the last layers contain the vast majority of the parameters.\nA ResNet block takes more computation than a plain convolutional block, since (in the stride-2 case) a ResNet block has three convolutions and a pooling layer. That’s why we want to have plain convolutions to start off our ResNet.\nWe’re now ready to show the implementation of a modern ResNet, with the “bag of tricks.” It uses four groups of ResNet blocks, with 64, 128, 256, then 512 filters. Each group starts with a stride-2 block, except for the first one, since it’s just after a MaxPooling layer:\n\nclass ResNet(nn.Sequential):\n    def __init__(self, n_out, layers, expansion=1):\n        stem = _resnet_stem(3,32,32,64)\n        self.block_szs = [64, 64, 128, 256, 512]\n        for i in range(1,5): self.block_szs[i] *= expansion\n        blocks = [self._make_layer(*o) for o in enumerate(layers)]\n        super().__init__(*stem, *blocks,\n                         nn.AdaptiveAvgPool2d(1), Flatten(),\n                         nn.Linear(self.block_szs[-1], n_out))\n    \n    def _make_layer(self, idx, n_layers):\n        stride = 1 if idx==0 else 2\n        ch_in,ch_out = self.block_szs[idx:idx+2]\n        return nn.Sequential(*[\n            ResBlock(ch_in if i==0 else ch_out, ch_out, stride if i==0 else 1)\n            for i in range(n_layers)\n        ])\n\nThe _make_layer function is just there to create a series of n_layers blocks. The first one is going from ch_in to ch_out with the indicated stride and all the others are blocks of stride 1 with ch_out to ch_out tensors. Once the blocks are defined, our model is purely sequential, which is why we define it as a subclass of nn.Sequential. (Ignore the expansion parameter for now; we’ll discuss it in the next section. For now, it’ll be 1, so it doesn’t do anything.)\nThe various versions of the models (ResNet-18, -34, -50, etc.) just change the number of blocks in each of those groups. This is the definition of a ResNet-18:\n\nrn = ResNet(dls.c, [2,2,2,2])\n\nLet’s train it for a little bit and see how it fares compared to the previous model:\n\nlearn = get_learner(rn)\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.673882\n1.828394\n0.413758\n00:13\n\n\n1\n1.331675\n1.572685\n0.518217\n00:13\n\n\n2\n1.087224\n1.086102\n0.650701\n00:13\n\n\n3\n0.900428\n0.968219\n0.684331\n00:12\n\n\n4\n0.760280\n0.782558\n0.757197\n00:12\n\n\n\n\n\nEven though we have more channels (and our model is therefore even more accurate), our training is just as fast as before, thanks to our optimized stem.\nTo make our model deeper without taking too much compute or memory, we can use another kind of layer introduced by the ResNet paper for ResNets with a depth of 50 or more: the bottleneck layer.\n\n\nBottleneck Layers\nInstead of stacking two convolutions with a kernel size of 3, bottleneck layers use three different convolutions: two 1×1 (at the beginning and the end) and one 3×3, as shown on the right in &lt;&gt;.\n\nWhy is that useful? 1×1 convolutions are much faster, so even if this seems to be a more complex design, this block executes faster than the first ResNet block we saw. This then lets us use more filters: as we see in the illustration, the number of filters in and out is 4 times higher (256 instead of 64) diminish then restore the number of channels (hence the name bottleneck). The overall impact is that we can use more filters in the same amount of time.\nLet’s try replacing our ResBlock with this bottleneck design:\n\ndef _conv_block(ni,nf,stride):\n    return nn.Sequential(\n        ConvLayer(ni, nf//4, 1),\n        ConvLayer(nf//4, nf//4, stride=stride), \n        ConvLayer(nf//4, nf, 1, act_cls=None, norm_type=NormType.BatchZero))\n\nWe’ll use this to create a ResNet-50 with group sizes of (3,4,6,3). We now need to pass 4 in to the expansion parameter of ResNet, since we need to start with four times less channels and we’ll end with four times more channels.\nDeeper networks like this don’t generally show improvements when training for only 5 epochs, so we’ll bump it up to 20 epochs this time to make the most of our bigger model. And to really get great results, let’s use bigger images too:\n\ndls = get_data(URLs.IMAGENETTE_320, presize=320, resize=224)\n\nWe don’t have to do anything to account for the larger 224-pixel images; thanks to our fully convolutional network, it just works. This is also why we were able to do progressive resizing earlier in the book—the models we used were fully convolutional, so we were even able to fine-tune models trained with different sizes. We can now train our model and see the effects:\n\nrn = ResNet(dls.c, [3,4,6,3], 4)\n\n\nlearn = get_learner(rn)\nlearn.fit_one_cycle(20, 3e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.613448\n1.473355\n0.514140\n00:31\n\n\n1\n1.359604\n2.050794\n0.397452\n00:31\n\n\n2\n1.253112\n4.511735\n0.387006\n00:31\n\n\n3\n1.133450\n2.575221\n0.396178\n00:31\n\n\n4\n1.054752\n1.264525\n0.613758\n00:32\n\n\n5\n0.927930\n2.670484\n0.422675\n00:32\n\n\n6\n0.838268\n1.724588\n0.528662\n00:32\n\n\n7\n0.748289\n1.180668\n0.666497\n00:31\n\n\n8\n0.688637\n1.245039\n0.650446\n00:32\n\n\n9\n0.645530\n1.053691\n0.674904\n00:31\n\n\n10\n0.593401\n1.180786\n0.676433\n00:32\n\n\n11\n0.536634\n0.879937\n0.713885\n00:32\n\n\n12\n0.479208\n0.798356\n0.741656\n00:32\n\n\n13\n0.440071\n0.600644\n0.806879\n00:32\n\n\n14\n0.402952\n0.450296\n0.858599\n00:32\n\n\n15\n0.359117\n0.486126\n0.846369\n00:32\n\n\n16\n0.313642\n0.442215\n0.861911\n00:32\n\n\n17\n0.294050\n0.485967\n0.853503\n00:32\n\n\n18\n0.270583\n0.408566\n0.875924\n00:32\n\n\n19\n0.266003\n0.411752\n0.872611\n00:33\n\n\n\n\n\nWe’re getting a great result now! Try adding Mixup, and then training this for a hundred epochs while you go get lunch. You’ll have yourself a very accurate image classifier, trained from scratch.\nThe bottleneck design we’ve shown here is typically only used in ResNet-50, -101, and -152 models. ResNet-18 and -34 models usually use the non-bottleneck design seen in the previous section. However, we’ve noticed that the bottleneck layer generally works better even for the shallower networks. This just goes to show that the little details in papers tend to stick around for years, even if they’re actually not quite the best design! Questioning assumptions and “stuff everyone knows” is always a good idea, because this is still a new field, and there are lots of details that aren’t always done well."
  },
  {
    "objectID": "Fastbook/14_resnet.html#conclusion",
    "href": "Fastbook/14_resnet.html#conclusion",
    "title": "ResNets",
    "section": "Conclusion",
    "text": "Conclusion\nYou have now seen how the models we have been using for computer vision since the first chapter are built, using skip connections to allow deeper models to be trained. Even if there has been a lot of research into better architectures, they all use one version or another of this trick, to make a direct path from the input to the end of the network. When using transfer learning, the ResNet is the pretrained model. In the next chapter, we will look at the final details of how the models we actually used were built from it."
  },
  {
    "objectID": "Fastbook/14_resnet.html#questionnaire",
    "href": "Fastbook/14_resnet.html#questionnaire",
    "title": "ResNets",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nHow did we get to a single vector of activations in the CNNs used for MNIST in previous chapters? Why isn’t that suitable for Imagenette?\nWhat do we do for Imagenette instead?\nWhat is “adaptive pooling”?\nWhat is “average pooling”?\nWhy do we need Flatten after an adaptive average pooling layer?\nWhat is a “skip connection”?\nWhy do skip connections allow us to train deeper models?\nWhat does &lt;&gt; show? How did that lead to the idea of skip connections?\nWhat is “identity mapping”?\nWhat is the basic equation for a ResNet block (ignoring batchnorm and ReLU layers)?\nWhat do ResNets have to do with residuals?\nHow do we deal with the skip connection when there is a stride-2 convolution? How about when the number of filters changes?\nHow can we express a 1×1 convolution in terms of a vector dot product?\nCreate a 1x1 convolution with F.conv2d or nn.Conv2d and apply it to an image. What happens to the shape of the image?\nWhat does the noop function return?\nExplain what is shown in &lt;&gt;.\nWhen is top-5 accuracy a better metric than top-1 accuracy?\nWhat is the “stem” of a CNN?\nWhy do we use plain convolutions in the CNN stem, instead of ResNet blocks?\nHow does a bottleneck block differ from a plain ResNet block?\nWhy is a bottleneck block faster?\nHow do fully convolutional nets (and nets with adaptive pooling in general) allow for progressive resizing?\n\n\nFurther Research\n\nTry creating a fully convolutional net with adaptive average pooling for MNIST (note that you’ll need fewer stride-2 layers). How does it compare to a network without such a pooling layer?\nIn &lt;&gt; we introduce Einstein summation notation. Skip ahead to see how this works, and then write an implementation of the 1×1 convolution operation using torch.einsum. Compare it to the same operation using torch.conv2d.\nWrite a “top-5 accuracy” function using plain PyTorch or plain Python.\nTrain a model on Imagenette for more epochs, with and without label smoothing. Take a look at the Imagenette leaderboards and see how close you can get to the best results shown. Read the linked pages describing the leading approaches."
  },
  {
    "objectID": "Fastbook/01_intro.html",
    "href": "Fastbook/01_intro.html",
    "title": "Your Deep Learning Journey",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\n[[chapter_intro]]\nHello, and thank you for letting us join you on your deep learning journey, however far along that you may be! In this chapter, we will tell you a little bit more about what to expect in this book, introduce the key concepts behind deep learning, and train our first models on different tasks. It doesn’t matter if you don’t come from a technical or a mathematical background (though it’s okay if you do too!); we wrote this book to make deep learning accessible to as many people as possible."
  },
  {
    "objectID": "Fastbook/01_intro.html#deep-learning-is-for-everyone",
    "href": "Fastbook/01_intro.html#deep-learning-is-for-everyone",
    "title": "Your Deep Learning Journey",
    "section": "Deep Learning Is for Everyone",
    "text": "Deep Learning Is for Everyone\nA lot of people assume that you need all kinds of hard-to-find stuff to get great results with deep learning, but as you’ll see in this book, those people are wrong. &lt;&gt; is a list of a few thing you absolutely don’t need to do world-class deep learning.\n[[myths]]\n.What you don't need to do deep learning\n[options=\"header\"]\n|======\n| Myth (don't need) | Truth\n| Lots of math | Just high school math is sufficient\n| Lots of data | We've seen record-breaking results with &lt;50 items of data\n| Lots of expensive computers | You can get what you need for state of the art work for free\n|======\nDeep learning is a computer technique to extract and transform data–-with use cases ranging from human speech recognition to animal imagery classification–-by using multiple layers of neural networks. Each of these layers takes its inputs from previous layers and progressively refines them. The layers are trained by algorithms that minimize their errors and improve their accuracy. In this way, the network learns to perform a specified task. We will discuss training algorithms in detail in the next section.\nDeep learning has power, flexibility, and simplicity. That’s why we believe it should be applied across many disciplines. These include the social and physical sciences, the arts, medicine, finance, scientific research, and many more. To give a personal example, despite having no background in medicine, Jeremy started Enlitic, a company that uses deep learning algorithms to diagnose illness and disease. Within months of starting the company, it was announced that its algorithm could identify malignant tumors more accurately than radiologists.\nHere’s a list of some of the thousands of tasks in different areas at which deep learning, or methods heavily using deep learning, is now the best in the world:\n\nNatural language processing (NLP):: Answering questions; speech recognition; summarizing documents; classifying documents; finding names, dates, etc. in documents; searching for articles mentioning a concept\nComputer vision:: Satellite and drone imagery interpretation (e.g., for disaster resilience); face recognition; image captioning; reading traffic signs; locating pedestrians and vehicles in autonomous vehicles\nMedicine:: Finding anomalies in radiology images, including CT, MRI, and X-ray images; counting features in pathology slides; measuring features in ultrasounds; diagnosing diabetic retinopathy\nBiology:: Folding proteins; classifying proteins; many genomics tasks, such as tumor-normal sequencing and classifying clinically actionable genetic mutations; cell classification; analyzing protein/protein interactions\nImage generation:: Colorizing images; increasing image resolution; removing noise from images; converting images to art in the style of famous artists\nRecommendation systems:: Web search; product recommendations; home page layout\nPlaying games:: Chess, Go, most Atari video games, and many real-time strategy games\nRobotics:: Handling objects that are challenging to locate (e.g., transparent, shiny, lacking texture) or hard to pick up\nOther applications:: Financial and logistical forecasting, text to speech, and much more…\n\nWhat is remarkable is that deep learning has such varied application yet nearly all of deep learning is based on a single type of model, the neural network.\nBut neural networks are not in fact completely new. In order to have a wider perspective on the field, it is worth it to start with a bit of history."
  },
  {
    "objectID": "Fastbook/01_intro.html#neural-networks-a-brief-history",
    "href": "Fastbook/01_intro.html#neural-networks-a-brief-history",
    "title": "Your Deep Learning Journey",
    "section": "Neural Networks: A Brief History",
    "text": "Neural Networks: A Brief History\nIn 1943 Warren McCulloch, a neurophysiologist, and Walter Pitts, a logician, teamed up to develop a mathematical model of an artificial neuron. In their paper “A Logical Calculus of the Ideas Immanent in Nervous Activity” they declared that:\n\n: Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms.\n\nMcCulloch and Pitts realized that a simplified model of a real neuron could be represented using simple addition and thresholding, as shown in &lt;&gt;. Pitts was self-taught, and by age 12, had received an offer to study at Cambridge University with the great Bertrand Russell. He did not take up this invitation, and indeed throughout his life did not accept any offers of advanced degrees or positions of authority. Most of his famous work was done while he was homeless. Despite his lack of an officially recognized position and increasing social isolation, his work with McCulloch was influential, and was taken up by a psychologist named Frank Rosenblatt.\n\nRosenblatt further developed the artificial neuron to give it the ability to learn. Even more importantly, he worked on building the first device that actually used these principles, the Mark I Perceptron. In “The Design of an Intelligent Automaton” Rosenblatt wrote about this work: “We are now about to witness the birth of such a machine–-a machine capable of perceiving, recognizing and identifying its surroundings without any human training or control.” The perceptron was built, and was able to successfully recognize simple shapes.\nAn MIT professor named Marvin Minsky (who was a grade behind Rosenblatt at the same high school!), along with Seymour Papert, wrote a book called Perceptrons (MIT Press), about Rosenblatt’s invention. They showed that a single layer of these devices was unable to learn some simple but critical mathematical functions (such as XOR). In the same book, they also showed that using multiple layers of the devices would allow these limitations to be addressed. Unfortunately, only the first of these insights was widely recognized. As a result, the global academic community nearly entirely gave up on neural networks for the next two decades.\nPerhaps the most pivotal work in neural networks in the last 50 years was the multi-volume Parallel Distributed Processing (PDP) by David Rumelhart, James McClellan, and the PDP Research Group, released in 1986 by MIT Press. Chapter 1 lays out a similar hope to that shown by Rosenblatt:\n\n: People are smarter than today’s computers because the brain employs a basic computational architecture that is more suited to deal with a central aspect of the natural information processing tasks that people are so good at. …We will introduce a computational framework for modeling cognitive processes that seems… closer than other frameworks to the style of computation as it might be done by the brain.\n\nThe premise that PDP is using here is that traditional computer programs work very differently to brains, and that might be why computer programs had been (at that point) so bad at doing things that brains find easy (such as recognizing objects in pictures). The authors claimed that the PDP approach was “closer than other frameworks” to how the brain works, and therefore it might be better able to handle these kinds of tasks.\nIn fact, the approach laid out in PDP is very similar to the approach used in today’s neural networks. The book defined parallel distributed processing as requiring:\n\nA set of processing units\nA state of activation\nAn output function for each unit\nA pattern of connectivity among units\nA propagation rule for propagating patterns of activities through the network of connectivities\nAn activation rule for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit\nA learning rule whereby patterns of connectivity are modified by experience\nAn environment within which the system must operate\n\nWe will see in this book that modern neural networks handle each of these requirements.\nIn the 1980’s most models were built with a second layer of neurons, thus avoiding the problem that had been identified by Minsky and Papert (this was their “pattern of connectivity among units,” to use the framework above). And indeed, neural networks were widely used during the ’80s and ’90s for real, practical projects. However, again a misunderstanding of the theoretical issues held back the field. In theory, adding just one extra layer of neurons was enough to allow any mathematical function to be approximated with these neural networks, but in practice such networks were often too big and too slow to be useful.\nAlthough researchers showed 30 years ago that to get practical good performance you need to use even more layers of neurons, it is only in the last decade that this principle has been more widely appreciated and applied. Neural networks are now finally living up to their potential, thanks to the use of more layers, coupled with the capacity to do so due to improvements in computer hardware, increases in data availability, and algorithmic tweaks that allow neural networks to be trained faster and more easily. We now have what Rosenblatt promised: “a machine capable of perceiving, recognizing, and identifying its surroundings without any human training or control.”\nThis is what you will learn how to build in this book. But first, since we are going to be spending a lot of time together, let’s get to know each other a bit…"
  },
  {
    "objectID": "Fastbook/01_intro.html#who-we-are",
    "href": "Fastbook/01_intro.html#who-we-are",
    "title": "Your Deep Learning Journey",
    "section": "Who We Are",
    "text": "Who We Are\nWe are Sylvain and Jeremy, your guides on this journey. We hope that you will find us well suited for this position.\nJeremy has been using and teaching machine learning for around 30 years. He started using neural networks 25 years ago. During this time, he has led many companies and projects that have machine learning at their core, including founding the first company to focus on deep learning and medicine, Enlitic, and taking on the role of President and Chief Scientist of the world’s largest machine learning community, Kaggle. He is the co-founder, along with Dr. Rachel Thomas, of fast.ai, the organization that built the course this book is based on.\nFrom time to time you will hear directly from us, in sidebars like this one from Jeremy:\n\nJ: Hi everybody, I’m Jeremy! You might be interested to know that I do not have any formal technical education. I completed a BA, with a major in philosophy, and didn’t have great grades. I was much more interested in doing real projects, rather than theoretical studies, so I worked full time at a management consulting firm called McKinsey and Company throughout my university years. If you’re somebody who would rather get their hands dirty building stuff than spend years learning abstract concepts, then you will understand where I am coming from! Look out for sidebars from me to find information most suited to people with a less mathematical or formal technical background—that is, people like me…\n\nSylvain, on the other hand, knows a lot about formal technical education. In fact, he has written 10 math textbooks, covering the entire advanced French maths curriculum!\n\nS: Unlike Jeremy, I have not spent many years coding and applying machine learning algorithms. Rather, I recently came to the machine learning world, by watching Jeremy’s fast.ai course videos. So, if you are somebody who has not opened a terminal and written commands at the command line, then you will understand where I am coming from! Look out for sidebars from me to find information most suited to people with a more mathematical or formal technical background, but less real-world coding experience—that is, people like me…\n\nThe fast.ai course has been studied by hundreds of thousands of students, from all walks of life, from all parts of the world. Sylvain stood out as the most impressive student of the course that Jeremy had ever seen, which led to him joining fast.ai, and then becoming the coauthor, along with Jeremy, of the fastai software library.\nAll this means that between us you have the best of both worlds: the people who know more about the software than anybody else, because they wrote it; an expert on math, and an expert on coding and machine learning; and also people who understand both what it feels like to be a relative outsider in math, and a relative outsider in coding and machine learning.\nAnybody who has watched sports knows that if you have a two-person commentary team then you also need a third person to do “special comments.” Our special commentator is Alexis Gallagher. Alexis has a very diverse background: he has been a researcher in mathematical biology, a screenplay writer, an improv performer, a McKinsey consultant (like Jeremy!), a Swift coder, and a CTO.\n\nA: I’ve decided it’s time for me to learn about this AI stuff! After all, I’ve tried pretty much everything else… But I don’t really have a background in building machine learning models. Still… how hard can it be? I’m going to be learning throughout this book, just like you are. Look out for my sidebars for learning tips that I found helpful on my journey, and hopefully you will find helpful too."
  },
  {
    "objectID": "Fastbook/01_intro.html#how-to-learn-deep-learning",
    "href": "Fastbook/01_intro.html#how-to-learn-deep-learning",
    "title": "Your Deep Learning Journey",
    "section": "How to Learn Deep Learning",
    "text": "How to Learn Deep Learning\nHarvard professor David Perkins, who wrote Making Learning Whole (Jossey-Bass), has much to say about teaching. The basic idea is to teach the whole game. That means that if you’re teaching baseball, you first take people to a baseball game or get them to play it. You don’t teach them how to wind twine to make a baseball from scratch, the physics of a parabola, or the coefficient of friction of a ball on a bat.\nPaul Lockhart, a Columbia math PhD, former Brown professor, and K-12 math teacher, imagines in the influential essay “A Mathematician’s Lament” a nightmare world where music and art are taught the way math is taught. Children are not allowed to listen to or play music until they have spent over a decade mastering music notation and theory, spending classes transposing sheet music into a different key. In art class, students study colors and applicators, but aren’t allowed to actually paint until college. Sound absurd? This is how math is taught–-we require students to spend years doing rote memorization and learning dry, disconnected fundamentals that we claim will pay off later, long after most of them quit the subject.\nUnfortunately, this is where many teaching resources on deep learning begin–-asking learners to follow along with the definition of the Hessian and theorems for the Taylor approximation of your loss functions, without ever giving examples of actual working code. We’re not knocking calculus. We love calculus, and Sylvain has even taught it at the college level, but we don’t think it’s the best place to start when learning deep learning!\nIn deep learning, it really helps if you have the motivation to fix your model to get it to do better. That’s when you start learning the relevant theory. But you need to have the model in the first place. We teach almost everything through real examples. As we build out those examples, we go deeper and deeper, and we’ll show you how to make your projects better and better. This means that you’ll be gradually learning all the theoretical foundations you need, in context, in such a way that you’ll see why it matters and how it works.\nSo, here’s our commitment to you. Throughout this book, we will follow these principles:\n\nTeaching the whole game. We’ll start by showing how to use a complete, working, very usable, state-of-the-art deep learning network to solve real-world problems, using simple, expressive tools. And then we’ll gradually dig deeper and deeper into understanding how those tools are made, and how the tools that make those tools are made, and so on…\nAlways teaching through examples. We’ll ensure that there is a context and a purpose that you can understand intuitively, rather than starting with algebraic symbol manipulation.\nSimplifying as much as possible. We’ve spent years building tools and teaching methods that make previously complex topics very simple.\nRemoving barriers. Deep learning has, until now, been a very exclusive game. We’re breaking it open, and ensuring that everyone can play.\n\nThe hardest part of deep learning is artisanal: how do you know if you’ve got enough data, whether it is in the right format, if your model is training properly, and, if it’s not, what you should do about it? That is why we believe in learning by doing. As with basic data science skills, with deep learning you only get better through practical experience. Trying to spend too much time on the theory can be counterproductive. The key is to just code and try to solve problems: the theory can come later, when you have context and motivation.\nThere will be times when the journey will feel hard. Times where you feel stuck. Don’t give up! Rewind through the book to find the last bit where you definitely weren’t stuck, and then read slowly through from there to find the first thing that isn’t clear. Then try some code experiments yourself, and Google around for more tutorials on whatever the issue you’re stuck with is—often you’ll find some different angle on the material might help it to click. Also, it’s expected and normal to not understand everything (especially the code) on first reading. Trying to understand the material serially before proceeding can sometimes be hard. Sometimes things click into place after you get more context from parts down the road, from having a bigger picture. So if you do get stuck on a section, try moving on anyway and make a note to come back to it later.\nRemember, you don’t need any particular academic background to succeed at deep learning. Many important breakthroughs are made in research and industry by folks without a PhD, such as “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks”—one of the most influential papers of the last decade—with over 5,000 citations, which was written by Alec Radford when he was an undergraduate. Even at Tesla, where they’re trying to solve the extremely tough challenge of making a self-driving car, CEO Elon Musk says:\n\n: A PhD is definitely not required. All that matters is a deep understanding of AI & ability to implement NNs in a way that is actually useful (latter point is what’s truly hard). Don’t care if you even graduated high school.\n\nWhat you will need to do to succeed however is to apply what you learn in this book to a personal project, and always persevere.\n\nYour Projects and Your Mindset\nWhether you’re excited to identify if plants are diseased from pictures of their leaves, auto-generate knitting patterns, diagnose TB from X-rays, or determine when a raccoon is using your cat door, we will get you using deep learning on your own problems (via pre-trained models from others) as quickly as possible, and then will progressively drill into more details. You’ll learn how to use deep learning to solve your own problems at state-of-the-art accuracy within the first 30 minutes of the next chapter! (And feel free to skip straight there now if you’re dying to get coding right away.) There is a pernicious myth out there that you need to have computing resources and datasets the size of those at Google to be able to do deep learning, but it’s not true.\nSo, what sorts of tasks make for good test cases? You could train your model to distinguish between Picasso and Monet paintings or to pick out pictures of your daughter instead of pictures of your son. It helps to focus on your hobbies and passions–-setting yourself four or five little projects rather than striving to solve a big, grand problem tends to work better when you’re getting started. Since it is easy to get stuck, trying to be too ambitious too early can often backfire. Then, once you’ve got the basics mastered, aim to complete something you’re really proud of!\n\nJ: Deep learning can be set to work on almost any problem. For instance, my first startup was a company called FastMail, which provided enhanced email services when it launched in 1999 (and still does to this day). In 2002 I set it up to use a primitive form of deep learning, single-layer neural networks, to help categorize emails and stop customers from receiving spam.\n\nCommon character traits in the people that do well at deep learning include playfulness and curiosity. The late physicist Richard Feynman is an example of someone who we’d expect to be great at deep learning: his development of an understanding of the movement of subatomic particles came from his amusement at how plates wobble when they spin in the air.\nLet’s now focus on what you will learn, starting with the software."
  },
  {
    "objectID": "Fastbook/01_intro.html#the-software-pytorch-fastai-and-jupyter",
    "href": "Fastbook/01_intro.html#the-software-pytorch-fastai-and-jupyter",
    "title": "Your Deep Learning Journey",
    "section": "The Software: PyTorch, fastai, and Jupyter",
    "text": "The Software: PyTorch, fastai, and Jupyter\n(And Why It Doesn’t Matter)\nWe’ve completed hundreds of machine learning projects using dozens of different packages, and many different programming languages. At fast.ai, we have written courses using most of the main deep learning and machine learning packages used today. After PyTorch came out in 2017 we spent over a thousand hours testing it before deciding that we would use it for future courses, software development, and research. Since that time PyTorch has become the world’s fastest-growing deep learning library and is already used for most research papers at top conferences. This is generally a leading indicator of usage in industry, because these are the papers that end up getting used in products and services commercially. We have found that PyTorch is the most flexible and expressive library for deep learning. It does not trade off speed for simplicity, but provides both.\nPyTorch works best as a low-level foundation library, providing the basic operations for higher-level functionality. The fastai library is the most popular library for adding this higher-level functionality on top of PyTorch. It’s also particularly well suited to the purposes of this book, because it is unique in providing a deeply layered software architecture (there’s even a peer-reviewed academic paper about this layered API). In this book, as we go deeper and deeper into the foundations of deep learning, we will also go deeper and deeper into the layers of fastai. This book covers version 2 of the fastai library, which is a from-scratch rewrite providing many unique features.\nHowever, it doesn’t really matter what software you learn, because it takes only a few days to learn to switch from one library to another. What really matters is learning the deep learning foundations and techniques properly. Our focus will be on using code that clearly expresses the concepts that you need to learn. Where we are teaching high-level concepts, we will use high-level fastai code. Where we are teaching low-level concepts, we will use low-level PyTorch, or even pure Python code.\nIf it feels like new deep learning libraries are appearing at a rapid pace nowadays, then you need to be prepared for a much faster rate of change in the coming months and years. As more people enter the field, they will bring more skills and ideas, and try more things. You should assume that whatever specific libraries and software you learn today will be obsolete in a year or two. Just think about the number of changes in libraries and technology stacks that occur all the time in the world of web programming—a much more mature and slow-growing area than deep learning. We strongly believe that the focus in learning needs to be on understanding the underlying techniques and how to apply them in practice, and how to quickly build expertise in new tools and techniques as they are released.\nBy the end of the book, you’ll understand nearly all the code that’s inside fastai (and much of PyTorch too), because in each chapter we’ll be digging a level deeper to show you exactly what’s going on as we build and train our models. This means that you’ll have learned the most important best practices used in modern deep learning—not just how to use them, but how they really work and are implemented. If you want to use those approaches in another framework, you’ll have the knowledge you need to do so if needed.\nSince the most important thing for learning deep learning is writing code and experimenting, it’s important that you have a great platform for experimenting with code. The most popular programming experimentation platform is called Jupyter. This is what we will be using throughout this book. We will show you how you can use Jupyter to train and experiment with models and introspect every stage of the data pre-processing and model development pipeline. Jupyter Notebook is the most popular tool for doing data science in Python, for good reason. It is powerful, flexible, and easy to use. We think you will love it!\nLet’s see it in practice and train our first model."
  },
  {
    "objectID": "Fastbook/01_intro.html#your-first-model",
    "href": "Fastbook/01_intro.html#your-first-model",
    "title": "Your Deep Learning Journey",
    "section": "Your First Model",
    "text": "Your First Model\nAs we said before, we will teach you how to do things before we explain why they work. Following this top-down approach, we will begin by actually training an image classifier to recognize dogs and cats with almost 100% accuracy. To train this model and run our experiments, you will need to do some initial setup. Don’t worry, it’s not as hard as it looks.\n\ns: Do not skip the setup part even if it looks intimidating at first, especially if you have little or no experience using things like a terminal or the command line. Most of that is actually not necessary and you will find that the easiest servers can be set up with just your usual web browser. It is crucial that you run your own experiments in parallel with this book in order to learn.\n\n\nGetting a GPU Deep Learning Server\nTo do nearly everything in this book, you’ll need access to a computer with an NVIDIA GPU (unfortunately other brands of GPU are not fully supported by the main deep learning libraries). However, we don’t recommend you buy one; in fact, even if you already have one, we don’t suggest you use it just yet! Setting up a computer takes time and energy, and you want all your energy to focus on deep learning right now. Therefore, we instead suggest you rent access to a computer that already has everything you need preinstalled and ready to go. Costs can be as little as US$0.25 per hour while you’re using it, and some options are even free.\n\njargon: Graphics Processing Unit (GPU): Also known as a graphics card. A special kind of processor in your computer that can handle thousands of single tasks at the same time, especially designed for displaying 3D environments on a computer for playing games. These same basic tasks are very similar to what neural networks do, such that GPUs can run neural networks hundreds of times faster than regular CPUs. All modern computers contain a GPU, but few contain the right kind of GPU necessary for deep learning.\n\nThe best choice of GPU servers to use with this book will change over time, as companies come and go and prices change. We maintain a list of our recommended options on the book’s website, so go there now and follow the instructions to get connected to a GPU deep learning server. Don’t worry, it only takes about two minutes to get set up on most platforms, and many don’t even require any payment, or even a credit card, to get started.\n\nA: My two cents: heed this advice! If you like computers you will be tempted to set up your own box. Beware! It is feasible but surprisingly involved and distracting. There is a good reason this book is not titled, Everything You Ever Wanted to Know About Ubuntu System Administration, NVIDIA Driver Installation, apt-get, conda, pip, and Jupyter Notebook Configuration. That would be a book of its own. Having designed and deployed our production machine learning infrastructure at work, I can testify it has its satisfactions, but it is as unrelated to modeling as maintaining an airplane is to flying one.\n\nEach option shown on the website includes a tutorial; after completing the tutorial, you will end up with a screen looking like &lt;&gt;.\n\nYou are now ready to run your first Jupyter notebook!\n\njargon: Jupyter Notebook: A piece of software that allows you to include formatted text, code, images, videos, and much more, all within a single interactive document. Jupyter received the highest honor for software, the ACM Software System Award, thanks to its wide use and enormous impact in many academic fields and in industry. Jupyter Notebook is the software most widely used by data scientists for developing and interacting with deep learning models.\n\n\n\nRunning Your First Notebook\nThe notebooks are labeled by chapter and then by notebook number, so that they are in the same order as they are presented in this book. So, the very first notebook you will see listed is the notebook that you need to use now. You will be using this notebook to train a model that can recognize dog and cat photos. To do this, you’ll be downloading a dataset of dog and cat photos, and using that to train a model. A dataset is simply a bunch of data—it could be images, emails, financial indicators, sounds, or anything else. There are many datasets made freely available that are suitable for training models. Many of these datasets are created by academics to help advance research, many are made available for competitions (there are competitions where data scientists can compete to see who has the most accurate model!), and some are by-products of other processes (such as financial filings).\n\nnote: Full and Stripped Notebooks: There are two folders containing different versions of the notebooks. The full folder contains the exact notebooks used to create the book you’re reading now, with all the prose and outputs. The stripped version has the same headings and code cells, but all outputs and prose have been removed. After reading a section of the book, we recommend working through the stripped notebooks, with the book closed, and seeing if you can figure out what each cell will show before you execute it. Also try to recall what the code is demonstrating.\n\nTo open a notebook, just click on it. The notebook will open, and it will look something like &lt;&gt; (note that there may be slight differences in details across different platforms; you can ignore those differences).\n\nA notebook consists of cells. There are two main types of cell:\n\nCells containing formatted text, images, and so forth. These use a format called markdown, which you will learn about soon.\nCells containing code that can be executed, and outputs will appear immediately underneath (which could be plain text, tables, images, animations, sounds, or even interactive applications).\n\nJupyter notebooks can be in one of two modes: edit mode or command mode. In edit mode typing on your keyboard enters the letters into the cell in the usual way. However, in command mode, you will not see any flashing cursor, and the keys on your keyboard will each have a special function.\nBefore continuing, press the Escape key on your keyboard to switch to command mode (if you are already in command mode, this does nothing, so press it now just in case). To see a complete list of all of the functions available, press H; press Escape to remove this help screen. Notice that in command mode, unlike most programs, commands do not require you to hold down Control, Alt, or similar—you simply press the required letter key.\nYou can make a copy of a cell by pressing C (the cell needs to be selected first, indicated with an outline around it; if it is not already selected, click on it once). Then press V to paste a copy of it.\nClick on the cell that begins with the line “# CLICK ME” to select it. The first character in that line indicates that what follows is a comment in Python, so it is ignored when executing the cell. The rest of the cell is, believe it or not, a complete system for creating and training a state-of-the-art model for recognizing cats versus dogs. So, let’s train it now! To do so, just press Shift-Enter on your keyboard, or press the Play button on the toolbar. Then wait a few minutes while the following things happen:\n\nA dataset called the Oxford-IIIT Pet Dataset that contains 7,349 images of cats and dogs from 37 different breeds will be downloaded from the fast.ai datasets collection to the GPU server you are using, and will then be extracted.\nA pretrained model that has already been trained on 1.3 million images, using a competition-winning model will be downloaded from the internet.\nThe pretrained model will be fine-tuned using the latest advances in transfer learning, to create a model that is specially customized for recognizing dogs and cats.\n\nThe first two steps only need to be run once on your GPU server. If you run the cell again, it will use the dataset and model that have already been downloaded, rather than downloading them again. Let’s take a look at the contents of the cell, and the results (&lt;&gt;):\n\n#id first_training\n#caption Results from the first training\n# CLICK ME\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)/'images'\n\ndef is_cat(x): return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=42,\n    label_func=is_cat, item_tfms=Resize(224))\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.180385\n0.023942\n0.006766\n00:16\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.056023\n0.007580\n0.004060\n00:20\n\n\n\n\n\nYou will probably not see exactly the same results that are in the book. There are a lot of sources of small random variation involved in training models. We generally see an error rate of well less than 0.02 in this example, however.\n\nimportant: Training Time: Depending on your network speed, it might take a few minutes to download the pretrained model and dataset. Running fine_tune might take a minute or so. Often models in this book take a few minutes to train, as will your own models, so it’s a good idea to come up with good techniques to make the most of this time. For instance, keep reading the next section while your model trains, or open up another notebook and use it for some coding experiments.\n\n\n\nSidebar: This Book Was Written in Jupyter Notebooks\nWe wrote this book using Jupyter notebooks, so for nearly every chart, table, and calculation in this book, we’ll be showing you the exact code required to replicate it yourself. That’s why very often in this book, you will see some code immediately followed by a table, a picture or just some text. If you go on the book’s website you will find all the code, and you can try running and modifying every example yourself.\nYou just saw how a cell that outputs a table looks inside the book. Here is an example of a cell that outputs text:\n\n1+1\n\n2\n\n\nJupyter will always print or show the result of the last line (if there is one). For instance, here is an example of a cell that outputs an image:\n\nimg = PILImage.create(image_cat())\nimg.to_thumb(192)\n\n\n\n\n\n\n\n\n\n\nEnd sidebar\nSo, how do we know if this model is any good? In the last column of the table you can see the error rate, which is the proportion of images that were incorrectly identified. The error rate serves as our metric—our measure of model quality, chosen to be intuitive and comprehensible. As you can see, the model is nearly perfect, even though the training time was only a few seconds (not including the one-time downloading of the dataset and the pretrained model). In fact, the accuracy you’ve achieved already is far better than anybody had ever achieved just 10 years ago!\nFinally, let’s check that this model actually works. Go and get a photo of a dog, or a cat; if you don’t have one handy, just search Google Images and download an image that you find there. Now execute the cell with uploader defined. It will output a button you can click, so you can select the image you want to classify:\n\n#hide_output\nuploader = widgets.FileUpload()\nuploader\n\n\n\n\n\nNow you can pass the uploaded file to the model. Make sure that it is a clear photo of a single dog or a cat, and not a line drawing, cartoon, or similar. The notebook will tell you whether it thinks it is a dog or a cat, and how confident it is. Hopefully, you’ll find that your model did a great job:\n\n#hide\n# For the book, we can't actually click an upload button, so we fake it\nuploader = SimpleNamespace(data = ['images/chapter1_cat_example.jpg'])\n\n\nimg = PILImage.create(uploader.data[0])\nis_cat,_,probs = learn.predict(img)\nprint(f\"Is this a cat?: {is_cat}.\")\nprint(f\"Probability it's a cat: {probs[1].item():.6f}\")\n\n\n\n\nIs this a cat?: True.\nProbability it's a cat: 1.000000\n\n\nCongratulations on your first classifier!\nBut what does this mean? What did you actually do? In order to explain this, let’s zoom out again to take in the big picture.\n\n\nWhat Is Machine Learning?\nYour classifier is a deep learning model. As was already mentioned, deep learning models use neural networks, which originally date from the 1950s and have become powerful very recently thanks to recent advancements.\nAnother key piece of context is that deep learning is just a modern area in the more general discipline of machine learning. To understand the essence of what you did when you trained your own classification model, you don’t need to understand deep learning. It is enough to see how your model and your training process are examples of the concepts that apply to machine learning in general.\nSo in this section, we will describe what machine learning is. We will look at the key concepts, and show how they can be traced back to the original essay that introduced them.\nMachine learning is, like regular programming, a way to get computers to complete a specific task. But how would we use regular programming to do what we just did in the last section: recognize dogs versus cats in photos? We would have to write down for the computer the exact steps necessary to complete the task.\nNormally, it’s easy enough for us to write down the steps to complete a task when we’re writing a program. We just think about the steps we’d take if we had to do the task by hand, and then we translate them into code. For instance, we can write a function that sorts a list. In general, we’d write a function that looks something like &lt;&gt; (where inputs might be an unsorted list, and results a sorted list).\n\n#hide_input\n#caption A traditional program\n#id basic_program\n#alt Pipeline inputs, program, results\ngv('''program[shape=box3d width=1 height=0.7]\ninputs-&gt;program-&gt;results''')\n\n\n\n\n\n\n\n\nBut for recognizing objects in a photo that’s a bit tricky; what are the steps we take when we recognize an object in a picture? We really don’t know, since it all happens in our brain without us being consciously aware of it!\nRight back at the dawn of computing, in 1949, an IBM researcher named Arthur Samuel started working on a different way to get computers to complete tasks, which he called machine learning. In his classic 1962 essay “Artificial Intelligence: A Frontier of Automation”, he wrote:\n\n: Programming a computer for such computations is, at best, a difficult task, not primarily because of any inherent complexity in the computer itself but, rather, because of the need to spell out every minute step of the process in the most exasperating detail. Computers, as any programmer will tell you, are giant morons, not giant brains.\n\nHis basic idea was this: instead of telling the computer the exact steps required to solve a problem, show it examples of the problem to solve, and let it figure out how to solve it itself. This turned out to be very effective: by 1961 his checkers-playing program had learned so much that it beat the Connecticut state champion! Here’s how he described his idea (from the same essay as above):\n\n: Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would “learn” from its experience.\n\nThere are a number of powerful concepts embedded in this short statement:\n\nThe idea of a “weight assignment”\nThe fact that every weight assignment has some “actual performance”\nThe requirement that there be an “automatic means” of testing that performance,\n\nThe need for a “mechanism” (i.e., another automatic process) for improving the performance by changing the weight assignments\n\nLet us take these concepts one by one, in order to understand how they fit together in practice. First, we need to understand what Samuel means by a weight assignment.\nWeights are just variables, and a weight assignment is a particular choice of values for those variables. The program’s inputs are values that it processes in order to produce its results—for instance, taking image pixels as inputs, and returning the classification “dog” as a result. The program’s weight assignments are other values that define how the program will operate.\nSince they will affect the program they are in a sense another kind of input, so we will update our basic picture in &lt;&gt; and replace it with &lt;&gt; in order to take this into account.\n\n#hide_input\n#caption A program using weight assignment\n#id weight_assignment\ngv('''model[shape=box3d width=1 height=0.7]\ninputs-&gt;model-&gt;results; weights-&gt;model''')\n\n\n\n\n\n\n\n\nWe’ve changed the name of our box from program to model. This is to follow modern terminology and to reflect that the model is a special kind of program: it’s one that can do many different things, depending on the weights. It can be implemented in many different ways. For instance, in Samuel’s checkers program, different values of the weights would result in different checkers-playing strategies.\n(By the way, what Samuel called “weights” are most generally referred to as model parameters these days, in case you have encountered that term. The term weights is reserved for a particular type of model parameter.)\nNext, Samuel said we need an automatic means of testing the effectiveness of any current weight assignment in terms of actual performance. In the case of his checkers program, the “actual performance” of a model would be how well it plays. And you could automatically test the performance of two models by setting them to play against each other, and seeing which one usually wins.\nFinally, he says we need a mechanism for altering the weight assignment so as to maximize the performance. For instance, we could look at the difference in weights between the winning model and the losing model, and adjust the weights a little further in the winning direction.\nWe can now see why he said that such a procedure could be made entirely automatic and… a machine so programmed would “learn” from its experience. Learning would become entirely automatic when the adjustment of the weights was also automatic—when instead of us improving a model by adjusting its weights manually, we relied on an automated mechanism that produced adjustments based on performance.\n&lt;&gt; shows the full picture of Samuel’s idea of training a machine learning model.\n\n#hide_input\n#caption Training a machine learning model\n#id training_loop\n#alt The basic training loop\ngv('''ordering=in\nmodel[shape=box3d width=1 height=0.7]\ninputs-&gt;model-&gt;results; weights-&gt;model; results-&gt;performance\nperformance-&gt;weights[constraint=false label=update]''')\n\n\n\n\n\n\n\n\nNotice the distinction between the model’s results (e.g., the moves in a checkers game) and its performance (e.g., whether it wins the game, or how quickly it wins).\nAlso note that once the model is trained—that is, once we’ve chosen our final, best, favorite weight assignment—then we can think of the weights as being part of the model, since we’re not varying them any more.\nTherefore, actually using a model after it’s trained looks like &lt;&gt;.\n\n#hide_input\n#caption Using a trained model as a program\n#id using_model\ngv('''model[shape=box3d width=1 height=0.7]\ninputs-&gt;model-&gt;results''')\n\n\n\n\n\n\n\n\nThis looks identical to our original diagram in &lt;&gt;, just with the word program replaced with model. This is an important insight: a trained model can be treated just like a regular computer program.\n\njargon: Machine Learning: The training of programs developed by allowing a computer to learn from its experience, rather than through manually coding the individual steps.\n\n\n\nWhat Is a Neural Network?\nIt’s not too hard to imagine what the model might look like for a checkers program. There might be a range of checkers strategies encoded, and some kind of search mechanism, and then the weights could vary how strategies are selected, what parts of the board are focused on during a search, and so forth. But it’s not at all obvious what the model might look like for an image recognition program, or for understanding text, or for many other interesting problems we might imagine.\nWhat we would like is some kind of function that is so flexible that it could be used to solve any given problem, just by varying its weights. Amazingly enough, this function actually exists! It’s the neural network, which we already discussed. That is, if you regard a neural network as a mathematical function, it turns out to be a function which is extremely flexible depending on its weights. A mathematical proof called the universal approximation theorem shows that this function can solve any problem to any level of accuracy, in theory. The fact that neural networks are so flexible means that, in practice, they are often a suitable kind of model, and you can focus your effort on the process of training them—that is, of finding good weight assignments.\nBut what about that process? One could imagine that you might need to find a new “mechanism” for automatically updating weights for every problem. This would be laborious. What we’d like here as well is a completely general way to update the weights of a neural network, to make it improve at any given task. Conveniently, this also exists!\nThis is called stochastic gradient descent (SGD). We’ll see how neural networks and SGD work in detail in &lt;&gt;, as well as explaining the universal approximation theorem. For now, however, we will instead use Samuel’s own words: We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would “learn” from its experience.\n\nJ: Don’t worry, neither SGD nor neural nets are mathematically complex. Both nearly entirely rely on addition and multiplication to do their work (but they do a lot of addition and multiplication!). The main reaction we hear from students when they see the details is: “Is that all it is?”\n\nIn other words, to recap, a neural network is a particular kind of machine learning model, which fits right in to Samuel’s original conception. Neural networks are special because they are highly flexible, which means they can solve an unusually wide range of problems just by finding the right weights. This is powerful, because stochastic gradient descent provides us a way to find those weight values automatically.\nHaving zoomed out, let’s now zoom back in and revisit our image classification problem using Samuel’s framework.\nOur inputs are the images. Our weights are the weights in the neural net. Our model is a neural net. Our results are the values that are calculated by the neural net, like “dog” or “cat.”\nWhat about the next piece, an automatic means of testing the effectiveness of any current weight assignment in terms of actual performance? Determining “actual performance” is easy enough: we can simply define our model’s performance as its accuracy at predicting the correct answers.\nPutting this all together, and assuming that SGD is our mechanism for updating the weight assignments, we can see how our image classifier is a machine learning model, much like Samuel envisioned.\n\n\nA Bit of Deep Learning Jargon\nSamuel was working in the 1960s, and since then terminology has changed. Here is the modern deep learning terminology for all the pieces we have discussed:\n\nThe functional form of the model is called its architecture (but be careful—sometimes people use model as a synonym of architecture, so this can get confusing).\nThe weights are called parameters.\nThe predictions are calculated from the independent variable, which is the data not including the labels.\nThe results of the model are called predictions.\nThe measure of performance is called the loss.\nThe loss depends not only on the predictions, but also the correct labels (also known as targets or the dependent variable); e.g., “dog” or “cat.”\n\nAfter making these changes, our diagram in &lt;&gt; looks like &lt;&gt;.\n\n#hide_input\n#caption Detailed training loop\n#id detailed_loop\ngv('''ordering=in\nmodel[shape=box3d width=1 height=0.7 label=architecture]\ninputs-&gt;model-&gt;predictions; parameters-&gt;model; labels-&gt;loss; predictions-&gt;loss\nloss-&gt;parameters[constraint=false label=update]''')\n\n\n\n\n\n\n\n\n\n\nLimitations Inherent To Machine Learning\nFrom this picture we can now see some fundamental things about training a deep learning model:\n\nA model cannot be created without data.\nA model can only learn to operate on the patterns seen in the input data used to train it.\nThis learning approach only creates predictions, not recommended actions.\nIt’s not enough to just have examples of input data; we need labels for that data too (e.g., pictures of dogs and cats aren’t enough to train a model; we need a label for each one, saying which ones are dogs, and which are cats).\n\nGenerally speaking, we’ve seen that most organizations that say they don’t have enough data, actually mean they don’t have enough labeled data. If any organization is interested in doing something in practice with a model, then presumably they have some inputs they plan to run their model against. And presumably they’ve been doing that some other way for a while (e.g., manually, or with some heuristic program), so they have data from those processes! For instance, a radiology practice will almost certainly have an archive of medical scans (since they need to be able to check how their patients are progressing over time), but those scans may not have structured labels containing a list of diagnoses or interventions (since radiologists generally create free-text natural language reports, not structured data). We’ll be discussing labeling approaches a lot in this book, because it’s such an important issue in practice.\nSince these kinds of machine learning models can only make predictions (i.e., attempt to replicate labels), this can result in a significant gap between organizational goals and model capabilities. For instance, in this book you’ll learn how to create a recommendation system that can predict what products a user might purchase. This is often used in e-commerce, such as to customize products shown on a home page by showing the highest-ranked items. But such a model is generally created by looking at a user and their buying history (inputs) and what they went on to buy or look at (labels), which means that the model is likely to tell you about products the user already has or already knows about, rather than new products that they are most likely to be interested in hearing about. That’s very different to what, say, an expert at your local bookseller might do, where they ask questions to figure out your taste, and then tell you about authors or series that you’ve never heard of before.\nAnother critical insight comes from considering how a model interacts with its environment. This can create feedback loops, as described here:\n\nA predictive policing model is created based on where arrests have been made in the past. In practice, this is not actually predicting crime, but rather predicting arrests, and is therefore partially simply reflecting biases in existing policing processes.\nLaw enforcement officers then might use that model to decide where to focus their police activity, resulting in increased arrests in those areas.\nData on these additional arrests would then be fed back in to retrain future versions of the model.\n\nThis is a positive feedback loop, where the more the model is used, the more biased the data becomes, making the model even more biased, and so forth.\nFeedback loops can also create problems in commercial settings. For instance, a video recommendation system might be biased toward recommending content consumed by the biggest watchers of video (e.g., conspiracy theorists and extremists tend to watch more online video content than the average), resulting in those users increasing their video consumption, resulting in more of those kinds of videos being recommended. We’ll consider this topic more in detail in &lt;&gt;.\nNow that you have seen the base of the theory, let’s go back to our code example and see in detail how the code corresponds to the process we just described.\n\n\nHow Our Image Recognizer Works\nLet’s see just how our image recognizer code maps to these ideas. We’ll put each line into a separate cell, and look at what each one is doing (we won’t explain every detail of every parameter yet, but will give a description of the important bits; full details will come later in the book).\nThe first line imports all of the fastai.vision library.\nfrom fastai.vision.all import *\nThis gives us all of the functions and classes we will need to create a wide variety of computer vision models.\n\nJ: A lot of Python coders recommend avoiding importing a whole library like this (using the import * syntax), because in large software projects it can cause problems. However, for interactive work such as in a Jupyter notebook, it works great. The fastai library is specially designed to support this kind of interactive use, and it will only import the necessary pieces into your environment.\n\nThe second line downloads a standard dataset from the fast.ai datasets collection (if not previously downloaded) to your server, extracts it (if not previously extracted), and returns a Path object with the extracted location:\npath = untar_data(URLs.PETS)/'images'\n\nS: Throughout my time studying at fast.ai, and even still today, I’ve learned a lot about productive coding practices. The fastai library and fast.ai notebooks are full of great little tips that have helped make me a better programmer. For instance, notice that the fastai library doesn’t just return a string containing the path to the dataset, but a Path object. This is a really useful class from the Python 3 standard library that makes accessing files and directories much easier. If you haven’t come across it before, be sure to check out its documentation or a tutorial and try it out. Note that the https://book.fast.ai[website] contains links to recommended tutorials for each chapter. I’ll keep letting you know about little coding tips I’ve found useful as we come across them.\n\nIn the third line we define a function, is_cat, which labels cats based on a filename rule provided by the dataset creators:\ndef is_cat(x): return x[0].isupper()\nWe use that function in the fourth line, which tells fastai what kind of dataset we have and how it is structured:\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=42,\n    label_func=is_cat, item_tfms=Resize(224))\nThere are various different classes for different kinds of deep learning datasets and problems—here we’re using ImageDataLoaders. The first part of the class name will generally be the type of data you have, such as image, or text.\nThe other important piece of information that we have to tell fastai is how to get the labels from the dataset. Computer vision datasets are normally structured in such a way that the label for an image is part of the filename, or path—most commonly the parent folder name. fastai comes with a number of standardized labeling methods, and ways to write your own. Here we’re telling fastai to use the is_cat function we just defined.\nFinally, we define the Transforms that we need. A Transform contains code that is applied automatically during training; fastai includes many predefined Transforms, and adding new ones is as simple as creating a Python function. There are two kinds: item_tfms are applied to each item (in this case, each item is resized to a 224-pixel square), while batch_tfms are applied to a batch of items at a time using the GPU, so they’re particularly fast (we’ll see many examples of these throughout this book).\nWhy 224 pixels? This is the standard size for historical reasons (old pretrained models require this size exactly), but you can pass pretty much anything. If you increase the size, you’ll often get a model with better results (since it will be able to focus on more details), but at the price of speed and memory consumption; the opposite is true if you decrease the size.\n\nNote: Classification and Regression: classification and regression have very specific meanings in machine learning. These are the two main types of model that we will be investigating in this book. A classification model is one which attempts to predict a class, or category. That is, it’s predicting from a number of discrete possibilities, such as “dog” or “cat.” A regression model is one which attempts to predict one or more numeric quantities, such as a temperature or a location. Sometimes people use the word regression to refer to a particular kind of model called a linear regression model; this is a bad practice, and we won’t be using that terminology in this book!\n\nThe Pet dataset contains 7,390 pictures of dogs and cats, consisting of 37 different breeds. Each image is labeled using its filename: for instance the file great_pyrenees_173.jpg is the 173rd example of an image of a Great Pyrenees breed dog in the dataset. The filenames start with an uppercase letter if the image is a cat, and a lowercase letter otherwise. We have to tell fastai how to get labels from the filenames, which we do by calling from_name_func (which means that labels can be extracted using a function applied to the filename), and passing is_cat, which returns x[0].isupper(), which evaluates to True if the first letter is uppercase (i.e., it’s a cat).\nThe most important parameter to mention here is valid_pct=0.2. This tells fastai to hold out 20% of the data and not use it for training the model at all. This 20% of the data is called the validation set; the remaining 80% is called the training set. The validation set is used to measure the accuracy of the model. By default, the 20% that is held out is selected randomly. The parameter seed=42 sets the random seed to the same value every time we run this code, which means we get the same validation set every time we run it—this way, if we change our model and retrain it, we know that any differences are due to the changes to the model, not due to having a different random validation set.\nfastai will always show you your model’s accuracy using only the validation set, never the training set. This is absolutely critical, because if you train a large enough model for a long enough time, it will eventually memorize the label of every item in your dataset! The result will not actually be a useful model, because what we care about is how well our model works on previously unseen images. That is always our goal when creating a model: for it to be useful on data that the model only sees in the future, after it has been trained.\nEven when your model has not fully memorized all your data, earlier on in training it may have memorized certain parts of it. As a result, the longer you train for, the better your accuracy will get on the training set; the validation set accuracy will also improve for a while, but eventually it will start getting worse as the model starts to memorize the training set, rather than finding generalizable underlying patterns in the data. When this happens, we say that the model is overfitting.\n&lt;&gt; shows what happens when you overfit, using a simplified example where we have just one parameter, and some randomly generated data based on the function x**2. As you can see, although the predictions in the overfit model are accurate for data near the observed data points, they are way off when outside of that range.\n\nOverfitting is the single most important and challenging issue when training for all machine learning practitioners, and all algorithms. As you will see, it is very easy to create a model that does a great job at making predictions on the exact data it has been trained on, but it is much harder to make accurate predictions on data the model has never seen before. And of course, this is the data that will actually matter in practice. For instance, if you create a handwritten digit classifier (as we will very soon!) and use it to recognize numbers written on checks, then you are never going to see any of the numbers that the model was trained on—checks will have slightly different variations of writing to deal with. You will learn many methods to avoid overfitting in this book. However, you should only use those methods after you have confirmed that overfitting is actually occurring (i.e., you have actually observed the validation accuracy getting worse during training). We often see practitioners using over-fitting avoidance techniques even when they have enough data that they didn’t need to do so, ending up with a model that may be less accurate than what they could have achieved.\n\nimportant: Validation Set: When you train a model, you must always have both a training set and a validation set, and must measure the accuracy of your model only on the validation set. If you train for too long, with not enough data, you will see the accuracy of your model start to get worse; this is called overfitting. fastai defaults valid_pct to 0.2, so even if you forget, fastai will create a validation set for you!\n\nThe fifth line of the code training our image recognizer tells fastai to create a convolutional neural network (CNN) and specifies what architecture to use (i.e. what kind of model to create), what data we want to train it on, and what metric to use:\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nWhy a CNN? It’s the current state-of-the-art approach to creating computer vision models. We’ll be learning all about how CNNs work in this book. Their structure is inspired by how the human vision system works.\nThere are many different architectures in fastai, which we will introduce in this book (as well as discussing how to create your own). Most of the time, however, picking an architecture isn’t a very important part of the deep learning process. It’s something that academics love to talk about, but in practice it is unlikely to be something you need to spend much time on. There are some standard architectures that work most of the time, and in this case we’re using one called ResNet that we’ll be talking a lot about during the book; it is both fast and accurate for many datasets and problems. The 34 in resnet34 refers to the number of layers in this variant of the architecture (other options are 18, 50, 101, and 152). Models using architectures with more layers take longer to train, and are more prone to overfitting (i.e. you can’t train them for as many epochs before the accuracy on the validation set starts getting worse). On the other hand, when using more data, they can be quite a bit more accurate.\nWhat is a metric? A metric is a function that measures the quality of the model’s predictions using the validation set, and will be printed at the end of each epoch. In this case, we’re using error_rate, which is a function provided by fastai that does just what it says: tells you what percentage of images in the validation set are being classified incorrectly. Another common metric for classification is accuracy (which is just 1.0 - error_rate). fastai provides many more, which will be discussed throughout this book.\nThe concept of a metric may remind you of loss, but there is an important distinction. The entire purpose of loss is to define a “measure of performance” that the training system can use to update weights automatically. In other words, a good choice for loss is a choice that is easy for stochastic gradient descent to use. But a metric is defined for human consumption, so a good metric is one that is easy for you to understand, and that hews as closely as possible to what you want the model to do. At times, you might decide that the loss function is a suitable metric, but that is not necessarily the case.\nvision_learner also has a parameter pretrained, which defaults to True (so it’s used in this case, even though we haven’t specified it), which sets the weights in your model to values that have already been trained by experts to recognize a thousand different categories across 1.3 million photos (using the famous ImageNet dataset). A model that has weights that have already been trained on some other dataset is called a pretrained model. You should nearly always use a pretrained model, because it means that your model, before you’ve even shown it any of your data, is already very capable. And, as you’ll see, in a deep learning model many of these capabilities are things you’ll need, almost regardless of the details of your project. For instance, parts of pretrained models will handle edge, gradient, and color detection, which are needed for many tasks.\nWhen using a pretrained model, vision_learner will remove the last layer, since that is always specifically customized to the original training task (i.e. ImageNet dataset classification), and replace it with one or more new layers with randomized weights, of an appropriate size for the dataset you are working with. This last part of the model is known as the head.\nUsing pretrained models is the most important method we have to allow us to train more accurate models, more quickly, with less data, and less time and money. You might think that would mean that using pretrained models would be the most studied area in academic deep learning… but you’d be very, very wrong! The importance of pretrained models is generally not recognized or discussed in most courses, books, or software library features, and is rarely considered in academic papers. As we write this at the start of 2020, things are just starting to change, but it’s likely to take a while. So be careful: most people you speak to will probably greatly underestimate what you can do in deep learning with few resources, because they probably won’t deeply understand how to use pretrained models.\nUsing a pretrained model for a task different to what it was originally trained for is known as transfer learning. Unfortunately, because transfer learning is so under-studied, few domains have pretrained models available. For instance, there are currently few pretrained models available in medicine, making transfer learning challenging to use in that domain. In addition, it is not yet well understood how to use transfer learning for tasks such as time series analysis.\n\njargon: Transfer learning: Using a pretrained model for a task different to what it was originally trained for.\n\nThe sixth line of our code tells fastai how to fit the model:\nlearn.fine_tune(1)\nAs we’ve discussed, the architecture only describes a template for a mathematical function; it doesn’t actually do anything until we provide values for the millions of parameters it contains.\nThis is the key to deep learning—determining how to fit the parameters of a model to get it to solve your problem. In order to fit a model, we have to provide at least one piece of information: how many times to look at each image (known as number of epochs). The number of epochs you select will largely depend on how much time you have available, and how long you find it takes in practice to fit your model. If you select a number that is too small, you can always train for more epochs later.\nBut why is the method called fine_tune, and not fit? fastai actually does have a method called fit, which does indeed fit a model (i.e. look at images in the training set multiple times, each time updating the parameters to make the predictions closer and closer to the target labels). But in this case, we’ve started with a pretrained model, and we don’t want to throw away all those capabilities that it already has. As you’ll learn in this book, there are some important tricks to adapt a pretrained model for a new dataset—a process called fine-tuning.\n\njargon: Fine-tuning: A transfer learning technique where the parameters of a pretrained model are updated by training for additional epochs using a different task to that used for pretraining.\n\nWhen you use the fine_tune method, fastai will use these tricks for you. There are a few parameters you can set (which we’ll discuss later), but in the default form shown here, it does two steps:\n\nUse one epoch to fit just those parts of the model necessary to get the new random head to work correctly with your dataset.\nUse the number of epochs requested when calling the method to fit the entire model, updating the weights of the later layers (especially the head) faster than the earlier layers (which, as we’ll see, generally don’t require many changes from the pretrained weights).\n\nThe head of a model is the part that is newly added to be specific to the new dataset. An epoch is one complete pass through the dataset. After calling fit, the results after each epoch are printed, showing the epoch number, the training and validation set losses (the “measure of performance” used for training the model), and any metrics you’ve requested (error rate, in this case).\nSo, with all this code our model learned to recognize cats and dogs just from labeled examples. But how did it do it?\n\n\nWhat Our Image Recognizer Learned\nAt this stage we have an image recognizer that is working very well, but we have no idea what it is actually doing! Although many people complain that deep learning results in impenetrable “black box” models (that is, something that gives predictions but that no one can understand), this really couldn’t be further from the truth. There is a vast body of research showing how to deeply inspect deep learning models, and get rich insights from them. Having said that, all kinds of machine learning models (including deep learning, and traditional statistical models) can be challenging to fully understand, especially when considering how they will behave when coming across data that is very different to the data used to train them. We’ll be discussing this issue throughout this book.\nIn 2013 a PhD student, Matt Zeiler, and his supervisor, Rob Fergus, published the paper “Visualizing and Understanding Convolutional Networks”, which showed how to visualize the neural network weights learned in each layer of a model. They carefully analyzed the model that won the 2012 ImageNet competition, and used this analysis to greatly improve the model, such that they were able to go on to win the 2013 competition! &lt;&gt; is the picture that they published of the first layer’s weights.\n\nThis picture requires some explanation. For each layer, the image part with the light gray background shows the reconstructed weights pictures, and the larger section at the bottom shows the parts of the training images that most strongly matched each set of weights. For layer 1, what we can see is that the model has discovered weights that represent diagonal, horizontal, and vertical edges, as well as various different gradients. (Note that for each layer only a subset of the features are shown; in practice there are thousands across all of the layers.) These are the basic building blocks that the model has learned for computer vision. They have been widely analyzed by neuroscientists and computer vision researchers, and it turns out that these learned building blocks are very similar to the basic visual machinery in the human eye, as well as the handcrafted computer vision features that were developed prior to the days of deep learning. The next layer is represented in &lt;&gt;.\n\nFor layer 2, there are nine examples of weight reconstructions for each of the features found by the model. We can see that the model has learned to create feature detectors that look for corners, repeating lines, circles, and other simple patterns. These are built from the basic building blocks developed in the first layer. For each of these, the right-hand side of the picture shows small patches from actual images which these features most closely match. For instance, the particular pattern in row 2, column 1 matches the gradients and textures associated with sunsets.\n&lt;&gt; shows the image from the paper showing the results of reconstructing the features of layer 3.\n\nAs you can see by looking at the righthand side of this picture, the features are now able to identify and match with higher-level semantic components, such as car wheels, text, and flower petals. Using these components, layers four and five can identify even higher-level concepts, as shown in &lt;&gt;.\n\nThis article was studying an older model called AlexNet that only contained five layers. Networks developed since then can have hundreds of layers—so you can imagine how rich the features developed by these models can be!\nWhen we fine-tuned our pretrained model earlier, we adapted what those last layers focus on (flowers, humans, animals) to specialize on the cats versus dogs problem. More generally, we could specialize such a pretrained model on many different tasks. Let’s have a look at some examples.\n\n\nImage Recognizers Can Tackle Non-Image Tasks\nAn image recognizer can, as its name suggests, only recognize images. But a lot of things can be represented as images, which means that an image recogniser can learn to complete many tasks.\nFor instance, a sound can be converted to a spectrogram, which is a chart that shows the amount of each frequency at each time in an audio file. Fast.ai student Ethan Sutin used this approach to easily beat the published accuracy of a state-of-the-art environmental sound detection model using a dataset of 8,732 urban sounds. fastai’s show_batch clearly shows how each different sound has a quite distinctive spectrogram, as you can see in &lt;&gt;.\n\nA time series can easily be converted into an image by simply plotting the time series on a graph. However, it is often a good idea to try to represent your data in a way that makes it as easy as possible to pull out the most important components. In a time series, things like seasonality and anomalies are most likely to be of interest. There are various transformations available for time series data. For instance, fast.ai student Ignacio Oguiza created images from a time series dataset for olive oil classification, using a technique called Gramian Angular Difference Field (GADF); you can see the result in &lt;&gt;. He then fed those images to an image classification model just like the one you see in this chapter. His results, despite having only 30 training set images, were well over 90% accurate, and close to the state of the art.\n\nAnother interesting fast.ai student project example comes from Gleb Esman. He was working on fraud detection at Splunk, using a dataset of users’ mouse movements and mouse clicks. He turned these into pictures by drawing an image where the position, speed, and acceleration of the mouse pointer was displayed using coloured lines, and the clicks were displayed using small colored circles, as shown in &lt;&gt;. He then fed this into an image recognition model just like the one we’ve used in this chapter, and it worked so well that it led to a patent for this approach to fraud analytics!\n\nAnother example comes from the paper “Malware Classification with Deep Convolutional Neural Networks” by Mahmoud Kalash et al., which explains that “the malware binary file is divided into 8-bit sequences which are then converted to equivalent decimal values. This decimal vector is reshaped and a gray-scale image is generated that represents the malware sample,” like in &lt;&gt;.\n\nThe authors then show “pictures” generated through this process of malware in different categories, as shown in &lt;&gt;.\n\nAs you can see, the different types of malware look very distinctive to the human eye. The model the researchers trained based on this image representation was more accurate at malware classification than any previous approach shown in the academic literature. This suggests a good rule of thumb for converting a dataset into an image representation: if the human eye can recognize categories from the images, then a deep learning model should be able to do so too.\nIn general, you’ll find that a small number of general approaches in deep learning can go a long way, if you’re a bit creative in how you represent your data! You shouldn’t think of approaches like the ones described here as “hacky workarounds,” because actually they often (as here) beat previously state-of-the-art results. These really are the right ways to think about these problem domains.\n\n\nJargon Recap\nWe just covered a lot of information so let’s recap briefly, &lt;&gt; provides a handy vocabulary.\n[[dljargon]]\n.Deep learning vocabulary\n[options=\"header\"]\n|=====\n| Term | Meaning\n|Label | The data that we're trying to predict, such as \"dog\" or \"cat\"\n|Architecture | The _template_ of the model that we're trying to fit; the actual mathematical function that we're passing the input data and parameters to\n|Model | The combination of the architecture with a particular set of parameters\n|Parameters | The values in the model that change what task it can do, and are updated through model training\n|Fit | Update the parameters of the model such that the predictions of the model using the input data match the target labels\n|Train | A synonym for _fit_\n|Pretrained model | A model that has already been trained, generally using a large dataset, and will be fine-tuned\n|Fine-tune | Update a pretrained model for a different task\n|Epoch | One complete pass through the input data\n|Loss | A measure of how good the model is, chosen to drive training via SGD\n|Metric | A measurement of how good the model is, using the validation set, chosen for human consumption\n|Validation set | A set of data held out from training, used only for measuring how good the model is\n|Training set | The data used for fitting the model; does not include any data from the validation set\n|Overfitting | Training a model in such a way that it _remembers_ specific features of the input data, rather than generalizing well to data not seen during training\n|CNN | Convolutional neural network; a type of neural network that works particularly well for computer vision tasks\n|=====\nWith this vocabulary in hand, we are now in a position to bring together all the key concepts introduced so far. Take a moment to review those definitions and read the following summary. If you can follow the explanation, then you’re well equipped to understand the discussions to come.\nMachine learning is a discipline where we define a program not by writing it entirely ourselves, but by learning from data. Deep learning is a specialty within machine learning that uses neural networks with multiple layers. Image classification is a representative example (also known as image recognition). We start with labeled data; that is, a set of images where we have assigned a label to each image indicating what it represents. Our goal is to produce a program, called a model, which, given a new image, will make an accurate prediction regarding what that new image represents.\nEvery model starts with a choice of architecture, a general template for how that kind of model works internally. The process of training (or fitting) the model is the process of finding a set of parameter values (or weights) that specialize that general architecture into a model that works well for our particular kind of data. In order to define how well a model does on a single prediction, we need to define a loss function, which determines how we score a prediction as good or bad.\nTo make the training process go faster, we might start with a pretrained model—a model that has already been trained on someone else’s data. We can then adapt it to our data by training it a bit more on our data, a process called fine-tuning.\nWhen we train a model, a key concern is to ensure that our model generalizes—that is, that it learns general lessons from our data which also apply to new items it will encounter, so that it can make good predictions on those items. The risk is that if we train our model badly, instead of learning general lessons it effectively memorizes what it has already seen, and then it will make poor predictions about new images. Such a failure is called overfitting. In order to avoid this, we always divide our data into two parts, the training set and the validation set. We train the model by showing it only the training set and then we evaluate how well the model is doing by seeing how well it performs on items from the validation set. In this way, we check if the lessons the model learns from the training set are lessons that generalize to the validation set. In order for a person to assess how well the model is doing on the validation set overall, we define a metric. During the training process, when the model has seen every item in the training set, we call that an epoch.\nAll these concepts apply to machine learning in general. That is, they apply to all sorts of schemes for defining a model by training it with data. What makes deep learning distinctive is a particular class of architectures: the architectures based on neural networks. In particular, tasks like image classification rely heavily on convolutional neural networks, which we will discuss shortly."
  },
  {
    "objectID": "Fastbook/01_intro.html#deep-learning-is-not-just-for-image-classification",
    "href": "Fastbook/01_intro.html#deep-learning-is-not-just-for-image-classification",
    "title": "Your Deep Learning Journey",
    "section": "Deep Learning Is Not Just for Image Classification",
    "text": "Deep Learning Is Not Just for Image Classification\nDeep learning’s effectiveness for classifying images has been widely discussed in recent years, even showing superhuman results on complex tasks like recognizing malignant tumors in CT scans. But it can do a lot more than this, as we will show here.\nFor instance, let’s talk about something that is critically important for autonomous vehicles: localizing objects in a picture. If a self-driving car doesn’t know where a pedestrian is, then it doesn’t know how to avoid one! Creating a model that can recognize the content of every individual pixel in an image is called segmentation. Here is how we can train a segmentation model with fastai, using a subset of the Camvid dataset from the paper “Semantic Object Classes in Video: A High-Definition Ground Truth Database” by Gabruel J. Brostow, Julien Fauqueur, and Roberto Cipolla:\n\npath = untar_data(URLs.CAMVID_TINY)\ndls = SegmentationDataLoaders.from_label_func(\n    path, bs=8, fnames = get_image_files(path/\"images\"),\n    label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n    codes = np.loadtxt(path/'codes.txt', dtype=str)\n)\n\nlearn = unet_learner(dls, resnet34)\nlearn.fine_tune(8)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n2.641862\n2.140568\n00:02\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.624964\n1.464210\n00:02\n\n\n1\n1.454148\n1.284032\n00:02\n\n\n2\n1.342955\n1.048562\n00:02\n\n\n3\n1.199765\n0.852787\n00:02\n\n\n4\n1.078090\n0.838206\n00:02\n\n\n5\n0.975496\n0.746806\n00:02\n\n\n6\n0.892793\n0.725384\n00:02\n\n\n7\n0.827645\n0.726778\n00:02\n\n\n\n\n\nWe are not even going to walk through this code line by line, because it is nearly identical to our previous example! (Although we will be doing a deep dive into segmentation models in &lt;&gt;, along with all of the other models that we are briefly introducing in this chapter, and many, many more.)\nWe can visualize how well it achieved its task, by asking the model to color-code each pixel of an image. As you can see, it nearly perfectly classifies every pixel in every object. For instance, notice that all of the cars are overlaid with the same color and all of the trees are overlaid with the same color (in each pair of images, the lefthand image is the ground truth label and the right is the prediction from the model):\n\nlearn.show_results(max_n=6, figsize=(7,8))\n\n\n\n\n\n\n\n\n\n\n\nOne other area where deep learning has dramatically improved in the last couple of years is natural language processing (NLP). Computers can now generate text, translate automatically from one language to another, analyze comments, label words in sentences, and much more. Here is all of the code necessary to train a model that can classify the sentiment of a movie review better than anything that existed in the world just five years ago:\n\nfrom fastai.text.all import *\n\ndls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\nlearn.fine_tune(4, 1e-2)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.878776\n0.748753\n0.500400\n01:27\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.679118\n0.674778\n0.584040\n02:45\n\n\n1\n0.653671\n0.670396\n0.618040\n02:55\n\n\n2\n0.598665\n0.551815\n0.718920\n05:28\n\n\n3\n0.556812\n0.507450\n0.752480\n03:11\n\n\n\n\n\n#clean If you hit a “CUDA out of memory error” after running this cell, click on the menu Kernel, then restart. Instead of executing the cell above, copy and paste the following code in it:\nfrom fastai.text.all import *\n\ndls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test', bs=32)\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\nlearn.fine_tune(4, 1e-2)\nThis reduces the batch size to 32 (we will explain this later). If you keep hitting the same error, change 32 to 16.\nThis model is using the “IMDb Large Movie Review dataset” from the paper “Learning Word Vectors for Sentiment Analysis” by Andrew Maas et al. It works well with movie reviews of many thousands of words, but let’s test it out on a very short one to see how it does its thing:\n\nlearn.predict(\"I really liked that movie!\")\n\n\n\n\n('pos', tensor(1), tensor([0.0040, 0.9960]))\n\n\nHere we can see the model has considered the review to be positive. The second part of the result is the index of “pos” in our data vocabulary and the last part is the probabilities attributed to each class (99.6% for “pos” and 0.4% for “neg”).\nNow it’s your turn! Write your own mini movie review, or copy one from the internet, and you can see what this model thinks about it.\n\nSidebar: The Order Matters\nIn a Jupyter notebook, the order in which you execute each cell is very important. It’s not like Excel, where everything gets updated as soon as you type something anywhere—it has an inner state that gets updated each time you execute a cell. For instance, when you run the first cell of the notebook (with the “CLICK ME” comment), you create an object called learn that contains a model and data for an image classification problem. If we were to run the cell just shown in the text (the one that predicts if a review is good or not) straight after, we would get an error as this learn object does not contain a text classification model. This cell needs to be run after the one containing:\nfrom fastai.text.all import *\n\ndls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, \n                                metrics=accuracy)\nlearn.fine_tune(4, 1e-2)\nThe outputs themselves can be deceiving, because they include the results of the last time the cell was executed; if you change the code inside a cell without executing it, the old (misleading) results will remain.\nExcept when we mention it explicitly, the notebooks provided on the book website are meant to be run in order, from top to bottom. In general, when experimenting, you will find yourself executing cells in any order to go fast (which is a super neat feature of Jupyter Notebook), but once you have explored and arrived at the final version of your code, make sure you can run the cells of your notebooks in order (your future self won’t necessarily remember the convoluted path you took otherwise!).\nIn command mode, pressing 0 twice will restart the kernel (which is the engine powering your notebook). This will wipe your state clean and make it as if you had just started in the notebook. Choose Run All Above from the Cell menu to run all cells above the point where you are. We have found this to be very useful when developing the fastai library.\n\n\nEnd sidebar\nIf you ever have any questions about a fastai method, you should use the function doc, passing it the method name:\ndoc(learn.predict)\nThis will make a small window pop up with content like this:\n\nA brief one-line explanation is provided by doc. The “Show in docs” link takes you to the full documentation, where you’ll find all the details and lots of examples. Also, most of fastai’s methods are just a handful of lines, so you can click the “source” link to see exactly what’s going on behind the scenes.\nLet’s move on to something much less sexy, but perhaps significantly more widely commercially useful: building models from plain tabular data.\n\njargon: Tabular: Data that is in the form of a table, such as from a spreadsheet, database, or CSV file. A tabular model is a model that tries to predict one column of a table based on information in other columns of the table.\n\nIt turns out that looks very similar too. Here is the code necessary to train a model that will predict whether a person is a high-income earner, based on their socioeconomic background:\n\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ADULT_SAMPLE)\n\ndls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names=\"salary\",\n    cat_names = ['workclass', 'education', 'marital-status', 'occupation',\n                 'relationship', 'race'],\n    cont_names = ['age', 'fnlwgt', 'education-num'],\n    procs = [Categorify, FillMissing, Normalize])\n\nlearn = tabular_learner(dls, metrics=accuracy)\n\nAs you see, we had to tell fastai which columns are categorical (that is, contain values that are one of a discrete set of choices, such as occupation) and which are continuous (that is, contain a number that represents a quantity, such as age).\nThere is no pretrained model available for this task (in general, pretrained models are not widely available for any tabular modeling tasks, although some organizations have created them for internal use), so we don’t use fine_tune in this case. Instead we use fit_one_cycle, the most commonly used method for training fastai models from scratch (i.e. without transfer learning):\n\nlearn.fit_one_cycle(3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.372397\n0.357177\n0.832463\n00:08\n\n\n1\n0.351544\n0.341505\n0.841523\n00:08\n\n\n2\n0.338763\n0.339184\n0.845670\n00:08\n\n\n\n\n\nThis model is using the Adult dataset, from the paper “Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid” by Rob Kohavi, which contains some demographic data about individuals (like their education, marital status, race, sex, and whether or not they have an annual income greater than $50k). The model is over 80% accurate, and took around 30 seconds to train.\nLet’s look at one more. Recommendation systems are very important, particularly in e-commerce. Companies like Amazon and Netflix try hard to recommend products or movies that users might like. Here’s how to train a model that will predict movies people might like, based on their previous viewing habits, using the MovieLens dataset:\n\nfrom fastai.collab import *\npath = untar_data(URLs.ML_SAMPLE)\ndls = CollabDataLoaders.from_csv(path/'ratings.csv')\nlearn = collab_learner(dls, y_range=(0.5,5.5))\nlearn.fine_tune(10)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.510897\n1.410028\n00:00\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.375435\n1.350930\n00:00\n\n\n1\n1.270062\n1.173962\n00:00\n\n\n2\n1.023159\n0.879298\n00:00\n\n\n3\n0.797398\n0.739787\n00:00\n\n\n4\n0.685500\n0.700903\n00:00\n\n\n5\n0.646508\n0.686387\n00:00\n\n\n6\n0.623985\n0.681087\n00:00\n\n\n7\n0.606319\n0.676885\n00:00\n\n\n8\n0.606975\n0.675833\n00:00\n\n\n9\n0.602670\n0.675682\n00:00\n\n\n\n\n\nThis model is predicting movie ratings on a scale of 0.5 to 5.0 to within around 0.6 average error. Since we’re predicting a continuous number, rather than a category, we have to tell fastai what range our target has, using the y_range parameter.\nAlthough we’re not actually using a pretrained model (for the same reason that we didn’t for the tabular model), this example shows that fastai lets us use fine_tune anyway in this case (you’ll learn how and why this works in &lt;&gt;). Sometimes it’s best to experiment with fine_tune versus fit_one_cycle to see which works best for your dataset.\nWe can use the same show_results call we saw earlier to view a few examples of user and movie IDs, actual ratings, and predictions:\n\nlearn.show_results()\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\nrating_pred\n\n\n\n\n0\n66.0\n79.0\n4.0\n3.978900\n\n\n1\n97.0\n15.0\n4.0\n3.851795\n\n\n2\n55.0\n79.0\n3.5\n3.945623\n\n\n3\n98.0\n91.0\n4.0\n4.458704\n\n\n4\n53.0\n7.0\n5.0\n4.670005\n\n\n5\n26.0\n69.0\n5.0\n4.319870\n\n\n6\n81.0\n16.0\n4.5\n4.426761\n\n\n7\n80.0\n7.0\n4.0\n4.046183\n\n\n8\n51.0\n94.0\n5.0\n3.499996\n\n\n\n\n\n\n\nSidebar: Datasets: Food for Models\nYou’ve already seen quite a few models in this section, each one trained using a different dataset to do a different task. In machine learning and deep learning, we can’t do anything without data. So, the people that create datasets for us to train our models on are the (often underappreciated) heroes. Some of the most useful and important datasets are those that become important academic baselines; that is, datasets that are widely studied by researchers and used to compare algorithmic changes. Some of these become household names (at least, among households that train models!), such as MNIST, CIFAR-10, and ImageNet.\nThe datasets used in this book have been selected because they provide great examples of the kinds of data that you are likely to encounter, and the academic literature has many examples of model results using these datasets to which you can compare your work.\nMost datasets used in this book took the creators a lot of work to build. For instance, later in the book we’ll be showing you how to create a model that can translate between French and English. The key input to this is a French/English parallel text corpus prepared back in 2009 by Professor Chris Callison-Burch of the University of Pennsylvania. This dataset contains over 20 million sentence pairs in French and English. He built the dataset in a really clever way: by crawling millions of Canadian web pages (which are often multilingual) and then using a set of simple heuristics to transform URLs of French content onto URLs pointing to the same content in English.\nAs you look at datasets throughout this book, think about where they might have come from, and how they might have been curated. Then think about what kinds of interesting datasets you could create for your own projects. (We’ll even take you step by step through the process of creating your own image dataset soon.)\nfast.ai has spent a lot of time creating cut-down versions of popular datasets that are specially designed to support rapid prototyping and experimentation, and to be easier to learn with. In this book we will often start by using one of the cut-down versions and later scale up to the full-size version (just as we’re doing in this chapter!). In fact, this is how the world’s top practitioners do their modeling in practice; they do most of their experimentation and prototyping with subsets of their data, and only use the full dataset when they have a good understanding of what they have to do.\n\n\nEnd sidebar\nEach of the models we trained showed a training and validation loss. A good validation set is one of the most important pieces of the training process. Let’s see why and learn how to create one."
  },
  {
    "objectID": "Fastbook/01_intro.html#validation-sets-and-test-sets",
    "href": "Fastbook/01_intro.html#validation-sets-and-test-sets",
    "title": "Your Deep Learning Journey",
    "section": "Validation Sets and Test Sets",
    "text": "Validation Sets and Test Sets\nAs we’ve discussed, the goal of a model is to make predictions about data. But the model training process is fundamentally dumb. If we trained a model with all our data, and then evaluated the model using that same data, we would not be able to tell how well our model can perform on data it hasn’t seen. Without this very valuable piece of information to guide us in training our model, there is a very good chance it would become good at making predictions about that data but would perform poorly on new data.\nTo avoid this, our first step was to split our dataset into two sets: the training set (which our model sees in training) and the validation set, also known as the development set (which is used only for evaluation). This lets us test that the model learns lessons from the training data that generalize to new data, the validation data.\nOne way to understand this situation is that, in a sense, we don’t want our model to get good results by “cheating.” If it makes an accurate prediction for a data item, that should be because it has learned characteristics of that kind of item, and not because the model has been shaped by actually having seen that particular item.\nSplitting off our validation data means our model never sees it in training and so is completely untainted by it, and is not cheating in any way. Right?\nIn fact, not necessarily. The situation is more subtle. This is because in realistic scenarios we rarely build a model just by training its weight parameters once. Instead, we are likely to explore many versions of a model through various modeling choices regarding network architecture, learning rates, data augmentation strategies, and other factors we will discuss in upcoming chapters. Many of these choices can be described as choices of hyperparameters. The word reflects that they are parameters about parameters, since they are the higher-level choices that govern the meaning of the weight parameters.\nThe problem is that even though the ordinary training process is only looking at predictions on the training data when it learns values for the weight parameters, the same is not true of us. We, as modelers, are evaluating the model by looking at predictions on the validation data when we decide to explore new hyperparameter values! So subsequent versions of the model are, indirectly, shaped by us having seen the validation data. Just as the automatic training process is in danger of overfitting the training data, we are in danger of overfitting the validation data through human trial and error and exploration.\nThe solution to this conundrum is to introduce another level of even more highly reserved data, the test set. Just as we hold back the validation data from the training process, we must hold back the test set data even from ourselves. It cannot be used to improve the model; it can only be used to evaluate the model at the very end of our efforts. In effect, we define a hierarchy of cuts of our data, based on how fully we want to hide it from training and modeling processes: training data is fully exposed, the validation data is less exposed, and test data is totally hidden. This hierarchy parallels the different kinds of modeling and evaluation processes themselves—the automatic training process with back propagation, the more manual process of trying different hyper-parameters between training sessions, and the assessment of our final result.\nThe test and validation sets should have enough data to ensure that you get a good estimate of your accuracy. If you’re creating a cat detector, for instance, you generally want at least 30 cats in your validation set. That means that if you have a dataset with thousands of items, using the default 20% validation set size may be more than you need. On the other hand, if you have lots of data, using some of it for validation probably doesn’t have any downsides.\nHaving two levels of “reserved data”—a validation set and a test set, with one level representing data that you are virtually hiding from yourself—may seem a bit extreme. But the reason it is often necessary is because models tend to gravitate toward the simplest way to do good predictions (memorization), and we as fallible humans tend to gravitate toward fooling ourselves about how well our models are performing. The discipline of the test set helps us keep ourselves intellectually honest. That doesn’t mean we always need a separate test set—if you have very little data, you may need to just have a validation set—but generally it’s best to use one if at all possible.\nThis same discipline can be critical if you intend to hire a third party to perform modeling work on your behalf. A third party might not understand your requirements accurately, or their incentives might even encourage them to misunderstand them. A good test set can greatly mitigate these risks and let you evaluate whether their work solves your actual problem.\nTo put it bluntly, if you’re a senior decision maker in your organization (or you’re advising senior decision makers), the most important takeaway is this: if you ensure that you really understand what test and validation sets are and why they’re important, then you’ll avoid the single biggest source of failures we’ve seen when organizations decide to use AI. For instance, if you’re considering bringing in an external vendor or service, make sure that you hold out some test data that the vendor never gets to see. Then you check their model on your test data, using a metric that you choose based on what actually matters to you in practice, and you decide what level of performance is adequate. (It’s also a good idea for you to try out some simple baseline yourself, so you know what a really simple model can achieve. Often it’ll turn out that your simple model performs just as well as one produced by an external “expert”!)\n\nUse Judgment in Defining Test Sets\nTo do a good job of defining a validation set (and possibly a test set), you will sometimes want to do more than just randomly grab a fraction of your original dataset. Remember: a key property of the validation and test sets is that they must be representative of the new data you will see in the future. This may sound like an impossible order! By definition, you haven’t seen this data yet. But you usually still do know some things.\nIt’s instructive to look at a few example cases. Many of these examples come from predictive modeling competitions on the Kaggle platform, which is a good representation of problems and methods you might see in practice.\nOne case might be if you are looking at time series data. For a time series, choosing a random subset of the data will be both too easy (you can look at the data both before and after the dates you are trying to predict) and not representative of most business use cases (where you are using historical data to build a model for use in the future). If your data includes the date and you are building a model to use in the future, you will want to choose a continuous section with the latest dates as your validation set (for instance, the last two weeks or last month of available data).\nSuppose you want to split the time series data in &lt;&gt; into training and validation sets.\n\nA random subset is a poor choice (too easy to fill in the gaps, and not indicative of what you’ll need in production), as we can see in &lt;&gt;.\n\nInstead, use the earlier data as your training set (and the later data for the validation set), as shown in &lt;&gt;.\n\nFor example, Kaggle had a competition to predict the sales in a chain of Ecuadorian grocery stores. Kaggle’s training data ran from Jan 1 2013 to Aug 15 2017, and the test data spanned Aug 16 2017 to Aug 31 2017. That way, the competition organizer ensured that entrants were making predictions for a time period that was in the future, from the perspective of their model. This is similar to the way quant hedge fund traders do back-testing to check whether their models are predictive of future periods, based on past data.\nA second common case is when you can easily anticipate ways the data you will be making predictions for in production may be qualitatively different from the data you have to train your model with.\nIn the Kaggle distracted driver competition, the independent variables are pictures of drivers at the wheel of a car, and the dependent variables are categories such as texting, eating, or safely looking ahead. Lots of pictures are of the same drivers in different positions, as we can see in &lt;&gt;. If you were an insurance company building a model from this data, note that you would be most interested in how the model performs on drivers it hasn’t seen before (since you would likely have training data only for a small group of people). In recognition of this, the test data for the competition consists of images of people that don’t appear in the training set.\n\nIf you put one of the images in &lt;&gt; in your training set and one in the validation set, your model will have an easy time making a prediction for the one in the validation set, so it will seem to be performing better than it would on new people. Another perspective is that if you used all the people in training your model, your model might be overfitting to particularities of those specific people, and not just learning the states (texting, eating, etc.).\nA similar dynamic was at work in the Kaggle fisheries competition to identify the species of fish caught by fishing boats in order to reduce illegal fishing of endangered populations. The test set consisted of boats that didn’t appear in the training data. This means that you’d want your validation set to include boats that are not in the training set.\nSometimes it may not be clear how your validation data will differ. For instance, for a problem using satellite imagery, you’d need to gather more information on whether the training set just contained certain geographic locations, or if it came from geographically scattered data.\nNow that you have gotten a taste of how to build a model, you can decide what you want to dig into next."
  },
  {
    "objectID": "Fastbook/01_intro.html#a-choose-your-own-adventure-moment",
    "href": "Fastbook/01_intro.html#a-choose-your-own-adventure-moment",
    "title": "Your Deep Learning Journey",
    "section": "A Choose Your Own Adventure moment",
    "text": "A Choose Your Own Adventure moment\nIf you would like to learn more about how to use deep learning models in practice, including how to identify and fix errors, create a real working web application, and avoid your model causing unexpected harm to your organization or society more generally, then keep reading the next two chapters. If you would like to start learning the foundations of how deep learning works under the hood, skip to &lt;&gt;. (Did you ever read Choose Your Own Adventure books as a kid? Well, this is kind of like that… except with more deep learning than that book series contained.)\nYou will need to read all these chapters to progress further in the book, but it is totally up to you which order you read them in. They don’t depend on each other. If you skip ahead to &lt;&gt;, we will remind you at the end to come back and read the chapters you skipped over before you go any further."
  },
  {
    "objectID": "Fastbook/01_intro.html#questionnaire",
    "href": "Fastbook/01_intro.html#questionnaire",
    "title": "Your Deep Learning Journey",
    "section": "Questionnaire",
    "text": "Questionnaire\nIt can be hard to know in pages and pages of prose what the key things are that you really need to focus on and remember. So, we’ve prepared a list of questions and suggested steps to complete at the end of each chapter. All the answers are in the text of the chapter, so if you’re not sure about anything here, reread that part of the text and make sure you understand it. Answers to all these questions are also available on the book’s website. You can also visit the forums if you get stuck to get help from other folks studying this material.\nFor more questions, including detailed answers and links to the video timeline, have a look at Radek Osmulski’s aiquizzes.\n\nDo you need these for deep learning?\n\nLots of math T / F\nLots of data T / F\nLots of expensive computers T / F\nA PhD T / F\n\nName five areas where deep learning is now the best in the world.\nWhat was the name of the first device that was based on the principle of the artificial neuron?\nBased on the book of the same name, what are the requirements for parallel distributed processing (PDP)?\nWhat were the two theoretical misunderstandings that held back the field of neural networks?\nWhat is a GPU?\nOpen a notebook and execute a cell containing: 1+1. What happens?\nFollow through each cell of the stripped version of the notebook for this chapter. Before executing each cell, guess what will happen.\nComplete the Jupyter Notebook online appendix.\nWhy is it hard to use a traditional computer program to recognize images in a photo?\nWhat did Samuel mean by “weight assignment”?\nWhat term do we normally use in deep learning for what Samuel called “weights”?\nDraw a picture that summarizes Samuel’s view of a machine learning model.\nWhy is it hard to understand why a deep learning model makes a particular prediction?\nWhat is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy?\nWhat do you need in order to train a model?\nHow could a feedback loop impact the rollout of a predictive policing model?\nDo we always have to use 224×224-pixel images with the cat recognition model?\nWhat is the difference between classification and regression?\nWhat is a validation set? What is a test set? Why do we need them?\nWhat will fastai do if you don’t provide a validation set?\nCan we always use a random sample for a validation set? Why or why not?\nWhat is overfitting? Provide an example.\nWhat is a metric? How does it differ from “loss”?\nHow can pretrained models help?\nWhat is the “head” of a model?\nWhat kinds of features do the early layers of a CNN find? How about the later layers?\nAre image models only useful for photos?\nWhat is an “architecture”?\nWhat is segmentation?\nWhat is y_range used for? When do we need it?\nWhat are “hyperparameters”?\nWhat’s the best way to avoid failures when using AI in an organization?\n\n\nFurther Research\nEach chapter also has a “Further Research” section that poses questions that aren’t fully answered in the text, or gives more advanced assignments. Answers to these questions aren’t on the book’s website; you’ll need to do your own research!\n\nWhy is a GPU useful for deep learning? How is a CPU different, and why is it less effective for deep learning?\nTry to think of three areas where feedback loops might impact the use of machine learning. See if you can find documented examples of that happening in practice."
  },
  {
    "objectID": "Fastbook/16_accel_sgd.html",
    "href": "Fastbook/16_accel_sgd.html",
    "title": "The Training Process",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\n[[chapter_accel_sgd]]\nYou now know how to create state-of-the-art architectures for computer vision, natural language processing, tabular analysis, and collaborative filtering, and you know how to train them quickly. So we’re done, right? Not quite yet. We still have to explore a little bit more the training process.\nWe explained in &lt;&gt; the basis of stochastic gradient descent: pass a mini-batch to the model, compare it to our target with the loss function, then compute the gradients of this loss function with regard to each weight before updating the weights with the formula:\nWe implemented this from scratch in a training loop, and also saw that PyTorch provides a simple nn.SGD class that does this calculation for each parameter for us. In this chapter we will build some faster optimizers, using a flexible foundation. But that’s not all we might want to change in the training process. For any tweak of the training loop, we will need a way to add some code to the basis of SGD. The fastai library has a system of callbacks to do this, and we will teach you all about it.\nLet’s start with standard SGD to get a baseline, then we will introduce the most commonly used optimizers."
  },
  {
    "objectID": "Fastbook/16_accel_sgd.html#establishing-a-baseline",
    "href": "Fastbook/16_accel_sgd.html#establishing-a-baseline",
    "title": "The Training Process",
    "section": "Establishing a Baseline",
    "text": "Establishing a Baseline\nFirst, we’ll create a baseline, using plain SGD, and compare it to fastai’s default optimizer. We’ll start by grabbing Imagenette with the same get_data we used in &lt;&gt;:\n\n#hide_input\ndef get_data(url, presize, resize):\n    path = untar_data(url)\n    return DataBlock(\n        blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, \n        splitter=GrandparentSplitter(valid_name='val'),\n        get_y=parent_label, item_tfms=Resize(presize),\n        batch_tfms=[*aug_transforms(min_scale=0.5, size=resize),\n                    Normalize.from_stats(*imagenet_stats)],\n    ).dataloaders(path, bs=128)\n\n\ndls = get_data(URLs.IMAGENETTE_160, 160, 128)\n\nWe’ll create a ResNet-34 without pretraining, and pass along any arguments received:\n\ndef get_learner(**kwargs):\n    return vision_learner(dls, resnet34, pretrained=False,\n                    metrics=accuracy, **kwargs).to_fp16()\n\nHere’s the default fastai optimizer, with the usual 3e-3 learning rate:\n\nlearn = get_learner()\nlearn.fit_one_cycle(3, 0.003)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.571932\n2.685040\n0.322548\n00:11\n\n\n1\n1.904674\n1.852589\n0.437452\n00:11\n\n\n2\n1.586909\n1.374908\n0.594904\n00:11\n\n\n\n\n\nNow let’s try plain SGD. We can pass opt_func (optimization function) to vision_learner to get fastai to use any optimizer:\n\nlearn = get_learner(opt_func=SGD)\n\nThe first thing to look at is lr_find:\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\n\n\n\nIt looks like we’ll need to use a higher learning rate than we normally use:\n\nlearn.fit_one_cycle(3, 0.03, moms=(0,0,0))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.969412\n2.214596\n0.242038\n00:09\n\n\n1\n2.442730\n1.845950\n0.362548\n00:09\n\n\n2\n2.157159\n1.741143\n0.408917\n00:09\n\n\n\n\n\nBecause accelerating SGD with momentum is such a good idea, fastai does this by default in fit_one_cycle, so we turn it off with moms=(0,0,0). We’ll be discussing momentum shortly.)\nClearly, plain SGD isn’t training as fast as we’d like. So let’s learn some tricks to get accelerated training!"
  },
  {
    "objectID": "Fastbook/16_accel_sgd.html#a-generic-optimizer",
    "href": "Fastbook/16_accel_sgd.html#a-generic-optimizer",
    "title": "The Training Process",
    "section": "A Generic Optimizer",
    "text": "A Generic Optimizer\nTo build up our accelerated SGD tricks, we’ll need to start with a nice flexible optimizer foundation. No library prior to fastai provided such a foundation, but during fastai’s development we realized that all the optimizer improvements we’d seen in the academic literature could be handled using optimizer callbacks. These are small pieces of code that we can compose, mix and match in an optimizer to build the optimizer step. They are called by fastai’s lightweight Optimizer class. These are the definitions in Optimizer of the two key methods that we’ve been using in this book:\ndef zero_grad(self):\n    for p,*_ in self.all_params():\n        p.grad.detach_()\n        p.grad.zero_()\n\ndef step(self):\n    for p,pg,state,hyper in self.all_params():\n        for cb in self.cbs:\n            state = _update(state, cb(p, **{**state, **hyper}))\n        self.state[p] = state\nAs we saw when training an MNIST model from scratch, zero_grad just loops through the parameters of the model and sets the gradients to zero. It also calls detach_, which removes any history of gradient computation, since it won’t be needed after zero_grad.\nThe more interesting method is step, which loops through the callbacks (cbs) and calls them to update the parameters (the _update function just calls state.update if there’s anything returned by cb). As you can see, Optimizer doesn’t actually do any SGD steps itself. Let’s see how we can add SGD to Optimizer.\nHere’s an optimizer callback that does a single SGD step, by multiplying -lr by the gradients and adding that to the parameter (when Tensor.add_ in PyTorch is passed two parameters, they are multiplied together before the addition):\n\ndef sgd_cb(p, lr, **kwargs): p.data.add_(-lr, p.grad.data)\n\nWe can pass this to Optimizer using the cbs parameter; we’ll need to use partial since Learner will call this function to create our optimizer later:\n\nopt_func = partial(Optimizer, cbs=[sgd_cb])\n\nLet’s see if this trains:\n\nlearn = get_learner(opt_func=opt_func)\nlearn.fit(3, 0.03)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.730918\n2.009971\n0.332739\n00:09\n\n\n1\n2.204893\n1.747202\n0.441529\n00:09\n\n\n2\n1.875621\n1.684515\n0.445350\n00:09\n\n\n\n\n\nIt’s working! So that’s how we create SGD from scratch in fastai. Now let’s see what “momentum” is."
  },
  {
    "objectID": "Fastbook/16_accel_sgd.html#momentum",
    "href": "Fastbook/16_accel_sgd.html#momentum",
    "title": "The Training Process",
    "section": "Momentum",
    "text": "Momentum\nAs described in &lt;&gt;, SGD can be thought of as standing at the top of a mountain and working your way down by taking a step in the direction of the steepest slope at each point in time. But what if we have a ball rolling down the mountain? It won’t, at each given point, exactly follow the direction of the gradient, as it will have momentum. A ball with more momentum (for instance, a heavier ball) will skip over little bumps and holes, and be more likely to get to the bottom of a bumpy mountain. A ping pong ball, on the other hand, will get stuck in every little crevice.\nSo how can we bring this idea over to SGD? We can use a moving average, instead of only the current gradient, to make our step:\nweight.avg = beta * weight.avg + (1-beta) * weight.grad\nnew_weight = weight - lr * weight.avg\nHere beta is some number we choose which defines how much momentum to use. If beta is 0, then the first equation becomes weight.avg = weight.grad, so we end up with plain SGD. But if it’s a number close to 1, then the main direction chosen is an average of the previous steps. (If you have done a bit of statistics, you may recognize in the first equation an exponentially weighted moving average, which is very often used to denoise data and get the underlying tendency.)\nNote that we are writing weight.avg to highlight the fact that we need to store the moving averages for each parameter of the model (they all have their own independent moving averages).\n&lt;&gt; shows an example of noisy data for a single parameter, with the momentum curve plotted in red, and the gradients of the parameter plotted in blue. The gradients increase, then decrease, and the momentum does a good job of following the general trend without getting too influenced by noise.\n\n#hide_input\n#id img_momentum\n#caption An example of momentum\n#alt Graph showing an example of momentum\nx = np.linspace(-4, 4, 100)\ny = 1 - (x/3) ** 2\nx1 = x + np.random.randn(100) * 0.1\ny1 = y + np.random.randn(100) * 0.1\nplt.scatter(x1,y1)\nidx = x1.argsort()\nbeta,avg,res = 0.7,0,[]\nfor i in idx:\n    avg = beta * avg + (1-beta) * y1[i]\n    res.append(avg/(1-beta**(i+1)))\nplt.plot(x1[idx],np.array(res), color='red');\n\n\n\n\n\n\n\n\nIt works particularly well if the loss function has narrow canyons we need to navigate: vanilla SGD would send us bouncing from one side to the other, while SGD with momentum will average those to roll smoothly down the side. The parameter beta determines the strength of the momentum we are using: with a small beta we stay closer to the actual gradient values, whereas with a high beta we will mostly go in the direction of the average of the gradients and it will take a while before any change in the gradients makes that trend move.\nWith a large beta, we might miss that the gradients have changed directions and roll over a small local minima. This is a desired side effect: intuitively, when we show a new input to our model, it will look like something in the training set but won’t be exactly like it. That means it will correspond to a point in the loss function that is close to the minimum we ended up with at the end of training, but not exactly at that minimum. So, we would rather end up training in a wide minimum, where nearby points have approximately the same loss (or if you prefer, a point where the loss is as flat as possible). &lt;&gt; shows how the chart in &lt;&gt; varies as we change beta.\n\n#hide_input\n#id img_betas\n#caption Momentum with different beta values\n#alt Graph showing how the beta value influences momentum\nx = np.linspace(-4, 4, 100)\ny = 1 - (x/3) ** 2\nx1 = x + np.random.randn(100) * 0.1\ny1 = y + np.random.randn(100) * 0.1\n_,axs = plt.subplots(2,2, figsize=(12,8))\nbetas = [0.5,0.7,0.9,0.99]\nidx = x1.argsort()\nfor beta,ax in zip(betas, axs.flatten()):\n    ax.scatter(x1,y1)\n    avg,res = 0,[]\n    for i in idx:\n        avg = beta * avg + (1-beta) * y1[i]\n        res.append(avg)#/(1-beta**(i+1)))\n    ax.plot(x1[idx],np.array(res), color='red');\n    ax.set_title(f'beta={beta}')\n\n\n\n\n\n\n\n\nWe can see in these examples that a beta that’s too high results in the overall changes in gradient getting ignored. In SGD with momentum, a value of beta that is often used is 0.9.\nfit_one_cycle by default starts with a beta of 0.95, gradually adjusts it to 0.85, and then gradually moves it back to 0.95 at the end of training. Let’s see how our training goes with momentum added to plain SGD.\nIn order to add momentum to our optimizer, we’ll first need to keep track of the moving average gradient, which we can do with another callback. When an optimizer callback returns a dict, it is used to update the state of the optimizer and is passed back to the optimizer on the next step. So this callback will keep track of the gradient averages in a parameter called grad_avg:\n\ndef average_grad(p, mom, grad_avg=None, **kwargs):\n    if grad_avg is None: grad_avg = torch.zeros_like(p.grad.data)\n    return {'grad_avg': grad_avg*mom + p.grad.data}\n\nTo use it, we just have to replace p.grad.data with grad_avg in our step function:\n\ndef momentum_step(p, lr, grad_avg, **kwargs): p.data.add_(-lr, grad_avg)\n\n\nopt_func = partial(Optimizer, cbs=[average_grad,momentum_step], mom=0.9)\n\nLearner will automatically schedule mom and lr, so fit_one_cycle will even work with our custom Optimizer:\n\nlearn = get_learner(opt_func=opt_func)\nlearn.fit_one_cycle(3, 0.03)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.856000\n2.493429\n0.246115\n00:10\n\n\n1\n2.504205\n2.463813\n0.348280\n00:10\n\n\n2\n2.187387\n1.755670\n0.418853\n00:10\n\n\n\n\n\n\nlearn.recorder.plot_sched()\n\n\n\n\n\n\n\n\nWe’re still not getting great results, so let’s see what else we can do."
  },
  {
    "objectID": "Fastbook/16_accel_sgd.html#rmsprop",
    "href": "Fastbook/16_accel_sgd.html#rmsprop",
    "title": "The Training Process",
    "section": "RMSProp",
    "text": "RMSProp\nRMSProp is another variant of SGD introduced by Geoffrey Hinton in Lecture 6e of his Coursera class “Neural Networks for Machine Learning”. The main difference from SGD is that it uses an adaptive learning rate: instead of using the same learning rate for every parameter, each parameter gets its own specific learning rate controlled by a global learning rate. That way we can speed up training by giving a higher learning rate to the weights that need to change a lot while the ones that are good enough get a lower learning rate.\nHow do we decide which parameters should have a high learning rate and which should not? We can look at the gradients to get an idea. If a parameter’s gradients have been close to zero for a while, that parameter will need a higher learning rate because the loss is flat. On the other hand, if the gradients are all over the place, we should probably be careful and pick a low learning rate to avoid divergence. We can’t just average the gradients to see if they’re changing a lot, because the average of a large positive and a large negative number is close to zero. Instead, we can use the usual trick of either taking the absolute value or the squared values (and then taking the square root after the mean).\nOnce again, to determine the general tendency behind the noise, we will use a moving average—specifically the moving average of the gradients squared. Then we will update the corresponding weight by using the current gradient (for the direction) divided by the square root of this moving average (that way if it’s low, the effective learning rate will be higher, and if it’s high, the effective learning rate will be lower):\nw.square_avg = alpha * w.square_avg + (1-alpha) * (w.grad ** 2)\nnew_w = w - lr * w.grad / math.sqrt(w.square_avg + eps)\nThe eps (epsilon) is added for numerical stability (usually set at 1e-8), and the default value for alpha is usually 0.99.\nWe can add this to Optimizer by doing much the same thing we did for avg_grad, but with an extra **2:\n\ndef average_sqr_grad(p, sqr_mom, sqr_avg=None, **kwargs):\n    if sqr_avg is None: sqr_avg = torch.zeros_like(p.grad.data)\n    return {'sqr_avg': sqr_mom*sqr_avg + (1-sqr_mom)*p.grad.data**2}\n\nAnd we can define our step function and optimizer as before:\n\ndef rms_prop_step(p, lr, sqr_avg, eps, grad_avg=None, **kwargs):\n    denom = sqr_avg.sqrt().add_(eps)\n    p.data.addcdiv_(-lr, p.grad, denom)\n\nopt_func = partial(Optimizer, cbs=[average_sqr_grad,rms_prop_step],\n                   sqr_mom=0.99, eps=1e-7)\n\nLet’s try it out:\n\nlearn = get_learner(opt_func=opt_func)\nlearn.fit_one_cycle(3, 0.003)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.766912\n1.845900\n0.402548\n00:11\n\n\n1\n2.194586\n1.510269\n0.504459\n00:11\n\n\n2\n1.869099\n1.447939\n0.544968\n00:11\n\n\n\n\n\nMuch better! Now we just have to bring these ideas together, and we have Adam, fastai’s default optimizer."
  },
  {
    "objectID": "Fastbook/16_accel_sgd.html#adam",
    "href": "Fastbook/16_accel_sgd.html#adam",
    "title": "The Training Process",
    "section": "Adam",
    "text": "Adam\nAdam mixes the ideas of SGD with momentum and RMSProp together: it uses the moving average of the gradients as a direction and divides by the square root of the moving average of the gradients squared to give an adaptive learning rate to each parameter.\nThere is one other difference in how Adam calculates moving averages. It takes the unbiased moving average, which is:\nw.avg = beta * w.avg + (1-beta) * w.grad\nunbias_avg = w.avg / (1 - (beta**(i+1)))\nif we are the i-th iteration (starting at 0 like Python does). This divisor of 1 - (beta**(i+1)) makes sure the unbiased average looks more like the gradients at the beginning (since beta &lt; 1, the denominator is very quickly close to 1).\nPutting everything together, our update step looks like:\nw.avg = beta1 * w.avg + (1-beta1) * w.grad\nunbias_avg = w.avg / (1 - (beta1**(i+1)))\nw.sqr_avg = beta2 * w.sqr_avg + (1-beta2) * (w.grad ** 2)\nnew_w = w - lr * unbias_avg / sqrt(w.sqr_avg + eps)\nLike for RMSProp, eps is usually set to 1e-8, and the default for (beta1,beta2) suggested by the literature is (0.9,0.999).\nIn fastai, Adam is the default optimizer we use since it allows faster training, but we’ve found that beta2=0.99 is better suited to the type of schedule we are using. beta1 is the momentum parameter, which we specify with the argument moms in our call to fit_one_cycle. As for eps, fastai uses a default of 1e-5. eps is not just useful for numerical stability. A higher eps limits the maximum value of the adjusted learning rate. To take an extreme example, if eps is 1, then the adjusted learning will never be higher than the base learning rate.\nRather than show all the code for this in the book, we’ll let you look at the optimizer notebook in fastai’s GitHub repository (browse the nbs folder and search for the notebook called optimizer). You’ll see all the code we’ve shown so far, along with Adam and other optimizers, and lots of examples and tests.\nOne thing that changes when we go from SGD to Adam is the way we apply weight decay, and it can have important consequences."
  },
  {
    "objectID": "Fastbook/16_accel_sgd.html#decoupled-weight-decay",
    "href": "Fastbook/16_accel_sgd.html#decoupled-weight-decay",
    "title": "The Training Process",
    "section": "Decoupled Weight Decay",
    "text": "Decoupled Weight Decay\nWeight decay, which we discussed in &lt;&gt;, is equivalent to (in the case of vanilla SGD) updating the parameters with:\nnew_weight = weight - lr*weight.grad - lr*wd*weight\nThe last part of this formula explains the name of this technique: each weight is decayed by a factor lr * wd.\nThe other name of weight decay is L2 regularization, which consists in adding the sum of all squared weights to the loss (multiplied by the weight decay). As we have seen in &lt;&gt;, this can be directly expressed on the gradients with:\nweight.grad += wd*weight\nFor SGD, those two formulas are equivalent. However, this equivalence only holds for standard SGD, because we have seen that with momentum, RMSProp or in Adam, the update has some additional formulas around the gradient.\nMost libraries use the second formulation, but it was pointed out in “Decoupled Weight Decay Regularization” by Ilya Loshchilov and Frank Hutter, that the first one is the only correct approach with the Adam optimizer or momentum, which is why fastai makes it its default.\nNow you know everything that is hidden behind the line learn.fit_one_cycle!\nOptimizers are only one part of the training process, however when you need to change the training loop with fastai, you can’t directly change the code inside the library. Instead, we have designed a system of callbacks to let you write any tweaks you like in independent blocks that you can then mix and match."
  },
  {
    "objectID": "Fastbook/16_accel_sgd.html#callbacks",
    "href": "Fastbook/16_accel_sgd.html#callbacks",
    "title": "The Training Process",
    "section": "Callbacks",
    "text": "Callbacks\nSometimes you need to change how things work a little bit. In fact, we have already seen examples of this: Mixup, fp16 training, resetting the model after each epoch for training RNNs, and so forth. How do we go about making these kinds of tweaks to the training process?\nWe’ve seen the basic training loop, which, with the help of the Optimizer class, looks like this for a single epoch:\nfor xb,yb in dl:\n    loss = loss_func(model(xb), yb)\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n&lt;&gt; shows how to picture that.\n\nThe usual way for deep learning practitioners to customize the training loop is to make a copy of an existing training loop, and then insert the code necessary for their particular changes into it. This is how nearly all code that you find online will look. But it has some very serious problems.\nIt’s not very likely that some particular tweaked training loop is going to meet your particular needs. There are hundreds of changes that can be made to a training loop, which means there are billions and billions of possible permutations. You can’t just copy one tweak from a training loop here, another from a training loop there, and expect them all to work together. Each will be based on different assumptions about the environment that it’s working in, use different naming conventions, and expect the data to be in different formats.\nWe need a way to allow users to insert their own code at any part of the training loop, but in a consistent and well-defined way. Computer scientists have already come up with an elegant solution: the callback. A callback is a piece of code that you write, and inject into another piece of code at some predefined point. In fact, callbacks have been used with deep learning training loops for years. The problem is that in previous libraries it was only possible to inject code in a small subset of places where this may have been required, and, more importantly, callbacks were not able to do all the things they needed to do.\nIn order to be just as flexible as manually copying and pasting a training loop and directly inserting code into it, a callback must be able to read every possible piece of information available in the training loop, modify all of it as needed, and fully control when a batch, epoch, or even the whole training loop should be terminated. fastai is the first library to provide all of this functionality. It modifies the training loop so it looks like &lt;&gt;.\n\nThe real effectiveness of this approach has been borne out over the last couple of years—it has turned out that, by using the fastai callback system, we were able to implement every single new paper we tried and fulfilled every user request for modifying the training loop. The training loop itself has not required modifications. &lt;&gt; shows just a few of the callbacks that have been added.\n\nThe reason that this is important is because it means that whatever idea we have in our head, we can implement it. We need never dig into the source code of PyTorch or fastai and hack together some one-off system to try out our ideas. And when we do implement our own callbacks to develop our own ideas, we know that they will work together with all of the other functionality provided by fastai–so we will get progress bars, mixed-precision training, hyperparameter annealing, and so forth.\nAnother advantage is that it makes it easy to gradually remove or add functionality and perform ablation studies. You just need to adjust the list of callbacks you pass along to your fit function.\nAs an example, here is the fastai source code that is run for each batch of the training loop:\ntry:\n    self._split(b);                                  self('before_batch')\n    self.pred = self.model(*self.xb);                self('after_pred')\n    self.loss = self.loss_func(self.pred, *self.yb); self('after_loss')\n    if not self.training: return\n    self.loss.backward();                            self('after_backward')\n    self.opt.step();                                 self('after_step')\n    self.opt.zero_grad()\nexcept CancelBatchException:                         self('after_cancel_batch')\nfinally:                                             self('after_batch')\nThe calls of the form self('...') are where the callbacks are called. As you see, this happens after every step. The callback will receive the entire state of training, and can also modify it. For instance, the input data and target labels are in self.xb and self.yb, respectively; a callback can modify these to alter the data the training loop sees. It can also modify self.loss, or even the gradients.\nLet’s see how this works in practice by writing a callback.\n\nCreating a Callback\nWhen you want to write your own callback, the full list of available events is:\n\nbefore_fit:: called before doing anything; ideal for initial setup.\nbefore_epoch:: called at the beginning of each epoch; useful for any behavior you need to reset at each epoch.\nbefore_train:: called at the beginning of the training part of an epoch.\nbefore_batch:: called at the beginning of each batch, just after drawing said batch. It can be used to do any setup necessary for the batch (like hyperparameter scheduling) or to change the input/target before it goes into the model (for instance, apply Mixup).\nafter_pred:: called after computing the output of the model on the batch. It can be used to change that output before it’s fed to the loss function.\nafter_loss:: called after the loss has been computed, but before the backward pass. It can be used to add penalty to the loss (AR or TAR in RNN training, for instance).\nafter_backward:: called after the backward pass, but before the update of the parameters. It can be used to make changes to the gradients before said update (via gradient clipping, for instance).\nafter_step:: called after the step and before the gradients are zeroed.\nafter_batch:: called at the end of a batch, to perform any required cleanup before the next one.\nafter_train:: called at the end of the training phase of an epoch.\nbefore_validate:: called at the beginning of the validation phase of an epoch; useful for any setup needed specifically for validation.\nafter_validate:: called at the end of the validation part of an epoch.\nafter_epoch:: called at the end of an epoch, for any cleanup before the next one.\nafter_fit:: called at the end of training, for final cleanup.\n\nThe elements of this list are available as attributes of the special variable event, so you can just type event. and hit Tab in your notebook to see a list of all the options.\nLet’s take a look at an example. Do you recall how in &lt;&gt; we needed to ensure that our special reset method was called at the start of training and validation for each epoch? We used the ModelResetter callback provided by fastai to do this for us. But how does it work? Here’s the full source code for that class:\n\nclass ModelResetter(Callback):\n    def before_train(self):    self.model.reset()\n    def before_validate(self): self.model.reset()\n\nYes, that’s actually it! It just does what we said in the preceding paragraph: after completing training or validation for an epoch, call a method named reset.\nCallbacks are often “short and sweet” like this one. In fact, let’s look at one more. Here’s the fastai source for the callback that adds RNN regularization (AR and TAR):\n\nclass RNNRegularizer(Callback):\n    def __init__(self, alpha=0., beta=0.): self.alpha,self.beta = alpha,beta\n\n    def after_pred(self):\n        self.raw_out,self.out = self.pred[1],self.pred[2]\n        self.learn.pred = self.pred[0]\n\n    def after_loss(self):\n        if not self.training: return\n        if self.alpha != 0.:\n            self.learn.loss += self.alpha * self.out[-1].float().pow(2).mean()\n        if self.beta != 0.:\n            h = self.raw_out[-1]\n            if len(h)&gt;1:\n                self.learn.loss += self.beta * (h[:,1:] - h[:,:-1]\n                                               ).float().pow(2).mean()\n\n\nnote: Code It Yourself: Go back and reread “Activation Regularization and Temporal Activation Regularization” in &lt;&gt; then take another look at the code here. Make sure you understand what it’s doing, and why.\n\nIn both of these examples, notice how we can access attributes of the training loop by directly checking self.model or self.pred. That’s because a Callback will always try to get an attribute it doesn’t have inside the Learner associated with it. These are shortcuts for self.learn.model or self.learn.pred. Note that they work for reading attributes, but not for writing them, which is why when RNNRegularizer changes the loss or the predictions you see self.learn.loss = or self.learn.pred =.\nWhen writing a callback, the following attributes of Learner are available:\n\nmodel:: The model used for training/validation.\ndata:: The underlying DataLoaders.\nloss_func:: The loss function used.\nopt:: The optimizer used to update the model parameters.\nopt_func:: The function used to create the optimizer.\ncbs:: The list containing all the Callbacks.\ndl:: The current DataLoader used for iteration.\nx/xb:: The last input drawn from self.dl (potentially modified by callbacks). xb is always a tuple (potentially with one element) and x is detuplified. You can only assign to xb.\ny/yb:: The last target drawn from self.dl (potentially modified by callbacks). yb is always a tuple (potentially with one element) and y is detuplified. You can only assign to yb.\npred:: The last predictions from self.model (potentially modified by callbacks).\nloss:: The last computed loss (potentially modified by callbacks).\nn_epoch:: The number of epochs in this training.\nn_iter:: The number of iterations in the current self.dl.\nepoch:: The current epoch index (from 0 to n_epoch-1).\niter:: The current iteration index in self.dl (from 0 to n_iter-1).\n\nThe following attributes are added by TrainEvalCallback and should be available unless you went out of your way to remove that callback:\n\ntrain_iter:: The number of training iterations done since the beginning of this training\npct_train:: The percentage of training iterations completed (from 0. to 1.)\ntraining:: A flag to indicate whether or not we’re in training mode\n\nThe following attribute is added by Recorder and should be available unless you went out of your way to remove that callback:\n\nsmooth_loss:: An exponentially averaged version of the training loss\n\nCallbacks can also interrupt any part of the training loop by using a system of exceptions.\n\n\nCallback Ordering and Exceptions\nSometimes, callbacks need to be able to tell fastai to skip over a batch, or an epoch, or stop training altogether. For instance, consider TerminateOnNaNCallback. This handy callback will automatically stop training any time the loss becomes infinite or NaN (not a number). Here’s the fastai source for this callback:\n\nclass TerminateOnNaNCallback(Callback):\n    run_before=Recorder\n    def after_batch(self):\n        if torch.isinf(self.loss) or torch.isnan(self.loss):\n            raise CancelFitException\n\nThe line raise CancelFitException tells the training loop to interrupt training at this point. The training loop catches this exception and does not run any further training or validation. The callback control flow exceptions available are:\n\nCancelBatchException:: Skip the rest of this batch and go to after_batch.\nCancelTrainException:: Skip the rest of the training part of the epoch and go to after_train.\nCancelValidException:: Skip the rest of the validation part of the epoch and go to after_validate.\nCancelEpochException:: Skip the rest of this epoch and go to after_epoch.\nCancelFitException:: Interrupt training and go to after_fit.\n\nYou can detect if one of those exceptions has occurred and add code that executes right after with the following events:\n\nafter_cancel_batch:: Reached immediately after a CancelBatchException before proceeding to after_batch\nafter_cancel_train:: Reached immediately after a CancelTrainException before proceeding to after_train\nafter_cancel_valid:: Reached immediately after a CancelValidException before proceeding to after_valid\nafter_cancel_epoch:: Reached immediately after a CancelEpochException before proceeding to after_epoch\nafter_cancel_fit:: Reached immediately after a CancelFitException before proceeding to after_fit\n\nSometimes, callbacks need to be called in a particular order. For example, in the case of TerminateOnNaNCallback, it’s important that Recorder runs its after_batch after this callback, to avoid registering an NaN loss. You can specify run_before (this callback must run before …) or run_after (this callback must run after …) in your callback to ensure the ordering that you need."
  },
  {
    "objectID": "Fastbook/16_accel_sgd.html#conclusion",
    "href": "Fastbook/16_accel_sgd.html#conclusion",
    "title": "The Training Process",
    "section": "Conclusion",
    "text": "Conclusion\nIn this chapter we took a close look at the training loop, exploring different variants of SGD and why they can be more powerful. At the time of writing, developing new optimizers is a very active area of research, so by the time you read this chapter there may be an addendum on the book’s website that presents new variants. Be sure to check out how our general optimizer framework can help you implement new optimizers very quickly.\nWe also examined the powerful callback system that allows you to customize every bit of the training loop by enabling you to inspect and modify any parameter you like between each step."
  },
  {
    "objectID": "Fastbook/16_accel_sgd.html#questionnaire",
    "href": "Fastbook/16_accel_sgd.html#questionnaire",
    "title": "The Training Process",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhat is the equation for a step of SGD, in math or code (as you prefer)?\nWhat do we pass to vision_learner to use a non-default optimizer?\nWhat are optimizer callbacks?\nWhat does zero_grad do in an optimizer?\nWhat does step do in an optimizer? How is it implemented in the general optimizer?\nRewrite sgd_cb to use the += operator, instead of add_.\nWhat is “momentum”? Write out the equation.\nWhat’s a physical analogy for momentum? How does it apply in our model training settings?\nWhat does a bigger value for momentum do to the gradients?\nWhat are the default values of momentum for 1cycle training?\nWhat is RMSProp? Write out the equation.\nWhat do the squared values of the gradients indicate?\nHow does Adam differ from momentum and RMSProp?\nWrite out the equation for Adam.\nCalculate the values of unbias_avg and w.avg for a few batches of dummy values.\nWhat’s the impact of having a high eps in Adam?\nRead through the optimizer notebook in fastai’s repo, and execute it.\nIn what situations do dynamic learning rate methods like Adam change the behavior of weight decay?\nWhat are the four steps of a training loop?\nWhy is using callbacks better than writing a new training loop for each tweak you want to add?\nWhat aspects of the design of fastai’s callback system make it as flexible as copying and pasting bits of code?\nHow can you get the list of events available to you when writing a callback?\nWrite the ModelResetter callback (without peeking).\nHow can you access the necessary attributes of the training loop inside a callback? When can you use or not use the shortcuts that go with them?\nHow can a callback influence the control flow of the training loop?\nWrite the TerminateOnNaN callback (without peeking, if possible).\nHow do you make sure your callback runs after or before another callback?\n\n\nFurther Research\n\nLook up the “Rectified Adam” paper, implement it using the general optimizer framework, and try it out. Search for other recent optimizers that work well in practice, and pick one to implement.\nLook at the mixed-precision callback with the documentation. Try to understand what each event and line of code does.\nImplement your own version of the learning rate finder from scratch. Compare it with fastai’s version.\nLook at the source code of the callbacks that ship with fastai. See if you can find one that’s similar to what you’re looking to do, to get some inspiration."
  },
  {
    "objectID": "Fastbook/16_accel_sgd.html#foundations-of-deep-learning-wrap-up",
    "href": "Fastbook/16_accel_sgd.html#foundations-of-deep-learning-wrap-up",
    "title": "The Training Process",
    "section": "Foundations of Deep Learning: Wrap up",
    "text": "Foundations of Deep Learning: Wrap up\nCongratulations, you have made it to the end of the “foundations of deep learning” section of the book! You now understand how all of fastai’s applications and most important architectures are built, and the recommended ways to train them—and you have all the information you need to build these from scratch. While you probably won’t need to create your own training loop, or batchnorm layer, for instance, knowing what is going on behind the scenes is very helpful for debugging, profiling, and deploying your solutions.\nSince you understand the foundations of fastai’s applications now, be sure to spend some time digging through the source notebooks and running and experimenting with parts of them. This will give you a better idea of how everything in fastai is developed.\nIn the next section, we will be looking even further under the covers: we’ll explore how the actual forward and backward passes of a neural network are done, and we will see what tools are at our disposal to get better performance. We will then continue with a project that brings together all the material in the book, which we will use to build a tool for interpreting convolutional neural networks. Last but not least, we’ll finish by building fastai’s Learner class from scratch."
  },
  {
    "objectID": "Fastbook/README_id.html",
    "href": "Fastbook/README_id.html",
    "title": "Buku fastai",
    "section": "",
    "text": "English / Spanish / Korean / Chinese / Bengali / Indonesian / Italian / Portuguese / Vietnamese / Japanese"
  },
  {
    "objectID": "Fastbook/README_id.html#kutipan",
    "href": "Fastbook/README_id.html#kutipan",
    "title": "Buku fastai",
    "section": "Kutipan",
    "text": "Kutipan\nJika Anda ingin mengutip buku tersebut, Anda dapat menggunakan:\n@book{howard2020deep,\ntitle={Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD},\nauthor={Howard, J. and Gugger, S.},\nisbn={9781492045526},\nurl={https://books.google.no/books?id=xd6LxgEACAAJ},\nyear={2020},\npublisher={O'Reilly Media, Incorporated}\n}"
  },
  {
    "objectID": "Fastbook/README_tr.html",
    "href": "Fastbook/README_tr.html",
    "title": "Сергей Мирошниченко",
    "section": "",
    "text": "English / Spanish / Korean / Chinese / Bengali / Indonesian / Italian / Portuguese / Vietnamese / Japanese / Turkish # Fastai kitabı\nBu Jupyter Notebook’ları, Derin Öğrenme, fastai ve PyTorch hakkında eğitim içermektedir. Fastai, Derin Öğrenme tekniklerine özel katmanlı API içermektedir; daha fazla bilgi için bkz. Bu Repodaki her şeyin telif hakkı, 2020’den itiabaren, Jeremy Howard ve Sylvain Gugger’a aittir. Seçili okunabilen bölümlere buradan ulaşabilirsiniz.\nRepoda bulunan Jupyter Notebook’ları, bu çevrimiçi kurs için hazırlanmıştır. Ayrıca, şuan satılık olan, bu kitabın temellerini oluşturmaktadır. Kitap, aynı GPL kısıtlamalarına tabii değildir.\nJupyter Notebook’ları ve python .py dosyalarının tamamı GPL v3 ile lisanslanmıştı; daha fazla detay için LICENSE’a bakınız. Geri kalanı (notebooklarda bulunan markdown hücreleri dahil olmak üzere), herhangi bir yeniden dağıtım veya format değişmi için, Repo’yu çatallamak ve özel kullanımınız hariç, lisanslanmamıştır. Ticari veya yayın amaçlı kullanıma izin verilmez. Deri Öğrenme konularını öğrenebilmeniz amacıyla bu içeriği ücretsiz sunuyoruz. Dolayısıyla, lütfen bahsi geçen kısıtlamalara ve telif hakkına saygı gösteriniz.\nBu içeriği başka yerlerde yayınlanan birisini görürseniz lütfen yaptıklarının izinli olmadığını ve onlar hakkında soruşturma başlatılabileceğini söyleyiniz. Dahası, topluluğa da zarar vermiş olacaklardır çünkü eğer insanlar telif hakkımızı göz ardı ederse bu şekilde içerik yayınlamamız pek olası değildir."
  },
  {
    "objectID": "Fastbook/README_tr.html#colab",
    "href": "Fastbook/README_tr.html#colab",
    "title": "Сергей Мирошниченко",
    "section": "Colab",
    "text": "Colab\nRepo’yu klonlayıp bilgisayarınızda çalıştırmak yerine, Google Colab kullanarak Notebook’ları çalışabilirsiniz. Bu, yeni başlayanlar için önerilen yaklaşımdır; doğrudan web tarayıcınızda çalışabileceğiniz için kendi makinenizde bir Python geliştirme ortamı kurmanıza da gerek yoktur.\nKitabın herhangi bir bölümüne Google Colab üzerine bu linklerden ulaşabilirsiniz: Jupyter’e Giriş | Bölüm 1, Giriş | Bölüm 2, Production | Bölüm 3, Etikler | Bölüm 4, MNIST temelleri | Bölüm 5, Pet Breeds | Bölüm 6, Multi-Category | Bölüm 7, Sizing and TTA | Bölüm 8, Collab | Bölüm 9, Tabular | Bölüm 10, NLP | Bölüm 11, Mid-Level API | Bölüm 12, NLP Deep-Dive | Bölüm 13, Convolutions | Bölüm 14, Resnet | Bölüm 15, Arch Details | Bölüm 16, Optimizers and Callbacks | Bölüm 17, Foundations | Bölüm 18, GradCAM | Bölüm 19, Learner | Bölüm 20, conclusion"
  },
  {
    "objectID": "Fastbook/README_tr.html#contributions-katkılar",
    "href": "Fastbook/README_tr.html#contributions-katkılar",
    "title": "Сергей Мирошниченко",
    "section": "Contributions / Katkılar",
    "text": "Contributions / Katkılar\nBu Repo’ya herhangi bir Pull isteğinde bulunursanız bu çalışmanın telif hakkını Jeremy Howard ve Sylvain Gugger’a devretmiş olursunuz. (Ayrıca, yazım veya metin üzerinde küçük düzenlemeler yapıyorsanız, lütfen dosyanın adını ve neyi düzelttiğinizin çok kısa bir açıklamasını belirtin. İncelemeyi yapanların hangi düzeltmelerin daha önce yapıldığını bilmesi zordur. Teşekkür ederiz.)"
  },
  {
    "objectID": "Fastbook/README_tr.html#atıf",
    "href": "Fastbook/README_tr.html#atıf",
    "title": "Сергей Мирошниченко",
    "section": "Atıf",
    "text": "Atıf\nKitaba atıfta bulunmak istiyorsanız, şunu kullanabilirsiniz:\n@book{howard2020deep,\ntitle={Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD},\nauthor={Howard, J. and Gugger, S.},\nisbn={9781492045526},\nurl={https://books.google.no/books?id=xd6LxgEACAAJ},\nyear={2020},\npublisher={O'Reilly Media, Incorporated}\n}"
  },
  {
    "objectID": "Fastbook/README_vn.html",
    "href": "Fastbook/README_vn.html",
    "title": "Cuốn sách fastai",
    "section": "",
    "text": "English / Spanish / Korean / Chinese / Bengali / Indonesian / Italian / Portuguese / Vietnamese / Japanese"
  },
  {
    "objectID": "Fastbook/README_vn.html#trích-dẫn",
    "href": "Fastbook/README_vn.html#trích-dẫn",
    "title": "Cuốn sách fastai",
    "section": "Trích dẫn",
    "text": "Trích dẫn\nNếu bạn muốn trích dẫn cuốn sách, bạn có thể sử dụng như sau:\n@book{howard2020deep,\ntitle={Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD},\nauthor={Howard, J. and Gugger, S.},\nisbn={9781492045526},\nurl={https://books.google.no/books?id=xd6LxgEACAAJ},\nyear={2020},\npublisher={O'Reilly Media, Incorporated}\n}"
  },
  {
    "objectID": "Fastbook/09_tabular.html",
    "href": "Fastbook/09_tabular.html",
    "title": "Tabular Modeling Deep Dive",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook kaggle waterfallcharts treeinterpreter dtreeviz==1.4.1\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG\n\npd.options.display.max_rows = 20\npd.options.display.max_columns = 8\n[[chapter_tabular]]\nTabular modeling takes data in the form of a table (like a spreadsheet or CSV). The objective is to predict the value in one column based on the values in the other columns. In this chapter we will not only look at deep learning but also more general machine learning techniques like random forests, as they can give better results depending on your problem.\nWe will look at how we should preprocess and clean the data as well as how to interpret the result of our models after training, but first, we will see how we can feed columns that contain categories into a model that expects numbers by using embeddings."
  },
  {
    "objectID": "Fastbook/09_tabular.html#categorical-embeddings",
    "href": "Fastbook/09_tabular.html#categorical-embeddings",
    "title": "Tabular Modeling Deep Dive",
    "section": "Categorical Embeddings",
    "text": "Categorical Embeddings\nIn tabular data some columns may contain numerical data, like “age,” while others contain string values, like “sex.” The numerical data can be directly fed to the model (with some optional preprocessing), but the other columns need to be converted to numbers. Since the values in those correspond to different categories, we often call this type of variables categorical variables. The first type are called continuous variables.\n\njargon: Continuous and Categorical Variables: Continuous variables are numerical data, such as “age,” that can be directly fed to the model, since you can add and multiply them directly. Categorical variables contain a number of discrete levels, such as “movie ID,” for which addition and multiplication don’t have meaning (even if they’re stored as numbers).\n\nAt the end of 2015, the Rossmann sales competition ran on Kaggle. Competitors were given a wide range of information about various stores in Germany, and were tasked with trying to predict sales on a number of days. The goal was to help the company to manage stock properly and be able to satisfy demand without holding unnecessary inventory. The official training set provided a lot of information about the stores. It was also permitted for competitors to use additional data, as long as that data was made public and available to all participants.\nOne of the gold medalists used deep learning, in one of the earliest known examples of a state-of-the-art deep learning tabular model. Their method involved far less feature engineering, based on domain knowledge, than those of the other gold medalists. The paper, “Entity Embeddings of Categorical Variables” describes their approach. In an online-only chapter on the book’s website we show how to replicate it from scratch and attain the same accuracy shown in the paper. In the abstract of the paper the authors (Cheng Guo and Felix Berkhahn) say:\n\n: Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables… [It] is especially useful for datasets with lots of high cardinality features, where other methods tend to overfit… As entity embedding defines a distance measure for categorical variables it can be used for visualizing categorical data and for data clustering.\n\nWe have already noticed all of these points when we built our collaborative filtering model. We can clearly see that these insights go far beyond just collaborative filtering, however.\nThe paper also points out that (as we discussed in the last chapter) an embedding layer is exactly equivalent to placing an ordinary linear layer after every one-hot-encoded input layer. The authors used the diagram in &lt;&gt; to show this equivalence. Note that “dense layer” is a term with the same meaning as “linear layer,” and the one-hot encoding layers represent inputs.\n\nThe insight is important because we already know how to train linear layers, so this shows that from the point of view of the architecture and our training algorithm the embedding layer is just another layer. We also saw this in practice in the last chapter, when we built a collaborative filtering neural network that looks exactly like this diagram.\nWhere we analyzed the embedding weights for movie reviews, the authors of the entity embeddings paper analyzed the embedding weights for their sales prediction model. What they found was quite amazing, and illustrates their second key insight. This is that the embedding transforms the categorical variables into inputs that are both continuous and meaningful.\nThe images in &lt;&gt; illustrate these ideas. They are based on the approaches used in the paper, along with some analysis we have added.\n\nOn the left is a plot of the embedding matrix for the possible values of the State category. For a categorical variable we call the possible values of the variable its “levels” (or “categories” or “classes”), so here one level is “Berlin,” another is “Hamburg,” etc. On the right is a map of Germany. The actual physical locations of the German states were not part of the provided data, yet the model itself learned where they must be, based only on the behavior of store sales!\nDo you remember how we talked about distance between embeddings? The authors of the paper plotted the distance between store embeddings against the actual geographic distance between the stores (see &lt;&gt;). They found that they matched very closely!\n\nWe’ve even tried plotting the embeddings for days of the week and months of the year, and found that days and months that are near each other on the calendar ended up close as embeddings too, as shown in &lt;&gt;.\n\nWhat stands out in these two examples is that we provide the model fundamentally categorical data about discrete entities (e.g., German states or days of the week), and then the model learns an embedding for these entities that defines a continuous notion of distance between them. Because the embedding distance was learned based on real patterns in the data, that distance tends to match up with our intuitions.\nIn addition, it is valuable in its own right that embeddings are continuous, because models are better at understanding continuous variables. This is unsurprising considering models are built of many continuous parameter weights and continuous activation values, which are updated via gradient descent (a learning algorithm for finding the minimums of continuous functions).\nAnother benefit is that we can combine our continuous embedding values with truly continuous input data in a straightforward manner: we just concatenate the variables, and feed the concatenation into our first dense layer. In other words, the raw categorical data is transformed by an embedding layer before it interacts with the raw continuous input data. This is how fastai and Guo and Berkhahn handle tabular models containing continuous and categorical variables.\nAn example using this concatenation approach is how Google does its recommendations on Google Play, as explained in the paper “Wide & Deep Learning for Recommender Systems”. &lt;&gt; illustrates.\n\nInterestingly, the Google team actually combined both approaches we saw in the previous chapter: the dot product (which they call cross product) and neural network approaches.\nLet’s pause for a moment. So far, the solution to all of our modeling problems has been: train a deep learning model. And indeed, that is a pretty good rule of thumb for complex unstructured data like images, sounds, natural language text, and so forth. Deep learning also works very well for collaborative filtering. But it is not always the best starting point for analyzing tabular data."
  },
  {
    "objectID": "Fastbook/09_tabular.html#beyond-deep-learning",
    "href": "Fastbook/09_tabular.html#beyond-deep-learning",
    "title": "Tabular Modeling Deep Dive",
    "section": "Beyond Deep Learning",
    "text": "Beyond Deep Learning\nMost machine learning courses will throw dozens of different algorithms at you, with a brief technical description of the math behind them and maybe a toy example. You’re left confused by the enormous range of techniques shown and have little practical understanding of how to apply them.\nThe good news is that modern machine learning can be distilled down to a couple of key techniques that are widely applicable. Recent studies have shown that the vast majority of datasets can be best modeled with just two methods:\n\nEnsembles of decision trees (i.e., random forests and gradient boosting machines), mainly for structured data (such as you might find in a database table at most companies)\nMultilayered neural networks learned with SGD (i.e., shallow and/or deep learning), mainly for unstructured data (such as audio, images, and natural language)\n\nAlthough deep learning is nearly always clearly superior for unstructured data, these two approaches tend to give quite similar results for many kinds of structured data. But ensembles of decision trees tend to train faster, are often easier to interpret, do not require special GPU hardware for inference at scale, and often require less hyperparameter tuning. They have also been popular for quite a lot longer than deep learning, so there is a more mature ecosystem of tooling and documentation around them.\nMost importantly, the critical step of interpreting a model of tabular data is significantly easier for decision tree ensembles. There are tools and methods for answering the pertinent questions, like: Which columns in the dataset were the most important for your predictions? How are they related to the dependent variable? How do they interact with each other? And which particular features were most important for some particular observation?\nTherefore, ensembles of decision trees are our first approach for analyzing a new tabular dataset.\nThe exception to this guideline is when the dataset meets one of these conditions:\n\nThere are some high-cardinality categorical variables that are very important (“cardinality” refers to the number of discrete levels representing categories, so a high-cardinality categorical variable is something like a zip code, which can take on thousands of possible levels).\nThere are some columns that contain data that would be best understood with a neural network, such as plain text data.\n\nIn practice, when we deal with datasets that meet these exceptional conditions, we always try both decision tree ensembles and deep learning to see which works best. It is likely that deep learning will be a useful approach in our example of collaborative filtering, as we have at least two high-cardinality categorical variables: the users and the movies. But in practice things tend to be less cut-and-dried, and there will often be a mixture of high- and low-cardinality categorical variables and continuous variables.\nEither way, it’s clear that we are going to need to add decision tree ensembles to our modeling toolbox!\nUp to now we’ve used PyTorch and fastai for pretty much all of our heavy lifting. But these libraries are mainly designed for algorithms that do lots of matrix multiplication and derivatives (that is, stuff like deep learning!). Decision trees don’t depend on these operations at all, so PyTorch isn’t much use.\nInstead, we will be largely relying on a library called scikit-learn (also known as sklearn). Scikit-learn is a popular library for creating machine learning models, using approaches that are not covered by deep learning. In addition, we’ll need to do some tabular data processing and querying, so we’ll want to use the Pandas library. Finally, we’ll also need NumPy, since that’s the main numeric programming library that both sklearn and Pandas rely on.\nWe don’t have time to do a deep dive into all these libraries in this book, so we’ll just be touching on some of the main parts of each. For a far more in depth discussion, we strongly suggest Wes McKinney’s Python for Data Analysis (O’Reilly). Wes is the creator of Pandas, so you can be sure that the information is accurate!\nFirst, let’s gather the data we will use."
  },
  {
    "objectID": "Fastbook/09_tabular.html#the-dataset",
    "href": "Fastbook/09_tabular.html#the-dataset",
    "title": "Tabular Modeling Deep Dive",
    "section": "The Dataset",
    "text": "The Dataset\nThe dataset we use in this chapter is from the Blue Book for Bulldozers Kaggle competition, which has the following description: “The goal of the contest is to predict the sale price of a particular piece of heavy equipment at auction based on its usage, equipment type, and configuration. The data is sourced from auction result postings and includes information on usage and equipment configurations.”\nThis is a very common type of dataset and prediction problem, similar to what you may see in your project or workplace. The dataset is available for download on Kaggle, a website that hosts data science competitions.\n\nKaggle Competitions\nKaggle is an awesome resource for aspiring data scientists or anyone looking to improve their machine learning skills. There is nothing like getting hands-on practice and receiving real-time feedback to help you improve your skills.\nKaggle provides:\n\nInteresting datasets\nFeedback on how you’re doing\nA leaderboard to see what’s good, what’s possible, and what’s state-of-the-art\nBlog posts by winning contestants sharing useful tips and techniques\n\nUntil now all our datasets have been available to download through fastai’s integrated dataset system. However, the dataset we will be using in this chapter is only available from Kaggle. Therefore, you will need to register on the site, then go to the page for the competition. On that page click “Rules,” then “I Understand and Accept.” (Although the competition has finished, and you will not be entering it, you still have to agree to the rules to be allowed to download the data.)\nThe easiest way to download Kaggle datasets is to use the Kaggle API. You can install this using pip by running this in a notebook cell:\n!pip install kaggle\nYou need an API key to use the Kaggle API; to get one, click on your profile picture on the Kaggle website, and choose My Account, then click Create New API Token. This will save a file called kaggle.json to your PC. You need to copy this key on your GPU server. To do so, open the file you downloaded, copy the contents, and paste them in the following cell in the notebook associated with this chapter (e.g., creds = '{\"username\":\"xxx\",\"key\":\"xxx\"}'):\n\ncreds = ''\n\nThen execute this cell (this only needs to be run once):\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\nNow you can download datasets from Kaggle! Pick a path to download the dataset to:\n\ncomp = 'bluebook-for-bulldozers'\npath = URLs.path(comp)\npath\n\nPath('/home/jhoward/.fastai/archive/bluebook-for-bulldozers')\n\n\n\n#hide\nPath.BASE_PATH = path\n\nAnd use the Kaggle API to download the dataset to that path, and extract it:\n\nfrom kaggle import api\n\nif not path.exists():\n    path.mkdir(parents=true)\n    api.competition_download_cli(comp, path=path)\n    shutil.unpack_archive(str(path/f'{comp}.zip'), str(path))\n\npath.ls(file_type='text')\n\n(#7) [Path('ValidSolution.csv'),Path('Machine_Appendix.csv'),Path('TrainAndValid.csv'),Path('median_benchmark.csv'),Path('random_forest_benchmark_test.csv'),Path('Test.csv'),Path('Valid.csv')]\n\n\nNow that we have downloaded our dataset, let’s take a look at it!\n\n\nLook at the Data\nKaggle provides information about some of the fields of our dataset. The Data explains that the key fields in train.csv are:\n\nSalesID:: The unique identifier of the sale.\nMachineID:: The unique identifier of a machine. A machine can be sold multiple times.\nsaleprice:: What the machine sold for at auction (only provided in train.csv).\nsaledate:: The date of the sale.\n\nIn any sort of data science work, it’s important to look at your data directly to make sure you understand the format, how it’s stored, what types of values it holds, etc. Even if you’ve read a description of the data, the actual data may not be what you expect. We’ll start by reading the training set into a Pandas DataFrame. Generally it’s a good idea to specify low_memory=False unless Pandas actually runs out of memory and returns an error. The low_memory parameter, which is True by default, tells Pandas to only look at a few rows of data at a time to figure out what type of data is in each column. This means that Pandas can actually end up using different data type for different rows, which generally leads to data processing errors or model training problems later.\nLet’s load our data and have a look at the columns:\n\ndf = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\n\n\ndf.columns\n\nIndex(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',\n       'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n       'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',\n       'fiModelSeries', 'fiModelDescriptor', 'ProductSize',\n       'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',\n       'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',\n       'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',\n       'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',\n       'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',\n       'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',\n       'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',\n       'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',\n       'Travel_Controls', 'Differential_Type', 'Steering_Controls'],\n      dtype='object')\n\n\nThat’s a lot of columns for us to look at! Try looking through the dataset to get a sense of what kind of information is in each one. We’ll shortly see how to “zero in” on the most interesting bits.\nAt this point, a good next step is to handle ordinal columns. This refers to columns containing strings or similar, but where those strings have a natural ordering. For instance, here are the levels of ProductSize:\n\ndf['ProductSize'].unique()\n\narray([nan, 'Medium', 'Small', 'Large / Medium', 'Mini', 'Large', 'Compact'], dtype=object)\n\n\nWe can tell Pandas about a suitable ordering of these levels like so:\n\nsizes = 'Large','Large / Medium','Medium','Small','Mini','Compact'\n\n\ndf['ProductSize'] = df['ProductSize'].astype('category')\ndf['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)\n\nThe most important data column is the dependent variable—that is, the one we want to predict. Recall that a model’s metric is a function that reflects how good the predictions are. It’s important to note what metric is being used for a project. Generally, selecting the metric is an important part of the project setup. In many cases, choosing a good metric will require more than just selecting a variable that already exists. It is more like a design process. You should think carefully about which metric, or set of metrics, actually measures the notion of model quality that matters to you. If no variable represents that metric, you should see if you can build the metric from the variables that are available.\nHowever, in this case Kaggle tells us what metric to use: root mean squared log error (RMSLE) between the actual and predicted auction prices. We need do only a small amount of processing to use this: we take the log of the prices, so that rmse of that value will give us what we ultimately need:\n\ndep_var = 'SalePrice'\n\n\ndf[dep_var] = np.log(df[dep_var])\n\nWe are now ready to explore our first machine learning algorithm for tabular data: decision trees."
  },
  {
    "objectID": "Fastbook/09_tabular.html#decision-trees",
    "href": "Fastbook/09_tabular.html#decision-trees",
    "title": "Tabular Modeling Deep Dive",
    "section": "Decision Trees",
    "text": "Decision Trees\nDecision tree ensembles, as the name suggests, rely on decision trees. So let’s start there! A decision tree asks a series of binary (that is, yes or no) questions about the data. After each question the data at that part of the tree is split between a “yes” and a “no” branch, as shown in &lt;&gt;. After one or more questions, either a prediction can be made on the basis of all previous answers or another question is required.\n\nThis sequence of questions is now a procedure for taking any data item, whether an item from the training set or a new one, and assigning that item to a group. Namely, after asking and answering the questions, we can say the item belongs to the same group as all the other training data items that yielded the same set of answers to the questions. But what good is this? The goal of our model is to predict values for items, not to assign them into groups from the training dataset. The value is that we can now assign a prediction value for each of these groups—for regression, we take the target mean of the items in the group.\nLet’s consider how we find the right questions to ask. Of course, we wouldn’t want to have to create all these questions ourselves—that’s what computers are for! The basic steps to train a decision tree can be written down very easily:\n\nLoop through each column of the dataset in turn.\nFor each column, loop through each possible level of that column in turn.\nTry splitting the data into two groups, based on whether they are greater than or less than that value (or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable).\nFind the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. That is, treat this as a very simple “model” where our predictions are simply the average sale price of the item’s group.\nAfter looping through all of the columns and all the possible levels for each, pick the split point that gave the best predictions using that simple model.\nWe now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, and find the best split for each by going back to step 1 for each group.\nContinue this process recursively, until you have reached some stopping criterion for each group—for instance, stop splitting a group further when it has only 20 items in it.\n\nAlthough this is an easy enough algorithm to implement yourself (and it is a good exercise to do so), we can save some time by using the implementation built into sklearn.\nFirst, however, we need to do a little data preparation.\n\nA: Here’s a productive question to ponder. If you consider that the procedure for defining a decision tree essentially chooses one sequence of splitting questions about variables, you might ask yourself, how do we know this procedure chooses the correct sequence? The rule is to choose the splitting question that produces the best split (i.e., that most accurately separates the items into two distinct categories), and then to apply the same rule to the groups that split produces, and so on. This is known in computer science as a “greedy” approach. Can you imagine a scenario in which asking a “less powerful” splitting question would enable a better split down the road (or should I say down the trunk!) and lead to a better result overall?\n\n\nHandling Dates\nThe first piece of data preparation we need to do is to enrich our representation of dates. The fundamental basis of the decision tree that we just described is bisection— dividing a group into two. We look at the ordinal variables and divide up the dataset based on whether the variable’s value is greater (or lower) than a threshold, and we look at the categorical variables and divide up the dataset based on whether the variable’s level is a particular level. So this algorithm has a way of dividing up the dataset based on both ordinal and categorical data.\nBut how does this apply to a common data type, the date? You might want to treat a date as an ordinal value, because it is meaningful to say that one date is greater than another. However, dates are a bit different from most ordinal values in that some dates are qualitatively different from others in a way that that is often relevant to the systems we are modeling.\nIn order to help our algorithm handle dates intelligently, we’d like our model to know more than whether a date is more recent or less recent than another. We might want our model to make decisions based on that date’s day of the week, on whether a day is a holiday, on what month it is in, and so forth. To do this, we replace every date column with a set of date metadata columns, such as holiday, day of week, and month. These columns provide categorical data that we suspect will be useful.\nfastai comes with a function that will do this for us—we just have to pass a column name that contains dates:\n\ndf = add_datepart(df, 'saledate')\n\nLet’s do the same for the test set while we’re there:\n\ndf_test = pd.read_csv(path/'Test.csv', low_memory=False)\ndf_test = add_datepart(df_test, 'saledate')\n\nWe can see that there are now lots of new columns in our DataFrame:\n\n' '.join(o for o in df.columns if o.startswith('sale'))\n\n'saleWeek saleYear saleMonth saleDay saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed'\n\n\nThis is a good first step, but we will need to do a bit more cleaning. For this, we will use fastai objects called TabularPandas and TabularProc.\n\n\nUsing TabularPandas and TabularProc\nA second piece of preparatory processing is to be sure we can handle strings and missing data. Out of the box, sklearn cannot do either. Instead we will use fastai’s class TabularPandas, which wraps a Pandas DataFrame and provides a few conveniences. To populate a TabularPandas, we will use two TabularProcs, Categorify and FillMissing. A TabularProc is like a regular Transform, except that:\n\nIt returns the exact same object that’s passed to it, after modifying the object in place.\nIt runs the transform once, when data is first passed in, rather than lazily as the data is accessed.\n\nCategorify is a TabularProc that replaces a column with a numeric categorical column. FillMissing is a TabularProc that replaces missing values with the median of the column, and creates a new Boolean column that is set to True for any row where the value was missing. These two transforms are needed for nearly every tabular dataset you will use, so this is a good starting point for your data processing:\n\nprocs = [Categorify, FillMissing]\n\nTabularPandas will also handle splitting the dataset into training and validation sets for us. However we need to be very careful about our validation set. We want to design it so that it is like the test set Kaggle will use to judge the contest.\nRecall the distinction between a validation set and a test set, as discussed in &lt;&gt;. A validation set is data we hold back from training in order to ensure that the training process does not overfit on the training data. A test set is data that is held back even more deeply, from us ourselves, in order to ensure that we don’t overfit on the validation data, as we explore various model architectures and hyperparameters.\nWe don’t get to see the test set. But we do want to define our validation data so that it has the same sort of relationship to the training data as the test set will have.\nIn some cases, just randomly choosing a subset of your data points will do that. This is not one of those cases, because it is a time series.\nIf you look at the date range represented in the test set, you will discover that it covers a six-month period from May 2012, which is later in time than any date in the training set. This is a good design, because the competition sponsor will want to ensure that a model is able to predict the future. But it means that if we are going to have a useful validation set, we also want the validation set to be later in time than the training set. The Kaggle training data ends in April 2012, so we will define a narrower training dataset which consists only of the Kaggle training data from before November 2011, and we’ll define a validation set consisting of data from after November 2011.\nTo do this we use np.where, a useful function that returns (as the first element of a tuple) the indices of all True values:\n\ncond = (df.saleYear&lt;2011) | (df.saleMonth&lt;10)\ntrain_idx = np.where( cond)[0]\nvalid_idx = np.where(~cond)[0]\n\nsplits = (list(train_idx),list(valid_idx))\n\nTabularPandas needs to be told which columns are continuous and which are categorical. We can handle that automatically using the helper function cont_cat_split:\n\ncont,cat = cont_cat_split(df, 1, dep_var=dep_var)\n\n\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)\n\nA TabularPandas behaves a lot like a fastai Datasets object, including providing train and valid attributes:\n\nlen(to.train),len(to.valid)\n\n(404710, 7988)\n\n\nWe can see that the data is still displayed as strings for categories (we only show a few columns here because the full table is too big to fit on a page):\n\n#hide_output\nto.show(3)\n\n\n\n\n\nsaleWeek\nUsageBand\nfiModelDesc\nfiBaseModel\nfiSecondaryDesc\nfiModelSeries\nfiModelDescriptor\nProductSize\nfiProductClassDesc\nstate\nProductGroup\nProductGroupDesc\nDrive_System\nEnclosure\nForks\nPad_Type\nRide_Control\nStick\nTransmission\nTurbocharged\nBlade_Extension\nBlade_Width\nEnclosure_Type\nEngine_Horsepower\nHydraulics\nPushblock\nRipper\nScarifier\nTip_Control\nTire_Size\nCoupler\nCoupler_System\nGrouser_Tracks\nHydraulics_Flow\nTrack_Type\nUndercarriage_Pad_Width\nStick_Length\nThumb\nPattern_Changer\nGrouser_Type\nBackhoe_Mounting\nBlade_Type\nTravel_Controls\nDifferential_Type\nSteering_Controls\nsaleIs_month_end\nsaleIs_month_start\nsaleIs_quarter_end\nsaleIs_quarter_start\nsaleIs_year_end\nsaleIs_year_start\nsaleElapsed\nauctioneerID_na\nMachineHoursCurrentMeter_na\nSalesID\nMachineID\nModelID\ndatasource\nauctioneerID\nYearMade\nMachineHoursCurrentMeter\nsaleYear\nsaleMonth\nsaleDay\nsaleDayofweek\nsaleDayofyear\nSalePrice\n\n\n\n\n0\n46\nLow\n521D\n521\nD\n#na#\n#na#\n#na#\nWheel Loader - 110.0 to 120.0 Horsepower\nAlabama\nWL\nWheel Loader\n#na#\nEROPS w AC\nNone or Unspecified\n#na#\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n2 Valve\n#na#\n#na#\n#na#\n#na#\nNone or Unspecified\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\nStandard\nConventional\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n1163635200\nFalse\nFalse\n1139246\n999089\n3157\n121\n3.0\n2004\n68.0\n2006\n11\n16\n3\n320\n11.097410\n\n\n1\n13\nLow\n950FII\n950\nF\nII\n#na#\nMedium\nWheel Loader - 150.0 to 175.0 Horsepower\nNorth Carolina\nWL\nWheel Loader\n#na#\nEROPS w AC\nNone or Unspecified\n#na#\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n2 Valve\n#na#\n#na#\n#na#\n#na#\n23.5\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\nStandard\nConventional\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n1080259200\nFalse\nFalse\n1139248\n117657\n77\n121\n3.0\n1996\n4640.0\n2004\n3\n26\n4\n86\n10.950807\n\n\n2\n9\nHigh\n226\n226\n#na#\n#na#\n#na#\n#na#\nSkid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity\nNew York\nSSL\nSkid Steer Loaders\n#na#\nOROPS\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\nAuxiliary\n#na#\n#na#\n#na#\n#na#\n#na#\nNone or Unspecified\nNone or Unspecified\nNone or Unspecified\nStandard\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n1077753600\nFalse\nFalse\n1139249\n434808\n7009\n121\n3.0\n2001\n2838.0\n2004\n2\n26\n3\n57\n9.210340\n\n\n\n\n\n\n#hide_input\nto1 = TabularPandas(df, procs, ['state', 'ProductGroup', 'Drive_System', 'Enclosure'], [], y_names=dep_var, splits=splits)\nto1.show(3)\n\n\n\n\n\nstate\nProductGroup\nDrive_System\nEnclosure\nSalePrice\n\n\n\n\n0\nAlabama\nWL\n#na#\nEROPS w AC\n11.097410\n\n\n1\nNorth Carolina\nWL\n#na#\nEROPS w AC\n10.950807\n\n\n2\nNew York\nSSL\n#na#\nOROPS\n9.210340\n\n\n\n\n\nHowever, the underlying items are all numeric:\n\n#hide_output\nto.items.head(3)\n\n\n\n\n\n\n\n\nSalesID\nSalePrice\nMachineID\nsaleWeek\n...\nsaleIs_year_start\nsaleElapsed\nauctioneerID_na\nMachineHoursCurrentMeter_na\n\n\n\n\n0\n1139246\n11.097410\n999089\n46\n...\n1\n2647\n1\n1\n\n\n1\n1139248\n10.950807\n117657\n13\n...\n1\n2148\n1\n1\n\n\n2\n1139249\n9.210340\n434808\n9\n...\n1\n2131\n1\n1\n\n\n\n\n3 rows × 67 columns\n\n\n\n\n#hide_input\nto1.items[['state', 'ProductGroup', 'Drive_System', 'Enclosure']].head(3)\n\n\n\n\n\n\n\n\nstate\nProductGroup\nDrive_System\nEnclosure\n\n\n\n\n0\n1\n6\n0\n3\n\n\n1\n33\n6\n0\n3\n\n\n2\n32\n3\n0\n6\n\n\n\n\n\n\n\nThe conversion of categorical columns to numbers is done by simply replacing each unique level with a number. The numbers associated with the levels are chosen consecutively as they are seen in a column, so there’s no particular meaning to the numbers in categorical columns after conversion. The exception is if you first convert a column to a Pandas ordered category (as we did for ProductSize earlier), in which case the ordering you chose is used. We can see the mapping by looking at the classes attribute:\n\nto.classes['ProductSize']\n\n['#na#', 'Large', 'Large / Medium', 'Medium', 'Small', 'Mini', 'Compact']\n\n\nSince it takes a minute or so to process the data to get to this point, we should save it—that way in the future we can continue our work from here without rerunning the previous steps. fastai provides a save method that uses Python’s pickle system to save nearly any Python object:\n\nsave_pickle(path/'to.pkl',to)\n\nTo read this back later, you would type:\nto = (path/'to.pkl').load()\nNow that all this preprocessing is done, we are ready to create a decision tree.\n\n\nCreating the Decision Tree\nTo begin, we define our independent and dependent variables:\n\n#hide\nto = load_pickle(path/'to.pkl')\n\n\nxs,y = to.train.xs,to.train.y\nvalid_xs,valid_y = to.valid.xs,to.valid.y\n\nNow that our data is all numeric, and there are no missing values, we can create a decision tree:\n\nm = DecisionTreeRegressor(max_leaf_nodes=4)\nm.fit(xs, y);\n\nTo keep it simple, we’ve told sklearn to just create four leaf nodes. To see what it’s learned, we can display the tree:\n\ndraw_tree(m, xs, size=10, leaves_parallel=True, precision=2)\n\n\n\n\n\n\n\n\nUnderstanding this picture is one of the best ways to understand decision trees, so we will start at the top and explain each part step by step.\nThe top node represents the initial model before any splits have been done, when all the data is in one group. This is the simplest possible model. It is the result of asking zero questions and will always predict the value to be the average value of the whole dataset. In this case, we can see it predicts a value of 10.10 for the logarithm of the sales price. It gives a mean squared error of 0.48. The square root of this is 0.69. (Remember that unless you see m_rmse, or a root mean squared error, then the value you are looking at is before taking the square root, so it is just the average of the square of the differences.) We can also see that there are 404,710 auction records in this group—that is the total size of our training set. The final piece of information shown here is the decision criterion for the best split that was found, which is to split based on the coupler_system column.\nMoving down and to the left, this node shows us that there were 360,847 auction records for equipment where coupler_system was less than 0.5. The average value of our dependent variable in this group is 10.21. Moving down and to the right from the initial model takes us to the records where coupler_system was greater than 0.5.\nThe bottom row contains our leaf nodes: the nodes with no answers coming out of them, because there are no more questions to be answered. At the far right of this row is the node containing records where coupler_system was greater than 0.5. The average value here is 9.21, so we can see the decision tree algorithm did find a single binary decision that separated high-value from low-value auction results. Asking only about coupler_system predicts an average value of 9.21 versus 10.1.\nReturning back to the top node after the first decision point, we can see that a second binary decision split has been made, based on asking whether YearMade is less than or equal to 1991.5. For the group where this is true (remember, this is now following two binary decisions, based on coupler_system and YearMade) the average value is 9.97, and there are 155,724 auction records in this group. For the group of auctions where this decision is false, the average value is 10.4, and there are 205,123 records. So again, we can see that the decision tree algorithm has successfully split our more expensive auction records into two more groups which differ in value significantly.\nWe can show the same information using Terence Parr’s powerful dtreeviz library:\n\nsamp_idx = np.random.permutation(len(y))[:500]\ndtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n        fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n        orientation='LR')\n\n\n\n\n\n\n\n\nThis shows a chart of the distribution of the data for each split point. We can clearly see that there’s a problem with our YearMade data: there are bulldozers made in the year 1000, apparently! Presumably this is actually just a missing value code (a value that doesn’t otherwise appear in the data and that is used as a placeholder in cases where a value is missing). For modeling purposes, 1000 is fine, but as you can see this outlier makes visualization of the values we are interested in more difficult. So, let’s replace it with 1950:\n\nxs.loc[xs['YearMade']&lt;1900, 'YearMade'] = 1950\nvalid_xs.loc[valid_xs['YearMade']&lt;1900, 'YearMade'] = 1950\n\nThat change makes the split much clearer in the tree visualization, even although it doesn’t actually change the result of the model in any significant way. This is a great example of how resilient decision trees are to data issues!\n\nm = DecisionTreeRegressor(max_leaf_nodes=4).fit(xs, y)\n\ndtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n        fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n        orientation='LR')\n\n\n\n\n\n\n\n\nLet’s now have the decision tree algorithm build a bigger tree. Here, we are not passing in any stopping criteria such as max_leaf_nodes:\n\nm = DecisionTreeRegressor()\nm.fit(xs, y);\n\nWe’ll create a little function to check the root mean squared error of our model (m_rmse), since that’s how the competition was judged:\n\ndef r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6)\ndef m_rmse(m, xs, y): return r_mse(m.predict(xs), y)\n\n\nm_rmse(m, xs, y)\n\n0.0\n\n\nSo, our model is perfect, right? Not so fast… remember we really need to check the validation set, to ensure we’re not overfitting:\n\nm_rmse(m, valid_xs, valid_y)\n\n0.331466\n\n\nOops—it looks like we might be overfitting pretty badly. Here’s why:\n\nm.get_n_leaves(), len(xs)\n\n(324544, 404710)\n\n\nWe’ve got nearly as many leaf nodes as data points! That seems a little over-enthusiastic. Indeed, sklearn’s default settings allow it to continue splitting nodes until there is only one item in each leaf node. Let’s change the stopping rule to tell sklearn to ensure every leaf node contains at least 25 auction records:\n\nm = DecisionTreeRegressor(min_samples_leaf=25)\nm.fit(to.train.xs, to.train.y)\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n\n(0.248562, 0.323396)\n\n\nThat looks much better. Let’s check the number of leaves again:\n\nm.get_n_leaves()\n\n12397\n\n\nMuch more reasonable!\n\nA: Here’s my intuition for an overfitting decision tree with more leaf nodes than data items. Consider the game Twenty Questions. In that game, the chooser secretly imagines an object (like, “our television set”), and the guesser gets to pose 20 yes or no questions to try to guess what the object is (like “Is it bigger than a breadbox?”). The guesser is not trying to predict a numerical value, but just to identify a particular object out of the set of all imaginable objects. When your decision tree has more leaves than there are possible objects in your domain, then it is essentially a well-trained guesser. It has learned the sequence of questions needed to identify a particular data item in the training set, and it is “predicting” only by describing that item’s value. This is a way of memorizing the training set—i.e., of overfitting.\n\nBuilding a decision tree is a good way to create a model of our data. It is very flexible, since it can clearly handle nonlinear relationships and interactions between variables. But we can see there is a fundamental compromise between how well it generalizes (which we can achieve by creating small trees) and how accurate it is on the training set (which we can achieve by using large trees).\nSo how do we get the best of both worlds? We’ll show you right after we handle an important missing detail: how to handle categorical variables.\n\n\nCategorical Variables\nIn the previous chapter, when working with deep learning networks, we dealt with categorical variables by one-hot encoding them and feeding them to an embedding layer. The embedding layer helped the model to discover the meaning of the different levels of these variables (the levels of a categorical variable do not have an intrinsic meaning, unless we manually specify an ordering using Pandas). In a decision tree, we don’t have embeddings layers—so how can these untreated categorical variables do anything useful in a decision tree? For instance, how could something like a product code be used?\nThe short answer is: it just works! Think about a situation where there is one product code that is far more expensive at auction than any other one. In that case, any binary split will result in that one product code being in some group, and that group will be more expensive than the other group. Therefore, our simple decision tree building algorithm will choose that split. Later during training the algorithm will be able to further split the subgroup that contains the expensive product code, and over time, the tree will home in on that one expensive product.\nIt is also possible to use one-hot encoding to replace a single categorical variable with multiple one-hot-encoded columns, where each column represents a possible level of the variable. Pandas has a get_dummies method which does just that.\nHowever, there is not really any evidence that such an approach improves the end result. So, we generally avoid it where possible, because it does end up making your dataset harder to work with. In 2019 this issue was explored in the paper “Splitting on Categorical Predictors in Random Forests” by Marvin Wright and Inke König, which said:\n\n: The standard approach for nominal predictors is to consider all \\(2^{k-1} − 1\\) 2-partitions of the k predictor categories. However, this exponential relationship produces a large number of potential splits to be evaluated, increasing computational complexity and restricting the possible number of categories in most implementations. For binary classification and regression, it was shown that ordering the predictor categories in each split leads to exactly the same splits as the standard approach. This reduces computational complexity because only k − 1 splits have to be considered for a nominal predictor with k categories.\n\nNow that you understand how decisions tree work, it’s time for the best-of-both-worlds solution: random forests."
  },
  {
    "objectID": "Fastbook/09_tabular.html#random-forests",
    "href": "Fastbook/09_tabular.html#random-forests",
    "title": "Tabular Modeling Deep Dive",
    "section": "Random Forests",
    "text": "Random Forests\nIn 1994 Berkeley professor Leo Breiman, one year after his retirement, published a small technical report called “Bagging Predictors”, which turned out to be one of the most influential ideas in modern machine learning. The report began:\n\n: Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions… The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests… show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.\n\nHere is the procedure that Breiman is proposing:\n\nRandomly choose a subset of the rows of your data (i.e., “bootstrap replicates of your learning set”).\nTrain a model using this subset.\nSave that model, and then return to step 1 a few times.\nThis will give you a number of trained models. To make a prediction, predict using all of the models, and then take the average of each of those model’s predictions.\n\nThis procedure is known as “bagging.” It is based on a deep and important insight: although each of the models trained on a subset of data will make more errors than a model trained on the full dataset, those errors will not be correlated with each other. Different models will make different errors. The average of those errors, therefore, is: zero! So if we take the average of all of the models’ predictions, then we should end up with a prediction that gets closer and closer to the correct answer, the more models we have. This is an extraordinary result—it means that we can improve the accuracy of nearly any kind of machine learning algorithm by training it multiple times, each time on a different random subset of the data, and averaging its predictions.\nIn 2001 Leo Breiman went on to demonstrate that this approach to building models, when applied to decision tree building algorithms, was particularly powerful. He went even further than just randomly choosing rows for each model’s training, but also randomly selected from a subset of columns when choosing each split in each decision tree. He called this method the random forest. Today it is, perhaps, the most widely used and practically important machine learning method.\nIn essence a random forest is a model that averages the predictions of a large number of decision trees, which are generated by randomly varying various parameters that specify what data is used to train the tree and other tree parameters. Bagging is a particular approach to “ensembling,” or combining the results of multiple models together. To see how it works in practice, let’s get started on creating our own random forest!\n\n#hide\n# pip install —pre -f https://sklearn-nightly.scdn8.secure.raxcdn.com scikit-learn —U\n\n\nCreating a Random Forest\nWe can create a random forest just like we created a decision tree, except now, we are also specifying parameters that indicate how many trees should be in the forest, how we should subset the data items (the rows), and how we should subset the fields (the columns).\nIn the following function definition n_estimators defines the number of trees we want, max_samples defines how many rows to sample for training each tree, and max_features defines how many columns to sample at each split point (where 0.5 means “take half the total number of columns”). We can also specify when to stop splitting the tree nodes, effectively limiting the depth of the tree, by including the same min_samples_leaf parameter we used in the last section. Finally, we pass n_jobs=-1 to tell sklearn to use all our CPUs to build the trees in parallel. By creating a little function for this, we can more quickly try different variations in the rest of this chapter:\n\ndef rf(xs, y, n_estimators=40, max_samples=200_000,\n       max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,\n        max_samples=max_samples, max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\n\n\nm = rf(xs, y);\n\nOur validation RMSE is now much improved over our last result produced by the DecisionTreeRegressor, which made just one tree using all the available data:\n\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n\n(0.170917, 0.233975)\n\n\nOne of the most important properties of random forests is that they aren’t very sensitive to the hyperparameter choices, such as max_features. You can set n_estimators to as high a number as you have time to train—the more trees you have, the more accurate the model will be. max_samples can often be left at its default, unless you have over 200,000 data points, in which case setting it to 200,000 will make it train faster with little impact on accuracy. max_features=0.5 and min_samples_leaf=4 both tend to work well, although sklearn’s defaults work well too.\nThe sklearn docs show an example of the effects of different max_features choices, with increasing numbers of trees. In the plot, the blue plot line uses the fewest features and the green line uses the most (it uses all the features). As you can see in &lt;&gt;, the models with the lowest error result from using a subset of features but with a larger number of trees.\n\nTo see the impact of n_estimators, let’s get the predictions from each individual tree in our forest (these are in the estimators_ attribute):\n\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_])\n\nAs you can see, preds.mean(0) gives the same results as our random forest:\n\nr_mse(preds.mean(0), valid_y)\n\n0.233975\n\n\nLet’s see what happens to the RMSE as we add more and more trees. As you can see, the improvement levels off quite a bit after around 30 trees:\n\nplt.plot([r_mse(preds[:i+1].mean(0), valid_y) for i in range(40)]);\n\n\n\n\n\n\n\n\nThe performance on our validation set is worse than on our training set. But is that because we’re overfitting, or because the validation set covers a different time period, or a bit of both? With the existing information we’ve seen, we can’t tell. However, random forests have a very clever trick called out-of-bag (OOB) error that can help us with this (and more!).\n\n\nOut-of-Bag Error\nRecall that in a random forest, each tree is trained on a different subset of the training data. The OOB error is a way of measuring prediction error on the training set by only including in the calculation of a row’s error trees where that row was not included in training. This allows us to see whether the model is overfitting, without needing a separate validation set.\n\nA: My intuition for this is that, since every tree was trained with a different randomly selected subset of rows, out-of-bag error is a little like imagining that every tree therefore also has its own validation set. That validation set is simply the rows that were not selected for that tree’s training.\n\nThis is particularly beneficial in cases where we have only a small amount of training data, as it allows us to see whether our model generalizes without removing items to create a validation set. The OOB predictions are available in the oob_prediction_ attribute. Note that we compare them to the training labels, since this is being calculated on trees using the training set.\n\nr_mse(m.oob_prediction_, y)\n\n0.210681\n\n\nWe can see that our OOB error is much lower than our validation set error. This means that something else is causing that error, in addition to normal generalization error. We’ll discuss the reasons for this later in this chapter.\nThis is one way to interpret our model’s predictions—let’s focus on more of those now."
  },
  {
    "objectID": "Fastbook/09_tabular.html#model-interpretation",
    "href": "Fastbook/09_tabular.html#model-interpretation",
    "title": "Tabular Modeling Deep Dive",
    "section": "Model Interpretation",
    "text": "Model Interpretation\nFor tabular data, model interpretation is particularly important. For a given model, the things we are most likely to be interested in are:\n\nHow confident are we in our predictions using a particular row of data?\nFor predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?\nWhich columns are the strongest predictors, which can we ignore?\nWhich columns are effectively redundant with each other, for purposes of prediction?\nHow do predictions vary, as we vary these columns?\n\nAs we will see, random forests are particularly well suited to answering these questions. Let’s start with the first one!\n\nTree Variance for Prediction Confidence\nWe saw how the model averages the individual tree’s predictions to get an overall prediction—that is, an estimate of the value. But how can we know the confidence of the estimate? One simple way is to use the standard deviation of predictions across the trees, instead of just the mean. This tells us the relative confidence of predictions. In general, we would want to be more cautious of using the results for rows where trees give very different results (higher standard deviations), compared to cases where they are more consistent (lower standard deviations).\nIn the earlier section on creating a random forest, we saw how to get predictions over the validation set, using a Python list comprehension to do this for each tree in the forest:\n\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_])\n\n\npreds.shape\n\n(40, 7988)\n\n\nNow we have a prediction for every tree and every auction (40 trees and 7,988 auctions) in the validation set.\nUsing this we can get the standard deviation of the predictions over all the trees, for each auction:\n\npreds_std = preds.std(0)\n\nHere are the standard deviations for the predictions for the first five auctions—that is, the first five rows of the validation set:\n\npreds_std[:5]\n\narray([0.25065395, 0.11043862, 0.08242067, 0.26988508, 0.15730173])\n\n\nAs you can see, the confidence in the predictions varies widely. For some auctions, there is a low standard deviation because the trees agree. For others it’s higher, as the trees don’t agree. This is information that would be useful in a production setting; for instance, if you were using this model to decide what items to bid on at auction, a low-confidence prediction might cause you to look more carefully at an item before you made a bid.\n\n\nFeature Importance\nIt’s not normally enough just to know that a model can make accurate predictions—we also want to know how it’s making predictions. feature importance gives us insight into this. We can get these directly from sklearn’s random forest by looking in the feature_importances_ attribute. Here’s a simple function we can use to pop them into a DataFrame and sort them:\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n\nThe feature importances for our model show that the first few most important columns have much higher importance scores than the rest, with (not surprisingly) YearMade and ProductSize being at the top of the list:\n\nfi = rf_feat_importance(m, xs)\nfi[:10]\n\n\n\n\n\n\n\n\ncols\nimp\n\n\n\n\n59\nYearMade\n0.180070\n\n\n7\nProductSize\n0.113915\n\n\n31\nCoupler_System\n0.104699\n\n\n8\nfiProductClassDesc\n0.064118\n\n\n33\nHydraulics_Flow\n0.059110\n\n\n56\nModelID\n0.059087\n\n\n51\nsaleElapsed\n0.051231\n\n\n4\nfiSecondaryDesc\n0.041778\n\n\n32\nGrouser_Tracks\n0.037560\n\n\n2\nfiModelDesc\n0.030933\n\n\n\n\n\n\n\nA plot of the feature importances shows the relative importances more clearly:\n\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30]);\n\n\n\n\n\n\n\n\nThe way these importances are calculated is quite simple yet elegant. The feature importance algorithm loops through each tree, and then recursively explores each branch. At each branch, it looks to see what feature was used for that split, and how much the model improves as a result of that split. The improvement (weighted by the number of rows in that group) is added to the importance score for that feature. This is summed across all branches of all trees, and finally the scores are normalized such that they add to 1.\n\n\nRemoving Low-Importance Variables\nIt seems likely that we could use just a subset of the columns by removing the variables of low importance and still get good results. Let’s try just keeping those with a feature importance greater than 0.005:\n\nto_keep = fi[fi.imp&gt;0.005].cols\nlen(to_keep)\n\n21\n\n\nWe can retrain our model using just this subset of the columns:\n\nxs_imp = xs[to_keep]\nvalid_xs_imp = valid_xs[to_keep]\n\n\nm = rf(xs_imp, y)\n\nAnd here’s the result:\n\nm_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y)\n\n(0.181204, 0.230329)\n\n\nOur accuracy is about the same, but we have far fewer columns to study:\n\nlen(xs.columns), len(xs_imp.columns)\n\n(66, 21)\n\n\nWe’ve found that generally the first step to improving a model is simplifying it—78 columns was too many for us to study them all in depth! Furthermore, in practice often a simpler, more interpretable model is easier to roll out and maintain.\nThis also makes our feature importance plot easier to interpret. Let’s look at it again:\n\nplot_fi(rf_feat_importance(m, xs_imp));\n\n\n\n\n\n\n\n\nOne thing that makes this harder to interpret is that there seem to be some variables with very similar meanings: for example, ProductGroup and ProductGroupDesc. Let’s try to remove any redundent features.\n\n\nRemoving Redundant Features\nLet’s start with:\n\ncluster_columns(xs_imp)\n\n\n\n\n\n\n\n\nIn this chart, the pairs of columns that are most similar are the ones that were merged together early, far from the “root” of the tree at the left. Unsurprisingly, the fields ProductGroup and ProductGroupDesc were merged quite early, as were saleYear and saleElapsed and fiModelDesc and fiBaseModel. These might be so closely correlated they are practically synonyms for each other.\n\nnote: Determining Similarity: The most similar pairs are found by calculating the rank correlation, which means that all the values are replaced with their rank (i.e., first, second, third, etc. within the column), and then the correlation is calculated. (Feel free to skip over this minor detail though, since it’s not going to come up again in the book!)\n\nLet’s try removing some of these closely related features to see if the model can be simplified without impacting the accuracy. First, we create a function that quickly trains a random forest and returns the OOB score, by using a lower max_samples and higher min_samples_leaf. The OOB score is a number returned by sklearn that ranges between 1.0 for a perfect model and 0.0 for a random model. (In statistics it’s called R^2, although the details aren’t important for this explanation.) We don’t need it to be very accurate—we’re just going to use it to compare different models, based on removing some of the possibly redundant columns:\n\ndef get_oob(df):\n    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,\n        max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(df, y)\n    return m.oob_score_\n\nHere’s our baseline:\n\nget_oob(xs_imp)\n\n0.8768243241012634\n\n\nNow we try removing each of our potentially redundant variables, one at a time:\n\n{c:get_oob(xs_imp.drop(c, axis=1)) for c in (\n    'saleYear', 'saleElapsed', 'ProductGroupDesc','ProductGroup',\n    'fiModelDesc', 'fiBaseModel',\n    'Hydraulics_Flow','Grouser_Tracks', 'Coupler_System')}\n\n{'saleYear': 0.8766429216799364,\n 'saleElapsed': 0.8725120463477113,\n 'ProductGroupDesc': 0.8773289113713139,\n 'ProductGroup': 0.8768277447901079,\n 'fiModelDesc': 0.8760365396140016,\n 'fiBaseModel': 0.8769194097714894,\n 'Hydraulics_Flow': 0.8775975083138958,\n 'Grouser_Tracks': 0.8780246481379101,\n 'Coupler_System': 0.8780158691125818}\n\n\nNow let’s try dropping multiple variables. We’ll drop one from each of the tightly aligned pairs we noticed earlier. Let’s see what that does:\n\nto_drop = ['saleYear', 'ProductGroupDesc', 'fiBaseModel', 'Grouser_Tracks']\nget_oob(xs_imp.drop(to_drop, axis=1))\n\n0.8747772191306009\n\n\nLooking good! This is really not much worse than the model with all the fields. Let’s create DataFrames without these columns, and save them:\n\nxs_final = xs_imp.drop(to_drop, axis=1)\nvalid_xs_final = valid_xs_imp.drop(to_drop, axis=1)\n\n\nsave_pickle(path/'xs_final.pkl', xs_final)\nsave_pickle(path/'valid_xs_final.pkl', valid_xs_final)\n\nWe can load them back later with:\n\nxs_final = load_pickle(path/'xs_final.pkl')\nvalid_xs_final = load_pickle(path/'valid_xs_final.pkl')\n\nNow we can check our RMSE again, to confirm that the accuracy hasn’t substantially changed.\n\nm = rf(xs_final, y)\nm_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y)\n\n(0.183426, 0.231894)\n\n\nBy focusing on the most important variables, and removing some redundant ones, we’ve greatly simplified our model. Now, let’s see how those variables affect our predictions using partial dependence plots.\n\n\nPartial Dependence\nAs we’ve seen, the two most important predictors are ProductSize and YearMade. We’d like to understand the relationship between these predictors and sale price. It’s a good idea to first check the count of values per category (provided by the Pandas value_counts method), to see how common each category is:\n\np = valid_xs_final['ProductSize'].value_counts(sort=False).plot.barh()\nc = to.classes['ProductSize']\nplt.yticks(range(len(c)), c);\n\n\n\n\n\n\n\n\nThe largrest group is #na#, which is the label fastai applies to missing values.\nLet’s do the same thing for YearMade. Since this is a numeric feature, we’ll need to draw a histogram, which groups the year values into a few discrete bins:\n\nax = valid_xs_final['YearMade'].hist()\n\n\n\n\n\n\n\n\nOther than the special value 1950 which we used for coding missing year values, most of the data is from after 1990.\nNow we’re ready to look at partial dependence plots. Partial dependence plots try to answer the question: if a row varied on nothing other than the feature in question, how would it impact the dependent variable?\nFor instance, how does YearMade impact sale price, all other things being equal?\nTo answer this question, we can’t just take the average sale price for each YearMade. The problem with that approach is that many other things vary from year to year as well, such as which products are sold, how many products have air-conditioning, inflation, and so forth. So, merely averaging over all the auctions that have the same YearMade would also capture the effect of how every other field also changed along with YearMade and how that overall change affected price.\nInstead, what we do is replace every single value in the YearMade column with 1950, and then calculate the predicted sale price for every auction, and take the average over all auctions. Then we do the same for 1951, 1952, and so forth until our final year of 2011. This isolates the effect of only YearMade (even if it does so by averaging over some imagined records where we assign a YearMade value that might never actually exist alongside some other values).\n\nA: If you are philosophically minded it is somewhat dizzying to contemplate the different kinds of hypotheticality that we are juggling to make this calculation. First, there’s the fact that every prediction is hypothetical, because we are not noting empirical data. Second, there’s the point that we’re not merely interested in asking how sale price would change if we changed YearMade and everything else along with it. Rather, we’re very specifically asking, how sale price would change in a hypothetical world where only YearMade changed. Phew! It is impressive that we can ask such questions. I recommend Judea Pearl and Dana Mackenzie’s recent book on causality, The Book of Why (Basic Books), if you’re interested in more deeply exploring formalisms for analyzing these subtleties.\n\nWith these averages, we can then plot each of these years on the x-axis, and each of the predictions on the y-axis. This, finally, is a partial dependence plot. Let’s take a look:\n\nfrom sklearn.inspection import plot_partial_dependence\n\nfig,ax = plt.subplots(figsize=(12, 4))\nplot_partial_dependence(m, valid_xs_final, ['YearMade','ProductSize'],\n                        grid_resolution=20, ax=ax);\n\n\n\n\n\n\n\n\nLooking first of all at the YearMade plot, and specifically at the section covering the years after 1990 (since as we noted this is where we have the most data), we can see a nearly linear relationship between year and price. Remember that our dependent variable is after taking the logarithm, so this means that in practice there is an exponential increase in price. This is what we would expect: depreciation is generally recognized as being a multiplicative factor over time, so, for a given sale date, varying year made ought to show an exponential relationship with sale price.\nThe ProductSize partial plot is a bit concerning. It shows that the final group, which we saw is for missing values, has the lowest price. To use this insight in practice, we would want to find out why it’s missing so often, and what that means. Missing values can sometimes be useful predictors—it entirely depends on what causes them to be missing. Sometimes, however, they can indicate data leakage.\n\n\nData Leakage\nIn the paper “Leakage in Data Mining: Formulation, Detection, and Avoidance”, Shachar Kaufman, Saharon Rosset, and Claudia Perlich describe leakage as:\n\n: The introduction of information about the target of a data mining problem, which should not be legitimately available to mine from. A trivial example of leakage would be a model that uses the target itself as an input, thus concluding for example that ‘it rains on rainy days’. In practice, the introduction of this illegitimate information is unintentional, and facilitated by the data collection, aggregation and preparation process.\n\nThey give as an example:\n\n: A real-life business intelligence project at IBM where potential customers for certain products were identified, among other things, based on keywords found on their websites. This turned out to be leakage since the website content used for training had been sampled at the point in time where the potential customer has already become a customer, and where the website contained traces of the IBM products purchased, such as the word ‘Websphere’ (e.g., in a press release about the purchase or a specific product feature the client uses).\n\nData leakage is subtle and can take many forms. In particular, missing values often represent data leakage.\nFor instance, Jeremy competed in a Kaggle competition designed to predict which researchers would end up receiving research grants. The information was provided by a university and included thousands of examples of research projects, along with information about the researchers involved and data on whether or not each grant was eventually accepted. The university hoped to be able to use the models developed in this competition to rank which grant applications were most likely to succeed, so it could prioritize its processing.\nJeremy used a random forest to model the data, and then used feature importance to find out which features were most predictive. He noticed three surprising things:\n\nThe model was able to correctly predict who would receive grants over 95% of the time.\nApparently meaningless identifier columns were the most important predictors.\nThe day of week and day of year columns were also highly predictive; for instance, the vast majority of grant applications dated on a Sunday were accepted, and many accepted grant applications were dated on January 1.\n\nFor the identifier columns, one partial dependence plot per column showed that when the information was missing the application was almost always rejected. It turned out that in practice, the university only filled out much of this information after a grant application was accepted. Often, for applications that were not accepted, it was just left blank. Therefore, this information was not something that was actually available at the time that the application was received, and it would not be available for a predictive model—it was data leakage.\nIn the same way, the final processing of successful applications was often done automatically as a batch at the end of the week, or the end of the year. It was this final processing date which ended up in the data, so again, this information, while predictive, was not actually available at the time that the application was received.\nThis example showcases the most practical and simple approaches to identifying data leakage, which are to build a model and then:\n\nCheck whether the accuracy of the model is too good to be true.\nLook for important predictors that don’t make sense in practice.\nLook for partial dependence plot results that don’t make sense in practice.\n\nThinking back to our bear detector, this mirrors the advice that we provided in &lt;&gt;—it is often a good idea to build a model first and then do your data cleaning, rather than vice versa. The model can help you identify potentially problematic data issues.\nIt can also help you identify which factors influence specific predictions, with tree interpreters.\n\n\nTree Interpreter\n\n#hide\nimport warnings\nwarnings.simplefilter('ignore', FutureWarning)\n\nfrom treeinterpreter import treeinterpreter\nfrom waterfall_chart import plot as waterfall\n\nAt the start of this section, we said that we wanted to be able to answer five questions:\n\nHow confident are we in our predictions using a particular row of data?\nFor predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?\nWhich columns are the strongest predictors?\nWhich columns are effectively redundant with each other, for purposes of prediction?\nHow do predictions vary, as we vary these columns?\n\nWe’ve handled four of these already; only the second question remains. To answer this question, we need to use the treeinterpreter library. We’ll also use the waterfallcharts library to draw the chart of the results.\n!pip install treeinterpreter\n!pip install waterfallcharts\nWe have already seen how to compute feature importances across the entire random forest. The basic idea was to look at the contribution of each variable to improving the model, at each branch of every tree, and then add up all of these contributions per variable.\nWe can do exactly the same thing, but for just a single row of data. For instance, let’s say we are looking at some particular item at auction. Our model might predict that this item will be very expensive, and we want to know why. So, we take that one row of data and put it through the first decision tree, looking to see what split is used at each point throughout the tree. For each split, we see what the increase or decrease in the addition is, compared to the parent node of the tree. We do this for every tree, and add up the total change in importance by split variable.\nFor instance, let’s pick the first few rows of our validation set:\n\nrow = valid_xs_final.iloc[:5]\n\nWe can then pass these to treeinterpreter:\n\nprediction,bias,contributions = treeinterpreter.predict(m, row.values)\n\nprediction is simply the prediction that the random forest makes. bias is the prediction based on taking the mean of the dependent variable (i.e., the model that is the root of every tree). contributions is the most interesting bit—it tells us the total change in predicition due to each of the independent variables. Therefore, the sum of contributions plus bias must equal the prediction, for each row. Let’s look just at the first row:\n\nprediction[0], bias[0], contributions[0].sum()\n\n(array([10.01216396]), 10.104746057831765, -0.0925820990266335)\n\n\nThe clearest way to display the contributions is with a waterfall plot. This shows how the positive and negative contributions from all the independent variables sum up to create the final prediction, which is the righthand column labeled “net” here:\n\nwaterfall(valid_xs_final.columns, contributions[0], threshold=0.08, \n          rotation_value=45,formatting='{:,.3f}');\n\n\n\n\n\n\n\n\nThis kind of information is most useful in production, rather than during model development. You can use it to provide useful information to users of your data product about the underlying reasoning behind the predictions.\nNow that we covered some classic machine learning techniques to solve this problem, let’s see how deep learning can help!"
  },
  {
    "objectID": "Fastbook/09_tabular.html#extrapolation-and-neural-networks",
    "href": "Fastbook/09_tabular.html#extrapolation-and-neural-networks",
    "title": "Tabular Modeling Deep Dive",
    "section": "Extrapolation and Neural Networks",
    "text": "Extrapolation and Neural Networks\nA problem with random forests, like all machine learning or deep learning algorithms, is that they don’t always generalize well to new data. We will see in which situations neural networks generalize better, but first, let’s look at the extrapolation problem that random forests have.\n\nThe Extrapolation Problem\n\n#hide\nnp.random.seed(42)\n\nLet’s consider the simple task of making predictions from 40 data points showing a slightly noisy linear relationship:\n\nx_lin = torch.linspace(0,20, steps=40)\ny_lin = x_lin + torch.randn_like(x_lin)\nplt.scatter(x_lin, y_lin);\n\n\n\n\n\n\n\n\nAlthough we only have a single independent variable, sklearn expects a matrix of independent variables, not a single vector. So we have to turn our vector into a matrix with one column. In other words, we have to change the shape from [40] to [40,1]. One way to do that is with the unsqueeze method, which adds a new unit axis to a tensor at the requested dimension:\n\nxs_lin = x_lin.unsqueeze(1)\nx_lin.shape,xs_lin.shape\n\n(torch.Size([40]), torch.Size([40, 1]))\n\n\nA more flexible approach is to slice an array or tensor with the special value None, which introduces an additional unit axis at that location:\n\nx_lin[:,None].shape\n\ntorch.Size([40, 1])\n\n\nWe can now create a random forest for this data. We’ll use only the first 30 rows to train the model:\n\nm_lin = RandomForestRegressor().fit(xs_lin[:30],y_lin[:30])\n\nThen we’ll test the model on the full dataset. The blue dots are the training data, and the red dots are the predictions:\n\nplt.scatter(x_lin, y_lin, 20)\nplt.scatter(x_lin, m_lin.predict(xs_lin), color='red', alpha=0.5);\n\n\n\n\n\n\n\n\nWe have a big problem! Our predictions outside of the domain that our training data covered are all too low. Why do you suppose this is?\nRemember, a random forest just averages the predictions of a number of trees. And a tree simply predicts the average value of the rows in a leaf. Therefore, a tree and a random forest can never predict values outside of the range of the training data. This is particularly problematic for data where there is a trend over time, such as inflation, and you wish to make predictions for a future time. Your predictions will be systematically too low.\nBut the problem extends beyond time variables. Random forests are not able to extrapolate outside of the types of data they have seen, in a more general sense. That’s why we need to make sure our validation set does not contain out-of-domain data.\n\n\nFinding Out-of-Domain Data\nSometimes it is hard to know whether your test set is distributed in the same way as your training data, or, if it is different, what columns reflect that difference. There’s actually an easy way to figure this out, which is to use a random forest!\nBut in this case we don’t use the random forest to predict our actual dependent variable. Instead, we try to predict whether a row is in the validation set or the training set. To see this in action, let’s combine our training and validation sets together, create a dependent variable that represents which dataset each row comes from, build a random forest using that data, and get its feature importance:\n\ndf_dom = pd.concat([xs_final, valid_xs_final])\nis_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final))\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:6]\n\n\n\n\n\n\n\n\ncols\nimp\n\n\n\n\n6\nsaleElapsed\n0.891571\n\n\n9\nSalesID\n0.091174\n\n\n14\nMachineID\n0.012950\n\n\n0\nYearMade\n0.001520\n\n\n10\nEnclosure\n0.000430\n\n\n5\nModelID\n0.000395\n\n\n\n\n\n\n\nThis shows that there are three columns that differ significantly between the training and validation sets: saleElapsed, SalesID, and MachineID. It’s fairly obvious why this is the case for saleElapsed: it’s the number of days between the start of the dataset and each row, so it directly encodes the date. The difference in SalesID suggests that identifiers for auction sales might increment over time. MachineID suggests something similar might be happening for individual items sold in those auctions.\nLet’s get a baseline of the original random forest model’s RMSE, then see what the effect is of removing each of these columns in turn:\n\nm = rf(xs_final, y)\nprint('orig', m_rmse(m, valid_xs_final, valid_y))\n\nfor c in ('SalesID','saleElapsed','MachineID'):\n    m = rf(xs_final.drop(c,axis=1), y)\n    print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y))\n\norig 0.232883\nSalesID 0.230347\nsaleElapsed 0.235529\nMachineID 0.230735\n\n\nIt looks like we should be able to remove SalesID and MachineID without losing any accuracy. Let’s check:\n\ntime_vars = ['SalesID','MachineID']\nxs_final_time = xs_final.drop(time_vars, axis=1)\nvalid_xs_time = valid_xs_final.drop(time_vars, axis=1)\n\nm = rf(xs_final_time, y)\nm_rmse(m, valid_xs_time, valid_y)\n\n0.229498\n\n\nRemoving these variables has slightly improved the model’s accuracy; but more importantly, it should make it more resilient over time, and easier to maintain and understand. We recommend that for all datasets you try building a model where your dependent variable is is_valid, like we did here. It can often uncover subtle domain shift issues that you may otherwise miss.\nOne thing that might help in our case is to simply avoid using old data. Often, old data shows relationships that just aren’t valid any more. Let’s try just using the most recent few years of the data:\n\nxs['saleYear'].hist();\n\n\n\n\n\n\n\n\nHere’s the result of training on this subset:\n\nfilt = xs['saleYear']&gt;2004\nxs_filt = xs_final_time[filt]\ny_filt = y[filt]\n\n\nm = rf(xs_filt, y_filt)\nm_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y)\n\n(0.177284, 0.228008)\n\n\nIt’s a tiny bit better, which shows that you shouldn’t always just use your entire dataset; sometimes a subset can be better.\nLet’s see if using a neural network helps.\n\n\nUsing a Neural Network\nWe can use the same approach to build a neural network model. Let’s first replicate the steps we took to set up the TabularPandas object:\n\ndf_nn = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\ndf_nn['ProductSize'] = df_nn['ProductSize'].astype('category')\ndf_nn['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)\ndf_nn[dep_var] = np.log(df_nn[dep_var])\ndf_nn = add_datepart(df_nn, 'saledate')\n\nWe can leverage the work we did to trim unwanted columns in the random forest by using the same set of columns for our neural network:\n\ndf_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]]\n\nCategorical columns are handled very differently in neural networks, compared to decision tree approaches. As we saw in &lt;&gt;, in a neural net a great way to handle categorical variables is by using embeddings. To create embeddings, fastai needs to determine which columns should be treated as categorical variables. It does this by comparing the number of distinct levels in the variable to the value of the max_card parameter. If it’s lower, fastai will treat the variable as categorical. Embedding sizes larger than 10,000 should generally only be used after you’ve tested whether there are better ways to group the variable, so we’ll use 9,000 as our max_card:\n\ncont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)\n\nIn this case, there’s one variable that we absolutely do not want to treat as categorical: the saleElapsed variable. A categorical variable cannot, by definition, extrapolate outside the range of values that it has seen, but we want to be able to predict auction sale prices in the future. Let’s verify that cont_cat_split did the correct thing.\n\ncont_nn\n\n['saleElapsed']\n\n\nLet’s take a look at the cardinality of each of the categorical variables that we have chosen so far:\n\ndf_nn_final[cat_nn].nunique()\n\nYearMade                73\nProductSize              6\nCoupler_System           2\nfiProductClassDesc      74\nHydraulics_Flow          3\nModelID               5281\nfiSecondaryDesc        177\nfiModelDesc           5059\nEnclosure                6\nHydraulics              12\nProductGroup             6\nDrive_System             4\nTire_Size               17\ndtype: int64\n\n\nThe fact that there are two variables pertaining to the “model” of the equipment, both with similar very high cardinalities, suggests that they may contain similar, redundant information. Note that we would not necessarily see this when analyzing redundant features, since that relies on similar variables being sorted in the same order (that is, they need to have similarly named levels). Having a column with 5,000 levels means needing 5,000 columns in our embedding matrix, which would be nice to avoid if possible. Let’s see what the impact of removing one of these model columns has on the random forest:\n\nxs_filt2 = xs_filt.drop('fiModelDescriptor', axis=1)\nvalid_xs_time2 = valid_xs_time.drop('fiModelDescriptor', axis=1)\nm2 = rf(xs_filt2, y_filt)\nm_rmse(m2, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y)\n\n(0.176713, 0.230195)\n\n\nThere’s minimal impact, so we will remove it as a predictor for our neural network:\n\ncat_nn.remove('fiModelDescriptor')\n\nWe can create our TabularPandas object in the same way as when we created our random forest, with one very important addition: normalization. A random forest does not need any normalization—the tree building procedure cares only about the order of values in a variable, not at all about how they are scaled. But as we have seen, a neural network definitely does care about this. Therefore, we add the Normalize processor when we build our TabularPandas object:\n\nprocs_nn = [Categorify, FillMissing, Normalize]\nto_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn,\n                      splits=splits, y_names=dep_var)\n\nTabular models and data don’t generally require much GPU RAM, so we can use larger batch sizes:\n\ndls = to_nn.dataloaders(1024)\n\nAs we’ve discussed, it’s a good idea to set y_range for regression models, so let’s find the min and max of our dependent variable:\n\ny = to_nn.train.y\ny.min(),y.max()\n\n(8.465899467468262, 11.863582611083984)\n\n\nWe can now create the Learner to create this tabular model. As usual, we use the application-specific learner function, to take advantage of its application-customized defaults. We set the loss function to MSE, since that’s what this competition uses.\nBy default, for tabular data fastai creates a neural network with two hidden layers, with 200 and 100 activations, respectively. This works quite well for small datasets, but here we’ve got quite a large dataset, so we increase the layer sizes to 500 and 250:\n\nlearn = tabular_learner(dls, y_range=(8,12), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\n\n\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.002754228748381138, lr_steep=0.00015848931798245758)\n\n\n\n\n\n\n\n\n\nThere’s no need to use fine_tune, so we’ll train with fit_one_cycle for a few epochs and see how it looks:\n\nlearn.fit_one_cycle(5, 1e-2)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.068459\n0.061185\n00:09\n\n\n1\n0.056469\n0.058471\n00:09\n\n\n2\n0.048689\n0.052404\n00:09\n\n\n3\n0.044529\n0.052138\n00:09\n\n\n4\n0.040860\n0.051236\n00:09\n\n\n\n\n\nWe can use our r_mse function to compare the result to the random forest result we got earlier:\n\npreds,targs = learn.get_preds()\nr_mse(preds,targs)\n\n\n\n\n0.226353\n\n\nIt’s quite a bit better than the random forest (although it took longer to train, and it’s fussier about hyperparameter tuning).\nBefore we move on, let’s save our model in case we want to come back to it again later:\n\nlearn.save('nn')\n\nPath('models/nn.pth')\n\n\n\n\nSidebar: fastai’s Tabular Classes\nIn fastai, a tabular model is simply a model that takes columns of continuous or categorical data, and predicts a category (a classification model) or a continuous value (a regression model). Categorical independent variables are passed through an embedding, and concatenated, as we saw in the neural net we used for collaborative filtering, and then continuous variables are concatenated as well.\nThe model created in tabular_learner is an object of class TabularModel. Take a look at the source for tabular_learner now (remember, that’s tabular_learner?? in Jupyter). You’ll see that like collab_learner, it first calls get_emb_sz to calculate appropriate embedding sizes (you can override these by using the emb_szs parameter, which is a dictionary containing any column names you want to set sizes for manually), and it sets a few other defaults. Other than that, it just creates the TabularModel, and passes that to TabularLearner (note that TabularLearner is identical to Learner, except for a customized predict method).\nThat means that really all the work is happening in TabularModel, so take a look at the source for that now. With the exception of the BatchNorm1d and Dropout layers (which we’ll be learning about shortly), you now have the knowledge required to understand this whole class. Take a look at the discussion of EmbeddingNN at the end of the last chapter. Recall that it passed n_cont=0 to TabularModel. We now can see why that was: because there are zero continuous variables (in fastai the n_ prefix means “number of,” and cont is an abbreviation for “continuous”).\n\n\nEnd sidebar\nAnother thing that can help with generalization is to use several models and average their predictions—a technique, as mentioned earlier, known as ensembling."
  },
  {
    "objectID": "Fastbook/09_tabular.html#ensembling",
    "href": "Fastbook/09_tabular.html#ensembling",
    "title": "Tabular Modeling Deep Dive",
    "section": "Ensembling",
    "text": "Ensembling\nThink back to the original reasoning behind why random forests work so well: each tree has errors, but those errors are not correlated with each other, so the average of those errors should tend towards zero once there are enough trees. Similar reasoning could be used to consider averaging the predictions of models trained using different algorithms.\nIn our case, we have two very different models, trained using very different algorithms: a random forest, and a neural network. It would be reasonable to expect that the kinds of errors that each one makes would be quite different. Therefore, we might expect that the average of their predictions would be better than either one’s individual predictions.\nAs we saw earlier, a random forest is itself an ensemble. But we can then include a random forest in another ensemble—an ensemble of the random forest and the neural network! While ensembling won’t make the difference between a successful and an unsuccessful modeling process, it can certainly add a nice little boost to any models that you have built.\nOne minor issue we have to be aware of is that our PyTorch model and our sklearn model create data of different types: PyTorch gives us a rank-2 tensor (i.e, a column matrix), whereas NumPy gives us a rank-1 array (a vector). squeeze removes any unit axes from a tensor, and to_np converts it into a NumPy array:\n\nrf_preds = m.predict(valid_xs_time)\nens_preds = (to_np(preds.squeeze()) + rf_preds) /2\n\nThis gives us a better result than either model achieved on its own:\n\nr_mse(ens_preds,valid_y)\n\n0.222134\n\n\nIn fact, this result is better than any score shown on the Kaggle leaderboard. It’s not directly comparable, however, because the Kaggle leaderboard uses a separate dataset that we do not have access to. Kaggle does not allow us to submit to this old competition to find out how we would have done, but our results certainly look very encouraging!\n\nBoosting\nSo far our approach to ensembling has been to use bagging, which involves combining many models (each trained on a different data subset) together by averaging them. As we saw, when this is applied to decision trees, this is called a random forest.\nThere is another important approach to ensembling, called boosting, where we add models instead of averaging them. Here is how boosting works:\n\nTrain a small model that underfits your dataset.\nCalculate the predictions in the training set for this model.\nSubtract the predictions from the targets; these are called the “residuals” and represent the error for each point in the training set.\nGo back to step 1, but instead of using the original targets, use the residuals as the targets for the training.\nContinue doing this until you reach some stopping criterion, such as a maximum number of trees, or you observe your validation set error getting worse.\n\nUsing this approach, each new tree will be attempting to fit the error of all of the previous trees combined. Because we are continually creating new residuals, by subtracting the predictions of each new tree from the residuals from the previous tree, the residuals will get smaller and smaller.\nTo make predictions with an ensemble of boosted trees, we calculate the predictions from each tree, and then add them all together. There are many models following this basic approach, and many names for the same models. Gradient boosting machines (GBMs) and gradient boosted decision trees (GBDTs) are the terms you’re most likely to come across, or you may see the names of specific libraries implementing these; at the time of writing, XGBoost is the most popular.\nNote that, unlike with random forests, with this approach there is nothing to stop us from overfitting. Using more trees in a random forest does not lead to overfitting, because each tree is independent of the others. But in a boosted ensemble, the more trees you have, the better the training error becomes, and eventually you will see overfitting on the validation set.\nWe are not going to go into detail on how to train a gradient boosted tree ensemble here, because the field is moving rapidly, and any guidance we give will almost certainly be outdated by the time you read this. As we write this, sklearn has just added a HistGradientBoostingRegressor class that provides excellent performance. There are many hyperparameters to tweak for this class, and for all gradient boosted tree methods we have seen. Unlike random forests, gradient boosted trees are extremely sensitive to the choices of these hyperparameters; in practice, most people use a loop that tries a range of different hyperparameters to find the ones that work best.\nOne more technique that has gotten great results is to use embeddings learned by a neural net in a machine learning model.\n\n\nCombining Embeddings with Other Methods\nThe abstract of the entity embedding paper we mentioned at the start of this chapter states: “the embeddings obtained from the trained neural network boost the performance of all tested machine learning methods considerably when used as the input features instead”. It includes the very interesting table in &lt;&gt;.\n\nThis is showing the mean average percent error (MAPE) compared among four different modeling techniques, three of which we have already seen, along with k-nearest neighbors (KNN), which is a very simple baseline method. The first numeric column contains the results of using the methods on the data provided in the competition; the second column shows what happens if you first train a neural network with categorical embeddings, and then use those categorical embeddings instead of the raw categorical columns in the model. As you see, in every case, the models are dramatically improved by using the embeddings instead of the raw categories.\nThis is a really important result, because it shows that you can get much of the performance improvement of a neural network without actually having to use a neural network at inference time. You could just use an embedding, which is literally just an array lookup, along with a small decision tree ensemble.\nThese embeddings need not even be necessarily learned separately for each model or task in an organization. Instead, once a set of embeddings are learned for some column for some task, they could be stored in a central place, and reused across multiple models. In fact, we know from private communication with other practitioners at large companies that this is already happening in many places."
  },
  {
    "objectID": "Fastbook/09_tabular.html#conclusion-our-advice-for-tabular-modeling",
    "href": "Fastbook/09_tabular.html#conclusion-our-advice-for-tabular-modeling",
    "title": "Tabular Modeling Deep Dive",
    "section": "Conclusion: Our Advice for Tabular Modeling",
    "text": "Conclusion: Our Advice for Tabular Modeling\nWe have dicussed two approaches to tabular modeling: decision tree ensembles and neural networks. We’ve also mentioned two different decision tree ensembles: random forests, and gradient boosting machines. Each is very effective, but each also has compromises:\n\nRandom forests are the easiest to train, because they are extremely resilient to hyperparameter choices and require very little preprocessing. They are very fast to train, and should not overfit if you have enough trees. But they can be a little less accurate, especially if extrapolation is required, such as predicting future time periods.\nGradient boosting machines in theory are just as fast to train as random forests, but in practice you will have to try lots of different hyperparameters. They can overfit, but they are often a little more accurate than random forests.\nNeural networks take the longest time to train, and require extra preprocessing, such as normalization; this normalization needs to be used at inference time as well. They can provide great results and extrapolate well, but only if you are careful with your hyperparameters and take care to avoid overfitting.\n\nWe suggest starting your analysis with a random forest. This will give you a strong baseline, and you can be confident that it’s a reasonable starting point. You can then use that model for feature selection and partial dependence analysis, to get a better understanding of your data.\nFrom that foundation, you can try neural nets and GBMs, and if they give you significantly better results on your validation set in a reasonable amount of time, you can use them. If decision tree ensembles are working well for you, try adding the embeddings for the categorical variables to the data, and see if that helps your decision trees learn better."
  },
  {
    "objectID": "Fastbook/09_tabular.html#questionnaire",
    "href": "Fastbook/09_tabular.html#questionnaire",
    "title": "Tabular Modeling Deep Dive",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhat is a continuous variable?\nWhat is a categorical variable?\nProvide two of the words that are used for the possible values of a categorical variable.\nWhat is a “dense layer”?\nHow do entity embeddings reduce memory usage and speed up neural networks?\nWhat kinds of datasets are entity embeddings especially useful for?\nWhat are the two main families of machine learning algorithms?\nWhy do some categorical columns need a special ordering in their classes? How do you do this in Pandas?\nSummarize what a decision tree algorithm does.\nWhy is a date different from a regular categorical or continuous variable, and how can you preprocess it to allow it to be used in a model?\nShould you pick a random validation set in the bulldozer competition? If no, what kind of validation set should you pick?\nWhat is pickle and what is it useful for?\nHow are mse, samples, and values calculated in the decision tree drawn in this chapter?\nHow do we deal with outliers, before building a decision tree?\nHow do we handle categorical variables in a decision tree?\nWhat is bagging?\nWhat is the difference between max_samples and max_features when creating a random forest?\nIf you increase n_estimators to a very high value, can that lead to overfitting? Why or why not?\nIn the section “Creating a Random Forest”, just after &lt;&gt;, why did preds.mean(0) give the same result as our random forest?\nWhat is “out-of-bag-error”?\nMake a list of reasons why a model’s validation set error might be worse than the OOB error. How could you test your hypotheses?\nExplain why random forests are well suited to answering each of the following question:\n\nHow confident are we in our predictions using a particular row of data?\nFor predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?\nWhich columns are the strongest predictors?\nHow do predictions vary as we vary these columns?\n\nWhat’s the purpose of removing unimportant variables?\nWhat’s a good type of plot for showing tree interpreter results?\nWhat is the “extrapolation problem”?\nHow can you tell if your test or validation set is distributed in a different way than your training set?\nWhy do we ensure saleElapsed is a continuous variable, even although it has less than 9,000 distinct values?\nWhat is “boosting”?\nHow could we use embeddings with a random forest? Would we expect this to help?\nWhy might we not always use a neural net for tabular modeling?\n\n\nFurther Research\n\nPick a competition on Kaggle with tabular data (current or past) and try to adapt the techniques seen in this chapter to get the best possible results. Compare your results to the private leaderboard.\nImplement the decision tree algorithm in this chapter from scratch yourself, and try it on the dataset you used in the first exercise.\nUse the embeddings from the neural net in this chapter in a random forest, and see if you can improve on the random forest results we saw.\nExplain what each line of the source of TabularModel does (with the exception of the BatchNorm1d and Dropout layers)."
  },
  {
    "objectID": "Fastbook/12_nlp_dive.html",
    "href": "Fastbook/12_nlp_dive.html",
    "title": "A Language Model from Scratch",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\n[[chapter_nlp_dive]]\nWe’re now ready to go deep… deep into deep learning! You already learned how to train a basic neural network, but how do you go from there to creating state-of-the-art models? In this part of the book we’re going to uncover all of the mysteries, starting with language models.\nYou saw in &lt;&gt; how to fine-tune a pretrained language model to build a text classifier. In this chapter, we will explain to you what exactly is inside that model, and what an RNN is. First, let’s gather some data that will allow us to quickly prototype our various models."
  },
  {
    "objectID": "Fastbook/12_nlp_dive.html#the-data",
    "href": "Fastbook/12_nlp_dive.html#the-data",
    "title": "A Language Model from Scratch",
    "section": "The Data",
    "text": "The Data\nWhenever we start working on a new problem, we always first try to think of the simplest dataset we can that will allow us to try out methods quickly and easily, and interpret the results. When we started working on language modeling a few years ago we didn’t find any datasets that would allow for quick prototyping, so we made one. We call it Human Numbers, and it simply contains the first 10,000 numbers written out in English.\n\nj: One of the most common practical mistakes I see even amongst highly experienced practitioners is failing to use appropriate datasets at appropriate times during the analysis process. In particular, most people tend to start with datasets that are too big and too complicated.\n\nWe can download, extract, and take a look at our dataset in the usual way:\n\nfrom fastai.text.all import *\npath = untar_data(URLs.HUMAN_NUMBERS)\n\n\n#hide\nPath.BASE_PATH = path\n\n\npath.ls()\n\n(#2) [Path('train.txt'),Path('valid.txt')]\n\n\nLet’s open those two files and see what’s inside. At first we’ll join all of the texts together and ignore the train/valid split given by the dataset (we’ll come back to that later):\n\nlines = L()\nwith open(path/'train.txt') as f: lines += L(*f.readlines())\nwith open(path/'valid.txt') as f: lines += L(*f.readlines())\nlines\n\n(#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven \\n','eight \\n','nine \\n','ten \\n'...]\n\n\nWe take all those lines and concatenate them in one big stream. To mark when we go from one number to the next, we use a . as a separator:\n\ntext = ' . '.join([l.strip() for l in lines])\ntext[:100]\n\n'one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo'\n\n\nWe can tokenize this dataset by splitting on spaces:\n\ntokens = text.split(' ')\ntokens[:10]\n\n['one', '.', 'two', '.', 'three', '.', 'four', '.', 'five', '.']\n\n\nTo numericalize, we have to create a list of all the unique tokens (our vocab):\n\nvocab = L(*tokens).unique()\nvocab\n\n(#30) ['one','.','two','three','four','five','six','seven','eight','nine'...]\n\n\nThen we can convert our tokens into numbers by looking up the index of each in the vocab:\n\nword2idx = {w:i for i,w in enumerate(vocab)}\nnums = L(word2idx[i] for i in tokens)\nnums\n\n(#63095) [0,1,2,1,3,1,4,1,5,1...]\n\n\nNow that we have a small dataset on which language modeling should be an easy task, we can build our first model."
  },
  {
    "objectID": "Fastbook/12_nlp_dive.html#our-first-language-model-from-scratch",
    "href": "Fastbook/12_nlp_dive.html#our-first-language-model-from-scratch",
    "title": "A Language Model from Scratch",
    "section": "Our First Language Model from Scratch",
    "text": "Our First Language Model from Scratch\nOne simple way to turn this into a neural network would be to specify that we are going to predict each word based on the previous three words. We could create a list of every sequence of three words as our independent variables, and the next word after each sequence as the dependent variable.\nWe can do that with plain Python. Let’s do it first with tokens just to confirm what it looks like:\n\nL((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3))\n\n(#21031) [(['one', '.', 'two'], '.'),(['.', 'three', '.'], 'four'),(['four', '.', 'five'], '.'),(['.', 'six', '.'], 'seven'),(['seven', '.', 'eight'], '.'),(['.', 'nine', '.'], 'ten'),(['ten', '.', 'eleven'], '.'),(['.', 'twelve', '.'], 'thirteen'),(['thirteen', '.', 'fourteen'], '.'),(['.', 'fifteen', '.'], 'sixteen')...]\n\n\nNow we will do it with tensors of the numericalized values, which is what the model will actually use:\n\nseqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3))\nseqs\n\n(#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10,  1, 11]), 1),(tensor([ 1, 12,  1]), 13),(tensor([13,  1, 14]), 1),(tensor([ 1, 15,  1]), 16)...]\n\n\nWe can batch those easily using the DataLoader class. For now we will split the sequences randomly:\n\nbs = 64\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False)\n\nWe can now create a neural network architecture that takes three words as input, and returns a prediction of the probability of each possible next word in the vocab. We will use three standard linear layers, but with two tweaks.\nThe first tweak is that the first linear layer will use only the first word’s embedding as activations, the second layer will use the second word’s embedding plus the first layer’s output activations, and the third layer will use the third word’s embedding plus the second layer’s output activations. The key effect of this is that every word is interpreted in the information context of any words preceding it.\nThe second tweak is that each of these three layers will use the same weight matrix. The way that one word impacts the activations from previous words should not change depending on the position of a word. In other words, activation values will change as data moves through the layers, but the layer weights themselves will not change from layer to layer. So, a layer does not learn one sequence position; it must learn to handle all positions.\nSince layer weights do not change, you might think of the sequential layers as “the same layer” repeated. In fact, PyTorch makes this concrete; we can just create one layer, and use it multiple times.\n\nOur Language Model in PyTorch\nWe can now create the language model module that we described earlier:\n\nclass LMModel1(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        \n    def forward(self, x):\n        h = F.relu(self.h_h(self.i_h(x[:,0])))\n        h = h + self.i_h(x[:,1])\n        h = F.relu(self.h_h(h))\n        h = h + self.i_h(x[:,2])\n        h = F.relu(self.h_h(h))\n        return self.h_o(h)\n\nAs you see, we have created three layers:\n\nThe embedding layer (i_h, for input to hidden)\nThe linear layer to create the activations for the next word (h_h, for hidden to hidden)\nA final linear layer to predict the fourth word (h_o, for hidden to output)\n\nThis might be easier to represent in pictorial form, so let’s define a simple pictorial representation of basic neural networks. &lt;&gt; shows how we’re going to represent a neural net with one hidden layer.\n\nEach shape represents activations: rectangle for input, circle for hidden (inner) layer activations, and triangle for output activations. We will use those shapes (summarized in &lt;&gt;) in all the diagrams in this chapter.\n\nAn arrow represents the actual layer computation—i.e., the linear layer followed by the activation function. Using this notation, &lt;&gt; shows what our simple language model looks like.\n\nTo simplify things, we’ve removed the details of the layer computation from each arrow. We’ve also color-coded the arrows, such that all arrows with the same color have the same weight matrix. For instance, all the input layers use the same embedding matrix, so they all have the same color (green).\nLet’s try training this model and see how it goes:\n\nlearn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, \n                metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.824297\n1.970941\n0.467554\n00:02\n\n\n1\n1.386973\n1.823242\n0.467554\n00:02\n\n\n2\n1.417556\n1.654497\n0.494414\n00:02\n\n\n3\n1.376440\n1.650849\n0.494414\n00:02\n\n\n\n\n\nTo see if this is any good, let’s check what a very simple model would give us. In this case we could always predict the most common token, so let’s find out which token is most often the target in our validation set:\n\nn,counts = 0,torch.zeros(len(vocab))\nfor x,y in dls.valid:\n    n += y.shape[0]\n    for i in range_of(vocab): counts[i] += (y==i).long().sum()\nidx = torch.argmax(counts)\nidx, vocab[idx.item()], counts[idx].item()/n\n\n(tensor(29), 'thousand', 0.15165200855716662)\n\n\nThe most common token has the index 29, which corresponds to the token thousand. Always predicting this token would give us an accuracy of roughly 15%, so we are faring way better!\n\nA: My first guess was that the separator would be the most common token, since there is one for every number. But looking at tokens reminded me that large numbers are written with many words, so on the way to 10,000 you write “thousand” a lot: five thousand, five thousand and one, five thousand and two, etc. Oops! Looking at your data is great for noticing subtle features and also embarrassingly obvious ones.\n\nThis is a nice first baseline. Let’s see how we can refactor it with a loop.\n\n\nOur First Recurrent Neural Network\nLooking at the code for our module, we could simplify it by replacing the duplicated code that calls the layers with a for loop. As well as making our code simpler, this will also have the benefit that we will be able to apply our module equally well to token sequences of different lengths—we won’t be restricted to token lists of length three:\n\nclass LMModel2(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        \n    def forward(self, x):\n        h = 0\n        for i in range(3):\n            h = h + self.i_h(x[:,i])\n            h = F.relu(self.h_h(h))\n        return self.h_o(h)\n\nLet’s check that we get the same results using this refactoring:\n\nlearn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, \n                metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.816274\n1.964143\n0.460185\n00:02\n\n\n1\n1.423805\n1.739964\n0.473259\n00:02\n\n\n2\n1.430327\n1.685172\n0.485382\n00:02\n\n\n3\n1.388390\n1.657033\n0.470406\n00:02\n\n\n\n\n\nWe can also refactor our pictorial representation in exactly the same way, as shown in &lt;&gt; (we’re also removing the details of activation sizes here, and using the same arrow colors as in &lt;&gt;).\n\nYou will see that there is a set of activations that are being updated each time through the loop, stored in the variable h—this is called the hidden state.\n\nJargon: hidden state: The activations that are updated at each step of a recurrent neural network.\n\nA neural network that is defined using a loop like this is called a recurrent neural network (RNN). It is important to realize that an RNN is not a complicated new architecture, but simply a refactoring of a multilayer neural network using a for loop.\n\nA: My true opinion: if they were called “looping neural networks,” or LNNs, they would seem 50% less daunting!\n\nNow that we know what an RNN is, let’s try to make it a little bit better."
  },
  {
    "objectID": "Fastbook/12_nlp_dive.html#improving-the-rnn",
    "href": "Fastbook/12_nlp_dive.html#improving-the-rnn",
    "title": "A Language Model from Scratch",
    "section": "Improving the RNN",
    "text": "Improving the RNN\nLooking at the code for our RNN, one thing that seems problematic is that we are initializing our hidden state to zero for every new input sequence. Why is that a problem? We made our sample sequences short so they would fit easily into batches. But if we order the samples correctly, those sample sequences will be read in order by the model, exposing the model to long stretches of the original sequence.\nAnother thing we can look at is having more signal: why only predict the fourth word when we could use the intermediate predictions to also predict the second and third words?\nLet’s see how we can implement those changes, starting with adding some state.\n\nMaintaining the State of an RNN\nBecause we initialize the model’s hidden state to zero for each new sample, we are throwing away all the information we have about the sentences we have seen so far, which means that our model doesn’t actually know where we are up to in the overall counting sequence. This is easily fixed; we can simply move the initialization of the hidden state to __init__.\nBut this fix will create its own subtle, but important, problem. It effectively makes our neural network as deep as the entire number of tokens in our document. For instance, if there were 10,000 tokens in our dataset, we would be creating a 10,000-layer neural network.\nTo see why this is the case, consider the original pictorial representation of our recurrent neural network in &lt;&gt;, before refactoring it with a for loop. You can see each layer corresponds with one token input. When we talk about the representation of a recurrent neural network before refactoring with the for loop, we call this the unrolled representation. It is often helpful to consider the unrolled representation when trying to understand an RNN.\nThe problem with a 10,000-layer neural network is that if and when you get to the 10,000th word of the dataset, you will still need to calculate the derivatives all the way back to the first layer. This is going to be very slow indeed, and very memory-intensive. It is unlikely that you’ll be able to store even one mini-batch on your GPU.\nThe solution to this problem is to tell PyTorch that we do not want to back propagate the derivatives through the entire implicit neural network. Instead, we will just keep the last three layers of gradients. To remove all of the gradient history in PyTorch, we use the detach method.\nHere is the new version of our RNN. It is now stateful, because it remembers its activations between different calls to forward, which represent its use for different samples in the batch:\n\nclass LMModel3(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        self.h = 0\n        \n    def forward(self, x):\n        for i in range(3):\n            self.h = self.h + self.i_h(x[:,i])\n            self.h = F.relu(self.h_h(self.h))\n        out = self.h_o(self.h)\n        self.h = self.h.detach()\n        return out\n    \n    def reset(self): self.h = 0\n\nThis model will have the same activations whatever sequence length we pick, because the hidden state will remember the last activation from the previous batch. The only thing that will be different is the gradients computed at each step: they will only be calculated on sequence length tokens in the past, instead of the whole stream. This approach is called backpropagation through time (BPTT).\n\njargon: Back propagation through time (BPTT): Treating a neural net with effectively one layer per time step (usually refactored using a loop) as one big model, and calculating gradients on it in the usual way. To avoid running out of memory and time, we usually use truncated BPTT, which “detaches” the history of computation steps in the hidden state every few time steps.\n\nTo use LMModel3, we need to make sure the samples are going to be seen in a certain order. As we saw in &lt;&gt;, if the first line of the first batch is our dset[0] then the second batch should have dset[1] as the first line, so that the model sees the text flowing.\nLMDataLoader was doing this for us in &lt;&gt;. This time we’re going to do it ourselves.\nTo do this, we are going to rearrange our dataset. First we divide the samples into m = len(dset) // bs groups (this is the equivalent of splitting the whole concatenated dataset into, for example, 64 equally sized pieces, since we’re using bs=64 here). m is the length of each of these pieces. For instance, if we’re using our whole dataset (although we’ll actually split it into train versus valid in a moment), that will be:\n\nm = len(seqs)//bs\nm,bs,len(seqs)\n\n(328, 64, 21031)\n\n\nThe first batch will be composed of the samples:\n(0, m, 2*m, ..., (bs-1)*m)\nthe second batch of the samples:\n(1, m+1, 2*m+1, ..., (bs-1)*m+1)\nand so forth. This way, at each epoch, the model will see a chunk of contiguous text of size 3*m (since each text is of size 3) on each line of the batch.\nThe following function does that reindexing:\n\ndef group_chunks(ds, bs):\n    m = len(ds) // bs\n    new_ds = L()\n    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n    return new_ds\n\nThen we just pass drop_last=True when building our DataLoaders to drop the last batch that does not have a shape of bs. We also pass shuffle=False to make sure the texts are read in order:\n\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(\n    group_chunks(seqs[:cut], bs), \n    group_chunks(seqs[cut:], bs), \n    bs=bs, drop_last=True, shuffle=False)\n\nThe last thing we add is a little tweak of the training loop via a Callback. We will talk more about callbacks in &lt;&gt;; this one will call the reset method of our model at the beginning of each epoch and before each validation phase. Since we implemented that method to zero the hidden state of the model, this will make sure we start with a clean state before reading those continuous chunks of text. We can also start training a bit longer:\n\nlearn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy,\n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(10, 3e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.677074\n1.827367\n0.467548\n00:02\n\n\n1\n1.282722\n1.870913\n0.388942\n00:02\n\n\n2\n1.090705\n1.651793\n0.462500\n00:02\n\n\n3\n1.005092\n1.613794\n0.516587\n00:02\n\n\n4\n0.965975\n1.560775\n0.551202\n00:02\n\n\n5\n0.916182\n1.595857\n0.560577\n00:02\n\n\n6\n0.897657\n1.539733\n0.574279\n00:02\n\n\n7\n0.836274\n1.585141\n0.583173\n00:02\n\n\n8\n0.805877\n1.629808\n0.586779\n00:02\n\n\n9\n0.795096\n1.651267\n0.588942\n00:02\n\n\n\n\n\nThis is already better! The next step is to use more targets and compare them to the intermediate predictions.\n\n\nCreating More Signal\nAnother problem with our current approach is that we only predict one output word for each three input words. That means that the amount of signal that we are feeding back to update weights with is not as large as it could be. It would be better if we predicted the next word after every single word, rather than every three words, as shown in &lt;&gt;.\n\nThis is easy enough to add. We need to first change our data so that the dependent variable has each of the three next words after each of our three input words. Instead of 3, we use an attribute, sl (for sequence length), and make it a bit bigger:\n\nsl = 16\nseqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n         for i in range(0,len(nums)-sl-1,sl))\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n                             group_chunks(seqs[cut:], bs),\n                             bs=bs, drop_last=True, shuffle=False)\n\nLooking at the first element of seqs, we can see that it contains two lists of the same size. The second list is the same as the first, but offset by one element:\n\n[L(vocab[o] for o in s) for s in seqs[0]]\n\n[(#16) ['one','.','two','.','three','.','four','.','five','.'...],\n (#16) ['.','two','.','three','.','four','.','five','.','six'...]]\n\n\nNow we need to modify our model so that it outputs a prediction after every word, rather than just at the end of a three-word sequence:\n\nclass LMModel4(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        self.h = 0\n        \n    def forward(self, x):\n        outs = []\n        for i in range(sl):\n            self.h = self.h + self.i_h(x[:,i])\n            self.h = F.relu(self.h_h(self.h))\n            outs.append(self.h_o(self.h))\n        self.h = self.h.detach()\n        return torch.stack(outs, dim=1)\n    \n    def reset(self): self.h = 0\n\nThis model will return outputs of shape bs x sl x vocab_sz (since we stacked on dim=1). Our targets are of shape bs x sl, so we need to flatten those before using them in F.cross_entropy:\n\ndef loss_func(inp, targ):\n    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))\n\nWe can now use this loss function to train the model:\n\nlearn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func,\n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n3.103298\n2.874341\n0.212565\n00:01\n\n\n1\n2.231964\n1.971280\n0.462158\n00:01\n\n\n2\n1.711358\n1.813547\n0.461182\n00:01\n\n\n3\n1.448516\n1.828176\n0.483236\n00:01\n\n\n4\n1.288630\n1.659564\n0.520671\n00:01\n\n\n5\n1.161470\n1.714023\n0.554932\n00:01\n\n\n6\n1.055568\n1.660916\n0.575033\n00:01\n\n\n7\n0.960765\n1.719624\n0.591064\n00:01\n\n\n8\n0.870153\n1.839560\n0.614665\n00:01\n\n\n9\n0.808545\n1.770278\n0.624349\n00:01\n\n\n10\n0.758084\n1.842931\n0.610758\n00:01\n\n\n11\n0.719320\n1.799527\n0.646566\n00:01\n\n\n12\n0.683439\n1.917928\n0.649821\n00:01\n\n\n13\n0.660283\n1.874712\n0.628581\n00:01\n\n\n14\n0.646154\n1.877519\n0.640055\n00:01\n\n\n\n\n\nWe need to train for longer, since the task has changed a bit and is more complicated now. But we end up with a good result… At least, sometimes. If you run it a few times, you’ll see that you can get quite different results on different runs. That’s because effectively we have a very deep network here, which can result in very large or very small gradients. We’ll see in the next part of this chapter how to deal with this.\nNow, the obvious way to get a better model is to go deeper: we only have one linear layer between the hidden state and the output activations in our basic RNN, so maybe we’ll get better results with more."
  },
  {
    "objectID": "Fastbook/12_nlp_dive.html#multilayer-rnns",
    "href": "Fastbook/12_nlp_dive.html#multilayer-rnns",
    "title": "A Language Model from Scratch",
    "section": "Multilayer RNNs",
    "text": "Multilayer RNNs\nIn a multilayer RNN, we pass the activations from our recurrent neural network into a second recurrent neural network, like in &lt;&gt;.\n\nThe unrolled representation is shown in &lt;&gt; (similar to &lt;&gt;).\n\nLet’s see how to implement this in practice.\n\nThe Model\nWe can save some time by using PyTorch’s RNN class, which implements exactly what we created earlier, but also gives us the option to stack multiple RNNs, as we have discussed:\n\nclass LMModel5(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h = torch.zeros(n_layers, bs, n_hidden)\n        \n    def forward(self, x):\n        res,h = self.rnn(self.i_h(x), self.h)\n        self.h = h.detach()\n        return self.h_o(res)\n    \n    def reset(self): self.h.zero_()\n\n\nlearn = Learner(dls, LMModel5(len(vocab), 64, 2), \n                loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n3.055853\n2.591640\n0.437907\n00:01\n\n\n1\n2.162359\n1.787310\n0.471598\n00:01\n\n\n2\n1.710663\n1.941807\n0.321777\n00:01\n\n\n3\n1.520783\n1.999726\n0.312012\n00:01\n\n\n4\n1.330846\n2.012902\n0.413249\n00:01\n\n\n5\n1.163297\n1.896192\n0.450684\n00:01\n\n\n6\n1.033813\n2.005209\n0.434814\n00:01\n\n\n7\n0.919090\n2.047083\n0.456706\n00:01\n\n\n8\n0.822939\n2.068031\n0.468831\n00:01\n\n\n9\n0.750180\n2.136064\n0.475098\n00:01\n\n\n10\n0.695120\n2.139140\n0.485433\n00:01\n\n\n11\n0.655752\n2.155081\n0.493652\n00:01\n\n\n12\n0.629650\n2.162583\n0.498535\n00:01\n\n\n13\n0.613583\n2.171649\n0.491048\n00:01\n\n\n14\n0.604309\n2.180355\n0.487874\n00:01\n\n\n\n\n\nNow that’s disappointing… our previous single-layer RNN performed better. Why? The reason is that we have a deeper model, leading to exploding or vanishing activations.\n\n\nExploding or Disappearing Activations\nIn practice, creating accurate models from this kind of RNN is difficult. We will get better results if we call detach less often, and have more layers—this gives our RNN a longer time horizon to learn from, and richer features to create. But it also means we have a deeper model to train. The key challenge in the development of deep learning has been figuring out how to train these kinds of models.\nThe reason this is challenging is because of what happens when you multiply by a matrix many times. Think about what happens when you multiply by a number many times. For example, if you multiply by 2, starting at 1, you get the sequence 1, 2, 4, 8,… after 32 steps you are already at 4,294,967,296. A similar issue happens if you multiply by 0.5: you get 0.5, 0.25, 0.125… and after 32 steps it’s 0.00000000023. As you can see, multiplying by a number even slightly higher or lower than 1 results in an explosion or disappearance of our starting number, after just a few repeated multiplications.\nBecause matrix multiplication is just multiplying numbers and adding them up, exactly the same thing happens with repeated matrix multiplications. And that’s all a deep neural network is —each extra layer is another matrix multiplication. This means that it is very easy for a deep neural network to end up with extremely large or extremely small numbers.\nThis is a problem, because the way computers store numbers (known as “floating point”) means that they become less and less accurate the further away the numbers get from zero. The diagram in &lt;&gt;, from the excellent article “What You Never Wanted to Know About Floating Point but Will Be Forced to Find Out”, shows how the precision of floating-point numbers varies over the number line.\n\nThis inaccuracy means that often the gradients calculated for updating the weights end up as zero or infinity for deep networks. This is commonly referred to as the vanishing gradients or exploding gradients problem. It means that in SGD, the weights are either not updated at all or jump to infinity. Either way, they won’t improve with training.\nResearchers have developed a number of ways to tackle this problem, which we will be discussing later in the book. One option is to change the definition of a layer in a way that makes it less likely to have exploding activations. We’ll look at the details of how this is done in &lt;&gt;, when we discuss batch normalization, and &lt;&gt;, when we discuss ResNets, although these details don’t generally matter in practice (unless you are a researcher that is creating new approaches to solving this problem). Another strategy for dealing with this is by being careful about initialization, which is a topic we’ll investigate in &lt;&gt;.\nFor RNNs, there are two types of layers that are frequently used to avoid exploding activations: gated recurrent units (GRUs) and long short-term memory (LSTM) layers. Both of these are available in PyTorch, and are drop-in replacements for the RNN layer. We will only cover LSTMs in this book; there are plenty of good tutorials online explaining GRUs, which are a minor variant on the LSTM design."
  },
  {
    "objectID": "Fastbook/12_nlp_dive.html#lstm",
    "href": "Fastbook/12_nlp_dive.html#lstm",
    "title": "A Language Model from Scratch",
    "section": "LSTM",
    "text": "LSTM\nLSTM is an architecture that was introduced back in 1997 by Jürgen Schmidhuber and Sepp Hochreiter. In this architecture, there are not one but two hidden states. In our base RNN, the hidden state is the output of the RNN at the previous time step. That hidden state is then responsible for two things:\n\nHaving the right information for the output layer to predict the correct next token\nRetaining memory of everything that happened in the sentence\n\nConsider, for example, the sentences “Henry has a dog and he likes his dog very much” and “Sophie has a dog and she likes her dog very much.” It’s very clear that the RNN needs to remember the name at the beginning of the sentence to be able to predict he/she or his/her.\nIn practice, RNNs are really bad at retaining memory of what happened much earlier in the sentence, which is the motivation to have another hidden state (called cell state) in the LSTM. The cell state will be responsible for keeping long short-term memory, while the hidden state will focus on the next token to predict. Let’s take a closer look at how this is achieved and build an LSTM from scratch.\n\nBuilding an LSTM from Scratch\nIn order to build an LSTM, we first have to understand its architecture. &lt;&gt; shows its inner structure.\n\nIn this picture, our input \\(x_{t}\\) enters on the left with the previous hidden state (\\(h_{t-1}\\)) and cell state (\\(c_{t-1}\\)). The four orange boxes represent four layers (our neural nets) with the activation being either sigmoid (\\(\\sigma\\)) or tanh. tanh is just a sigmoid function rescaled to the range -1 to 1. Its mathematical expression can be written like this:\n\\[\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x}+e^{-x}} = 2 \\sigma(2x) - 1\\]\nwhere \\(\\sigma\\) is the sigmoid function. The green circles are elementwise operations. What goes out on the right is the new hidden state (\\(h_{t}\\)) and new cell state (\\(c_{t}\\)), ready for our next input. The new hidden state is also used as output, which is why the arrow splits to go up.\nLet’s go over the four neural nets (called gates) one by one and explain the diagram—but before this, notice how very little the cell state (at the top) is changed. It doesn’t even go directly through a neural net! This is exactly why it will carry on a longer-term state.\nFirst, the arrows for input and old hidden state are joined together. In the RNN we wrote earlier in this chapter, we were adding them together. In the LSTM, we stack them in one big tensor. This means the dimension of our embeddings (which is the dimension of \\(x_{t}\\)) can be different than the dimension of our hidden state. If we call those n_in and n_hid, the arrow at the bottom is of size n_in + n_hid; thus all the neural nets (orange boxes) are linear layers with n_in + n_hid inputs and n_hid outputs.\nThe first gate (looking from left to right) is called the forget gate. Since it’s a linear layer followed by a sigmoid, its output will consist of scalars between 0 and 1. We multiply this result by the cell state to determine which information to keep and which to throw away: values closer to 0 are discarded and values closer to 1 are kept. This gives the LSTM the ability to forget things about its long-term state. For instance, when crossing a period or an xxbos token, we would expect to it to (have learned to) reset its cell state.\nThe second gate is called the input gate. It works with the third gate (which doesn’t really have a name but is sometimes called the cell gate) to update the cell state. For instance, we may see a new gender pronoun, in which case we’ll need to replace the information about gender that the forget gate removed. Similar to the forget gate, the input gate decides which elements of the cell state to update (values close to 1) or not (values close to 0). The third gate determines what those updated values are, in the range of –1 to 1 (thanks to the tanh function). The result is then added to the cell state.\nThe last gate is the output gate. It determines which information from the cell state to use to generate the output. The cell state goes through a tanh before being combined with the sigmoid output from the output gate, and the result is the new hidden state.\nIn terms of code, we can write the same steps like this:\n\nclass LSTMCell(Module):\n    def __init__(self, ni, nh):\n        self.forget_gate = nn.Linear(ni + nh, nh)\n        self.input_gate  = nn.Linear(ni + nh, nh)\n        self.cell_gate   = nn.Linear(ni + nh, nh)\n        self.output_gate = nn.Linear(ni + nh, nh)\n\n    def forward(self, input, state):\n        h,c = state\n        h = torch.cat([h, input], dim=1)\n        forget = torch.sigmoid(self.forget_gate(h))\n        c = c * forget\n        inp = torch.sigmoid(self.input_gate(h))\n        cell = torch.tanh(self.cell_gate(h))\n        c = c + inp * cell\n        out = torch.sigmoid(self.output_gate(h))\n        h = out * torch.tanh(c)\n        return h, (h,c)\n\nIn practice, we can then refactor the code. Also, in terms of performance, it’s better to do one big matrix multiplication than four smaller ones (that’s because we only launch the special fast kernel on the GPU once, and it gives the GPU more work to do in parallel). The stacking takes a bit of time (since we have to move one of the tensors around on the GPU to have it all in a contiguous array), so we use two separate layers for the input and the hidden state. The optimized and refactored code then looks like this:\n\nclass LSTMCell(Module):\n    def __init__(self, ni, nh):\n        self.ih = nn.Linear(ni,4*nh)\n        self.hh = nn.Linear(nh,4*nh)\n\n    def forward(self, input, state):\n        h,c = state\n        # One big multiplication for all the gates is better than 4 smaller ones\n        gates = (self.ih(input) + self.hh(h)).chunk(4, 1)\n        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n        cellgate = gates[3].tanh()\n\n        c = (forgetgate*c) + (ingate*cellgate)\n        h = outgate * c.tanh()\n        return h, (h,c)\n\nHere we use the PyTorch chunk method to split our tensor into four pieces. It works like this:\n\nt = torch.arange(0,10); t\n\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\nt.chunk(2)\n\n(tensor([0, 1, 2, 3, 4]), tensor([5, 6, 7, 8, 9]))\n\n\nLet’s now use this architecture to train a language model!\n\n\nTraining a Language Model Using LSTMs\nHere is the same network as LMModel5, using a two-layer LSTM. We can train it at a higher learning rate, for a shorter time, and get better accuracy:\n\nclass LMModel6(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n        \n    def forward(self, x):\n        res,h = self.rnn(self.i_h(x), self.h)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(res)\n    \n    def reset(self): \n        for h in self.h: h.zero_()\n\n\nlearn = Learner(dls, LMModel6(len(vocab), 64, 2), \n                loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 1e-2)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n3.000821\n2.663942\n0.438314\n00:02\n\n\n1\n2.139642\n2.184780\n0.240479\n00:02\n\n\n2\n1.607275\n1.812682\n0.439779\n00:02\n\n\n3\n1.347711\n1.830982\n0.497477\n00:02\n\n\n4\n1.123113\n1.937766\n0.594401\n00:02\n\n\n5\n0.852042\n2.012127\n0.631592\n00:02\n\n\n6\n0.565494\n1.312742\n0.725749\n00:02\n\n\n7\n0.347445\n1.297934\n0.711263\n00:02\n\n\n8\n0.208191\n1.441269\n0.731201\n00:02\n\n\n9\n0.126335\n1.569952\n0.737305\n00:02\n\n\n10\n0.079761\n1.427187\n0.754150\n00:02\n\n\n11\n0.052990\n1.494990\n0.745117\n00:02\n\n\n12\n0.039008\n1.393731\n0.757894\n00:02\n\n\n13\n0.031502\n1.373210\n0.758464\n00:02\n\n\n14\n0.028068\n1.368083\n0.758464\n00:02\n\n\n\n\n\nNow that’s better than a multilayer RNN! We can still see there is a bit of overfitting, however, which is a sign that a bit of regularization might help."
  },
  {
    "objectID": "Fastbook/12_nlp_dive.html#regularizing-an-lstm",
    "href": "Fastbook/12_nlp_dive.html#regularizing-an-lstm",
    "title": "A Language Model from Scratch",
    "section": "Regularizing an LSTM",
    "text": "Regularizing an LSTM\nRecurrent neural networks, in general, are hard to train, because of the problem of vanishing activations and gradients we saw before. Using LSTM (or GRU) cells makes training easier than with vanilla RNNs, but they are still very prone to overfitting. Data augmentation, while a possibility, is less often used for text data than for images because in most cases it requires another model to generate random augmentations (e.g., by translating the text into another language and then back into the original language). Overall, data augmentation for text data is currently not a well-explored space.\nHowever, there are other regularization techniques we can use instead to reduce overfitting, which were thoroughly studied for use with LSTMs in the paper “Regularizing and Optimizing LSTM Language Models” by Stephen Merity, Nitish Shirish Keskar, and Richard Socher. This paper showed how effective use of dropout, activation regularization, and temporal activation regularization could allow an LSTM to beat state-of-the-art results that previously required much more complicated models. The authors called an LSTM using these techniques an AWD-LSTM. We’ll look at each of these techniques in turn.\n\nDropout\nDropout is a regularization technique that was introduced by Geoffrey Hinton et al. in Improving neural networks by preventing co-adaptation of feature detectors. The basic idea is to randomly change some activations to zero at training time. This makes sure all neurons actively work toward the output, as seen in &lt;&gt; (from “Dropout: A Simple Way to Prevent Neural Networks from Overfitting” by Nitish Srivastava et al.).\n\nHinton used a nice metaphor when he explained, in an interview, the inspiration for dropout:\n\n: I went to my bank. The tellers kept changing and I asked one of them why. He said he didn’t know but they got moved around a lot. I figured it must be because it would require cooperation between employees to successfully defraud the bank. This made me realize that randomly removing a different subset of neurons on each example would prevent conspiracies and thus reduce overfitting.\n\nIn the same interview, he also explained that neuroscience provided additional inspiration:\n\n: We don’t really know why neurons spike. One theory is that they want to be noisy so as to regularize, because we have many more parameters than we have data points. The idea of dropout is that if you have noisy activations, you can afford to use a much bigger model.\n\nThis explains the idea behind why dropout helps to generalize: first it helps the neurons to cooperate better together, then it makes the activations more noisy, thus making the model more robust.\nWe can see, however, that if we were to just zero those activations without doing anything else, our model would have problems training: if we go from the sum of five activations (that are all positive numbers since we apply a ReLU) to just two, this won’t have the same scale. Therefore, if we apply dropout with a probability p, we rescale all activations by dividing them by 1-p (on average p will be zeroed, so it leaves 1-p), as shown in &lt;&gt;.\n\nThis is a full implementation of the dropout layer in PyTorch (although PyTorch’s native layer is actually written in C, not Python):\n\nclass Dropout(Module):\n    def __init__(self, p): self.p = p\n    def forward(self, x):\n        if not self.training: return x\n        mask = x.new(*x.shape).bernoulli_(1-p)\n        return x * mask.div_(1-p)\n\nThe bernoulli_ method is creating a tensor of random zeros (with probability p) and ones (with probability 1-p), which is then multiplied with our input before dividing by 1-p. Note the use of the training attribute, which is available in any PyTorch nn.Module, and tells us if we are doing training or inference.\n\nnote: Do Your Own Experiments: In previous chapters of the book we’d be adding a code example for bernoulli_ here, so you can see exactly how it works. But now that you know enough to do this yourself, we’re going to be doing fewer and fewer examples for you, and instead expecting you to do your own experiments to see how things work. In this case, you’ll see in the end-of-chapter questionnaire that we’re asking you to experiment with bernoulli_—but don’t wait for us to ask you to experiment to develop your understanding of the code we’re studying; go ahead and do it anyway!\n\nUsing dropout before passing the output of our LSTM to the final layer will help reduce overfitting. Dropout is also used in many other models, including the default CNN head used in fastai.vision, and is available in fastai.tabular by passing the ps parameter (where each “p” is passed to each added Dropout layer), as we’ll see in &lt;&gt;.\nDropout has different behavior in training and validation mode, which we specified using the training attribute in Dropout. Calling the train method on a Module sets training to True (both for the module you call the method on and for every module it recursively contains), and eval sets it to False. This is done automatically when calling the methods of Learner, but if you are not using that class, remember to switch from one to the other as needed.\n\n\nActivation Regularization and Temporal Activation Regularization\nActivation regularization (AR) and temporal activation regularization (TAR) are two regularization methods very similar to weight decay, discussed in &lt;&gt;. When applying weight decay, we add a small penalty to the loss that aims at making the weights as small as possible. For activation regularization, it’s the final activations produced by the LSTM that we will try to make as small as possible, instead of the weights.\nTo regularize the final activations, we have to store those somewhere, then add the means of the squares of them to the loss (along with a multiplier alpha, which is just like wd for weight decay):\nloss += alpha * activations.pow(2).mean()\nTemporal activation regularization is linked to the fact we are predicting tokens in a sentence. That means it’s likely that the outputs of our LSTMs should somewhat make sense when we read them in order. TAR is there to encourage that behavior by adding a penalty to the loss to make the difference between two consecutive activations as small as possible: our activations tensor has a shape bs x sl x n_hid, and we read consecutive activations on the sequence length axis (the dimension in the middle). With this, TAR can be expressed as:\nloss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean()\nalpha and beta are then two hyperparameters to tune. To make this work, we need our model with dropout to return three things: the proper output, the activations of the LSTM pre-dropout, and the activations of the LSTM post-dropout. AR is often applied on the dropped-out activations (to not penalize the activations we turned into zeros afterward) while TAR is applied on the non-dropped-out activations (because those zeros create big differences between two consecutive time steps). There is then a callback called RNNRegularizer that will apply this regularization for us.\n\n\nTraining a Weight-Tied Regularized LSTM\nWe can combine dropout (applied before we go into our output layer) with AR and TAR to train our previous LSTM. We just need to return three things instead of one: the normal output of our LSTM, the dropped-out activations, and the activations from our LSTMs. The last two will be picked up by the callback RNNRegularization for the contributions it has to make to the loss.\nAnother useful trick we can add from the AWD LSTM paper is weight tying. In a language model, the input embeddings represent a mapping from English words to activations, and the output hidden layer represents a mapping from activations to English words. We might expect, intuitively, that these mappings could be the same. We can represent this in PyTorch by assigning the same weight matrix to each of these layers:\nself.h_o.weight = self.i_h.weight\nIn LMModel7, we include these final tweaks:\n\nclass LMModel7(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.drop = nn.Dropout(p)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h_o.weight = self.i_h.weight\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n        \n    def forward(self, x):\n        raw,h = self.rnn(self.i_h(x), self.h)\n        out = self.drop(raw)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(out),raw,out\n    \n    def reset(self): \n        for h in self.h: h.zero_()\n\nWe can create a regularized Learner using the RNNRegularizer callback:\n\nlearn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5),\n                loss_func=CrossEntropyLossFlat(), metrics=accuracy,\n                cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)])\n\nA TextLearner automatically adds those two callbacks for us (with those values for alpha and beta as defaults), so we can simplify the preceding line to:\n\nlearn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),\n                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n\nWe can then train the model, and add additional regularization by increasing the weight decay to 0.1:\n\nlearn.fit_one_cycle(15, 1e-2, wd=0.1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.693885\n2.013484\n0.466634\n00:02\n\n\n1\n1.685549\n1.187310\n0.629313\n00:02\n\n\n2\n0.973307\n0.791398\n0.745605\n00:02\n\n\n3\n0.555823\n0.640412\n0.794108\n00:02\n\n\n4\n0.351802\n0.557247\n0.836100\n00:02\n\n\n5\n0.244986\n0.594977\n0.807292\n00:02\n\n\n6\n0.192231\n0.511690\n0.846761\n00:02\n\n\n7\n0.162456\n0.520370\n0.858073\n00:02\n\n\n8\n0.142664\n0.525918\n0.842285\n00:02\n\n\n9\n0.128493\n0.495029\n0.858073\n00:02\n\n\n10\n0.117589\n0.464236\n0.867188\n00:02\n\n\n11\n0.109808\n0.466550\n0.869303\n00:02\n\n\n12\n0.104216\n0.455151\n0.871826\n00:02\n\n\n13\n0.100271\n0.452659\n0.873617\n00:02\n\n\n14\n0.098121\n0.458372\n0.869385\n00:02\n\n\n\n\n\nNow this is far better than our previous model!"
  },
  {
    "objectID": "Fastbook/12_nlp_dive.html#conclusion",
    "href": "Fastbook/12_nlp_dive.html#conclusion",
    "title": "A Language Model from Scratch",
    "section": "Conclusion",
    "text": "Conclusion\nYou have now seen everything that is inside the AWD-LSTM architecture we used in text classification in &lt;&gt;. It uses dropout in a lot more places:\n\nEmbedding dropout (inside the embedding layer, drops some random lines of embeddings)\nInput dropout (applied after the embedding layer)\nWeight dropout (applied to the weights of the LSTM at each training step)\nHidden dropout (applied to the hidden state between two layers)\n\nThis makes it even more regularized. Since fine-tuning those five dropout values (including the dropout before the output layer) is complicated, we have determined good defaults and allow the magnitude of dropout to be tuned overall with the drop_mult parameter you saw in that chapter (which is multiplied by each dropout).\nAnother architecture that is very powerful, especially in “sequence-to-sequence” problems (that is, problems where the dependent variable is itself a variable-length sequence, such as language translation), is the Transformers architecture. You can find it in a bonus chapter on the book’s website."
  },
  {
    "objectID": "Fastbook/12_nlp_dive.html#questionnaire",
    "href": "Fastbook/12_nlp_dive.html#questionnaire",
    "title": "A Language Model from Scratch",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nIf the dataset for your project is so big and complicated that working with it takes a significant amount of time, what should you do?\nWhy do we concatenate the documents in our dataset before creating a language model?\nTo use a standard fully connected network to predict the fourth word given the previous three words, what two tweaks do we need to make to our model?\nHow can we share a weight matrix across multiple layers in PyTorch?\nWrite a module that predicts the third word given the previous two words of a sentence, without peeking.\nWhat is a recurrent neural network?\nWhat is “hidden state”?\nWhat is the equivalent of hidden state in LMModel1?\nTo maintain the state in an RNN, why is it important to pass the text to the model in order?\nWhat is an “unrolled” representation of an RNN?\nWhy can maintaining the hidden state in an RNN lead to memory and performance problems? How do we fix this problem?\nWhat is “BPTT”?\nWrite code to print out the first few batches of the validation set, including converting the token IDs back into English strings, as we showed for batches of IMDb data in &lt;&gt;.\nWhat does the ModelResetter callback do? Why do we need it?\nWhat are the downsides of predicting just one output word for each three input words?\nWhy do we need a custom loss function for LMModel4?\nWhy is the training of LMModel4 unstable?\nIn the unrolled representation, we can see that a recurrent neural network actually has many layers. So why do we need to stack RNNs to get better results?\nDraw a representation of a stacked (multilayer) RNN.\nWhy should we get better results in an RNN if we call detach less often? Why might this not happen in practice with a simple RNN?\nWhy can a deep network result in very large or very small activations? Why does this matter?\nIn a computer’s floating-point representation of numbers, which numbers are the most precise?\nWhy do vanishing gradients prevent training?\nWhy does it help to have two hidden states in the LSTM architecture? What is the purpose of each one?\nWhat are these two states called in an LSTM?\nWhat is tanh, and how is it related to sigmoid?\nWhat is the purpose of this code in LSTMCell: h = torch.cat([h, input], dim=1)\nWhat does chunk do in PyTorch?\nStudy the refactored version of LSTMCell carefully to ensure you understand how and why it does the same thing as the non-refactored version.\nWhy can we use a higher learning rate for LMModel6?\nWhat are the three regularization techniques used in an AWD-LSTM model?\nWhat is “dropout”?\nWhy do we scale the acitvations with dropout? Is this applied during training, inference, or both?\nWhat is the purpose of this line from Dropout: if not self.training: return x\nExperiment with bernoulli_ to understand how it works.\nHow do you set your model in training mode in PyTorch? In evaluation mode?\nWrite the equation for activation regularization (in math or code, as you prefer). How is it different from weight decay?\nWrite the equation for temporal activation regularization (in math or code, as you prefer). Why wouldn’t we use this for computer vision problems?\nWhat is “weight tying” in a language model?\n\n\nFurther Research\n\nIn LMModel2, why can forward start with h=0? Why don’t we need to say h=torch.zeros(...)?\nWrite the code for an LSTM from scratch (you may refer to &lt;&gt;).\nSearch the internet for the GRU architecture and implement it from scratch, and try training a model. See if you can get results similar to those we saw in this chapter. Compare your results to the results of PyTorch’s built in GRU module.\nTake a look at the source code for AWD-LSTM in fastai, and try to map each of the lines of code to the concepts shown in this chapter."
  },
  {
    "objectID": "Fastbook/11_midlevel_data.html",
    "href": "Fastbook/11_midlevel_data.html",
    "title": "Data Munging with fastai’s Mid-Level API",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\nfrom IPython.display import display,HTML\n[[chapter_midlevel_data]]\nWe have seen what Tokenizer and Numericalize do to a collection of texts, and how they’re used inside the data block API, which handles those transforms for us directly using the TextBlock. But what if we want to only apply one of those transforms, either to see intermediate results or because we have already tokenized texts? More generally, what can we do when the data block API is not flexible enough to accommodate our particular use case? For this, we need to use fastai’s mid-level API for processing data. The data block API is built on top of that layer, so it will allow you to do everything the data block API does, and much much more."
  },
  {
    "objectID": "Fastbook/11_midlevel_data.html#going-deeper-into-fastais-layered-api",
    "href": "Fastbook/11_midlevel_data.html#going-deeper-into-fastais-layered-api",
    "title": "Data Munging with fastai’s Mid-Level API",
    "section": "Going Deeper into fastai’s Layered API",
    "text": "Going Deeper into fastai’s Layered API\nThe fastai library is built on a layered API. In the very top layer there are applications that allow us to train a model in five lines of codes, as we saw in &lt;&gt;. In the case of creating DataLoaders for a text classifier, for instance, we used the line:\n\nfrom fastai.text.all import *\n\ndls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\n\nThe factory method TextDataLoaders.from_folder is very convenient when your data is arranged the exact same way as the IMDb dataset, but in practice, that often won’t be the case. The data block API offers more flexibility. As we saw in the last chapter, we can get the same result with:\n\npath = untar_data(URLs.IMDB)\ndls = DataBlock(\n    blocks=(TextBlock.from_folder(path),CategoryBlock),\n    get_y = parent_label,\n    get_items=partial(get_text_files, folders=['train', 'test']),\n    splitter=GrandparentSplitter(valid_name='test')\n).dataloaders(path)\n\nBut it’s sometimes not flexible enough. For debugging purposes, for instance, we might need to apply just parts of the transforms that come with this data block. Or we might want to create a DataLoaders for some application that isn’t directly supported by fastai. In this section, we’ll dig into the pieces that are used inside fastai to implement the data block API. Understanding these will enable you to leverage the power and flexibility of this mid-tier API.\n\nnote: Mid-Level API: The mid-level API does not only contain functionality for creating DataLoaders. It also has the callback system, which allows us to customize the training loop any way we like, and the general optimizer. Both will be covered in &lt;&gt;.\n\n\nTransforms\nWhen we studied tokenization and numericalization in the last chapter, we started by grabbing a bunch of texts:\n\nfiles = get_text_files(path, folders = ['train', 'test'])\ntxts = L(o.open().read() for o in files[:2000])\n\nWe then showed how to tokenize them with a Tokenizer:\n\ntok = Tokenizer.from_folder(path)\ntok.setup(txts)\ntoks = txts.map(tok)\ntoks[0]\n\n(#374) ['xxbos','xxmaj','well',',','\"','cube','\"','(','1997',')'...]\n\n\nand how to numericalize, including automatically creating the vocab for our corpus:\n\nnum = Numericalize()\nnum.setup(toks)\nnums = toks.map(num)\nnums[0][:10]\n\ntensor([   2,    8,   76,   10,   23, 3112,   23,   34, 3113,   33])\n\n\nThe classes also have a decode method. For instance, Numericalize.decode gives us back the string tokens:\n\nnums_dec = num.decode(nums[0][:10]); nums_dec\n\n(#10) ['xxbos','xxmaj','well',',','\"','cube','\"','(','1997',')']\n\n\nand Tokenizer.decode turns this back into a single string (it may not, however, be exactly the same as the original string; this depends on whether the tokenizer is reversible, which the default word tokenizer is not at the time we’re writing this book):\n\ntok.decode(nums_dec)\n\n'xxbos xxmaj well , \" cube \" ( 1997 )'\n\n\ndecode is used by fastai’s show_batch and show_results, as well as some other inference methods, to convert predictions and mini-batches into a human-understandable representation.\nFor each of tok or num in the preceding example, we created an object, called the setup method (which trains the tokenizer if needed for tok and creates the vocab for num), applied it to our raw texts (by calling the object as a function), and then finally decoded the result back to an understandable representation. These steps are needed for most data preprocessing tasks, so fastai provides a class that encapsulates them. This is the Transform class. Both Tokenize and Numericalize are Transforms.\nIn general, a Transform is an object that behaves like a function and has an optional setup method that will initialize some inner state (like the vocab inside num) and an optional decode that will reverse the function (this reversal may not be perfect, as we saw with tok).\nA good example of decode is found in the Normalize transform that we saw in &lt;&gt;: to be able to plot the images its decode method undoes the normalization (i.e., it multiplies by the standard deviation and adds back the mean). On the other hand, data augmentation transforms do not have a decode method, since we want to show the effects on images to make sure the data augmentation is working as we want.\nA special behavior of Transforms is that they always get applied over tuples. In general, our data is always a tuple (input,target) (sometimes with more than one input or more than one target). When applying a transform on an item like this, such as Resize, we don’t want to resize the tuple as a whole; instead, we want to resize the input (if applicable) and the target (if applicable) separately. It’s the same for batch transforms that do data augmentation: when the input is an image and the target is a segmentation mask, the transform needs to be applied (the same way) to the input and the target.\nWe can see this behavior if we pass a tuple of texts to tok:\n\ntok((txts[0], txts[1]))\n\n((#374) ['xxbos','xxmaj','well',',','\"','cube','\"','(','1997',')'...],\n (#207) ['xxbos','xxmaj','conrad','xxmaj','hall','went','out','with','a','bang'...])\n\n\n\n\nWriting Your Own Transform\nIf you want to write a custom transform to apply to your data, the easiest way is to write a function. As you can see in this example, a Transform will only be applied to a matching type, if a type is provided (otherwise it will always be applied). In the following code, the :int in the function signature means that f only gets applied to ints. That’s why tfm(2.0) returns 2.0, but tfm(2) returns 3 here:\n\ndef f(x:int): return x+1\ntfm = Transform(f)\ntfm(2),tfm(2.0)\n\n(3, 2.0)\n\n\nHere, f is converted to a Transform with no setup and no decode method.\nPython has a special syntax for passing a function (like f) to another function (or something that behaves like a function, known as a callable in Python), called a decorator. A decorator is used by prepending a callable with @ and placing it before a function definition (there are lots of good online tutorials about Python decorators, so take a look at one if this is a new concept for you). The following is identical to the previous code:\n\n@Transform\ndef f(x:int): return x+1\nf(2),f(2.0)\n\n(3, 2.0)\n\n\nIf you need either setup or decode, you will need to subclass Transform to implement the actual encoding behavior in encodes, then (optionally), the setup behavior in setups and the decoding behavior in decodes:\n\nclass NormalizeMean(Transform):\n    def setups(self, items): self.mean = sum(items)/len(items)\n    def encodes(self, x): return x-self.mean\n    def decodes(self, x): return x+self.mean\n\nHere, NormalizeMean will initialize some state during the setup (the mean of all elements passed), then the transformation is to subtract that mean. For decoding purposes, we implement the reverse of that transformation by adding the mean. Here is an example of NormalizeMean in action:\n\ntfm = NormalizeMean()\ntfm.setup([1,2,3,4,5])\nstart = 2\ny = tfm(start)\nz = tfm.decode(y)\ntfm.mean,y,z\n\n(3.0, -1.0, 2.0)\n\n\nNote that the method called and the method implemented are different, for each of these methods:\n[options=\"header\"]\n|======\n| Class | To call | To implement\n| `nn.Module` (PyTorch) | `()` (i.e., call as function) | `forward`\n| `Transform` | `()` | `encodes`\n| `Transform` | `decode()` | `decodes`\n| `Transform` | `setup()` | `setups`\n|======\nSo, for instance, you would never call setups directly, but instead would call setup. The reason for this is that setup does some work before and after calling setups for you. To learn more about Transforms and how you can use them to implement different behavior depending on the type of the input, be sure to check the tutorials in the fastai docs.\n\n\nPipeline\nTo compose several transforms together, fastai provides the Pipeline class. We define a Pipeline by passing it a list of Transforms; it will then compose the transforms inside it. When you call Pipeline on an object, it will automatically call the transforms inside, in order:\n\ntfms = Pipeline([tok, num])\nt = tfms(txts[0]); t[:20]\n\ntensor([   2,    8,   76,   10,   23, 3112,   23,   34, 3113,   33,   10,    8, 4477,   22,   88,   32,   10,   27,   42,   14])\n\n\nAnd you can call decode on the result of your encoding, to get back something you can display and analyze:\n\ntfms.decode(t)[:100]\n\n'xxbos xxmaj well , \" cube \" ( 1997 ) , xxmaj vincenzo \\'s first movie , was one of the most interesti'\n\n\nThe only part that doesn’t work the same way as in Transform is the setup. To properly set up a Pipeline of Transforms on some data, you need to use a TfmdLists."
  },
  {
    "objectID": "Fastbook/11_midlevel_data.html#tfmdlists-and-datasets-transformed-collections",
    "href": "Fastbook/11_midlevel_data.html#tfmdlists-and-datasets-transformed-collections",
    "title": "Data Munging with fastai’s Mid-Level API",
    "section": "TfmdLists and Datasets: Transformed Collections",
    "text": "TfmdLists and Datasets: Transformed Collections\nYour data is usually a set of raw items (like filenames, or rows in a DataFrame) to which you want to apply a succession of transformations. We just saw that a succession of transformations is represented by a Pipeline in fastai. The class that groups together this Pipeline with your raw items is called TfmdLists.\n\nTfmdLists\nHere is the short way of doing the transformation we saw in the previous section:\n\ntls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize])\n\nAt initialization, the TfmdLists will automatically call the setup method of each Transform in order, providing them not with the raw items but the items transformed by all the previous Transforms in order. We can get the result of our Pipeline on any raw element just by indexing into the TfmdLists:\n\nt = tls[0]; t[:20]\n\ntensor([    2,     8,    91,    11,    22,  5793,    22,    37,  4910,    34,    11,     8, 13042,    23,   107,    30,    11,    25,    44,    14])\n\n\nAnd the TfmdLists knows how to decode for show purposes:\n\ntls.decode(t)[:100]\n\n'xxbos xxmaj well , \" cube \" ( 1997 ) , xxmaj vincenzo \\'s first movie , was one of the most interesti'\n\n\nIn fact, it even has a show method:\n\ntls.show(t)\n\nxxbos xxmaj well , \" cube \" ( 1997 ) , xxmaj vincenzo 's first movie , was one of the most interesting and tricky ideas that xxmaj i 've ever seen when talking about movies . xxmaj they had just one scenery , a bunch of actors and a plot . xxmaj so , what made it so special were all the effective direction , great dialogs and a bizarre condition that characters had to deal like rats in a labyrinth . xxmaj his second movie , \" cypher \" ( 2002 ) , was all about its story , but it was n't so good as \" cube \" but here are the characters being tested like rats again . \n\n \" nothing \" is something very interesting and gets xxmaj vincenzo coming back to his ' cube days ' , locking the characters once again in a very different space with no time once more playing with the characters like playing with rats in an experience room . xxmaj but instead of a thriller sci - fi ( even some of the promotional teasers and trailers erroneous seemed like that ) , \" nothing \" is a loose and light comedy that for sure can be called a modern satire about our society and also about the intolerant world we 're living . xxmaj once again xxmaj xxunk amaze us with a great idea into a so small kind of thing . 2 actors and a blinding white scenario , that 's all you got most part of time and you do n't need more than that . xxmaj while \" cube \" is a claustrophobic experience and \" cypher \" confusing , \" nothing \" is completely the opposite but at the same time also desperate . \n\n xxmaj this movie proves once again that a smart idea means much more than just a millionaire budget . xxmaj of course that the movie fails sometimes , but its prime idea means a lot and offsets any flaws . xxmaj there 's nothing more to be said about this movie because everything is a brilliant surprise and a totally different experience that i had in movies since \" cube \" .\n\n\nThe TfmdLists is named with an “s” because it can handle a training and a validation set with a splits argument. You just need to pass the indices of which elements are in the training set, and which are in the validation set:\n\ncut = int(len(files)*0.8)\nsplits = [list(range(cut)), list(range(cut,len(files)))]\ntls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize], \n                splits=splits)\n\nYou can then access them through the train and valid attributes:\n\ntls.valid[0][:20]\n\ntensor([    2,     8,    20,    30,    87,   510,  1570,    12,   408,   379,  4196,    10,     8,    20,    30,    16,    13, 12216,   202,   509])\n\n\nIf you have manually written a Transform that performs all of your preprocessing at once, turning raw items into a tuple with inputs and targets, then TfmdLists is the class you need. You can directly convert it to a DataLoaders object with the dataloaders method. This is what we will do in our Siamese example later in this chapter.\nIn general, though, you will have two (or more) parallel pipelines of transforms: one for processing your raw items into inputs and one to process your raw items into targets. For instance, here, the pipeline we defined only processes the raw text into inputs. If we want to do text classification, we also have to process the labels into targets.\nFor this we need to do two things. First we take the label name from the parent folder. There is a function, parent_label, for this:\n\nlbls = files.map(parent_label)\nlbls\n\n(#50000) ['pos','pos','pos','pos','pos','pos','pos','pos','pos','pos'...]\n\n\nThen we need a Transform that will grab the unique items and build a vocab with them during setup, then transform the string labels into integers when called. fastai provides this for us; it’s called Categorize:\n\ncat = Categorize()\ncat.setup(lbls)\ncat.vocab, cat(lbls[0])\n\n((#2) ['neg','pos'], TensorCategory(1))\n\n\nTo do the whole setup automatically on our list of files, we can create a TfmdLists as before:\n\ntls_y = TfmdLists(files, [parent_label, Categorize()])\ntls_y[0]\n\nTensorCategory(1)\n\n\nBut then we end up with two separate objects for our inputs and targets, which is not what we want. This is where Datasets comes to the rescue.\n\n\nDatasets\nDatasets will apply two (or more) pipelines in parallel to the same raw object and build a tuple with the result. Like TfmdLists, it will automatically do the setup for us, and when we index into a Datasets, it will return us a tuple with the results of each pipeline:\n\nx_tfms = [Tokenizer.from_folder(path), Numericalize]\ny_tfms = [parent_label, Categorize()]\ndsets = Datasets(files, [x_tfms, y_tfms])\nx,y = dsets[0]\nx[:20],y\n\nLike a TfmdLists, we can pass along splits to a Datasets to split our data between training and validation sets:\n\nx_tfms = [Tokenizer.from_folder(path), Numericalize]\ny_tfms = [parent_label, Categorize()]\ndsets = Datasets(files, [x_tfms, y_tfms], splits=splits)\nx,y = dsets.valid[0]\nx[:20],y\n\n(tensor([    2,     8,    20,    30,    87,   510,  1570,    12,   408,   379,  4196,    10,     8,    20,    30,    16,    13, 12216,   202,   509]),\n TensorCategory(0))\n\n\nIt can also decode any processed tuple or show it directly:\n\nt = dsets.valid[0]\ndsets.decode(t)\n\n('xxbos xxmaj this movie had horrible lighting and terrible camera movements . xxmaj this movie is a jumpy horror flick with no meaning at all . xxmaj the slashes are totally fake looking . xxmaj it looks like some 17 year - old idiot wrote this movie and a 10 year old kid shot it . xxmaj with the worst acting you can ever find . xxmaj people are tired of knives . xxmaj at least move on to guns or fire . xxmaj it has almost exact lines from \" when a xxmaj stranger xxmaj calls \" . xxmaj with gruesome killings , only crazy people would enjoy this movie . xxmaj it is obvious the writer does n\\'t have kids or even care for them . i mean at show some mercy . xxmaj just to sum it up , this movie is a \" b \" movie and it sucked . xxmaj just for your own sake , do n\\'t even think about wasting your time watching this crappy movie .',\n 'neg')\n\n\nThe last step is to convert our Datasets object to a DataLoaders, which can be done with the dataloaders method. Here we need to pass along a special argument to take care of the padding problem (as we saw in the last chapter). This needs to happen just before we batch the elements, so we pass it to before_batch:\n\ndls = dsets.dataloaders(bs=64, before_batch=pad_input)\n\ndataloaders directly calls DataLoader on each subset of our Datasets. fastai’s DataLoader expands the PyTorch class of the same name and is responsible for collating the items from our datasets into batches. It has a lot of points of customization, but the most important ones that you should know are:\n\nafter_item:: Applied on each item after grabbing it inside the dataset. This is the equivalent of item_tfms in DataBlock.\nbefore_batch:: Applied on the list of items before they are collated. This is the ideal place to pad items to the same size.\nafter_batch:: Applied on the batch as a whole after its construction. This is the equivalent of batch_tfms in DataBlock.\n\nAs a conclusion, here is the full code necessary to prepare the data for text classification:\n\ntfms = [[Tokenizer.from_folder(path), Numericalize], [parent_label, Categorize]]\nfiles = get_text_files(path, folders = ['train', 'test'])\nsplits = GrandparentSplitter(valid_name='test')(files)\ndsets = Datasets(files, tfms, splits=splits)\ndls = dsets.dataloaders(dl_type=SortedDL, before_batch=pad_input)\n\nThe two differences from the previous code are the use of GrandparentSplitter to split our training and validation data, and the dl_type argument. This is to tell dataloaders to use the SortedDL class of DataLoader, and not the usual one. SortedDL constructs batches by putting samples of roughly the same lengths into batches.\nThis does the exact same thing as our previous DataBlock:\n\npath = untar_data(URLs.IMDB)\ndls = DataBlock(\n    blocks=(TextBlock.from_folder(path),CategoryBlock),\n    get_y = parent_label,\n    get_items=partial(get_text_files, folders=['train', 'test']),\n    splitter=GrandparentSplitter(valid_name='test')\n).dataloaders(path)\n\nBut now, you know how to customize every single piece of it!\nLet’s practice what we just learned about this mid-level API for data preprocessing, using a computer vision example now."
  },
  {
    "objectID": "Fastbook/11_midlevel_data.html#applying-the-mid-level-data-api-siamesepair",
    "href": "Fastbook/11_midlevel_data.html#applying-the-mid-level-data-api-siamesepair",
    "title": "Data Munging with fastai’s Mid-Level API",
    "section": "Applying the Mid-Level Data API: SiamesePair",
    "text": "Applying the Mid-Level Data API: SiamesePair\nA Siamese model takes two images and has to determine if they are of the same class or not. For this example, we will use the Pet dataset again and prepare the data for a model that will have to predict if two images of pets are of the same breed or not. We will explain here how to prepare the data for such a model, then we will train that model in &lt;&gt;.\nFirst things first, let’s get the images in our dataset:\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\nIf we didn’t care about showing our objects at all, we could directly create one transform to completely preprocess that list of files. We will want to look at those images though, so we need to create a custom type. When you call the show method on a TfmdLists or a Datasets object, it will decode items until it reaches a type that contains a show method and use it to show the object. That show method gets passed a ctx, which could be a matplotlib axis for images, or a row of a DataFrame for texts.\nHere we create a SiameseImage object that subclasses fastuple and is intended to contain three things: two images, and a Boolean that’s True if the images are of the same breed. We also implement the special show method, such that it concatenates the two images with a black line in the middle. Don’t worry too much about the part that is in the if test (which is to show the SiameseImage when the images are Python images, not tensors); the important part is in the last three lines:\n\nclass SiameseImage(fastuple):\n    def show(self, ctx=None, **kwargs): \n        img1,img2,same_breed = self\n        if not isinstance(img1, Tensor):\n            if img2.size != img1.size: img2 = img2.resize(img1.size)\n            t1,t2 = tensor(img1),tensor(img2)\n            t1,t2 = t1.permute(2,0,1),t2.permute(2,0,1)\n        else: t1,t2 = img1,img2\n        line = t1.new_zeros(t1.shape[0], t1.shape[1], 10)\n        return show_image(torch.cat([t1,line,t2], dim=2), \n                          title=same_breed, ctx=ctx)\n\nLet’s create a first SiameseImage and check our show method works:\n\nimg = PILImage.create(files[0])\ns = SiameseImage(img, img, True)\ns.show();\n\n\n\n\n\n\n\n\nWe can also try with a second image that’s not from the same class:\n\nimg1 = PILImage.create(files[1])\ns1 = SiameseImage(img, img1, False)\ns1.show();\n\n\n\n\n\n\n\n\nThe important thing with transforms that we saw before is that they dispatch over tuples or their subclasses. That’s precisely why we chose to subclass fastuple in this instance—this way we can apply any transform that works on images to our SiameseImage and it will be applied on each image in the tuple:\n\ns2 = Resize(224)(s1)\ns2.show();\n\n\n\n\n\n\n\n\nHere the Resize transform is applied to each of the two images, but not the Boolean flag. Even if we have a custom type, we can thus benefit from all the data augmentation transforms inside the library.\nWe are now ready to build the Transform that we will use to get our data ready for a Siamese model. First, we will need a function to determine the classes of all our images:\n\ndef label_func(fname):\n    return re.match(r'^(.*)_\\d+.jpg$', fname.name).groups()[0]\n\nFor each image our tranform will, with a probability of 0.5, draw an image from the same class and return a SiameseImage with a true label, or draw an image from another class and return a SiameseImage with a false label. This is all done in the private _draw function. There is one difference between the training and validation sets, which is why the transform needs to be initialized with the splits: on the training set we will make that random pick each time we read an image, whereas on the validation set we make this random pick once and for all at initialization. This way, we get more varied samples during training, but always the same validation set:\n\nclass SiameseTransform(Transform):\n    def __init__(self, files, label_func, splits):\n        self.labels = files.map(label_func).unique()\n        self.lbl2files = {l: L(f for f in files if label_func(f) == l) \n                          for l in self.labels}\n        self.label_func = label_func\n        self.valid = {f: self._draw(f) for f in files[splits[1]]}\n        \n    def encodes(self, f):\n        f2,t = self.valid.get(f, self._draw(f))\n        img1,img2 = PILImage.create(f),PILImage.create(f2)\n        return SiameseImage(img1, img2, t)\n    \n    def _draw(self, f):\n        same = random.random() &lt; 0.5\n        cls = self.label_func(f)\n        if not same: \n            cls = random.choice(L(l for l in self.labels if l != cls)) \n        return random.choice(self.lbl2files[cls]),same\n\nWe can then create our main transform:\n\nsplits = RandomSplitter()(files)\ntfm = SiameseTransform(files, label_func, splits)\ntfm(files[0]).show();\n\n\n\n\n\n\n\n\nIn the mid-level API for data collection we have two objects that can help us apply transforms on a set of items, TfmdLists and Datasets. If you remember what we have just seen, one applies a Pipeline of transforms and the other applies several Pipelines of transforms in parallel, to build tuples. Here, our main transform already builds the tuples, so we use TfmdLists:\n\ntls = TfmdLists(files, tfm, splits=splits)\nshow_at(tls.valid, 0);\n\n\n\n\n\n\n\n\nAnd we can finally get our data in DataLoaders by calling the dataloaders method. One thing to be careful of here is that this method does not take item_tfms and batch_tfms like a DataBlock. The fastai DataLoader has several hooks that are named after events; here what we apply on the items after they are grabbed is called after_item, and what we apply on the batch once it’s built is called after_batch:\n\ndls = tls.dataloaders(after_item=[Resize(224), ToTensor], \n    after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)])\n\nNote that we need to pass more transforms than usual—that’s because the data block API usually adds them automatically:\n\nToTensor is the one that converts images to tensors (again, it’s applied on every part of the tuple).\nIntToFloatTensor converts the tensor of images containing integers from 0 to 255 to a tensor of floats, and divides by 255 to make the values between 0 and 1.\n\nWe can now train a model using this DataLoaders. It will need a bit more customization than the usual model provided by vision_learner since it has to take two images instead of one, but we will see how to create such a model and train it in &lt;&gt;."
  },
  {
    "objectID": "Fastbook/11_midlevel_data.html#conclusion",
    "href": "Fastbook/11_midlevel_data.html#conclusion",
    "title": "Data Munging with fastai’s Mid-Level API",
    "section": "Conclusion",
    "text": "Conclusion\nfastai provides a layered API. It takes one line of code to grab the data when it’s in one of the usual settings, making it easy for beginners to focus on training a model without spending too much time assembling the data. Then, the high-level data block API gives you more flexibility by allowing you to mix and match some building blocks. Underneath it, the mid-level API gives you greater flexibility to apply any transformations on your items. In your real-world problems, this is probably what you will need to use, and we hope it makes the step of data-munging as easy as possible."
  },
  {
    "objectID": "Fastbook/11_midlevel_data.html#questionnaire",
    "href": "Fastbook/11_midlevel_data.html#questionnaire",
    "title": "Data Munging with fastai’s Mid-Level API",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhy do we say that fastai has a “layered” API? What does it mean?\nWhy does a Transform have a decode method? What does it do?\nWhy does a Transform have a setup method? What does it do?\nHow does a Transform work when called on a tuple?\nWhich methods do you need to implement when writing your own Transform?\nWrite a Normalize transform that fully normalizes items (subtract the mean and divide by the standard deviation of the dataset), and that can decode that behavior. Try not to peek!\nWrite a Transform that does the numericalization of tokenized texts (it should set its vocab automatically from the dataset seen and have a decode method). Look at the source code of fastai if you need help.\nWhat is a Pipeline?\nWhat is a TfmdLists?\nWhat is a Datasets? How is it different from a TfmdLists?\nWhy are TfmdLists and Datasets named with an “s”?\nHow can you build a DataLoaders from a TfmdLists or a Datasets?\nHow do you pass item_tfms and batch_tfms when building a DataLoaders from a TfmdLists or a Datasets?\nWhat do you need to do when you want to have your custom items work with methods like show_batch or show_results?\nWhy can we easily apply fastai data augmentation transforms to the SiamesePair we built?\n\n\nFurther Research\n\nUse the mid-level API to prepare the data in DataLoaders on your own datasets. Try this with the Pet dataset and the Adult dataset from Chapter 1.\nLook at the Siamese tutorial in the fastai documentation to learn how to customize the behavior of show_batch and show_results for new type of items. Implement it in your own project."
  },
  {
    "objectID": "Fastbook/11_midlevel_data.html#understanding-fastais-applications-wrap-up",
    "href": "Fastbook/11_midlevel_data.html#understanding-fastais-applications-wrap-up",
    "title": "Data Munging with fastai’s Mid-Level API",
    "section": "Understanding fastai’s Applications: Wrap Up",
    "text": "Understanding fastai’s Applications: Wrap Up\nCongratulations—you’ve completed all of the chapters in this book that cover the key practical parts of training models and using deep learning! You know how to use all of fastai’s built-in applications, and how to customize them using the data block API and loss functions. You even know how to create a neural network from scratch, and train it! (And hopefully you now know some of the questions to ask to make sure your creations help improve society too.)\nThe knowledge you already have is enough to create full working prototypes of many types of neural network applications. More importantly, it will help you understand the capabilities and limitations of deep learning models, and how to design a system that’s well adapted to them.\nIn the rest of this book we will be pulling apart those applications, piece by piece, to understand the foundations they are built on. This is important knowledge for a deep learning practitioner, because it is what allows you to inspect and debug models that you build and create new applications that are customized for your particular projects."
  },
  {
    "objectID": "Fastbook/README_it.html",
    "href": "Fastbook/README_it.html",
    "title": "Il libro di fastai",
    "section": "",
    "text": "English / Spanish / Korean / Chinese / Bengali / Indonesian / Italian / Portuguese / Vietnamese / Japanese"
  },
  {
    "objectID": "Fastbook/README_it.html#citazioni",
    "href": "Fastbook/README_it.html#citazioni",
    "title": "Il libro di fastai",
    "section": "Citazioni",
    "text": "Citazioni\nQualora si desiderasse citare il libro, è possibile farlo nella seguente maniera:\n@book{howard2020deep,\ntitle={Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD},\nauthor={Howard, J. and Gugger, S.},\nisbn={9781492045526},\nurl={https://books.google.no/books?id=xd6LxgEACAAJ},\nyear={2020},\npublisher={O'Reilly Media, Incorporated}\n}"
  },
  {
    "objectID": "Fastbook/08_collab.html",
    "href": "Fastbook/08_collab.html",
    "title": "Collaborative Filtering Deep Dive",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\n[[chapter_collab]]\nOne very common problem to solve is when you have a number of users and a number of products, and you want to recommend which products are most likely to be useful for which users. There are many variations of this: for example, recommending movies (such as on Netflix), figuring out what to highlight for a user on a home page, deciding what stories to show in a social media feed, and so forth. There is a general solution to this problem, called collaborative filtering, which works like this: look at what products the current user has used or liked, find other users that have used or liked similar products, and then recommend other products that those users have used or liked.\nFor example, on Netflix you may have watched lots of movies that are science fiction, full of action, and were made in the 1970s. Netflix may not know these particular properties of the films you have watched, but it will be able to see that other people that have watched the same movies that you watched also tended to watch other movies that are science fiction, full of action, and were made in the 1970s. In other words, to use this approach we don’t necessarily need to know anything about the movies, except who like to watch them.\nThere is actually a more general class of problems that this approach can solve, not necessarily involving users and products. Indeed, for collaborative filtering we more commonly refer to items, rather than products. Items could be links that people click on, diagnoses that are selected for patients, and so forth.\nThe key foundational idea is that of latent factors. In the Netflix example, we started with the assumption that you like old, action-packed sci-fi movies. But you never actually told Netflix that you like these kinds of movies. And Netflix never actually needed to add columns to its movies table saying which movies are of these types. Still, there must be some underlying concept of sci-fi, action, and movie age, and these concepts must be relevant for at least some people’s movie watching decisions.\nFor this chapter we are going to work on this movie recommendation problem. We’ll start by getting some data suitable for a collaborative filtering model."
  },
  {
    "objectID": "Fastbook/08_collab.html#a-first-look-at-the-data",
    "href": "Fastbook/08_collab.html#a-first-look-at-the-data",
    "title": "Collaborative Filtering Deep Dive",
    "section": "A First Look at the Data",
    "text": "A First Look at the Data\nWe do not have access to Netflix’s entire dataset of movie watching history, but there is a great dataset that we can use, called MovieLens. This dataset contains tens of millions of movie rankings (a combination of a movie ID, a user ID, and a numeric rating), although we will just use a subset of 100,000 of them for our example. If you’re interested, it would be a great learning project to try and replicate this approach on the full 25-million recommendation dataset, which you can get from their website.\nThe dataset is available through the usual fastai function:\n\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ML_100k)\n\nAccording to the README, the main table is in the file u.data. It is tab-separated and the columns are, respectively user, movie, rating, and timestamp. Since those names are not encoded, we need to indicate them when reading the file with Pandas. Here is a way to open this table and take a look:\n\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None,\n                      names=['user','movie','rating','timestamp'])\nratings.head()\n\n\n\n\n\n\n\n\nuser\nmovie\nrating\ntimestamp\n\n\n\n\n0\n196\n242\n3\n881250949\n\n\n1\n186\n302\n3\n891717742\n\n\n2\n22\n377\n1\n878887116\n\n\n3\n244\n51\n2\n880606923\n\n\n4\n166\n346\n1\n886397596\n\n\n\n\n\n\n\nAlthough this has all the information we need, it is not a particularly helpful way for humans to look at this data. &lt;&gt; shows the same data cross-tabulated into a human-friendly table.\n\nWe have selected just a few of the most popular movies, and users who watch the most movies, for this crosstab example. The empty cells in this table are the things that we would like our model to learn to fill in. Those are the places where a user has not reviewed the movie yet, presumably because they have not watched it. For each user, we would like to figure out which of those movies they might be most likely to enjoy.\nIf we knew for each user to what degree they liked each important category that a movie might fall into, such as genre, age, preferred directors and actors, and so forth, and we knew the same information about each movie, then a simple way to fill in this table would be to multiply this information together for each movie and use a combination. For instance, assuming these factors range between -1 and +1, with positive numbers indicating stronger matches and negative numbers weaker ones, and the categories are science-fiction, action, and old movies, then we could represent the movie The Last Skywalker as:\n\nlast_skywalker = np.array([0.98,0.9,-0.9])\n\nHere, for instance, we are scoring very science-fiction as 0.98, very action as 0.9, and very not old as -0.9. We could represent a user who likes modern sci-fi action movies as:\n\nuser1 = np.array([0.9,0.8,-0.6])\n\nand we can now calculate the match between this combination:\n\n(user1*last_skywalker).sum()\n\n2.1420000000000003\n\n\nWhen we multiply two vectors together and add up the results, this is known as the dot product. It is used a lot in machine learning, and forms the basis of matrix multiplication. We will be looking a lot more at matrix multiplication and dot products in &lt;&gt;.\n\njargon: dot product: The mathematical operation of multiplying the elements of two vectors together, and then summing up the result.\n\nOn the other hand, we might represent the movie Casablanca as:\n\ncasablanca = np.array([-0.99,-0.3,0.8])\n\nThe match between this combination is:\n\n(user1*casablanca).sum()\n\n-1.611\n\n\nSince we don’t know what the latent factors actually are, and we don’t know how to score them for each user and movie, we should learn them."
  },
  {
    "objectID": "Fastbook/08_collab.html#learning-the-latent-factors",
    "href": "Fastbook/08_collab.html#learning-the-latent-factors",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Learning the Latent Factors",
    "text": "Learning the Latent Factors\nThere is surprisingly little difference between specifying the structure of a model, as we did in the last section, and learning one, since we can just use our general gradient descent approach.\nStep 1 of this approach is to randomly initialize some parameters. These parameters will be a set of latent factors for each user and movie. We will have to decide how many to use. We will discuss how to select this shortly, but for illustrative purposes let’s use 5 for now. Because each user will have a set of these factors and each movie will have a set of these factors, we can show these randomly initialized values right next to the users and movies in our crosstab, and we can then fill in the dot products for each of these combinations in the middle. For example, &lt;&gt; shows what it looks like in Microsoft Excel, with the top-left cell formula displayed as an example.\n\nStep 2 of this approach is to calculate our predictions. As we’ve discussed, we can do this by simply taking the dot product of each movie with each user. If, for instance, the first latent user factor represents how much the user likes action movies and the first latent movie factor represents if the movie has a lot of action or not, the product of those will be particularly high if either the user likes action movies and the movie has a lot of action in it or the user doesn’t like action movies and the movie doesn’t have any action in it. On the other hand, if we have a mismatch (a user loves action movies but the movie isn’t an action film, or the user doesn’t like action movies and it is one), the product will be very low.\nStep 3 is to calculate our loss. We can use any loss function that we wish; let’s pick mean squared error for now, since that is one reasonable way to represent the accuracy of a prediction.\nThat’s all we need. With this in place, we can optimize our parameters (that is, the latent factors) using stochastic gradient descent, such as to minimize the loss. At each step, the stochastic gradient descent optimizer will calculate the match between each movie and each user using the dot product, and will compare it to the actual rating that each user gave to each movie. It will then calculate the derivative of this value and will step the weights by multiplying this by the learning rate. After doing this lots of times, the loss will get better and better, and the recommendations will also get better and better.\nTo use the usual Learner.fit function we will need to get our data into a DataLoaders, so let’s focus on that now."
  },
  {
    "objectID": "Fastbook/08_collab.html#creating-the-dataloaders",
    "href": "Fastbook/08_collab.html#creating-the-dataloaders",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Creating the DataLoaders",
    "text": "Creating the DataLoaders\nWhen showing the data, we would rather see movie titles than their IDs. The table u.item contains the correspondence of IDs to titles:\n\nmovies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1',\n                     usecols=(0,1), names=('movie','title'), header=None)\nmovies.head()\n\n\n\n\n\n\n\n\nmovie\ntitle\n\n\n\n\n0\n1\nToy Story (1995)\n\n\n1\n2\nGoldenEye (1995)\n\n\n2\n3\nFour Rooms (1995)\n\n\n3\n4\nGet Shorty (1995)\n\n\n4\n5\nCopycat (1995)\n\n\n\n\n\n\n\nWe can merge this with our ratings table to get the user ratings by title:\n\nratings = ratings.merge(movies)\nratings.head()\n\n\n\n\n\n\n\n\nuser\nmovie\nrating\ntimestamp\ntitle\n\n\n\n\n0\n196\n242\n3\n881250949\nKolya (1996)\n\n\n1\n63\n242\n3\n875747190\nKolya (1996)\n\n\n2\n226\n242\n5\n883888671\nKolya (1996)\n\n\n3\n154\n242\n3\n879138235\nKolya (1996)\n\n\n4\n306\n242\n5\n876503793\nKolya (1996)\n\n\n\n\n\n\n\nWe can then build a DataLoaders object from this table. By default, it takes the first column for the user, the second column for the item (here our movies), and the third column for the ratings. We need to change the value of item_name in our case to use the titles instead of the IDs:\n\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\ndls.show_batch()\n\n\n\n\n\nuser\ntitle\nrating\n\n\n\n\n0\n542\nMy Left Foot (1989)\n4\n\n\n1\n422\nEvent Horizon (1997)\n3\n\n\n2\n311\nAfrican Queen, The (1951)\n4\n\n\n3\n595\nFace/Off (1997)\n4\n\n\n4\n617\nEvil Dead II (1987)\n1\n\n\n5\n158\nJurassic Park (1993)\n5\n\n\n6\n836\nChasing Amy (1997)\n3\n\n\n7\n474\nEmma (1996)\n3\n\n\n8\n466\nJackie Chan's First Strike (1996)\n3\n\n\n9\n554\nScream (1996)\n3\n\n\n\n\n\nTo represent collaborative filtering in PyTorch we can’t just use the crosstab representation directly, especially if we want it to fit into our deep learning framework. We can represent our movie and user latent factor tables as simple matrices:\n\ndls.classes\n\n{'user': (#944) ['#na#',1,2,3,4,5,6,7,8,9...],\n 'title': (#1635) ['#na#',\"'Til There Was You (1997)\",'1-900 (1994)','101 Dalmatians (1996)','12 Angry Men (1957)','187 (1997)','2 Days in the Valley (1996)','20,000 Leagues Under the Sea (1954)','2001: A Space Odyssey (1968)','3 Ninjas: High Noon At Mega Mountain (1998)'...]}\n\n\n\nn_users  = len(dls.classes['user'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\n\nuser_factors = torch.randn(n_users, n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)\n\nTo calculate the result for a particular movie and user combination, we have to look up the index of the movie in our movie latent factor matrix and the index of the user in our user latent factor matrix; then we can do our dot product between the two latent factor vectors. But look up in an index is not an operation our deep learning models know how to do. They know how to do matrix products, and activation functions.\nFortunately, it turns out that we can represent look up in an index as a matrix product. The trick is to replace our indices with one-hot-encoded vectors. Here is an example of what happens if we multiply a vector by a one-hot-encoded vector representing the index 3:\n\none_hot_3 = one_hot(3, n_users).float()\n\n\nuser_factors.t() @ one_hot_3\n\ntensor([-0.4586, -0.9915, -0.4052, -0.3621, -0.5908])\n\n\nIt gives us the same vector as the one at index 3 in the matrix:\n\nuser_factors[3]\n\ntensor([-0.4586, -0.9915, -0.4052, -0.3621, -0.5908])\n\n\nIf we do that for a few indices at once, we will have a matrix of one-hot-encoded vectors, and that operation will be a matrix multiplication! This would be a perfectly acceptable way to build models using this kind of architecture, except that it would use a lot more memory and time than necessary. We know that there is no real underlying reason to store the one-hot-encoded vector, or to search through it to find the occurrence of the number one—we should just be able to index into an array directly with an integer. Therefore, most deep learning libraries, including PyTorch, include a special layer that does just this; it indexes into a vector using an integer, but has its derivative calculated in such a way that it is identical to what it would have been if it had done a matrix multiplication with a one-hot-encoded vector. This is called an embedding.\n\njargon: Embedding: Multiplying by a one-hot-encoded matrix, using the computational shortcut that it can be implemented by simply indexing directly. This is quite a fancy word for a very simple concept. The thing that you multiply the one-hot-encoded matrix by (or, using the computational shortcut, index into directly) is called the embedding matrix.\n\nIn computer vision, we have a very easy way to get all the information of a pixel through its RGB values: each pixel in a colored image is represented by three numbers. Those three numbers give us the redness, the greenness and the blueness, which is enough to get our model to work afterward.\nFor the problem at hand, we don’t have the same easy way to characterize a user or a movie. There are probably relations with genres: if a given user likes romance, they are likely to give higher scores to romance movies. Other factors might be whether the movie is more action-oriented versus heavy on dialogue, or the presence of a specific actor that a user might particularly like.\nHow do we determine numbers to characterize those? The answer is, we don’t. We will let our model learn them. By analyzing the existing relations between users and movies, our model can figure out itself the features that seem important or not.\nThis is what embeddings are. We will attribute to each of our users and each of our movies a random vector of a certain length (here, n_factors=5), and we will make those learnable parameters. That means that at each step, when we compute the loss by comparing our predictions to our targets, we will compute the gradients of the loss with respect to those embedding vectors and update them with the rules of SGD (or another optimizer).\nAt the beginning, those numbers don’t mean anything since we have chosen them randomly, but by the end of training, they will. By learning on existing data about the relations between users and movies, without having any other information, we will see that they still get some important features, and can isolate blockbusters from independent cinema, action movies from romance, and so on.\nWe are now in a position that we can create our whole model from scratch."
  },
  {
    "objectID": "Fastbook/08_collab.html#collaborative-filtering-from-scratch",
    "href": "Fastbook/08_collab.html#collaborative-filtering-from-scratch",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Collaborative Filtering from Scratch",
    "text": "Collaborative Filtering from Scratch\nBefore we can write a model in PyTorch, we first need to learn the basics of object-oriented programming and Python. If you haven’t done any object-oriented programming before, we will give you a quick introduction here, but we would recommend looking up a tutorial and getting some practice before moving on.\nThe key idea in object-oriented programming is the class. We have been using classes throughout this book, such as DataLoader, string, and Learner. Python also makes it easy for us to create new classes. Here is an example of a simple class:\n\nclass Example:\n    def __init__(self, a): self.a = a\n    def say(self,x): return f'Hello {self.a}, {x}.'\n\nThe most important piece of this is the special method called __init__ (pronounced dunder init). In Python, any method surrounded in double underscores like this is considered special. It indicates that there is some extra behavior associated with this method name. In the case of __init__, this is the method Python will call when your new object is created. So, this is where you can set up any state that needs to be initialized upon object creation. Any parameters included when the user constructs an instance of your class will be passed to the __init__ method as parameters. Note that the first parameter to any method defined inside a class is self, so you can use this to set and get any attributes that you will need:\n\nex = Example('Sylvain')\nex.say('nice to meet you')\n\n'Hello Sylvain, nice to meet you.'\n\n\nAlso note that creating a new PyTorch module requires inheriting from Module. Inheritance is an important object-oriented concept that we will not discuss in detail here—in short, it means that we can add additional behavior to an existing class. PyTorch already provides a Module class, which provides some basic foundations that we want to build on. So, we add the name of this superclass after the name of the class that we are defining, as shown in the following example.\nThe final thing that you need to know to create a new PyTorch module is that when your module is called, PyTorch will call a method in your class called forward, and will pass along to that any parameters that are included in the call. Here is the class defining our dot product model:\n\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        return (users * movies).sum(dim=1)\n\nIf you haven’t seen object-oriented programming before, then don’t worry, you won’t need to use it much in this book. We are just mentioning this approach here, because most online tutorials and documentation will use the object-oriented syntax.\nNote that the input of the model is a tensor of shape batch_size x 2, where the first column (x[:, 0]) contains the user IDs and the second column (x[:, 1]) contains the movie IDs. As explained before, we use the embedding layers to represent our matrices of user and movie latent factors:\n\nx,y = dls.one_batch()\nx.shape\n\ntorch.Size([64, 2])\n\n\nNow that we have defined our architecture, and created our parameter matrices, we need to create a Learner to optimize our model. In the past we have used special functions, such as vision_learner, which set up everything for us for a particular application. Since we are doing things from scratch here, we will use the plain Learner class:\n\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\nWe are now ready to fit our model:\n\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.993168\n0.990168\n00:12\n\n\n1\n0.884821\n0.911269\n00:12\n\n\n2\n0.671865\n0.875679\n00:12\n\n\n3\n0.471727\n0.878200\n00:11\n\n\n4\n0.361314\n0.884209\n00:12\n\n\n\n\n\nThe first thing we can do to make this model a little bit better is to force those predictions to be between 0 and 5. For this, we just need to use sigmoid_range, like in &lt;&gt;. One thing we discovered empirically is that it’s better to have the range go a little bit over 5, so we use (0, 5.5):\n\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\n\n\nmodel = DotProduct(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.973745\n0.993206\n00:12\n\n\n1\n0.869132\n0.914323\n00:12\n\n\n2\n0.676553\n0.870192\n00:12\n\n\n3\n0.485377\n0.873865\n00:12\n\n\n4\n0.377866\n0.877610\n00:11\n\n\n\n\n\nThis is a reasonable start, but we can do better. One obvious missing piece is that some users are just more positive or negative in their recommendations than others, and some movies are just plain better or worse than others. But in our dot product representation we do not have any way to encode either of these things. If all you can say about a movie is, for instance, that it is very sci-fi, very action-oriented, and very not old, then you don’t really have any way to say whether most people like it.\nThat’s because at this point we only have weights; we do not have biases. If we have a single number for each user that we can add to our scores, and ditto for each movie, that will handle this missing piece very nicely. So first of all, let’s adjust our model architecture:\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.user_bias = Embedding(n_users, 1)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.movie_bias = Embedding(n_movies, 1)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        res = (users * movies).sum(dim=1, keepdim=True)\n        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n        return sigmoid_range(res, *self.y_range)\n\nLet’s try training this and see how it goes:\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.929161\n0.936303\n00:13\n\n\n1\n0.820444\n0.861306\n00:13\n\n\n2\n0.621612\n0.865306\n00:14\n\n\n3\n0.404648\n0.886448\n00:13\n\n\n4\n0.292948\n0.892580\n00:13\n\n\n\n\n\nInstead of being better, it ends up being worse (at least at the end of training). Why is that? If we look at both trainings carefully, we can see the validation loss stopped improving in the middle and started to get worse. As we’ve seen, this is a clear indication of overfitting. In this case, there is no way to use data augmentation, so we will have to use another regularization technique. One approach that can be helpful is weight decay.\n\nWeight Decay\nWeight decay, or L2 regularization, consists in adding to your loss function the sum of all the weights squared. Why do that? Because when we compute the gradients, it will add a contribution to them that will encourage the weights to be as small as possible.\nWhy would it prevent overfitting? The idea is that the larger the coefficients are, the sharper canyons we will have in the loss function. If we take the basic example of a parabola, y = a * (x**2), the larger a is, the more narrow the parabola is (&lt;&gt;).\n\n#hide_input\n#id parabolas\nx = np.linspace(-2,2,100)\na_s = [1,2,5,10,50] \nys = [a * x**2 for a in a_s]\n_,ax = plt.subplots(figsize=(8,6))\nfor a,y in zip(a_s,ys): ax.plot(x,y, label=f'a={a}')\nax.set_ylim([0,5])\nax.legend();\n\n\n\n\n\n\n\n\nSo, letting our model learn high parameters might cause it to fit all the data points in the training set with an overcomplex function that has very sharp changes, which will lead to overfitting.\nLimiting our weights from growing too much is going to hinder the training of the model, but it will yield a state where it generalizes better. Going back to the theory briefly, weight decay (or just wd) is a parameter that controls that sum of squares we add to our loss (assuming parameters is a tensor of all parameters):\nloss_with_wd = loss + wd * (parameters**2).sum()\nIn practice, though, it would be very inefficient (and maybe numerically unstable) to compute that big sum and add it to the loss. If you remember a little bit of high school math, you might recall that the derivative of p**2 with respect to p is 2*p, so adding that big sum to our loss is exactly the same as doing:\nparameters.grad += wd * 2 * parameters\nIn practice, since wd is a parameter that we choose, we can just make it twice as big, so we don’t even need the *2 in this equation. To use weight decay in fastai, just pass wd in your call to fit or fit_one_cycle:\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.972090\n0.962366\n00:13\n\n\n1\n0.875591\n0.885106\n00:13\n\n\n2\n0.723798\n0.839880\n00:13\n\n\n3\n0.586002\n0.823225\n00:13\n\n\n4\n0.490980\n0.823060\n00:13\n\n\n\n\n\nMuch better!\n\n\nCreating Our Own Embedding Module\nSo far, we’ve used Embedding without thinking about how it really works. Let’s re-create DotProductBias without using this class. We’ll need a randomly initialized weight matrix for each of the embeddings. We have to be careful, however. Recall from &lt;&gt; that optimizers require that they can get all the parameters of a module from the module’s parameters method. However, this does not happen fully automatically. If we just add a tensor as an attribute to a Module, it will not be included in parameters:\n\nclass T(Module):\n    def __init__(self): self.a = torch.ones(3)\n\nL(T().parameters())\n\n(#0) []\n\n\nTo tell Module that we want to treat a tensor as a parameter, we have to wrap it in the nn.Parameter class. This class doesn’t actually add any functionality (other than automatically calling requires_grad_ for us). It’s only used as a “marker” to show what to include in parameters:\n\nclass T(Module):\n    def __init__(self): self.a = nn.Parameter(torch.ones(3))\n\nL(T().parameters())\n\n(#1) [Parameter containing:\ntensor([1., 1., 1.], requires_grad=True)]\n\n\nAll PyTorch modules use nn.Parameter for any trainable parameters, which is why we haven’t needed to explicitly use this wrapper up until now:\n\nclass T(Module):\n    def __init__(self): self.a = nn.Linear(1, 3, bias=False)\n\nt = T()\nL(t.parameters())\n\n(#1) [Parameter containing:\ntensor([[-0.9595],\n        [-0.8490],\n        [ 0.8159]], requires_grad=True)]\n\n\n\ntype(t.a.weight)\n\ntorch.nn.parameter.Parameter\n\n\nWe can create a tensor as a parameter, with random initialization, like so:\n\ndef create_params(size):\n    return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))\n\nLet’s use this to create DotProductBias again, but without Embedding:\n\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = create_params([n_users, n_factors])\n        self.user_bias = create_params([n_users])\n        self.movie_factors = create_params([n_movies, n_factors])\n        self.movie_bias = create_params([n_movies])\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors[x[:,0]]\n        movies = self.movie_factors[x[:,1]]\n        res = (users*movies).sum(dim=1)\n        res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n        return sigmoid_range(res, *self.y_range)\n\nThen let’s train it again to check we get around the same results we saw in the previous section:\n\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.962146\n0.936952\n00:14\n\n\n1\n0.858084\n0.884951\n00:14\n\n\n2\n0.740883\n0.838549\n00:14\n\n\n3\n0.592497\n0.823599\n00:14\n\n\n4\n0.473570\n0.824263\n00:14\n\n\n\n\n\nNow, let’s take a look at what our model has learned."
  },
  {
    "objectID": "Fastbook/08_collab.html#interpreting-embeddings-and-biases",
    "href": "Fastbook/08_collab.html#interpreting-embeddings-and-biases",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Interpreting Embeddings and Biases",
    "text": "Interpreting Embeddings and Biases\nOur model is already useful, in that it can provide us with movie recommendations for our users—but it is also interesting to see what parameters it has discovered. The easiest to interpret are the biases. Here are the movies with the lowest values in the bias vector:\n\nmovie_bias = learn.model.movie_bias.squeeze()\nidxs = movie_bias.argsort()[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Children of the Corn: The Gathering (1996)',\n 'Lawnmower Man 2: Beyond Cyberspace (1996)',\n 'Beautician and the Beast, The (1997)',\n 'Crow: City of Angels, The (1996)',\n 'Home Alone 3 (1997)']\n\n\nThink about what this means. What it’s saying is that for each of these movies, even when a user is very well matched to its latent factors (which, as we will see in a moment, tend to represent things like level of action, age of movie, and so forth), they still generally don’t like it. We could have simply sorted the movies directly by their average rating, but looking at the learned bias tells us something much more interesting. It tells us not just whether a movie is of a kind that people tend not to enjoy watching, but that people tend not to like watching it even if it is of a kind that they would otherwise enjoy! By the same token, here are the movies with the highest bias:\n\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['L.A. Confidential (1997)',\n 'Titanic (1997)',\n 'Silence of the Lambs, The (1991)',\n 'Shawshank Redemption, The (1994)',\n 'Star Wars (1977)']\n\n\nSo, for instance, even if you don’t normally enjoy detective movies, you might enjoy LA Confidential!\nIt is not quite so easy to directly interpret the embedding matrices. There are just too many factors for a human to look at. But there is a technique that can pull out the most important underlying directions in such a matrix, called principal component analysis (PCA). We will not be going into this in detail in this book, because it is not particularly important for you to understand to be a deep learning practitioner, but if you are interested then we suggest you check out the fast.ai course Computational Linear Algebra for Coders. &lt;&gt; shows what our movies look like based on two of the strongest PCA components.\n\n#hide_input\n#id img_pca_movie\n#caption Representation of movies based on two strongest PCA components\n#alt Representation of movies based on two strongest PCA components\ng = ratings.groupby('title')['rating'].count()\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\ntop_idxs = tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])\nmovie_w = learn.model.movie_factors[top_idxs].cpu().detach()\nmovie_pca = movie_w.pca(3)\nfac0,fac1,fac2 = movie_pca.t()\nidxs = list(range(50))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n\n\n\n\n\n\n\n\nWe can see here that the model seems to have discovered a concept of classic versus pop culture movies, or perhaps it is critically acclaimed that is represented here.\n\nj: No matter how many models I train, I never stop getting moved and surprised by how these randomly initialized bunches of numbers, trained with such simple mechanics, manage to discover things about my data all by themselves. It almost seems like cheating, that I can create code that does useful things without ever actually telling it how to do those things!\n\nWe defined our model from scratch to teach you what is inside, but you can directly use the fastai library to build it. We’ll look at how to do that next.\n\nUsing fastai.collab\nWe can create and train a collaborative filtering model using the exact structure shown earlier by using fastai’s collab_learner:\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\n\n\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.931751\n0.953806\n00:13\n\n\n1\n0.851826\n0.878119\n00:13\n\n\n2\n0.715254\n0.834711\n00:13\n\n\n3\n0.583173\n0.821470\n00:13\n\n\n4\n0.496625\n0.821688\n00:13\n\n\n\n\n\nThe names of the layers can be seen by printing the model:\n\nlearn.model\n\nEmbeddingDotBias(\n  (u_weight): Embedding(944, 50)\n  (i_weight): Embedding(1635, 50)\n  (u_bias): Embedding(944, 1)\n  (i_bias): Embedding(1635, 1)\n)\n\n\nWe can use these to replicate any of the analyses we did in the previous section—for instance:\n\nmovie_bias = learn.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Titanic (1997)',\n \"Schindler's List (1993)\",\n 'Shawshank Redemption, The (1994)',\n 'L.A. Confidential (1997)',\n 'Silence of the Lambs, The (1991)']\n\n\nAnother interesting thing we can do with these learned embeddings is to look at distance.\n\n\nEmbedding Distance\nOn a two-dimensional map we can calculate the distance between two coordinates using the formula of Pythagoras: \\(\\sqrt{x^{2}+y^{2}}\\) (assuming that x and y are the distances between the coordinates on each axis). For a 50-dimensional embedding we can do exactly the same thing, except that we add up the squares of all 50 of the coordinate distances.\nIf there were two movies that were nearly identical, then their embedding vectors would also have to be nearly identical, because the users that would like them would be nearly exactly the same. There is a more general idea here: movie similarity can be defined by the similarity of users that like those movies. And that directly means that the distance between two movies’ embedding vectors can define that similarity. We can use this to find the most similar movie to Silence of the Lambs:\n\nmovie_factors = learn.model.i_weight.weight\nidx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']\ndistances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])\nidx = distances.argsort(descending=True)[1]\ndls.classes['title'][idx]\n\n'Dial M for Murder (1954)'\n\n\nNow that we have succesfully trained a model, let’s see how to deal with the situation where we have no data for a user. How can we make recommendations to new users?"
  },
  {
    "objectID": "Fastbook/08_collab.html#bootstrapping-a-collaborative-filtering-model",
    "href": "Fastbook/08_collab.html#bootstrapping-a-collaborative-filtering-model",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Bootstrapping a Collaborative Filtering Model",
    "text": "Bootstrapping a Collaborative Filtering Model\nThe biggest challenge with using collaborative filtering models in practice is the bootstrapping problem. The most extreme version of this problem is when you have no users, and therefore no history to learn from. What products do you recommend to your very first user?\nBut even if you are a well-established company with a long history of user transactions, you still have the question: what do you do when a new user signs up? And indeed, what do you do when you add a new product to your portfolio? There is no magic solution to this problem, and really the solutions that we suggest are just variations of use your common sense. You could assign new users the mean of all of the embedding vectors of your other users, but this has the problem that that particular combination of latent factors may be not at all common (for instance, the average for the science-fiction factor may be high, and the average for the action factor may be low, but it is not that common to find people who like science-fiction without action). Better would probably be to pick some particular user to represent average taste.\nBetter still is to use a tabular model based on user meta data to construct your initial embedding vector. When a user signs up, think about what questions you could ask them that could help you to understand their tastes. Then you can create a model where the dependent variable is a user’s embedding vector, and the independent variables are the results of the questions that you ask them, along with their signup metadata. We will see in the next section how to create these kinds of tabular models. (You may have noticed that when you sign up for services such as Pandora and Netflix, they tend to ask you a few questions about what genres of movie or music you like; this is how they come up with your initial collaborative filtering recommendations.)\nOne thing to be careful of is that a small number of extremely enthusiastic users may end up effectively setting the recommendations for your whole user base. This is a very common problem, for instance, in movie recommendation systems. People that watch anime tend to watch a whole lot of it, and don’t watch very much else, and spend a lot of time putting their ratings on websites. As a result, anime tends to be heavily overrepresented in a lot of best ever movies lists. In this particular case, it can be fairly obvious that you have a problem of representation bias, but if the bias is occurring in the latent factors then it may not be obvious at all.\nSuch a problem can change the entire makeup of your user base, and the behavior of your system. This is particularly true because of positive feedback loops. If a small number of your users tend to set the direction of your recommendation system, then they are naturally going to end up attracting more people like them to your system. And that will, of course, amplify the original representation bias. This type of bias has a natural tendency to be amplified exponentially. You may have seen examples of company executives expressing surprise at how their online platforms rapidly deteriorated in such a way that they expressed values at odds with the values of the founders. In the presence of these kinds of feedback loops, it is easy to see how such a divergence can happen both quickly and in a way that is hidden until it is too late.\nIn a self-reinforcing system like this, we should probably expect these kinds of feedback loops to be the norm, not the exception. Therefore, you should assume that you will see them, plan for that, and identify up front how you will deal with these issues. Try to think about all of the ways in which feedback loops may be represented in your system, and how you might be able to identify them in your data. In the end, this is coming back to our original advice about how to avoid disaster when rolling out any kind of machine learning system. It’s all about ensuring that there are humans in the loop; that there is careful monitoring, and a gradual and thoughtful rollout.\nOur dot product model works quite well, and it is the basis of many successful real-world recommendation systems. This approach to collaborative filtering is known as probabilistic matrix factorization (PMF). Another approach, which generally works similarly well given the same data, is deep learning."
  },
  {
    "objectID": "Fastbook/08_collab.html#deep-learning-for-collaborative-filtering",
    "href": "Fastbook/08_collab.html#deep-learning-for-collaborative-filtering",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Deep Learning for Collaborative Filtering",
    "text": "Deep Learning for Collaborative Filtering\nTo turn our architecture into a deep learning model, the first step is to take the results of the embedding lookup and concatenate those activations together. This gives us a matrix which we can then pass through linear layers and nonlinearities in the usual way.\nSince we’ll be concatenating the embeddings, rather than taking their dot product, the two embedding matrices can have different sizes (i.e., different numbers of latent factors). fastai has a function get_emb_sz that returns recommended sizes for embedding matrices for your data, based on a heuristic that fast.ai has found tends to work well in practice:\n\nembs = get_emb_sz(dls)\nembs\n\n[(944, 74), (1635, 101)]\n\n\nLet’s implement this class:\n\nclass CollabNN(Module):\n    def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):\n        self.user_factors = Embedding(*user_sz)\n        self.item_factors = Embedding(*item_sz)\n        self.layers = nn.Sequential(\n            nn.Linear(user_sz[1]+item_sz[1], n_act),\n            nn.ReLU(),\n            nn.Linear(n_act, 1))\n        self.y_range = y_range\n        \n    def forward(self, x):\n        embs = self.user_factors(x[:,0]),self.item_factors(x[:,1])\n        x = self.layers(torch.cat(embs, dim=1))\n        return sigmoid_range(x, *self.y_range)\n\nAnd use it to create a model:\n\nmodel = CollabNN(*embs)\n\nCollabNN creates our Embedding layers in the same way as previous classes in this chapter, except that we now use the embs sizes. self.layers is identical to the mini-neural net we created in &lt;&gt; for MNIST. Then, in forward, we apply the embeddings, concatenate the results, and pass this through the mini-neural net. Finally, we apply sigmoid_range as we have in previous models.\nLet’s see if it trains:\n\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.01)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.940104\n0.959786\n00:15\n\n\n1\n0.893943\n0.905222\n00:14\n\n\n2\n0.865591\n0.875238\n00:14\n\n\n3\n0.800177\n0.867468\n00:14\n\n\n4\n0.760255\n0.867455\n00:14\n\n\n\n\n\nfastai provides this model in fastai.collab if you pass use_nn=True in your call to collab_learner (including calling get_emb_sz for you), and it lets you easily create more layers. For instance, here we’re creating two hidden layers, of size 100 and 50, respectively:\n\nlearn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50])\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.002747\n0.972392\n00:16\n\n\n1\n0.926903\n0.922348\n00:16\n\n\n2\n0.877160\n0.893401\n00:16\n\n\n3\n0.838334\n0.865040\n00:16\n\n\n4\n0.781666\n0.864936\n00:16\n\n\n\n\n\nlearn.model is an object of type EmbeddingNN. Let’s take a look at fastai’s code for this class:\n\n@delegates(TabularModel)\nclass EmbeddingNN(TabularModel):\n    def __init__(self, emb_szs, layers, **kwargs):\n        super().__init__(emb_szs, layers=layers, n_cont=0, out_sz=1, **kwargs)\n\nWow, that’s not a lot of code! This class inherits from TabularModel, which is where it gets all its functionality from. In __init__ it calls the same method in TabularModel, passing n_cont=0 and out_sz=1; other than that, it only passes along whatever arguments it received.\n\nSidebar: kwargs and Delegates\nEmbeddingNN includes **kwargs as a parameter to __init__. In Python **kwargs in a parameter list means “put any additional keyword arguments into a dict called kwargs. And **kwargs in an argument list means”insert all key/value pairs in the kwargs dict as named arguments here”. This approach is used in many popular libraries, such as matplotlib, in which the main plot function simply has the signature plot(*args, **kwargs). The plot documentation says “The kwargs are Line2D properties” and then lists those properties.\nWe’re using **kwargs in EmbeddingNN to avoid having to write all the arguments to TabularModel a second time, and keep them in sync. However, this makes our API quite difficult to work with, because now Jupyter Notebook doesn’t know what parameters are available. Consequently things like tab completion of parameter names and pop-up lists of signatures won’t work.\nfastai resolves this by providing a special @delegates decorator, which automatically changes the signature of the class or function (EmbeddingNN in this case) to insert all of its keyword arguments into the signature.\n\n\nEnd sidebar\nAlthough the results of EmbeddingNN are a bit worse than the dot product approach (which shows the power of carefully constructing an architecture for a domain), it does allow us to do something very important: we can now directly incorporate other user and movie information, date and time information, or any other information that may be relevant to the recommendation. That’s exactly what TabularModel does. In fact, we’ve now seen that EmbeddingNN is just a TabularModel, with n_cont=0 and out_sz=1. So, we’d better spend some time learning about TabularModel, and how to use it to get great results! We’ll do that in the next chapter."
  },
  {
    "objectID": "Fastbook/08_collab.html#conclusion",
    "href": "Fastbook/08_collab.html#conclusion",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Conclusion",
    "text": "Conclusion\nFor our first non-computer vision application, we looked at recommendation systems and saw how gradient descent can learn intrinsic factors or biases about items from a history of ratings. Those can then give us information about the data.\nWe also built our first model in PyTorch. We will do a lot more of this in the next section of the book, but first, let’s finish our dive into the other general applications of deep learning, continuing with tabular data."
  },
  {
    "objectID": "Fastbook/08_collab.html#questionnaire",
    "href": "Fastbook/08_collab.html#questionnaire",
    "title": "Collaborative Filtering Deep Dive",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhat problem does collaborative filtering solve?\nHow does it solve it?\nWhy might a collaborative filtering predictive model fail to be a very useful recommendation system?\nWhat does a crosstab representation of collaborative filtering data look like?\nWrite the code to create a crosstab representation of the MovieLens data (you might need to do some web searching!).\nWhat is a latent factor? Why is it “latent”?\nWhat is a dot product? Calculate a dot product manually using pure Python with lists.\nWhat does pandas.DataFrame.merge do?\nWhat is an embedding matrix?\nWhat is the relationship between an embedding and a matrix of one-hot-encoded vectors?\nWhy do we need Embedding if we could use one-hot-encoded vectors for the same thing?\nWhat does an embedding contain before we start training (assuming we’re not using a pretained model)?\nCreate a class (without peeking, if possible!) and use it.\nWhat does x[:,0] return?\nRewrite the DotProduct class (without peeking, if possible!) and train a model with it.\nWhat is a good loss function to use for MovieLens? Why?\nWhat would happen if we used cross-entropy loss with MovieLens? How would we need to change the model?\nWhat is the use of bias in a dot product model?\nWhat is another name for weight decay?\nWrite the equation for weight decay (without peeking!).\nWrite the equation for the gradient of weight decay. Why does it help reduce weights?\nWhy does reducing weights lead to better generalization?\nWhat does argsort do in PyTorch?\nDoes sorting the movie biases give the same result as averaging overall movie ratings by movie? Why/why not?\nHow do you print the names and details of the layers in a model?\nWhat is the “bootstrapping problem” in collaborative filtering?\nHow could you deal with the bootstrapping problem for new users? For new movies?\nHow can feedback loops impact collaborative filtering systems?\nWhen using a neural network in collaborative filtering, why can we have different numbers of factors for movies and users?\nWhy is there an nn.Sequential in the CollabNN model?\nWhat kind of model should we use if we want to add metadata about users and items, or information such as date and time, to a collaborative filtering model?\n\n\nFurther Research\n\nTake a look at all the differences between the Embedding version of DotProductBias and the create_params version, and try to understand why each of those changes is required. If you’re not sure, try reverting each change to see what happens. (NB: even the type of brackets used in forward has changed!)\nFind three other areas where collaborative filtering is being used, and find out what the pros and cons of this approach are in those areas.\nComplete this notebook using the full MovieLens dataset, and compare your results to online benchmarks. See if you can improve your accuracy. Look on the book’s website and the fast.ai forum for ideas. Note that there are more columns in the full dataset—see if you can use those too (the next chapter might give you ideas).\nCreate a model for MovieLens that works with cross-entropy loss, and compare it to the model in this chapter."
  },
  {
    "objectID": "Fastbook/CODE_OF_CONDUCT.html",
    "href": "Fastbook/CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\nExamples of unacceptable behavior by participants include:\n\nThe use of sexualized language or imagery and unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or electronic address, without explicit permission\n\nThese examples of unacceptable behaviour are requirements; we will not allow them in any fast.ai project, including this one.\n\n\n\nExamples of behavior that contributes to creating a positive environment include:\n\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\n\nThese examples are shown only to help you participate effectively – they are not requirements, just requests and guidance.\n\n\n\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\n\n\n\nThis Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\n\n\n\nInstances of abusive, harassing or otherwise unacceptable behavior may be reported by contacting the project team at info@fast.ai. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership.\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html"
  },
  {
    "objectID": "Fastbook/CODE_OF_CONDUCT.html#our-pledge",
    "href": "Fastbook/CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\nExamples of unacceptable behavior by participants include:\n\nThe use of sexualized language or imagery and unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or electronic address, without explicit permission\n\nThese examples of unacceptable behaviour are requirements; we will not allow them in any fast.ai project, including this one."
  },
  {
    "objectID": "Fastbook/CODE_OF_CONDUCT.html#our-standards",
    "href": "Fastbook/CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to creating a positive environment include:\n\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\n\nThese examples are shown only to help you participate effectively – they are not requirements, just requests and guidance."
  },
  {
    "objectID": "Fastbook/CODE_OF_CONDUCT.html#our-responsibilities",
    "href": "Fastbook/CODE_OF_CONDUCT.html#our-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful."
  },
  {
    "objectID": "Fastbook/CODE_OF_CONDUCT.html#scope",
    "href": "Fastbook/CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers."
  },
  {
    "objectID": "Fastbook/CODE_OF_CONDUCT.html#enforcement",
    "href": "Fastbook/CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing or otherwise unacceptable behavior may be reported by contacting the project team at info@fast.ai. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership."
  },
  {
    "objectID": "Fastbook/CODE_OF_CONDUCT.html#attribution",
    "href": "Fastbook/CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html"
  },
  {
    "objectID": "Fastbook/05_pet_breeds.html",
    "href": "Fastbook/05_pet_breeds.html",
    "title": "Image Classification",
    "section": "",
    "text": "#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\n[[chapter_pet_breeds]]\nNow that you understand what deep learning is, what it’s for, and how to create and deploy a model, it’s time for us to go deeper! In an ideal world deep learning practitioners wouldn’t have to know every detail of how things work under the hood… But as yet, we don’t live in an ideal world. The truth is, to make your model really work, and work reliably, there are a lot of details you have to get right, and a lot of details that you have to check. This process requires being able to look inside your neural network as it trains, and as it makes predictions, find possible problems, and know how to fix them.\nSo, from here on in the book we are going to do a deep dive into the mechanics of deep learning. What is the architecture of a computer vision model, an NLP model, a tabular model, and so on? How do you create an architecture that matches the needs of your particular domain? How do you get the best possible results from the training process? How do you make things faster? What do you have to change as your datasets change?\nWe will start by repeating the same basic applications that we looked at in the first chapter, but we are going to do two things:\nIn order to do these two things, we will have to learn all of the pieces of the deep learning puzzle. This includes different types of layers, regularization methods, optimizers, how to put layers together into architectures, labeling techniques, and much more. We are not just going to dump all of these things on you, though; we will introduce them progressively as needed, to solve actual problems related to the projects we are working on."
  },
  {
    "objectID": "Fastbook/05_pet_breeds.html#from-dogs-and-cats-to-pet-breeds",
    "href": "Fastbook/05_pet_breeds.html#from-dogs-and-cats-to-pet-breeds",
    "title": "Image Classification",
    "section": "From Dogs and Cats to Pet Breeds",
    "text": "From Dogs and Cats to Pet Breeds\nIn our very first model we learned how to classify dogs versus cats. Just a few years ago this was considered a very challenging task—but today, it’s far too easy! We will not be able to show you the nuances of training models with this problem, because we get a nearly perfect result without worrying about any of the details. But it turns out that the same dataset also allows us to work on a much more challenging problem: figuring out what breed of pet is shown in each image.\nIn &lt;&gt; we presented the applications as already-solved problems. But this is not how things work in real life. We start with some dataset that we know nothing about. We then have to figure out how it is put together, how to extract the data we need from it, and what that data looks like. For the rest of this book we will be showing you how to solve these problems in practice, including all of the intermediate steps necessary to understand the data that you are working with and test your modeling as you go.\nWe already downloaded the Pet dataset, and we can get a path to this dataset using the same code as in &lt;&gt;:\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)\n\nNow if we are going to understand how to extract the breed of each pet from each image we’re going to need to understand how this data is laid out. Such details of data layout are a vital piece of the deep learning puzzle. Data is usually provided in one of these two ways:\n\nIndividual files representing items of data, such as text documents or images, possibly organized into folders or with filenames representing information about those items\nA table of data, such as in CSV format, where each row is an item which may include filenames providing a connection between the data in the table and data in other formats, such as text documents and images\n\nThere are exceptions to these rules—particularly in domains such as genomics, where there can be binary database formats or even network streams—but overall the vast majority of the datasets you’ll work with will use some combination of these two formats.\nTo see what is in our dataset we can use the ls method:\n\n#hide\nPath.BASE_PATH = path\n\n\npath.ls()\n\n(#3) [Path('annotations'),Path('images'),Path('models')]\n\n\nWe can see that this dataset provides us with images and annotations directories. The website for the dataset tells us that the annotations directory contains information about where the pets are rather than what they are. In this chapter, we will be doing classification, not localization, which is to say that we care about what the pets are, not where they are. Therefore, we will ignore the annotations directory for now. So, let’s have a look inside the images directory:\n\n(path/\"images\").ls()\n\n(#7394) [Path('images/great_pyrenees_173.jpg'),Path('images/wheaten_terrier_46.jpg'),Path('images/Ragdoll_262.jpg'),Path('images/german_shorthaired_3.jpg'),Path('images/american_bulldog_196.jpg'),Path('images/boxer_188.jpg'),Path('images/staffordshire_bull_terrier_173.jpg'),Path('images/basset_hound_71.jpg'),Path('images/staffordshire_bull_terrier_37.jpg'),Path('images/yorkshire_terrier_18.jpg')...]\n\n\nMost functions and methods in fastai that return a collection use a class called L. L can be thought of as an enhanced version of the ordinary Python list type, with added conveniences for common operations. For instance, when we display an object of this class in a notebook it appears in the format shown there. The first thing that is shown is the number of items in the collection, prefixed with a #. You’ll also see in the preceding output that the list is suffixed with an ellipsis. This means that only the first few items are displayed—which is a good thing, because we would not want more than 7,000 filenames on our screen!\nBy examining these filenames, we can see how they appear to be structured. Each filename contains the pet breed, and then an underscore (_), a number, and finally the file extension. We need to create a piece of code that extracts the breed from a single Path. Jupyter notebooks make this easy, because we can gradually build up something that works, and then use it for the entire dataset. We do have to be careful to not make too many assumptions at this point. For instance, if you look carefully you may notice that some of the pet breeds contain multiple words, so we cannot simply break at the first _ character that we find. To allow us to test our code, let’s pick out one of these filenames:\n\nfname = (path/\"images\").ls()[0]\n\nThe most powerful and flexible way to extract information from strings like this is to use a regular expression, also known as a regex. A regular expression is a special string, written in the regular expression language, which specifies a general rule for deciding if another string passes a test (i.e., “matches” the regular expression), and also possibly for plucking a particular part or parts out of that other string.\nIn this case, we need a regular expression that extracts the pet breed from the filename.\nWe do not have the space to give you a complete regular expression tutorial here, but there are many excellent ones online and we know that many of you will already be familiar with this wonderful tool. If you’re not, that is totally fine—this is a great opportunity for you to rectify that! We find that regular expressions are one of the most useful tools in our programming toolkit, and many of our students tell us that this is one of the things they are most excited to learn about. So head over to Google and search for “regular expressions tutorial” now, and then come back here after you’ve had a good look around. The book’s website also provides a list of our favorites.\n\na: Not only are regular expressions dead handy, but they also have interesting roots. They are “regular” because they were originally examples of a “regular” language, the lowest rung within the Chomsky hierarchy, a grammar classification developed by linguist Noam Chomsky, who also wrote Syntactic Structures, the pioneering work searching for the formal grammar underlying human language. This is one of the charms of computing: it may be that the hammer you reach for every day in fact came from a spaceship.\n\nWhen you are writing a regular expression, the best way to start is just to try it against one example at first. Let’s use the findall method to try a regular expression against the filename of the fname object:\n\nre.findall(r'(.+)_\\d+.jpg$', fname.name)\n\n['great_pyrenees']\n\n\nThis regular expression plucks out all the characters leading up to the last underscore character, as long as the subsequence characters are numerical digits and then the JPEG file extension.\nNow that we confirmed the regular expression works for the example, let’s use it to label the whole dataset. fastai comes with many classes to help with labeling. For labeling with regular expressions, we can use the RegexLabeller class. In this example we use the data block API we saw in &lt;&gt; (in fact, we nearly always use the data block API—it’s so much more flexible than the simple factory methods we saw in &lt;&gt;):\n\npets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n                 item_tfms=Resize(460),\n                 batch_tfms=aug_transforms(size=224, min_scale=0.75))\ndls = pets.dataloaders(path/\"images\")\n\nOne important piece of this DataBlock call that we haven’t seen before is in these two lines:\nitem_tfms=Resize(460),\nbatch_tfms=aug_transforms(size=224, min_scale=0.75)\nThese lines implement a fastai data augmentation strategy which we call presizing. Presizing is a particular way to do image augmentation that is designed to minimize data destruction while maintaining good performance."
  },
  {
    "objectID": "Fastbook/05_pet_breeds.html#presizing",
    "href": "Fastbook/05_pet_breeds.html#presizing",
    "title": "Image Classification",
    "section": "Presizing",
    "text": "Presizing\nWe need our images to have the same dimensions, so that they can collate into tensors to be passed to the GPU. We also want to minimize the number of distinct augmentation computations we perform. The performance requirement suggests that we should, where possible, compose our augmentation transforms into fewer transforms (to reduce the number of computations and the number of lossy operations) and transform the images into uniform sizes (for more efficient processing on the GPU).\nThe challenge is that, if performed after resizing down to the augmented size, various common data augmentation transforms might introduce spurious empty zones, degrade data, or both. For instance, rotating an image by 45 degrees fills corner regions of the new bounds with emptiness, which will not teach the model anything. Many rotation and zooming operations will require interpolating to create pixels. These interpolated pixels are derived from the original image data but are still of lower quality.\nTo work around these challenges, presizing adopts two strategies that are shown in &lt;&gt;:\n\nResize images to relatively “large” dimensions—that is, dimensions significantly larger than the target training dimensions.\nCompose all of the common augmentation operations (including a resize to the final target size) into one, and perform the combined operation on the GPU only once at the end of processing, rather than performing the operations individually and interpolating multiple times.\n\nThe first step, the resize, creates images large enough that they have spare margin to allow further augmentation transforms on their inner regions without creating empty zones. This transformation works by resizing to a square, using a large crop size. On the training set, the crop area is chosen randomly, and the size of the crop is selected to cover the entire width or height of the image, whichever is smaller.\nIn the second step, the GPU is used for all data augmentation, and all of the potentially destructive operations are done together, with a single interpolation at the end.\n\nThis picture shows the two steps:\n\nCrop full width or height: This is in item_tfms, so it’s applied to each individual image before it is copied to the GPU. It’s used to ensure all images are the same size. On the training set, the crop area is chosen randomly. On the validation set, the center square of the image is always chosen.\nRandom crop and augment: This is in batch_tfms, so it’s applied to a batch all at once on the GPU, which means it’s fast. On the validation set, only the resize to the final size needed for the model is done here. On the training set, the random crop and any other augmentations are done first.\n\nTo implement this process in fastai you use Resize as an item transform with a large size, and RandomResizedCrop as a batch transform with a smaller size. RandomResizedCrop will be added for you if you include the min_scale parameter in your aug_transforms function, as was done in the DataBlock call in the previous section. Alternatively, you can use pad or squish instead of crop (the default) for the initial Resize.\n&lt;&gt; shows the difference between an image that has been zoomed, interpolated, rotated, and then interpolated again (which is the approach used by all other deep learning libraries), shown here on the right, and an image that has been zoomed and rotated as one operation and then interpolated just once on the left (the fastai approach), shown here on the left.\n\n#hide_input\n#id interpolations\n#caption A comparison of fastai's data augmentation strategy (left) and the traditional approach (right).\ndblock1 = DataBlock(blocks=(ImageBlock(), CategoryBlock()),\n                   get_y=parent_label,\n                   item_tfms=Resize(460))\n# Place an image in the 'images/grizzly.jpg' subfolder where this notebook is located before running this\ndls1 = dblock1.dataloaders([(Path.cwd()/'images'/'grizzly.jpg')]*100, bs=8)\ndls1.train.get_idxs = lambda: Inf.ones\nx,y = dls1.valid.one_batch()\n_,axs = subplots(1, 2)\n\nx1 = TensorImage(x.clone())\nx1 = x1.affine_coord(sz=224)\nx1 = x1.rotate(draw=30, p=1.)\nx1 = x1.zoom(draw=1.2, p=1.)\nx1 = x1.warp(draw_x=-0.2, draw_y=0.2, p=1.)\n\ntfms = setup_aug_tfms([Rotate(draw=30, p=1, size=224), Zoom(draw=1.2, p=1., size=224),\n                       Warp(draw_x=-0.2, draw_y=0.2, p=1., size=224)])\nx = Pipeline(tfms)(x)\n#x.affine_coord(coord_tfm=coord_tfm, sz=size, mode=mode, pad_mode=pad_mode)\nTensorImage(x[0]).show(ctx=axs[0])\nTensorImage(x1[0]).show(ctx=axs[1]);\n\n\n\n\n\n\n\n\nYou can see that the image on the right is less well defined and has reflection padding artifacts in the bottom-left corner; also, the grass at the top left has disappeared entirely. We find that in practice using presizing significantly improves the accuracy of models, and often results in speedups too.\nThe fastai library also provides simple ways to check your data looks right before training a model, which is an extremely important step. We’ll look at those next.\n\nChecking and Debugging a DataBlock\nWe can never just assume that our code is working perfectly. Writing a DataBlock is just like writing a blueprint. You will get an error message if you have a syntax error somewhere in your code, but you have no guarantee that your template is going to work on your data source as you intend. So, before training a model you should always check your data. You can do this using the show_batch method:\n\ndls.show_batch(nrows=1, ncols=3)\n\n\n\n\n\n\n\n\nTake a look at each image, and check that each one seems to have the correct label for that breed of pet. Often, data scientists work with data with which they are not as familiar as domain experts may be: for instance, I actually don’t know what a lot of these pet breeds are. Since I am not an expert on pet breeds, I would use Google images at this point to search for a few of these breeds, and make sure the images look similar to what I see in this output.\nIf you made a mistake while building your DataBlock, it is very likely you won’t see it before this step. To debug this, we encourage you to use the summary method. It will attempt to create a batch from the source you give it, with a lot of details. Also, if it fails, you will see exactly at which point the error happens, and the library will try to give you some help. For instance, one common mistake is to forget to use a Resize transform, so you end up with pictures of different sizes and are not able to batch them. Here is what the summary would look like in that case (note that the exact text may have changed since the time of writing, but it will give you an idea):\n\n#hide_output\npets1 = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'))\npets1.summary(path/\"images\")\n\nSetting-up type transforms pipelines\nCollecting items from /home/jhoward/.fastai/data/oxford-iiit-pet/images\nFound 7390 items\n2 datasets of sizes 5912,1478\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: partial -&gt; Categorize\n\nBuilding one sample\n  Pipeline: PILBase.create\n    starting from\n      /home/jhoward/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_31.jpg\n    applying PILBase.create gives\n      PILImage mode=RGB size=500x414\n  Pipeline: partial -&gt; Categorize\n    starting from\n      /home/jhoward/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_31.jpg\n    applying partial gives\n      american_pit_bull_terrier\n    applying Categorize gives\n      TensorCategory(13)\n\nFinal sample: (PILImage mode=RGB size=500x414, TensorCategory(13))\n\n\nSetting up after_item: Pipeline: ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: IntToFloatTensor\n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: ToTensor\n    starting from\n      (PILImage mode=RGB size=500x414, TensorCategory(13))\n    applying ToTensor gives\n      (TensorImage of size 3x414x500, TensorCategory(13))\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\nError! It's not possible to collate your items in a batch\nCould not collate the 0-th members of your tuples because got the following shapes\ntorch.Size([3, 414, 500]),torch.Size([3, 375, 500]),torch.Size([3, 500, 281]),torch.Size([3, 203, 300])\n\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-11-8c0a3d421ca2&gt; in &lt;module&gt;\n      4                  splitter=RandomSplitter(seed=42),\n      5                  get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'))\n----&gt; 6 pets1.summary(path/\"images\")\n\n~/git/fastai/fastai/data/block.py in summary(self, source, bs, show_batch, **kwargs)\n    182         why = _find_fail_collate(s)\n    183         print(\"Make sure all parts of your samples are tensors of the same size\" if why is None else why)\n--&gt; 184         raise e\n    185 \n    186     if len([f for f in dls.train.after_batch.fs if f.name != 'noop'])!=0:\n\n~/git/fastai/fastai/data/block.py in summary(self, source, bs, show_batch, **kwargs)\n    176     print(\"\\nCollating items in a batch\")\n    177     try:\n--&gt; 178         b = dls.train.create_batch(s)\n    179         b = retain_types(b, s[0] if is_listy(s) else s)\n    180     except Exception as e:\n\n~/git/fastai/fastai/data/load.py in create_batch(self, b)\n    125     def retain(self, res, b):  return retain_types(res, b[0] if is_listy(b) else b)\n    126     def create_item(self, s):  return next(self.it) if s is None else self.dataset[s]\n--&gt; 127     def create_batch(self, b): return (fa_collate,fa_convert)[self.prebatched](b)\n    128     def do_batch(self, b): return self.retain(self.create_batch(self.before_batch(b)), b)\n    129     def to(self, device): self.device = device\n\n~/git/fastai/fastai/data/load.py in fa_collate(t)\n     44     b = t[0]\n     45     return (default_collate(t) if isinstance(b, _collate_types)\n---&gt; 46             else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence)\n     47             else default_collate(t))\n     48 \n\n~/git/fastai/fastai/data/load.py in &lt;listcomp&gt;(.0)\n     44     b = t[0]\n     45     return (default_collate(t) if isinstance(b, _collate_types)\n---&gt; 46             else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence)\n     47             else default_collate(t))\n     48 \n\n~/git/fastai/fastai/data/load.py in fa_collate(t)\n     43 def fa_collate(t):\n     44     b = t[0]\n---&gt; 45     return (default_collate(t) if isinstance(b, _collate_types)\n     46             else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence)\n     47             else default_collate(t))\n\n~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py in default_collate(batch)\n     53             storage = elem.storage()._new_shared(numel)\n     54             out = elem.new(storage)\n---&gt; 55         return torch.stack(batch, 0, out=out)\n     56     elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n     57             and elem_type.__name__ != 'string_':\n\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 414 and 375 in dimension 2 at /opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/TH/generic/THTensor.cpp:612\n\n\n\nSetting-up type transforms pipelines\nCollecting items from /home/sgugger/.fastai/data/oxford-iiit-pet/images\nFound 7390 items\n2 datasets of sizes 5912,1478\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: partial -&gt; Categorize\n\nBuilding one sample\n  Pipeline: PILBase.create\n    starting from\n      /home/sgugger/.fastai/data/oxford-iiit-pet/images/american_bulldog_83.jpg\n    applying PILBase.create gives\n      PILImage mode=RGB size=375x500\n  Pipeline: partial -&gt; Categorize\n    starting from\n      /home/sgugger/.fastai/data/oxford-iiit-pet/images/american_bulldog_83.jpg\n    applying partial gives\n      american_bulldog\n    applying Categorize gives\n      TensorCategory(12)\n\nFinal sample: (PILImage mode=RGB size=375x500, TensorCategory(12))\n\nSetting up after_item: Pipeline: ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: IntToFloatTensor\n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: ToTensor\n    starting from\n      (PILImage mode=RGB size=375x500, TensorCategory(12))\n    applying ToTensor gives\n      (TensorImage of size 3x500x375, TensorCategory(12))\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\nError! It's not possible to collate your items in a batch\nCould not collate the 0-th members of your tuples because got the following \nshapes:\ntorch.Size([3, 500, 375]),torch.Size([3, 375, 500]),torch.Size([3, 333, 500]),\ntorch.Size([3, 375, 500])\nYou can see exactly how we gathered the data and split it, how we went from a filename to a sample (the tuple (image, category)), then what item transforms were applied and how it failed to collate those samples in a batch (because of the different shapes).\nOnce you think your data looks right, we generally recommend the next step should be using it to train a simple model. We often see people put off the training of an actual model for far too long. As a result, they don’t actually find out what their baseline results look like. Perhaps your problem doesn’t need lots of fancy domain-specific engineering. Or perhaps the data doesn’t seem to train the model at all. These are things that you want to know as soon as possible. For this initial test, we’ll use the same simple model that we used in &lt;&gt;:\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.551305\n0.322132\n0.106225\n00:19\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.529473\n0.312148\n0.095399\n00:23\n\n\n1\n0.330207\n0.245883\n0.080514\n00:24\n\n\n\n\n\nAs we’ve briefly discussed before, the table shown when we fit a model shows us the results after each epoch of training. Remember, an epoch is one complete pass through all of the images in the data. The columns shown are the average loss over the items of the training set, the loss on the validation set, and any metrics that we requested—in this case, the error rate.\nRemember that loss is whatever function we’ve decided to use to optimize the parameters of our model. But we haven’t actually told fastai what loss function we want to use. So what is it doing? fastai will generally try to select an appropriate loss function based on what kind of data and model you are using. In this case we have image data and a categorical outcome, so fastai will default to using cross-entropy loss."
  },
  {
    "objectID": "Fastbook/05_pet_breeds.html#cross-entropy-loss",
    "href": "Fastbook/05_pet_breeds.html#cross-entropy-loss",
    "title": "Image Classification",
    "section": "Cross-Entropy Loss",
    "text": "Cross-Entropy Loss\nCross-entropy loss is a loss function that is similar to the one we used in the previous chapter, but (as we’ll see) has two benefits:\n\nIt works even when our dependent variable has more than two categories.\nIt results in faster and more reliable training.\n\nIn order to understand how cross-entropy loss works for dependent variables with more than two categories, we first have to understand what the actual data and activations that are seen by the loss function look like.\n\nViewing Activations and Labels\nLet’s take a look at the activations of our model. To actually get a batch of real data from our DataLoaders, we can use the one_batch method:\n\nx,y = dls.one_batch()\n\nAs you see, this returns the dependent and independent variables, as a mini-batch. Let’s see what is actually contained in our dependent variable:\n\ny\n\nTensorCategory([ 0,  5, 23, 36,  5, 20, 29, 34, 33, 32, 31, 24, 12, 36,  8, 26, 30,  2, 12, 17,  7, 23, 12, 29, 21,  4, 35, 33,  0, 20, 26, 30,  3,  6, 36,  2, 17, 32, 11,  6,  3, 30,  5, 26, 26, 29,  7, 36,\n        31, 26, 26,  8, 13, 30, 11, 12, 36, 31, 34, 20, 15,  8,  8, 23], device='cuda:5')\n\n\nOur batch size is 64, so we have 64 rows in this tensor. Each row is a single integer between 0 and 36, representing our 37 possible pet breeds. We can view the predictions (that is, the activations of the final layer of our neural network) using Learner.get_preds. This function either takes a dataset index (0 for train and 1 for valid) or an iterator of batches. Thus, we can pass it a simple list with our batch to get our predictions. It returns predictions and targets by default, but since we already have the targets, we can effectively ignore them by assigning to the special variable _:\n\npreds,_ = learn.get_preds(dl=[(x,y)])\npreds[0]\n\n\n\n\ntensor([9.9911e-01, 5.0433e-05, 3.7515e-07, 8.8590e-07, 8.1794e-05, 1.8991e-05, 9.9280e-06, 5.4656e-07, 6.7920e-06, 2.3486e-04, 3.7872e-04, 2.0796e-05, 4.0443e-07, 1.6933e-07, 2.0502e-07, 3.1354e-08,\n        9.4115e-08, 2.9782e-06, 2.0243e-07, 8.5262e-08, 1.0900e-07, 1.0175e-07, 4.4780e-09, 1.4285e-07, 1.0718e-07, 8.1411e-07, 3.6618e-07, 4.0950e-07, 3.8525e-08, 2.3660e-07, 5.3747e-08, 2.5448e-07,\n        6.5860e-08, 8.0937e-05, 2.7464e-07, 5.6760e-07, 1.5462e-08])\n\n\nThe actual predictions are 37 probabilities between 0 and 1, which add up to 1 in total:\n\nlen(preds[0]),preds[0].sum()\n\n(37, tensor(1.0000))\n\n\nTo transform the activations of our model into predictions like this, we used something called the softmax activation function.\n\n\nSoftmax\nIn our classification model, we use the softmax activation function in the final layer to ensure that the activations are all between 0 and 1, and that they sum to 1.\nSoftmax is similar to the sigmoid function, which we saw earlier. As a reminder sigmoid looks like this:\n\nplot_function(torch.sigmoid, min=-4,max=4)\n\n\n\n\n\n\n\n\nWe can apply this function to a single column of activations from a neural network, and get back a column of numbers between 0 and 1, so it’s a very useful activation function for our final layer.\nNow think about what happens if we want to have more categories in our target (such as our 37 pet breeds). That means we’ll need more activations than just a single column: we need an activation per category. We can create, for instance, a neural net that predicts 3s and 7s that returns two activations, one for each class—this will be a good first step toward creating the more general approach. Let’s just use some random numbers with a standard deviation of 2 (so we multiply randn by 2) for this example, assuming we have 6 images and 2 possible categories (where the first column represents 3s and the second is 7s):\n\n#hide\ntorch.random.manual_seed(42);\n\n\nacts = torch.randn((6,2))*2\nacts\n\ntensor([[ 0.6734,  0.2576],\n        [ 0.4689,  0.4607],\n        [-2.2457, -0.3727],\n        [ 4.4164, -1.2760],\n        [ 0.9233,  0.5347],\n        [ 1.0698,  1.6187]])\n\n\nWe can’t just take the sigmoid of this directly, since we don’t get rows that add to 1 (i.e., we want the probability of being a 3 plus the probability of being a 7 to add up to 1):\n\nacts.sigmoid()\n\ntensor([[0.6623, 0.5641],\n        [0.6151, 0.6132],\n        [0.0957, 0.4079],\n        [0.9881, 0.2182],\n        [0.7157, 0.6306],\n        [0.7446, 0.8346]])\n\n\nIn &lt;&gt;, our neural net created a single activation per image, which we passed through the sigmoid function. That single activation represented the model’s confidence that the input was a 3. Binary problems are a special case of classification problems, because the target can be treated as a single boolean value, as we did in mnist_loss. But binary problems can also be thought of in the context of the more general group of classifiers with any number of categories: in this case, we happen to have two categories. As we saw in the bear classifier, our neural net will return one activation per category.\nSo in the binary case, what do those activations really indicate? A single pair of activations simply indicates the relative confidence of the input being a 3 versus being a 7. The overall values, whether they are both high, or both low, don’t matter—all that matters is which is higher, and by how much.\nWe would expect that since this is just another way of representing the same problem, that we would be able to use sigmoid directly on the two-activation version of our neural net. And indeed we can! We can just take the difference between the neural net activations, because that reflects how much more sure we are of the input being a 3 than a 7, and then take the sigmoid of that:\n\n(acts[:,0]-acts[:,1]).sigmoid()\n\ntensor([0.6025, 0.5021, 0.1332, 0.9966, 0.5959, 0.3661])\n\n\nThe second column (the probability of it being a 7) will then just be that value subtracted from 1. Now, we need a way to do all this that also works for more than two columns. It turns out that this function, called softmax, is exactly that:\ndef softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True)\n\njargon: Exponential function (exp): Literally defined as e**x, where e is a special number approximately equal to 2.718. It is the inverse of the natural logarithm function. Note that exp is always positive, and it increases very rapidly!\n\nLet’s check that softmax returns the same values as sigmoid for the first column, and those values subtracted from 1 for the second column:\n\nsm_acts = torch.softmax(acts, dim=1)\nsm_acts\n\ntensor([[0.6025, 0.3975],\n        [0.5021, 0.4979],\n        [0.1332, 0.8668],\n        [0.9966, 0.0034],\n        [0.5959, 0.4041],\n        [0.3661, 0.6339]])\n\n\nsoftmax is the multi-category equivalent of sigmoid—we have to use it any time we have more than two categories and the probabilities of the categories must add to 1, and we often use it even when there are just two categories, just to make things a bit more consistent. We could create other functions that have the properties that all activations are between 0 and 1, and sum to 1; however, no other function has the same relationship to the sigmoid function, which we’ve seen is smooth and symmetric. Also, we’ll see shortly that the softmax function works well hand-in-hand with the loss function we will look at in the next section.\nIf we have three output activations, such as in our bear classifier, calculating softmax for a single bear image would then look like something like &lt;&gt;.\n\nWhat does this function do in practice? Taking the exponential ensures all our numbers are positive, and then dividing by the sum ensures we are going to have a bunch of numbers that add up to 1. The exponential also has a nice property: if one of the numbers in our activations x is slightly bigger than the others, the exponential will amplify this (since it grows, well… exponentially), which means that in the softmax, that number will be closer to 1.\nIntuitively, the softmax function really wants to pick one class among the others, so it’s ideal for training a classifier when we know each picture has a definite label. (Note that it may be less ideal during inference, as you might want your model to sometimes tell you it doesn’t recognize any of the classes that it has seen during training, and not pick a class because it has a slightly bigger activation score. In this case, it might be better to train a model using multiple binary output columns, each using a sigmoid activation.)\nSoftmax is the first part of the cross-entropy loss—the second part is log likelihood.\n\n\nLog Likelihood\nWhen we calculated the loss for our MNIST example in the last chapter we used:\ndef mnist_loss(inputs, targets):\n    inputs = inputs.sigmoid()\n    return torch.where(targets==1, 1-inputs, inputs).mean()\nJust as we moved from sigmoid to softmax, we need to extend the loss function to work with more than just binary classification—it needs to be able to classify any number of categories (in this case, we have 37 categories). Our activations, after softmax, are between 0 and 1, and sum to 1 for each row in the batch of predictions. Our targets are integers between 0 and 36. Furthermore, cross-entropy loss generalizes our binary classification loss and allows for more than one correct label per example (which is called multi-label classificaiton, which we will discuss in Chapter 6).\nIn the binary case, we used torch.where to select between inputs and 1-inputs. When we treat a binary classification as a general classification problem with two categories, it actually becomes even easier, because (as we saw in the previous section) we now have two columns, containing the equivalent of inputs and 1-inputs. Since there is only one correct label per example, all we need to do is select the appropriate column (as opposed to multiplying multiple probabilities). Let’s try to implement this in PyTorch. For our synthetic 3s and 7s example, let’s say these are our labels:\n\ntarg = tensor([0,1,0,1,1,0])\n\nand these are the softmax activations:\n\nsm_acts\n\ntensor([[0.6025, 0.3975],\n        [0.5021, 0.4979],\n        [0.1332, 0.8668],\n        [0.9966, 0.0034],\n        [0.5959, 0.4041],\n        [0.3661, 0.6339]])\n\n\nThen for each item of targ we can use that to select the appropriate column of sm_acts using tensor indexing, like so:\n\nidx = range(6)\nsm_acts[idx, targ]\n\ntensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661])\n\n\nTo see exactly what’s happening here, let’s put all the columns together in a table. Here, the first two columns are our activations, then we have the targets and the row index. We explain the last column, result below:\n\n#hide_input\nfrom IPython.display import HTML\ndf = pd.DataFrame(sm_acts, columns=[\"3\",\"7\"])\ndf['targ'] = targ\ndf['idx'] = idx\ndf['result'] = sm_acts[range(6), targ]\nt = df.style.hide_index()\n#To have html code compatible with our script\nhtml = t._repr_html_().split('&lt;/style&gt;')[1]\nhtml = re.sub(r'&lt;table id=\"([^\"]+)\"\\s*&gt;', r'&lt;table &gt;', html)\ndisplay(HTML(html))\n\n\n\n\n3\n7\ntarg\nidx\nresult\n\n\n\n\n0.602469\n0.397531\n0\n0\n0.602469\n\n\n0.502065\n0.497935\n1\n1\n0.497935\n\n\n0.133188\n0.866811\n0\n2\n0.133188\n\n\n0.996640\n0.003360\n1\n3\n0.003360\n\n\n0.595949\n0.404051\n1\n4\n0.404051\n\n\n0.366118\n0.633882\n0\n5\n0.366118\n\n\n\n\n\nLooking at this table, you can see that the result column can be calculated by taking the targ and idx columns as indices into the two-column matrix containing the 3 and 7 columns. That’s what sm_acts[idx, targ] is actually doing. The really interesting thing here is that this actually works just as well with more than two columns. To see this, consider what would happen if we added an activation column for every digit (0 through 9), and then targ contained a number from 0 to 9.\nPyTorch provides a function that does exactly the same thing as sm_acts[range(n), targ] (except it takes the negative, because when applying the log afterward, we will have negative numbers), called nll_loss (NLL stands for negative log likelihood):\n\n-sm_acts[idx, targ]\n\ntensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])\n\n\n\nF.nll_loss(sm_acts, targ, reduction='none')\n\ntensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])\n\n\nDespite its name, this PyTorch function does not take the log. We’ll see why in the next section, but first, let’s see why taking the logarithm can be useful.\n\nwarning: Confusing Name, Beware: The nll in nll_loss stands for “negative log likelihood,” but it doesn’t actually take the log at all! It assumes you have already taken the log. PyTorch has a function called log_softmax that combines log and softmax in a fast and accurate way. nll_loss is designed to be used after log_softmax.\n\n\nTaking the Log\nRecall that cross entropy loss may involve the multiplication of many numbers. Multiplying lots of negative numbers together can cause problems like numerical underflow in computers. Therefore, we want to transform these probabilities to larger values so we can perform mathematical operations on them. There is a mathematical function that does exactly this: the logarithm (available as torch.log). It is not defined for numbers less than 0, and looks like this between 0 and 1:\n\nplot_function(torch.log, min=0,max=1, ty='log(x)', tx='x')\n\n\n\n\n\n\n\n\nAdditionally, we want to ensure our model is able to detect differences between small numbers. For example, consider the probabilities of .01 and .001. Indeed, those numbers are very close together—but in another sense, 0.01 is 10 times more confident than 0.001. By taking the log of our probabilities, we prevent these important differences from being ignored.\nDoes “logarithm” ring a bell? The logarithm function has this identity:\ny = b**a\na = log(y,b)\nIn this case, we’re assuming that log(y,b) returns log y base b. However, PyTorch actually doesn’t define log this way: log in Python uses the special number e (2.718…) as the base.\nPerhaps a logarithm is something that you have not thought about for the last 20 years or so. But it’s a mathematical idea that is going to be really critical for many things in deep learning, so now would be a great time to refresh your memory. The key thing to know about logarithms is this relationship:\nlog(a*b) = log(a)+log(b)\nWhen we see it in that format, it looks a bit boring; but think about what this really means. It means that logarithms increase linearly when the underlying signal increases exponentially or multiplicatively. This is used, for instance, in the Richter scale of earthquake severity, and the dB scale of noise levels. It’s also often used on financial charts, where we want to show compound growth rates more clearly. Computer scientists love using logarithms, because it means that multiplication, which can create really really large and really really small numbers, can be replaced by addition, which is much less likely to result in scales that are difficult for our computers to handle.\nObserve that the log of a number approaches negative infinity as the number approaches zero. In our case, since the result relfects the predicted probability of the correct label, we want our loss function to return a small value when the prediction is “good” (closer to 1) and a large value when the prediction is “bad” (closer to 0). We can achieve this by taking the negative of the log:\n\nplot_function(lambda x: -1*torch.log(x), min=0,max=1, tx='x', ty='- log(x)', title = 'Log Loss when true label = 1')\n\n\n\n\n\n\n\n\n\ns: It’s not just computer scientists that love logs! Until computers came along, engineers and scientists used a special ruler called a “slide rule” that did multiplication by adding logarithms. Logarithms are widely used in physics, for multiplying very big or very small numbers, and many other fields.\n\nLet’s go ahead and update our previous table with an additional column, loss to reflect this loss function:\n\n#hide_input\nfrom IPython.display import HTML\ndf['loss'] = -torch.log(tensor(df['result']))\nt = df.style.hide_index()\n#To have html code compatible with our script\nhtml = t._repr_html_().split('&lt;/style&gt;')[1]\nhtml = re.sub(r'&lt;table id=\"([^\"]+)\"\\s*&gt;', r'&lt;table &gt;', html)\ndisplay(HTML(html))\n\n\n\n\n3\n7\ntarg\nidx\nresult\nloss\n\n\n\n\n0.602469\n0.397531\n0\n0\n0.602469\n0.506720\n\n\n0.502065\n0.497935\n1\n1\n0.497935\n0.697285\n\n\n0.133188\n0.866811\n0\n2\n0.133188\n2.015990\n\n\n0.996640\n0.003360\n1\n3\n0.003360\n5.695763\n\n\n0.595949\n0.404051\n1\n4\n0.404051\n0.906213\n\n\n0.366118\n0.633882\n0\n5\n0.366118\n1.004798\n\n\n\n\n\nNotice how the loss is very large in the third and fourth rows where the predictions are confident and wrong, or in other words have high probabilities on the wrong class. One benefit of using the log to calculate the loss is that our loss function penalizes predictions that are both confident and wrong. This kind of penalty works well in practice to aid in more effective model training.\n\ns: There are other loss functions such as focal loss that allow you control this penalty with a parameter. We do not discuss that loss function in this book.\n\nWe’re calculating the loss from the column containing the correct label. Because there is only one “right” answer per example, we don’t need to consider the other columns, because by the definition of softmax, they add up to 1 minus the activation corresponding to the correct label. As long as the activation columns sum to 1 (as they will, if we use softmax), then we’ll have a loss function that shows how well we’re predicting each digit. Therefore, making the activation for the correct label as high as possible must mean we’re also decreasing the activations of the remaining columns.\n\n\n\nNegative Log Likelihood\nTaking the mean of the negative log of our probabilities (taking the mean of the loss column of our table) gives us the negative log likelihood loss, which is another name for cross-entropy loss. Recall that PyTorch’s nll_loss assumes that you already took the log of the softmax, so it doesn’t actually do the logarithm for you.\nWhen we first take the softmax, and then the log likelihood of that, that combination is called cross-entropy loss. In PyTorch, this is available as nn.CrossEntropyLoss (which, in practice, actually does log_softmax and then nll_loss):\n\nloss_func = nn.CrossEntropyLoss()\n\nAs you see, this is a class. Instantiating it gives you an object which behaves like a function:\n\nloss_func(acts, targ)\n\ntensor(1.8045)\n\n\nAll PyTorch loss functions are provided in two forms, the class just shown above, and also a plain functional form, available in the F namespace:\n\nF.cross_entropy(acts, targ)\n\ntensor(1.8045)\n\n\nEither one works fine and can be used in any situation. We’ve noticed that most people tend to use the class version, and that’s more often used in PyTorch’s official docs and examples, so we’ll tend to use that too.\nBy default PyTorch loss functions take the mean of the loss of all items. You can use reduction='none' to disable that:\n\nnn.CrossEntropyLoss(reduction='none')(acts, targ)\n\ntensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])\n\n\nYou will notice these values match the loss column in our table exactly.\n\ns: An interesting feature about cross-entropy loss appears when we consider its gradient. The gradient of cross_entropy(a,b) is just softmax(a)-b. Since softmax(a) is just the final activation of the model, that means that the gradient is proportional to the difference between the prediction and the target. This is the same as mean squared error in regression (assuming there’s no final activation function such as that added by y_range), since the gradient of (a-b)**2 is 2*(a-b). Because the gradient is linear, that means we won’t see sudden jumps or exponential increases in gradients, which should lead to smoother training of models.\n\nWe have now seen all the pieces hidden behind our loss function. But while this puts a number on how well (or badly) our model is doing, it does nothing to help us know if it’s actually any good. Let’s now see some ways to interpret our model’s predictions."
  },
  {
    "objectID": "Fastbook/05_pet_breeds.html#model-interpretation",
    "href": "Fastbook/05_pet_breeds.html#model-interpretation",
    "title": "Image Classification",
    "section": "Model Interpretation",
    "text": "Model Interpretation\nIt’s very hard to interpret loss functions directly, because they are designed to be things computers can differentiate and optimize, not things that people can understand. That’s why we have metrics. These are not used in the optimization process, but just to help us poor humans understand what’s going on. In this case, our accuracy is looking pretty good already! So where are we making mistakes?\nWe saw in &lt;&gt; that we can use a confusion matrix to see where our model is doing well, and where it’s doing badly:\n\n#width 600\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(12,12), dpi=60)\n\n\n\n\n\n\n\n\n\n\n\nOh dear—in this case, a confusion matrix is very hard to read. We have 37 different breeds of pet, which means we have 37×37 entries in this giant matrix! Instead, we can use the most_confused method, which just shows us the cells of the confusion matrix with the most incorrect predictions (here, with at least 5 or more):\n\ninterp.most_confused(min_val=5)\n\n[('american_pit_bull_terrier', 'staffordshire_bull_terrier', 10),\n ('Ragdoll', 'Birman', 8),\n ('Siamese', 'Birman', 6),\n ('Bengal', 'Egyptian_Mau', 5),\n ('american_pit_bull_terrier', 'american_bulldog', 5)]\n\n\nSince we are not pet breed experts, it is hard for us to know whether these category errors reflect actual difficulties in recognizing breeds. So again, we turn to Google. A little bit of Googling tells us that the most common category errors shown here are actually breed differences that even expert breeders sometimes disagree about. So this gives us some comfort that we are on the right track.\nWe seem to have a good baseline. What can we do now to make it even better?"
  },
  {
    "objectID": "Fastbook/05_pet_breeds.html#improving-our-model",
    "href": "Fastbook/05_pet_breeds.html#improving-our-model",
    "title": "Image Classification",
    "section": "Improving Our Model",
    "text": "Improving Our Model\nWe will now look at a range of techniques to improve the training of our model and make it better. While doing so, we will explain a little bit more about transfer learning and how to fine-tune our pretrained model as best as possible, without breaking the pretrained weights.\nThe first thing we need to set when training a model is the learning rate. We saw in the previous chapter that it needs to be just right to train as efficiently as possible, so how do we pick a good one? fastai provides a tool for this.\n\nThe Learning Rate Finder\nOne of the most important things we can do when training a model is to make sure that we have the right learning rate. If our learning rate is too low, it can take many, many epochs to train our model. Not only does this waste time, but it also means that we may have problems with overfitting, because every time we do a complete pass through the data, we give our model a chance to memorize it.\nSo let’s just make our learning rate really high, right? Sure, let’s try that and see what happens:\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1, base_lr=0.1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.778816\n5.150732\n0.504060\n00:20\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n4.354680\n3.003533\n0.834235\n00:24\n\n\n\n\n\nThat doesn’t look good. Here’s what happened. The optimizer stepped in the correct direction, but it stepped so far that it totally overshot the minimum loss. Repeating that multiple times makes it get further and further away, not closer and closer!\nWhat do we do to find the perfect learning rate—not too high, and not too low? In 2015 the researcher Leslie Smith came up with a brilliant idea, called the learning rate finder. His idea was to start with a very, very small learning rate, something so small that we would never expect it to be too big to handle. We use that for one mini-batch, find what the losses are afterwards, and then increase the learning rate by some percentage (e.g., doubling it each time). Then we do another mini-batch, track the loss, and double the learning rate again. We keep doing this until the loss gets worse, instead of better. This is the point where we know we have gone too far. We then select a learning rate a bit lower than this point. Our advice is to pick either:\n\nOne order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10)\nThe last point where the loss was clearly decreasing\n\nThe learning rate finder computes those points on the curve to help you. Both these rules usually give around the same value. In the first chapter, we didn’t specify a learning rate, using the default value from the fastai library (which is 1e-3):\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlr_min,lr_steep = learn.lr_find(suggest_funcs=(minimum, steep))\n\n\n\n\n\n\n\n\n\n\n\n\nprint(f\"Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}\")\n\nMinimum/10: 1.00e-02, steepest point: 5.25e-03\n\n\nWe can see on this plot that in the range 1e-6 to 1e-3, nothing really happens and the model doesn’t train. Then the loss starts to decrease until it reaches a minimum, and then increases again. We don’t want a learning rate greater than 1e-1 as it will give a training that diverges like the one before (you can try for yourself), but 1e-1 is already too high: at this stage we’ve left the period where the loss was decreasing steadily.\nIn this learning rate plot it appears that a learning rate around 3e-3 would be appropriate, so let’s choose that:\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2, base_lr=3e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.328591\n0.344678\n0.114344\n00:20\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.540180\n0.420945\n0.127876\n00:24\n\n\n1\n0.329827\n0.248813\n0.083221\n00:24\n\n\n\n\n\n\nNote: Logarithmic Scale: The learning rate finder plot has a logarithmic scale, which is why the middle point between 1e-3 and 1e-2 is between 3e-3 and 4e-3. This is because we care mostly about the order of magnitude of the learning rate.\n\nIt’s interesting that the learning rate finder was only discovered in 2015, while neural networks have been under development since the 1950s. Throughout that time finding a good learning rate has been, perhaps, the most important and challenging issue for practitioners. The solution does not require any advanced maths, giant computing resources, huge datasets, or anything else that would make it inaccessible to any curious researcher. Furthermore, Leslie Smith, was not part of some exclusive Silicon Valley lab, but was working as a naval researcher. All of this is to say: breakthrough work in deep learning absolutely does not require access to vast resources, elite teams, or advanced mathematical ideas. There is lots of work still to be done that requires just a bit of common sense, creativity, and tenacity.\nNow that we have a good learning rate to train our model, let’s look at how we can fine-tune the weights of a pretrained model.\n\n\nUnfreezing and Transfer Learning\nWe discussed briefly in &lt;&gt; how transfer learning works. We saw that the basic idea is that a pretrained model, trained potentially on millions of data points (such as ImageNet), is fine-tuned for some other task. But what does this really mean?\nWe now know that a convolutional neural network consists of many linear layers with a nonlinear activation function between each pair, followed by one or more final linear layers with an activation function such as softmax at the very end. The final linear layer uses a matrix with enough columns such that the output size is the same as the number of classes in our model (assuming that we are doing classification).\nThis final linear layer is unlikely to be of any use for us when we are fine-tuning in a transfer learning setting, because it is specifically designed to classify the categories in the original pretraining dataset. So when we do transfer learning we remove it, throw it away, and replace it with a new linear layer with the correct number of outputs for our desired task (in this case, there would be 37 activations).\nThis newly added linear layer will have entirely random weights. Therefore, our model prior to fine-tuning has entirely random outputs. But that does not mean that it is an entirely random model! All of the layers prior to the last one have been carefully trained to be good at image classification tasks in general. As we saw in the images from the Zeiler and Fergus paper in &lt;&gt; (see &lt;&gt; through &lt;&gt;), the first few layers encode very general concepts, such as finding gradients and edges, and later layers encode concepts that are still very useful for us, such as finding eyeballs and fur.\nWe want to train a model in such a way that we allow it to remember all of these generally useful ideas from the pretrained model, use them to solve our particular task (classify pet breeds), and only adjust them as required for the specifics of our particular task.\nOur challenge when fine-tuning is to replace the random weights in our added linear layers with weights that correctly achieve our desired task (classifying pet breeds) without breaking the carefully pretrained weights and the other layers. There is actually a very simple trick to allow this to happen: tell the optimizer to only update the weights in those randomly added final layers. Don’t change the weights in the rest of the neural network at all. This is called freezing those pretrained layers.\nWhen we create a model from a pretrained network fastai automatically freezes all of the pretrained layers for us. When we call the fine_tune method fastai does two things:\n\nTrains the randomly added layers for one epoch, with all other layers frozen\nUnfreezes all of the layers, and trains them all for the number of epochs requested\n\nAlthough this is a reasonable default approach, it is likely that for your particular dataset you may get better results by doing things slightly differently. The fine_tune method has a number of parameters you can use to change its behavior, but it might be easiest for you to just call the underlying methods directly if you want to get some custom behavior. Remember that you can see the source code for the method by using the following syntax:\nlearn.fine_tune??\nSo let’s try doing this manually ourselves. First of all we will train the randomly added layers for three epochs, using fit_one_cycle. As mentioned in &lt;&gt;, fit_one_cycle is the suggested way to train models without using fine_tune. We’ll see why later in the book; in short, what fit_one_cycle does is to start training at a low learning rate, gradually increase it for the first section of training, and then gradually decrease it again for the last section of training.\n\nlearn.fine_tune??\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.188042\n0.355024\n0.102842\n00:20\n\n\n1\n0.534234\n0.302453\n0.094723\n00:20\n\n\n2\n0.325031\n0.222268\n0.074425\n00:20\n\n\n\n\n\nThen we’ll unfreeze the model:\n\nlearn.unfreeze()\n\nand run lr_find again, because having more layers to train, and weights that have already been trained for three epochs, means our previously found learning rate isn’t appropriate any more:\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\n\n\n\nNote that the graph is a little different from when we had random weights: we don’t have that sharp descent that indicates the model is training. That’s because our model has been trained already. Here we have a somewhat flat area before a sharp increase, and we should take a point well before that sharp increase—for instance, 1e-5. The point with the maximum gradient isn’t what we look for here and should be ignored.\nLet’s train at a suitable learning rate:\n\nlearn.fit_one_cycle(6, lr_max=1e-5)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.263579\n0.217419\n0.069012\n00:24\n\n\n1\n0.253060\n0.210346\n0.062923\n00:24\n\n\n2\n0.224340\n0.207357\n0.060217\n00:24\n\n\n3\n0.200195\n0.207244\n0.061570\n00:24\n\n\n4\n0.194269\n0.200149\n0.059540\n00:25\n\n\n5\n0.173164\n0.202301\n0.059540\n00:25\n\n\n\n\n\nThis has improved our model a bit, but there’s more we can do. The deepest layers of our pretrained model might not need as high a learning rate as the last ones, so we should probably use different learning rates for those—this is known as using discriminative learning rates.\n\n\nDiscriminative Learning Rates\nEven after we unfreeze, we still care a lot about the quality of those pretrained weights. We would not expect that the best learning rate for those pretrained parameters would be as high as for the randomly added parameters, even after we have tuned those randomly added parameters for a few epochs. Remember, the pretrained weights have been trained for hundreds of epochs, on millions of images.\nIn addition, do you remember the images we saw in &lt;&gt;, showing what each layer learns? The first layer learns very simple foundations, like edge and gradient detectors; these are likely to be just as useful for nearly any task. The later layers learn much more complex concepts, like “eye” and “sunset,” which might not be useful in your task at all (maybe you’re classifying car models, for instance). So it makes sense to let the later layers fine-tune more quickly than earlier layers.\nTherefore, fastai’s default approach is to use discriminative learning rates. This was originally developed in the ULMFiT approach to NLP transfer learning that we will introduce in &lt;&gt;. Like many good ideas in deep learning, it is extremely simple: use a lower learning rate for the early layers of the neural network, and a higher learning rate for the later layers (and especially the randomly added layers). The idea is based on insights developed by Jason Yosinski, who showed in 2014 that with transfer learning different layers of a neural network should train at different speeds, as seen in &lt;&gt;.\n\nfastai lets you pass a Python slice object anywhere that a learning rate is expected. The first value passed will be the learning rate in the earliest layer of the neural network, and the second value will be the learning rate in the final layer. The layers in between will have learning rates that are multiplicatively equidistant throughout that range. Let’s use this approach to replicate the previous training, but this time we’ll only set the lowest layer of our net to a learning rate of 1e-6; the other layers will scale up to 1e-4. Let’s train for a while and see what happens:\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3)\nlearn.unfreeze()\nlearn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.145300\n0.345568\n0.119756\n00:20\n\n\n1\n0.533986\n0.251944\n0.077131\n00:20\n\n\n2\n0.317696\n0.208371\n0.069012\n00:20\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.257977\n0.205400\n0.067659\n00:25\n\n\n1\n0.246763\n0.205107\n0.066306\n00:25\n\n\n2\n0.240595\n0.193848\n0.062246\n00:25\n\n\n3\n0.209988\n0.198061\n0.062923\n00:25\n\n\n4\n0.194756\n0.193130\n0.064276\n00:25\n\n\n5\n0.169985\n0.187885\n0.056157\n00:25\n\n\n6\n0.153205\n0.186145\n0.058863\n00:25\n\n\n7\n0.141480\n0.185316\n0.053451\n00:25\n\n\n8\n0.128564\n0.180999\n0.051421\n00:25\n\n\n9\n0.126941\n0.186288\n0.054127\n00:25\n\n\n10\n0.130064\n0.181764\n0.054127\n00:25\n\n\n11\n0.124281\n0.181855\n0.054127\n00:25\n\n\n\n\n\nNow the fine-tuning is working great!\nfastai can show us a graph of the training and validation loss:\n\nlearn.recorder.plot_loss()\n\n\n\n\n\n\n\n\nAs you can see, the training loss keeps getting better and better. But notice that eventually the validation loss improvement slows, and sometimes even gets worse! This is the point at which the model is starting to over fit. In particular, the model is becoming overconfident of its predictions. But this does not mean that it is getting less accurate, necessarily. Take a look at the table of training results per epoch, and you will often see that the accuracy continues improving, even as the validation loss gets worse. In the end what matters is your accuracy, or more generally your chosen metrics, not the loss. The loss is just the function we’ve given the computer to help us to optimize.\nAnother decision you have to make when training the model is for how long to train for. We’ll consider that next.\n\n\nSelecting the Number of Epochs\nOften you will find that you are limited by time, rather than generalization and accuracy, when choosing how many epochs to train for. So your first approach to training should be to simply pick a number of epochs that will train in the amount of time that you are happy to wait for. Then look at the training and validation loss plots, as shown above, and in particular your metrics, and if you see that they are still getting better even in your final epochs, then you know that you have not trained for too long.\nOn the other hand, you may well see that the metrics you have chosen are really getting worse at the end of training. Remember, it’s not just that we’re looking for the validation loss to get worse, but the actual metrics. Your validation loss will first get worse during training because the model gets overconfident, and only later will get worse because it is incorrectly memorizing the data. We only care in practice about the latter issue. Remember, our loss function is just something that we use to allow our optimizer to have something it can differentiate and optimize; it’s not actually the thing we care about in practice.\nBefore the days of 1cycle training it was very common to save the model at the end of each epoch, and then select whichever model had the best accuracy out of all of the models saved in each epoch. This is known as early stopping. However, this is very unlikely to give you the best answer, because those epochs in the middle occur before the learning rate has had a chance to reach the small values, where it can really find the best result. Therefore, if you find that you have overfit, what you should actually do is retrain your model from scratch, and this time select a total number of epochs based on where your previous best results were found.\nIf you have the time to train for more epochs, you may want to instead use that time to train more parameters—that is, use a deeper architecture.\n\n\nDeeper Architectures\nIn general, a model with more parameters can model your data more accurately. (There are lots and lots of caveats to this generalization, and it depends on the specifics of the architectures you are using, but it is a reasonable rule of thumb for now.) For most of the architectures that we will be seeing in this book, you can create larger versions of them by simply adding more layers. However, since we want to use pretrained models, we need to make sure that we choose a number of layers that have already been pretrained for us.\nThis is why, in practice, architectures tend to come in a small number of variants. For instance, the ResNet architecture that we are using in this chapter comes in variants with 18, 34, 50, 101, and 152 layer, pretrained on ImageNet. A larger (more layers and parameters; sometimes described as the “capacity” of a model) version of a ResNet will always be able to give us a better training loss, but it can suffer more from overfitting, because it has more parameters to overfit with.\nIn general, a bigger model has the ability to better capture the real underlying relationships in your data, and also to capture and memorize the specific details of your individual images.\nHowever, using a deeper model is going to require more GPU RAM, so you may need to lower the size of your batches to avoid an out-of-memory error. This happens when you try to fit too much inside your GPU and looks like:\nCuda runtime error: out of memory\nYou may have to restart your notebook when this happens. The way to solve it is to use a smaller batch size, which means passing smaller groups of images at any given time through your model. You can pass the batch size you want to the call creating your DataLoaders with bs=.\nThe other downside of deeper architectures is that they take quite a bit longer to train. One technique that can speed things up a lot is mixed-precision training. This refers to using less-precise numbers (half-precision floating point, also called fp16) where possible during training. As we are writing these words in early 2020, nearly all current NVIDIA GPUs support a special feature called tensor cores that can dramatically speed up neural network training, by 2-3x. They also require a lot less GPU memory. To enable this feature in fastai, just add to_fp16() after your Learner creation (you also need to import the module).\nYou can’t really know ahead of time what the best architecture for your particular problem is—you need to try training some. So let’s try a ResNet-50 now with mixed precision:\n\nfrom fastai.callback.fp16 import *\nlearn = vision_learner(dls, resnet50, metrics=error_rate).to_fp16()\nlearn.fine_tune(6, freeze_epochs=3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.427505\n0.310554\n0.098782\n00:21\n\n\n1\n0.606785\n0.302325\n0.094723\n00:22\n\n\n2\n0.409267\n0.294803\n0.091340\n00:21\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.261121\n0.274507\n0.083897\n00:26\n\n\n1\n0.296653\n0.318649\n0.084574\n00:26\n\n\n2\n0.242356\n0.253677\n0.069012\n00:26\n\n\n3\n0.150684\n0.251438\n0.065629\n00:26\n\n\n4\n0.094997\n0.239772\n0.064276\n00:26\n\n\n5\n0.061144\n0.228082\n0.054804\n00:26\n\n\n\n\n\nYou’ll see here we’ve gone back to using fine_tune, since it’s so handy! We can pass freeze_epochs to tell fastai how many epochs to train for while frozen. It will automatically change learning rates appropriately for most datasets.\nIn this case, we’re not seeing a clear win from the deeper model. This is useful to remember—bigger models aren’t necessarily better models for your particular case! Make sure you try small models before you start scaling up."
  },
  {
    "objectID": "Fastbook/05_pet_breeds.html#conclusion",
    "href": "Fastbook/05_pet_breeds.html#conclusion",
    "title": "Image Classification",
    "section": "Conclusion",
    "text": "Conclusion\nIn this chapter you learned some important practical tips, both for getting your image data ready for modeling (presizing, data block summary) and for fitting the model (learning rate finder, unfreezing, discriminative learning rates, setting the number of epochs, and using deeper architectures). Using these tools will help you to build more accurate image models, more quickly.\nWe also discussed cross-entropy loss. This part of the book is worth spending plenty of time on. You aren’t likely to need to actually implement cross-entropy loss from scratch yourself in practice, but it’s really important you understand the inputs to and output from that function, because it (or a variant of it, as we’ll see in the next chapter) is used in nearly every classification model. So when you want to debug a model, or put a model in production, or improve the accuracy of a model, you’re going to need to be able to look at its activations and loss, and understand what’s going on, and why. You can’t do that properly if you don’t understand your loss function.\nIf cross-entropy loss hasn’t “clicked” for you just yet, don’t worry—you’ll get there! First, go back to the last chapter and make sure you really understand mnist_loss. Then work gradually through the cells of the notebook for this chapter, where we step through each piece of cross-entropy loss. Make sure you understand what each calculation is doing, and why. Try creating some small tensors yourself and pass them into the functions, to see what they return.\nRemember: the choices made in the implementation of cross-entropy loss are not the only possible choices that could have been made. Just like when we looked at regression we could choose between mean squared error and mean absolute difference (L1). If you have other ideas for possible functions that you think might work, feel free to give them a try in this chapter’s notebook! (Fair warning though: you’ll probably find that the model will be slower to train, and less accurate. That’s because the gradient of cross-entropy loss is proportional to the difference between the activation and the target, so SGD always gets a nicely scaled step for the weights.)"
  },
  {
    "objectID": "Fastbook/05_pet_breeds.html#questionnaire",
    "href": "Fastbook/05_pet_breeds.html#questionnaire",
    "title": "Image Classification",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhy do we first resize to a large size on the CPU, and then to a smaller size on the GPU?\nIf you are not familiar with regular expressions, find a regular expression tutorial, and some problem sets, and complete them. Have a look on the book’s website for suggestions.\nWhat are the two ways in which data is most commonly provided, for most deep learning datasets?\nLook up the documentation for L and try using a few of the new methods that it adds.\nLook up the documentation for the Python pathlib module and try using a few methods of the Path class.\nGive two examples of ways that image transformations can degrade the quality of the data.\nWhat method does fastai provide to view the data in a DataLoaders?\nWhat method does fastai provide to help you debug a DataBlock?\nShould you hold off on training a model until you have thoroughly cleaned your data?\nWhat are the two pieces that are combined into cross-entropy loss in PyTorch?\nWhat are the two properties of activations that softmax ensures? Why is this important?\nWhen might you want your activations to not have these two properties?\nCalculate the exp and softmax columns of &lt;&gt; yourself (i.e., in a spreadsheet, with a calculator, or in a notebook).\nWhy can’t we use torch.where to create a loss function for datasets where our label can have more than two categories?\nWhat is the value of log(-2)? Why?\nWhat are two good rules of thumb for picking a learning rate from the learning rate finder?\nWhat two steps does the fine_tune method do?\nIn Jupyter Notebook, how do you get the source code for a method or function?\nWhat are discriminative learning rates?\nHow is a Python slice object interpreted when passed as a learning rate to fastai?\nWhy is early stopping a poor choice when using 1cycle training?\nWhat is the difference between resnet50 and resnet101?\nWhat does to_fp16 do?\n\n\nFurther Research\n\nFind the paper by Leslie Smith that introduced the learning rate finder, and read it.\nSee if you can improve the accuracy of the classifier in this chapter. What’s the best accuracy you can achieve? Look on the forums and the book’s website to see what other students have achieved with this dataset, and how they did it."
  },
  {
    "objectID": "Fastbook/translations/cn/08_collab.html",
    "href": "Fastbook/translations/cn/08_collab.html",
    "title": "第八章：协同过滤深入探讨",
    "section": "",
    "text": "解决的一个常见问题是有一定数量的用户和产品，您想推荐哪些产品最有可能对哪些用户有用。存在许多变体：例如，推荐电影（如 Netflix 上），确定在主页上为用户突出显示什么，决定在社交媒体动态中显示什么故事等。解决这个问题的一般方法称为协同过滤，工作原理如下：查看当前用户使用或喜欢的产品，找到其他使用或喜欢类似产品的用户，然后推荐那些用户使用或喜欢的其他产品。\n例如，在 Netflix 上，您可能观看了很多科幻、充满动作并且是上世纪 70 年代制作的电影。Netflix 可能不知道您观看的这些电影的特定属性，但它将能够看到观看了与您观看相同电影的其他人也倾向于观看其他科幻、充满动作并且是上世纪 70 年代制作的电影。换句话说，要使用这种方法，我们不一定需要了解电影的任何信息，只需要知道谁喜欢观看它们。\n这种方法可以解决更一般的一类问题，不一定涉及用户和产品。实际上，在协同过滤中，我们更常用项目这个术语，而不是产品。项目可以是人们点击的链接、为患者选择的诊断等。\n关键的基础概念是潜在因素。在 Netflix 的例子中，我们假设您喜欢老式、充满动作的科幻电影。但您从未告诉 Netflix 您喜欢这类电影。Netflix 也不需要在其电影表中添加列，说明哪些电影属于这些类型。尽管如此，必须存在一些关于科幻、动作和电影年龄的潜在概念，这些概念对于至少一些人的电影观看决策是相关的。\n在本章中，我们将解决这个电影推荐问题。我们将从获取适合协同过滤模型的一些数据开始。"
  },
  {
    "objectID": "Fastbook/translations/cn/08_collab.html#weight-decay",
    "href": "Fastbook/translations/cn/08_collab.html#weight-decay",
    "title": "第八章：协同过滤深入探讨",
    "section": "Weight Decay",
    "text": "Weight Decay\n权重衰减，或L2 正则化，包括将所有权重的平方和添加到损失函数中。为什么这样做？因为当我们计算梯度时，它会为梯度增加一个贡献，鼓励权重尽可能小。\n为什么它可以防止过拟合？这个想法是，系数越大，损失函数中的峡谷就会越尖锐。如果我们以抛物线的基本例子y = a * (x**2)为例，a越大，抛物线就越狭窄：\n\n\n\n不同 a 值的抛物线\n\n\n因此，让我们的模型学习高参数可能导致它用一个过于复杂、具有非常尖锐变化的函数拟合训练集中的所有数据点，这将导致过拟合。\n限制我们的权重过大会阻碍模型的训练，但会产生一个更好泛化的状态。回顾一下理论，权重衰减（或wd）是一个控制我们在损失中添加的平方和的参数（假设parameters是所有参数的张量）：\nloss_with_wd = loss + wd * (parameters**2).sum()\n然而，在实践中，计算那个大和并将其添加到损失中将非常低效（也许在数值上不稳定）。如果你还记得一点高中数学，你可能会记得p**2关于p的导数是2*p，所以将那个大和添加到我们的损失中，实际上等同于这样做：\nparameters.grad += wd * 2 * parameters\n实际上，由于wd是我们选择的一个参数，我们可以使它变为两倍大，所以在这个方程中我们甚至不需要*2。要在 fastai 中使用权重衰减，在调用fit或fit_one_cycle时传递wd即可（可以同时传递）：\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.972090\n0.962366\n00:13\n\n\n1\n0.875591\n0.885106\n00:13\n\n\n2\n0.723798\n0.839880\n00:13\n\n\n3\n0.586002\n0.823225\n00:13\n\n\n4\n0.490980\n0.823060\n00:13\n\n\n\n好多了！"
  },
  {
    "objectID": "Fastbook/translations/cn/08_collab.html#创建我们自己的嵌入模块",
    "href": "Fastbook/translations/cn/08_collab.html#创建我们自己的嵌入模块",
    "title": "第八章：协同过滤深入探讨",
    "section": "创建我们自己的嵌入模块",
    "text": "创建我们自己的嵌入模块\n到目前为止，我们使用Embedding而没有考虑它是如何工作的。让我们重新创建DotProductBias，不使用这个类。我们需要为每个嵌入初始化一个随机权重矩阵。然而，我们必须小心。回想一下第四章中提到的，优化器要求能够从模块的parameters方法中获取模块的所有参数。然而，这并不是完全自动发生的。如果我们只是将一个张量作为Module的属性添加，它不会包含在parameters中：\nclass T(Module):\n    def __init__(self): self.a = torch.ones(3)\n\nL(T().parameters())\n(#0) []\n要告诉Module我们希望将一个张量视为参数，我们必须将其包装在nn.Parameter类中。这个类不添加任何功能（除了自动为我们调用requires_grad_）。它只用作一个“标记”，以显示要包含在parameters中的内容：\nclass T(Module):\n    def __init__(self): self.a = nn.Parameter(torch.ones(3))\n\nL(T().parameters())\n(#1) [Parameter containing:\ntensor([1., 1., 1.], requires_grad=True)]\n所有 PyTorch 模块都使用nn.Parameter来表示任何可训练参数，这就是为什么我们直到现在都不需要显式使用这个包装器：\nclass T(Module):\n    def __init__(self): self.a = nn.Linear(1, 3, bias=False)\n\nt = T()\nL(t.parameters())\n(#1) [Parameter containing:\ntensor([[-0.9595],\n        [-0.8490],\n        [ 0.8159]], requires_grad=True)]\ntype(t.a.weight)\ntorch.nn.parameter.Parameter\n我们可以创建一个张量作为参数，进行随机初始化，如下所示：\ndef create_params(size):\n    return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))\n让我们再次使用这个来创建DotProductBias，但不使用Embedding：\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = create_params([n_users, n_factors])\n        self.user_bias = create_params([n_users])\n        self.movie_factors = create_params([n_movies, n_factors])\n        self.movie_bias = create_params([n_movies])\n        self.y_range = y_range\n\n    def forward(self, x):\n        users = self.user_factors[x[:,0]]\n        movies = self.movie_factors[x[:,1]]\n        res = (users*movies).sum(dim=1)\n        res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n        return sigmoid_range(res, *self.y_range)\n然后让我们再次训练它，以检查我们是否得到了与前一节中看到的大致相同的结果：\nmodel = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.962146\n0.936952\n00:14\n\n\n1\n0.858084\n0.884951\n00:14\n\n\n2\n0.740883\n0.838549\n00:14\n\n\n3\n0.592497\n0.823599\n00:14\n\n\n4\n0.473570\n0.824263\n00:14\n\n\n\n现在，让我们看看我们的模型学到了什么。"
  },
  {
    "objectID": "Fastbook/translations/cn/08_collab.html#使用-fastai.collab",
    "href": "Fastbook/translations/cn/08_collab.html#使用-fastai.collab",
    "title": "第八章：协同过滤深入探讨",
    "section": "使用 fastai.collab",
    "text": "使用 fastai.collab\n我们可以使用 fastai 的collab_learner使用先前显示的确切结构创建和训练协同过滤模型：\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.931751\n0.953806\n00:13\n\n\n1\n0.851826\n0.878119\n00:13\n\n\n2\n0.715254\n0.834711\n00:13\n\n\n3\n0.583173\n0.821470\n00:13\n\n\n4\n0.496625\n0.821688\n00:13\n\n\n\n通过打印模型可以看到层的名称：\nlearn.model\nEmbeddingDotBias(\n  (u_weight): Embedding(944, 50)\n  (i_weight): Embedding(1635, 50)\n  (u_bias): Embedding(944, 1)\n  (i_bias): Embedding(1635, 1)\n)\n我们可以使用这些来复制我们在上一节中所做的任何分析，例如：\nmovie_bias = learn.model.i_bias.weight.squeeze()\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n['Titanic (1997)',\n \"Schindler's List (1993)\",\n 'Shawshank Redemption, The (1994)',\n 'L.A. Confidential (1997)',\n 'Silence of the Lambs, The (1991)']\n我们可以使用这些学到的嵌入来查看距离。"
  },
  {
    "objectID": "Fastbook/translations/cn/08_collab.html#嵌入距离",
    "href": "Fastbook/translations/cn/08_collab.html#嵌入距离",
    "title": "第八章：协同过滤深入探讨",
    "section": "嵌入距离",
    "text": "嵌入距离\n在二维地图上，我们可以通过使用毕达哥拉斯定理的公式来计算两个坐标之间的距离：x 2 + y 2（假设x和y是每个轴上坐标之间的距离）。对于一个 50 维的嵌入，我们可以做完全相同的事情，只是将所有 50 个坐标距离的平方相加。\n如果有两部几乎相同的电影，它们的嵌入向量也必须几乎相同，因为喜欢它们的用户几乎完全相同。这里有一个更一般的想法：电影的相似性可以由喜欢这些电影的用户的相似性来定义。这直接意味着两部电影的嵌入向量之间的距离可以定义这种相似性。我们可以利用这一点找到与“沉默的羔羊”最相似的电影：\nmovie_factors = learn.model.i_weight.weight\nidx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']\ndistances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])\nidx = distances.argsort(descending=True)[1]\ndls.classes['title'][idx]\n'Dial M for Murder (1954)'\n现在我们已经成功训练了一个模型，让我们看看如何处理没有用户数据的情况。我们如何向新用户推荐？"
  },
  {
    "objectID": "Fastbook/translations/cn/08_collab.html#进一步研究",
    "href": "Fastbook/translations/cn/08_collab.html#进一步研究",
    "title": "第八章：协同过滤深入探讨",
    "section": "进一步研究",
    "text": "进一步研究\n\n看看Embedding版本的DotProductBias和create_params版本之间的所有差异，并尝试理解为什么需要进行每一项更改。如果不确定，尝试撤销每个更改以查看发生了什么。（注意：甚至在forward中使用的括号类型也已更改！）\n找到另外三个协同过滤正在使用的领域，并在这些领域中确定这种方法的优缺点。\n使用完整的 MovieLens 数据集完成这个笔记本，并将结果与在线基准进行比较。看看你能否提高准确性。在书的网站和 fast.ai 论坛上寻找想法。请注意，完整数据集中有更多列，看看你是否也可以使用这些列（下一章可能会给你一些想法）。\n为 MovieLens 创建一个使用交叉熵损失的模型，并将其与本章中的模型进行比较。"
  },
  {
    "objectID": "Fastbook/translations/cn/05_pet_breeds.html",
    "href": "Fastbook/translations/cn/05_pet_breeds.html",
    "title": "第五章：图像分类",
    "section": "",
    "text": "现在您了解了深度学习是什么、它的用途以及如何创建和部署模型，现在是时候深入了！在理想的世界中，深度学习从业者不必了解每个细节是如何在底层工作的。但事实上，我们还没有生活在理想的世界中。事实是，要使您的模型真正起作用并可靠地工作，您必须正确处理很多细节，并检查很多细节。这个过程需要能够在训练神经网络时查看内部情况，找到可能的问题，并知道如何解决它们。\n因此，从本书开始，我们将深入研究深度学习的机制。计算机视觉模型的架构是什么，自然语言处理模型的架构是什么，表格模型的架构是什么等等？如何创建一个与您特定领域需求匹配的架构？如何从训练过程中获得最佳结果？如何加快速度？随着数据集的变化，您必须做出哪些改变？\n我们将从重复第一章中查看的相同基本应用程序开始，但我们将做两件事：\n\n让它们变得更好。\n将它们应用于更多类型的数据。\n\n为了做这两件事，我们将不得不学习深度学习难题的所有部分。这包括不同类型的层、正则化方法、优化器、如何将层组合成架构、标记技术等等。但我们不会一次性把所有这些东西都扔给你；我们将根据需要逐步引入它们，以解决与我们正在处理的项目相关的实际问题。"
  },
  {
    "objectID": "Fastbook/translations/cn/05_pet_breeds.html#检查和调试-datablock",
    "href": "Fastbook/translations/cn/05_pet_breeds.html#检查和调试-datablock",
    "title": "第五章：图像分类",
    "section": "检查和调试 DataBlock",
    "text": "检查和调试 DataBlock\n我们永远不能假设我们的代码完美运行。编写DataBlock就像编写蓝图一样。如果您的代码中有语法错误，您将收到错误消息，但是您无法保证您的模板会按照您的意图在数据源上运行。因此，在训练模型之前，您应该始终检查您的数据。\n您可以使用show_batch方法来执行此操作：\ndls.show_batch(nrows=1, ncols=3)\n\n查看每个图像，并检查每个图像是否具有正确的宠物品种标签。通常，数据科学家使用的数据可能不如领域专家熟悉：例如，我实际上不知道这些宠物品种中的许多是什么。由于我不是宠物品种的专家，我会在这一点上使用谷歌图像搜索一些这些品种，并确保图像看起来与我在输出中看到的相似。\n如果在构建DataBlock时出现错误，您可能在此步骤之前不会看到它。为了调试这个问题，我们鼓励您使用summary方法。它将尝试从您提供的源创建一个批次，并提供大量细节。此外，如果失败，您将准确地看到错误发生的位置，并且库将尝试为您提供一些帮助。例如，一个常见的错误是忘记使用Resize转换，因此最终得到不同大小的图片并且无法将它们整理成批次。在这种情况下，摘要将如下所示（请注意，自撰写时可能已更改确切文本，但它将给您一个概念）：\npets1 = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files,\n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'))\npets1.summary(path/\"images\")\nSetting-up type transforms pipelines\nCollecting items from /home/sgugger/.fastai/data/oxford-iiit-pet/images\nFound 7390 items\n2 datasets of sizes 5912,1478\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: partial -&gt; Categorize\n\nBuilding one sample\n  Pipeline: PILBase.create\n    starting from\n      /home/sgugger/.fastai/data/oxford-iiit-pet/images/american_bulldog_83.jpg\n    applying PILBase.create gives\n      PILImage mode=RGB size=375x500\n  Pipeline: partial -&gt; Categorize\n    starting from\n      /home/sgugger/.fastai/data/oxford-iiit-pet/images/american_bulldog_83.jpg\n    applying partial gives\n      american_bulldog\n    applying Categorize gives\n      TensorCategory(12)\n\nFinal sample: (PILImage mode=RGB size=375x500, TensorCategory(12))\n\nSetting up after_item: Pipeline: ToTensor\nSetting up before_batch: Pipeline:\nSetting up after_batch: Pipeline: IntToFloatTensor\n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: ToTensor\n    starting from\n      (PILImage mode=RGB size=375x500, TensorCategory(12))\n    applying ToTensor gives\n      (TensorImage of size 3x500x375, TensorCategory(12))\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\nError! It's not possible to collate your items in a batch\nCould not collate the 0-th members of your tuples because got the following\nshapes:\ntorch.Size([3, 500, 375]),torch.Size([3, 375, 500]),torch.Size([3, 333, 500]),\ntorch.Size([3, 375, 500])\n您可以看到我们如何收集数据并拆分数据，如何从文件名转换为样本（元组（图像，类别）），然后应用了哪些项目转换以及如何在批处理中无法整理这些样本（因为形状不同）。\n一旦您认为数据看起来正确，我们通常建议下一步应该使用它来训练一个简单的模型。我们经常看到人们将实际模型的训练推迟得太久。结果，他们不知道他们的基准结果是什么样的。也许您的问题不需要大量花哨的领域特定工程。或者数据似乎根本无法训练模型。这些都是您希望尽快了解的事情。\n对于这个初始测试，我们将使用与第一章中使用的相同简单模型：\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2)\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.491732\n0.337355\n0.108254\n00:18\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n—\n—\n—\n—\n—\n\n\n0\n0.503154\n0.293404\n0.096076\n00:23\n\n\n1\n0.314759\n0.225316\n0.066306\n00:23\n\n\n\n正如我们之前简要讨论过的，当我们拟合模型时显示的表格展示了每个训练周期后的结果。记住，一个周期是对数据中所有图像的完整遍历。显示的列是训练集中项目的平均损失、验证集上的损失，以及我们请求的任何指标——在这种情况下是错误率。\n请记住损失是我们决定用来优化模型参数的任何函数。但是我们实际上并没有告诉 fastai 我们想要使用什么损失函数。那么它在做什么呢？fastai 通常会根据您使用的数据和模型类型尝试选择适当的损失函数。在这种情况下，我们有图像数据和分类结果，所以 fastai 会默认使用交叉熵损失。"
  },
  {
    "objectID": "Fastbook/translations/cn/05_pet_breeds.html#查看激活和标签",
    "href": "Fastbook/translations/cn/05_pet_breeds.html#查看激活和标签",
    "title": "第五章：图像分类",
    "section": "查看激活和标签",
    "text": "查看激活和标签\n让我们看看我们模型的激活。要从我们的DataLoaders中获取一批真实数据，我们可以使用one_batch方法：\nx,y = dls.one_batch()\n正如您所见，这返回了因变量和自变量，作为一个小批量。让我们看看我们的因变量中包含什么：\ny\nTensorCategory([11,  0,  0,  5, 20,  4, 22, 31, 23, 10, 20,  2,  3, 27, 18, 23,\n &gt; 33,  5, 24,  7,  6, 12,  9, 11, 35, 14, 10, 15,  3,  3, 21,  5, 19, 14, 12,\n &gt; 15, 27,  1, 17, 10,  7,  6, 15, 23, 36,  1, 35,  6,\n         4, 29, 24, 32,  2, 14, 26, 25, 21,  0, 29, 31, 18,  7,  7, 17],\n &gt; device='cuda:5')\n我们的批量大小是 64，因此在这个张量中有 64 行。每行是一个介于 0 和 36 之间的整数，代表我们 37 种可能的宠物品种。我们可以通过使用Learner.get_preds来查看预测（我们神经网络最后一层的激活）。这个函数默认返回预测和目标，但由于我们已经有了目标，我们可以通过将其赋值给特殊变量_来有效地忽略它们：\npreds,_ = learn.get_preds(dl=[(x,y)])\npreds[0]\ntensor([7.9069e-04, 6.2350e-05, 3.7607e-05, 2.9260e-06, 1.3032e-05, 2.5760e-05,\n &gt; 6.2341e-08, 3.6400e-07, 4.1311e-06, 1.3310e-04, 2.3090e-03, 9.9281e-01,\n &gt; 4.6494e-05, 6.4266e-07, 1.9780e-06, 5.7005e-07,\n        3.3448e-06, 3.5691e-03, 3.4385e-06, 1.1578e-05, 1.5916e-06, 8.5567e-08,\n &gt; 5.0773e-08, 2.2978e-06, 1.4150e-06, 3.5459e-07, 1.4599e-04, 5.6198e-08,\n &gt; 3.4108e-07, 2.0813e-06, 8.0568e-07, 4.3381e-07,\n        1.0069e-05, 9.1020e-07, 4.8714e-06, 1.2734e-06, 2.4735e-06])\n实际预测是 37 个介于 0 和 1 之间的概率，总和为 1：\nlen(preds[0]),preds[0].sum()\n(37, tensor(1.0000))\n为了将我们模型的激活转换为这样的预测，我们使用了一个叫做softmax的激活函数。"
  },
  {
    "objectID": "Fastbook/translations/cn/05_pet_breeds.html#softmax",
    "href": "Fastbook/translations/cn/05_pet_breeds.html#softmax",
    "title": "第五章：图像分类",
    "section": "Softmax",
    "text": "Softmax\n在我们的分类模型中，我们在最后一层使用 softmax 激活函数，以确保激活值都在 0 到 1 之间，并且它们总和为 1。\nSoftmax 类似于我们之前看到的 sigmoid 函数。作为提醒，sigmoid 看起来像这样：\nplot_function(torch.sigmoid, min=-4,max=4)\n\n我们可以将这个函数应用于神经网络的一个激活列，并得到一个介于 0 和 1 之间的数字列，因此对于我们的最后一层来说，这是一个非常有用的激活函数。\n现在想象一下，如果我们希望目标中有更多类别（比如我们的 37 种宠物品种）。这意味着我们需要比单个列更多的激活：我们需要一个激活每个类别。例如，我们可以创建一个预测 3 和 7 的神经网络，返回两个激活，每个类别一个——这将是创建更一般方法的一个很好的第一步。让我们只是使用一些标准差为 2 的随机数（因此我们将randn乘以 2）作为示例，假设我们有六个图像和两个可能的类别（其中第一列代表 3，第二列代表 7）：\nacts = torch.randn((6,2))*2\nacts\ntensor([[ 0.6734,  0.2576],\n        [ 0.4689,  0.4607],\n        [-2.2457, -0.3727],\n        [ 4.4164, -1.2760],\n        [ 0.9233,  0.5347],\n        [ 1.0698,  1.6187]])\n我们不能直接对这个进行 sigmoid 运算，因为我们得不到行相加为 1 的结果（我们希望 3 的概率加上 7 的概率等于 1）：\nacts.sigmoid()\ntensor([[0.6623, 0.5641],\n        [0.6151, 0.6132],\n        [0.0957, 0.4079],\n        [0.9881, 0.2182],\n        [0.7157, 0.6306],\n        [0.7446, 0.8346]])\n在第四章中，我们的神经网络为每个图像创建了一个单一激活，然后通过sigmoid函数传递。这个单一激活代表了模型对输入是 3 的置信度。二进制问题是分类问题的一种特殊情况，因为目标可以被视为单个布尔值，就像我们在mnist_loss中所做的那样。但是二进制问题也可以在任意数量的类别的分类器的更一般上下文中考虑：在这种情况下，我们碰巧有两个类别。正如我们在熊分类器中看到的，我们的神经网络将为每个类别返回一个激活。\n那么在二进制情况下，这些激活实际上表示什么？一对激活仅仅表示输入是 3 还是 7 的相对置信度。总体值，无论它们是高还是低，都不重要，重要的是哪个更高，以及高多少。\n我们期望，由于这只是表示相同问题的另一种方式，我们应该能够直接在我们的神经网络的两个激活版本上使用sigmoid。事实上我们可以！我们只需取神经网络激活之间的差异，因为这反映了我们对输入是 3 还是 7 更有把握的程度，然后取其 sigmoid：\n(acts[:,0]-acts[:,1]).sigmoid()\ntensor([0.6025, 0.5021, 0.1332, 0.9966, 0.5959, 0.3661])\n第二列（它是 7 的概率）将是该值从 1 中减去的值。现在，我们需要一种适用于多于两列的方法。事实证明，这个名为softmax的函数正是这样的：\ndef softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True)"
  },
  {
    "objectID": "Fastbook/translations/cn/05_pet_breeds.html#对数似然",
    "href": "Fastbook/translations/cn/05_pet_breeds.html#对数似然",
    "title": "第五章：图像分类",
    "section": "对数似然",
    "text": "对数似然\n在上一章中为我们的 MNIST 示例计算损失时，我们使用了这个：\ndef mnist_loss(inputs, targets):\n    inputs = inputs.sigmoid()\n    return torch.where(targets==1, 1-inputs, inputs).mean()\n就像我们从 sigmoid 到 softmax 的转变一样，我们需要扩展损失函数，使其能够处理不仅仅是二元分类，还需要能够对任意数量的类别进行分类（在本例中，我们有 37 个类别）。我们的激活，在 softmax 之后，介于 0 和 1 之间，并且对于预测批次中的每一行，总和为 1。我们的目标是介于 0 和 36 之间的整数。\n在二元情况下，我们使用torch.where在inputs和1-inputs之间进行选择。当我们将二元分类作为具有两个类别的一般分类问题处理时，它变得更容易，因为（正如我们在前一节中看到的）现在有两列包含等同于inputs和1-inputs的内容。因此，我们只需要从适当的列中进行选择。让我们尝试在 PyTorch 中实现这一点。对于我们合成的 3 和 7 的示例，假设这些是我们的标签：\ntarg = tensor([0,1,0,1,1,0])\n这些是 softmax 激活：\nsm_acts\ntensor([[0.6025, 0.3975],\n        [0.5021, 0.4979],\n        [0.1332, 0.8668],\n        [0.9966, 0.0034],\n        [0.5959, 0.4041],\n        [0.3661, 0.6339]])\n然后对于每个targ项，我们可以使用它来使用张量索引选择sm_acts的适当列，如下所示：\nidx = range(6)\nsm_acts[idx, targ]\ntensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661])\n为了准确了解这里发生了什么，让我们将所有列放在一起放在一个表中。这里，前两列是我们的激活，然后是目标，行索引，最后是前面代码中显示的结果：\n\n\n\n3\n7\ntarg\nidx\nloss\n\n\n\n\n0.602469\n0.397531\n0\n0\n0.602469\n\n\n0.502065\n0.497935\n1\n1\n0.497935\n\n\n0.133188\n0.866811\n0\n2\n0.133188\n\n\n0.99664\n0.00336017\n1\n3\n0.00336017\n\n\n0.595949\n0.404051\n1\n4\n0.404051\n\n\n0.366118\n0.633882\n0\n5\n0.366118\n\n\n\n从这个表中可以看出，最后一列可以通过将targ和idx列作为索引，指向包含3和7列的两列矩阵来计算。这就是sm_acts[idx, targ]的作用。\n这里真正有趣的是，这种方法同样适用于超过两列的情况。想象一下，如果我们为每个数字（0 到 9）添加一个激活列，然后targ包含从 0 到 9 的数字。只要激活列总和为 1（如果我们使用 softmax，它们将是这样），我们将有一个损失函数，显示我们预测每个数字的准确程度。\n我们只从包含正确标签的列中选择损失。我们不需要考虑其他列，因为根据 softmax 的定义，它们加起来等于 1 减去与正确标签对应的激活。因此，使正确标签的激活尽可能高必须意味着我们也在降低其余列的激活。\nPyTorch 提供了一个与sm_acts[range(n), targ]完全相同的函数（除了它取负数，因为之后应用对数时，我们将得到负数），称为nll_loss（NLL代表负对数似然）：\n-sm_acts[idx, targ]\ntensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])\nF.nll_loss(sm_acts, targ, reduction='none')\ntensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])\n尽管它的名字是这样的，但这个 PyTorch 函数并不取对数。我们将在下一节看到原因，但首先，让我们看看为什么取对数会有用。"
  },
  {
    "objectID": "Fastbook/translations/cn/05_pet_breeds.html#取对数",
    "href": "Fastbook/translations/cn/05_pet_breeds.html#取对数",
    "title": "第五章：图像分类",
    "section": "取对数",
    "text": "取对数\n在前一节中我们看到的函数作为损失函数效果很好，但我们可以让它更好一些。问题在于我们使用的是概率，概率不能小于 0 或大于 1。这意味着我们的模型不会在乎它是预测 0.99 还是 0.999。确实，这些数字非常接近，但从另一个角度来看，0.999 比 0.99 自信程度高 10 倍。因此，我们希望将我们的数字从 0 到 1 转换为从负无穷到无穷。有一个数学函数可以做到这一点：对数（可用torch.log）。它对小于 0 的数字没有定义，并且如下所示：\nplot_function(torch.log, min=0,max=4)\n\n“对数”这个词让你想起了什么吗？对数函数有这个恒等式：\ny = b**a\na = log(y,b)\n在这种情况下，我们假设log(y,b)返回log y 以 b 为底。然而，PyTorch 并没有这样定义log：Python 中的log使用特殊数字e（2.718…）作为底。\n也许对数是您在过去 20 年中没有考虑过的东西。但对于深度学习中的许多事情来说，对数是一个非常关键的数学概念，所以现在是一个很好的时机来刷新您的记忆。关于对数的关键事情是这样的关系：\nlog(a*b) = log(a)+log(b)\n当我们以这种格式看到它时，它看起来有点无聊；但想想这实际上意味着什么。这意味着当基础信号呈指数或乘法增长时，对数会线性增加。例如，在地震严重程度的里氏震级和噪音级别的分贝尺中使用。它也经常用于金融图表中，我们希望更清楚地显示复合增长率。计算机科学家喜欢使用对数，因为这意味着可以用加法代替修改，这样可以避免产生计算机难以处理的难以处理的规模。"
  },
  {
    "objectID": "Fastbook/translations/cn/05_pet_breeds.html#学习率查找器",
    "href": "Fastbook/translations/cn/05_pet_breeds.html#学习率查找器",
    "title": "第五章：图像分类",
    "section": "学习率查找器",
    "text": "学习率查找器\n在训练模型时，我们可以做的最重要的事情之一是确保我们有正确的学习率。如果我们的学习率太低，训练模型可能需要很多个 epoch。这不仅浪费时间，还意味着我们可能会出现过拟合的问题，因为每次完整地遍历数据时，我们都给了模型记住数据的机会。\n那么我们就把学习率调得很高，对吗？当然，让我们试试看会发生什么：\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1, base_lr=0.1)\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n8.946717\n47.954632\n0.893775\n00:20\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n—\n—\n—\n—\n—\n\n\n0\n7.231843\n4.119265\n0.954668\n00:24\n\n\n\n这看起来不太好。发生了什么呢。优化器朝着正确的方向迈出了一步，但它迈得太远，完全超过了最小损失。多次重复这样的过程会使其越来越远，而不是越来越接近！\n我们该如何找到完美的学习率——既不太高也不太低？在 2015 年，研究员 Leslie Smith 提出了一个绝妙的想法，称为学习率查找器。他的想法是从一个非常非常小的学习率开始，一个我们永远不会认为它太大而无法处理的学习率。我们用这个学习率进行一个 mini-batch，找到之后的损失，然后按一定百分比增加学习率（例如每次加倍）。然后我们再做另一个 mini-batch，跟踪损失，并再次加倍学习率。我们一直这样做，直到损失变得更糟，而不是更好。这是我们知道我们走得太远的时候。然后我们选择一个比这个点稍低的学习率。我们建议选择以下任一：\n\n比最小损失达到的地方少一个数量级（即最小值除以 10）\n最后一次损失明显减少的点\n\n学习率查找器计算曲线上的这些点来帮助您。这两个规则通常给出大致相同的值。在第一章中，我们没有指定学习率，而是使用了 fastai 库的默认值（即 1e-3）：\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlr_min,lr_steep = learn.lr_find()\n\nprint(f\"Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}\")\nMinimum/10: 8.32e-03, steepest point: 6.31e-03\n我们可以看到在 1e-6 到 1e-3 的范围内，没有什么特别的事情发生，模型不会训练。然后损失开始减少，直到达到最小值，然后再次增加。我们不希望学习率大于 1e-1，因为这会导致训练发散（您可以自行尝试），但 1e-1 已经太高了：在这个阶段，我们已经离开了损失稳定下降的阶段。\n在这个学习率图中，看起来学习率约为 3e-3 可能是合适的，所以让我们选择这个：\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2, base_lr=3e-3)\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.071820\n0.427476\n0.133965\n00:19\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n—\n—\n—\n—\n—\n\n\n0\n0.738273\n0.541828\n0.150880\n00:24\n\n\n1\n0.401544\n0.266623\n0.081867\n00:24"
  },
  {
    "objectID": "Fastbook/translations/cn/05_pet_breeds.html#解冻和迁移学习",
    "href": "Fastbook/translations/cn/05_pet_breeds.html#解冻和迁移学习",
    "title": "第五章：图像分类",
    "section": "解冻和迁移学习",
    "text": "解冻和迁移学习\n我们在第一章中简要讨论了迁移学习的工作原理。我们看到基本思想是，一个预训练模型，可能在数百万数据点（如 ImageNet）上训练，被为另一个任务进行微调。但这到底意味着什么？\n我们现在知道，卷积神经网络由许多线性层组成，每对之间有一个非线性激活函数，然后是一个或多个最终的线性层，最后是一个诸如 softmax 之类的激活函数。最终的线性层使用一个具有足够列数的矩阵，使得输出大小与我们模型中的类数相同（假设我们正在进行分类）。\n当我们在迁移学习设置中进行微调时，这个最终的线性层对我们来说可能没有任何用处，因为它专门设计用于对原始预训练数据集中的类别进行分类。因此，在进行迁移学习时，我们会将其移除、丢弃，并用一个新的线性层替换，该线性层具有我们所需任务的正确输出数量（在这种情况下，将有 37 个激活）。\n这个新添加的线性层将完全随机的权重。因此，在微调之前，我们的模型具有完全随机的输出。但这并不意味着它是一个完全随机的模型！最后一个层之前的所有层都经过精心训练，以便在一般的图像分类任务中表现良好。正如我们在Zeiler 和 Fergus 论文中看到的那样，在第一章中（参见图 1-10 到 1-13），前几层编码了一般概念，比如找到梯度和边缘，后面的层编码了对我们仍然有用的概念，比如找到眼球和毛发。\n我们希望以这样的方式训练模型，使其能够记住预训练模型中的所有这些通常有用的想法，用它们来解决我们的特定任务（分类宠物品种），并仅根据我们特定任务的具体要求进行调整。\n在微调时，我们的挑战是用能够正确实现我们所需任务（分类宠物品种）的权重替换我们添加的线性层中的随机权重，而不破坏精心预训练的权重和其他层。一个简单的技巧可以实现这一点：告诉优化器仅更新那些随机添加的最终层中的权重。根本不要改变神经网络的其他部分的权重。这被称为冻结那些预训练的层。\n当我们从预训练网络创建模型时，fastai 会自动为我们冻结所有预训练层。当我们调用fine_tune方法时，fastai 会做两件事：\n\n训练随机添加的层一个周期，同时冻结所有其他层\n解冻所有层，并根据请求的周期数进行训练\n\n尽管这是一个合理的默认方法，但对于您的特定数据集，您可能通过稍微不同的方式做事情来获得更好的结果。fine_tune方法有一些参数可以用来改变其行为，但如果您想获得自定义行为，直接调用底层方法可能更容易。请记住，您可以使用以下语法查看该方法的源代码：\nlearn.fine_tune??\n所以让我们尝试手动操作。首先，我们将使用fit_one_cycle训练随机添加的层三个周期。正如在第一章中提到的，fit_one_cycle是在不使用fine_tune的情况下训练模型的建议方法。我们将在本书后面看到原因；简而言之，fit_one_cycle的作用是以低学习率开始训练，逐渐增加学习率进行第一部分的训练，然后在最后一部分的训练中逐渐降低学习率：\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3)\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.188042\n0.355024\n0.102842\n00:20\n\n\n1\n0.534234\n0.302453\n0.094723\n00:20\n\n\n2\n0.325031\n0.222268\n0.074425\n00:20\n\n\n\n然后我们将解冻模型：\nlearn.unfreeze()\n并再次运行lr_find，因为有更多层要训练，而且已经训练了三个周期的权重，意味着我们之前找到的学习率不再合适：\nlearn.lr_find()\n(1.0964782268274575e-05, 1.5848931980144698e-06)\n\n请注意，图表与随机权重时有所不同：我们没有那种表明模型正在训练的陡峭下降。这是因为我们的模型已经训练过了。在这里，我们有一个相对平坦的区域，然后是一个急剧增加的区域，我们应该选择在那个急剧增加之前的一个点，例如 1e-5。具有最大梯度的点不是我们在这里寻找的，应该被忽略。\n让我们以适当的学习率进行训练：\nlearn.fit_one_cycle(6, lr_max=1e-5)\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.263579\n0.217419\n0.069012\n00:24\n\n\n1\n0.253060\n0.210346\n0.062923\n00:24\n\n\n2\n0.224340\n0.207357\n0.060217\n00:24\n\n\n3\n0.200195\n0.207244\n0.061570\n00:24\n\n\n4\n0.194269\n0.200149\n0.059540\n00:25\n\n\n5\n0.173164\n0.202301\n0.059540\n00:25\n\n\n\n这稍微改进了我们的模型，但我们还可以做更多。预训练模型的最深层可能不需要像最后一层那样高的学习率，因此我们可能应该为这些层使用不同的学习率——这被称为使用区分性学习率。"
  },
  {
    "objectID": "Fastbook/translations/cn/05_pet_breeds.html#区分性学习率",
    "href": "Fastbook/translations/cn/05_pet_breeds.html#区分性学习率",
    "title": "第五章：图像分类",
    "section": "区分性学习率",
    "text": "区分性学习率\n即使我们解冻后，我们仍然非常关心那些预训练权重的质量。我们不会期望那些预训练参数的最佳学习率与随机添加参数的学习率一样高，即使在我们为随机添加参数调整了几个轮数之后。请记住，预训练权重已经在数百个轮数中，在数百万张图像上进行了训练。\n此外，您还记得我们在第一章中看到的图像吗？显示每个层学习的内容？第一层学习非常简单的基础知识，如边缘和梯度检测器；这些对于几乎任何任务都可能非常有用。后面的层学习更复杂的概念，如“眼睛”和“日落”，这些对您的任务可能完全没有用（也许您正在对汽车型号进行分类）。因此，让后面的层比前面的层更快地微调是有道理的。\n因此，fastai 的默认方法是使用区分性学习率。这种技术最初是在我们将在第十章中介绍的 NLP 迁移学习的 ULMFiT 方法中开发的。就像深度学习中的许多好主意一样，这个方法非常简单：对神经网络的早期层使用较低的学习率，对后期层（尤其是随机添加的层）使用较高的学习率。这个想法基于Jason Yosinski 等人在 2014 年展示的见解，即在迁移学习中，神经网络的不同层应该以不同的速度训练，如图 5-4 所示。\n\n\n\n不同层和训练方法对迁移学习的影响（Yosinski）\n\n\n\n图 5-4。不同层和训练方法对迁移学习的影响（由 Jason Yosinski 等人提供）\nfastai 允许您在任何需要学习率的地方传递 Python slice对象。传递的第一个值将是神经网络最早层的学习率，第二个值将是最后一层的学习率。中间的层将在该范围内等距地乘法地具有学习率。让我们使用这种方法复制先前的训练，但这次我们只将我们网络的最低层的学习率设置为 1e-6；其他层将增加到 1e-4。让我们训练一段时间，看看会发生什么：\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3)\nlearn.unfreeze()\nlearn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4))\n\n\n\n轮数\n训练损失\n验证损失\n错误率\n时间\n\n\n\n\n0\n1.145300\n0.345568\n0.119756\n00:20\n\n\n1\n0.533986\n0.251944\n0.077131\n00:20\n\n\n2\n0.317696\n0.208371\n0.069012\n00:20\n\n\n轮数\n训练损失\n验证损失\n错误率\n时间\n\n\n—\n—\n—\n—\n—\n\n\n0\n0.257977\n0.205400\n0.067659\n00:25\n\n\n1\n0.246763\n0.205107\n0.066306\n00:25\n\n\n2\n0.240595\n0.193848\n0.062246\n00:25\n\n\n3\n0.209988\n0.198061\n0.062923\n00:25\n\n\n4\n0.194756\n0.193130\n0.064276\n00:25\n\n\n5\n0.169985\n0.187885\n0.056157\n00:25\n\n\n6\n0.153205\n0.186145\n0.058863\n00:25\n\n\n7\n0.141480\n0.185316\n0.053451\n00:25\n\n\n8\n0.128564\n0.180999\n0.051421\n00:25\n\n\n9\n0.126941\n0.186288\n0.054127\n00:25\n\n\n10\n0.130064\n0.181764\n0.054127\n00:25\n\n\n11\n0.124281\n0.181855\n0.054127\n00:25\n\n\n\n现在微调效果很好！\nfastai 可以展示训练和验证损失的图表：\nlearn.recorder.plot_loss()\n\n正如你所看到的，训练损失一直在变得越来越好。但请注意，最终验证损失的改善会减缓，有时甚至会变得更糟！这是模型开始过拟合的时候。特别是，模型开始对其预测变得过于自信。但这并不意味着它一定变得不准确。看一下每个 epoch 的训练结果表，你会经常看到准确率持续提高，即使验证损失变得更糟。最终，重要的是你的准确率，或者更一般地说是你选择的指标，而不是损失。损失只是我们给计算机的函数，帮助我们优化。\n在训练模型时，你还需要做出的另一个决定是训练多长时间。我们将在下面考虑这个问题。"
  },
  {
    "objectID": "Fastbook/translations/cn/05_pet_breeds.html#选择-epochs-的数量",
    "href": "Fastbook/translations/cn/05_pet_breeds.html#选择-epochs-的数量",
    "title": "第五章：图像分类",
    "section": "选择 epochs 的数量",
    "text": "选择 epochs 的数量\n通常情况下，你会发现在选择训练多少个 epochs 时，你受到的限制更多是时间，而不是泛化和准确性。因此，你训练的第一步应该是简单地选择一个你愿意等待的时间内可以完成的 epochs 数量。然后查看训练和验证损失图，特别是你的指标。如果你看到它们甚至在最后几个 epochs 中仍在变得更好，那么你就知道你没有训练得太久。\n另一方面，你可能会发现你选择的指标在训练结束时确实变得更糟。记住，我们不仅仅是在寻找验证损失变得更糟，而是实际的指标。你的验证损失在训练过程中会先变得更糟，因为模型变得过于自信，只有后来才会因为错误地记忆数据而变得更糟。在实践中，我们只关心后一种情况。记住，我们的损失函数是我们用来让优化器有东西可以区分和优化的，实际上我们关心的不是这个。\n在 1cycle 训练出现之前，通常会在每个 epoch 结束时保存模型，然后从所有保存的模型中选择准确率最高的模型。这被称为早停。然而，这不太可能给出最好的答案，因为那些中间的 epochs 出现在学习率还没有机会达到小值的情况下，这时它才能真正找到最佳结果。因此，如果你发现你过拟合了，你应该重新从头开始训练模型，并根据之前找到最佳结果的地方选择一个总的 epochs 数量。\n如果你有时间训练更多的 epochs，你可能会选择用这段时间来训练更多的参数，也就是使用更深的架构。"
  },
  {
    "objectID": "Fastbook/translations/cn/05_pet_breeds.html#更深的架构",
    "href": "Fastbook/translations/cn/05_pet_breeds.html#更深的架构",
    "title": "第五章：图像分类",
    "section": "更深的架构",
    "text": "更深的架构\n一般来说，具有更多参数的模型可以更准确地对数据进行建模。（对于这个泛化有很多很多的例外情况，这取决于你使用的架构的具体情况，但现在这是一个合理的经验法则。）对于我们将在本书中看到的大多数架构，你可以通过简单地添加更多层来创建更大的版本。然而，由于我们想使用预训练模型，我们需要确保选择已经为我们预训练的层数。\n这就是为什么在实践中，架构往往只有少数几种变体。例如，在本章中使用的 ResNet 架构有 18、34、50、101 和 152 层的变体，都是在 ImageNet 上预训练的。一个更大的（更多层和参数；有时被描述为模型的容量）ResNet 版本总是能够给我们更好的训练损失，但它可能更容易过拟合，因为它有更多参数可以过拟合。\n总的来说，一个更大的模型能够更好地捕捉数据的真实基本关系，以及捕捉和记忆你个别图像的具体细节。\n然而，使用更深的模型将需要更多的 GPU 内存，因此你可能需要降低批量大小以避免内存不足错误。当你尝试将太多内容装入 GPU 时，就会发生这种情况，看起来像这样：\nCuda runtime error: out of memory\n当发生这种情况时，你可能需要重新启动你的笔记本。解决方法是使用较小的批量大小，这意味着在任何给定时间通过你的模型传递较小的图像组。你可以通过使用bs=创建你想要的批量大小来调用。\n更深层次架构的另一个缺点是训练时间要长得多。一个可以大大加快速度的技术是混合精度训练。这指的是在训练过程中尽可能使用不那么精确的数字（半精度浮点数，也称为 fp16）。截至 2020 年初，几乎所有当前的 NVIDIA GPU 都支持一种特殊功能，称为张量核心，可以将神经网络训练速度提高 2-3 倍。它们还需要更少的 GPU 内存。要在 fastai 中启用此功能，只需在创建Learner后添加to_fp16()（你还需要导入模块）。\n你实际上无法提前知道适合你特定问题的最佳架构——你需要尝试一些训练。所以现在让我们尝试使用混合精度的 ResNet-50：\nfrom fastai2.callback.fp16 import *\nlearn = cnn_learner(dls, resnet50, metrics=error_rate).to_fp16()\nlearn.fine_tune(6, freeze_epochs=3)\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.427505\n0.310554\n0.098782\n00:21\n\n\n1\n0.606785\n0.302325\n0.094723\n00:22\n\n\n2\n0.409267\n0.294803\n0.091340\n00:21\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n—\n—\n—\n—\n—\n\n\n0\n0.261121\n0.274507\n0.083897\n00:26\n\n\n1\n0.296653\n0.318649\n0.084574\n00:26\n\n\n2\n0.242356\n0.253677\n0.069012\n00:26\n\n\n3\n0.150684\n0.251438\n0.065629\n00:26\n\n\n4\n0.094997\n0.239772\n0.064276\n00:26\n\n\n5\n0.061144\n0.228082\n0.054804\n00:26\n\n\n\n你会看到我们又回到使用fine_tune，因为它非常方便！我们可以传递freeze_epochs告诉 fastai 在冻结时训练多少个周期。它将自动为大多数数据集更改学习率。\n在这种情况下，我们没有从更深的模型中看到明显的优势。这是值得记住的——对于你的特定情况，更大的模型不一定是更好的模型！确保在扩大规模之前尝试小模型。"
  },
  {
    "objectID": "Fastbook/translations/cn/05_pet_breeds.html#进一步研究",
    "href": "Fastbook/translations/cn/05_pet_breeds.html#进一步研究",
    "title": "第五章：图像分类",
    "section": "进一步研究",
    "text": "进一步研究\n\n找到 Leslie Smith 撰写的介绍学习率查找器的论文，并阅读。\n看看是否可以提高本章分类器的准确性。您能达到的最佳准确性是多少？查看论坛和书籍网站，看看其他学生在这个数据集上取得了什么成就以及他们是如何做到的。"
  },
  {
    "objectID": "Fastbook/translations/cn/11_midlevel_data.html",
    "href": "Fastbook/translations/cn/11_midlevel_data.html",
    "title": "第十一章：使用 fastai 的中级 API 进行数据整理",
    "section": "",
    "text": "我们已经看到了Tokenizer和Numericalize对文本集合的处理方式，以及它们如何在数据块 API 中使用，该 API 直接使用TextBlock处理这些转换。但是，如果我们只想应用这些转换中的一个，要么是为了查看中间结果，要么是因为我们已经对文本进行了标记化，我们该怎么办？更一般地说，当数据块 API 不足以满足我们特定用例的需求时，我们需要使用 fastai 的中级 API来处理数据。数据块 API 是建立在该层之上的，因此它将允许您执行数据块 API 所做的一切，以及更多更多。"
  },
  {
    "objectID": "Fastbook/translations/cn/11_midlevel_data.html#转换",
    "href": "Fastbook/translations/cn/11_midlevel_data.html#转换",
    "title": "第十一章：使用 fastai 的中级 API 进行数据整理",
    "section": "转换",
    "text": "转换\n在前一章中研究标记化和数值化时，我们首先获取了一堆文本：\nfiles = get_text_files(path, folders = ['train', 'test'])\ntxts = L(o.open().read() for o in files[:2000])\n然后我们展示了如何使用Tokenizer对它们进行标记化\ntok = Tokenizer.from_folder(path)\ntok.setup(txts)\ntoks = txts.map(tok)\ntoks[0]\n(#374) ['xxbos','xxmaj','well',',','\"','cube','\"','(','1997',')'...]\n以及如何进行数值化，包括自动为我们的语料库创建词汇表：\nnum = Numericalize()\nnum.setup(toks)\nnums = toks.map(num)\nnums[0][:10]\ntensor([   2,    8,   76,   10,   23, 3112,   23,   34, 3113,   33])\n这些类还有一个decode方法。例如，Numericalize.decode会将字符串标记返回给我们：\nnums_dec = num.decode(nums[0][:10]); nums_dec\n(#10) ['xxbos','xxmaj','well',',','\"','cube','\"','(','1997',')']\nTokenizer.decode将其转换回一个字符串（但可能不完全与原始字符串相同；这取决于标记器是否是可逆的，在我们撰写本书时，默认的单词标记器不是）：\ntok.decode(nums_dec)\n'xxbos xxmaj well , \" cube \" ( 1997 )'\ndecode被 fastai 的show_batch和show_results以及其他一些推断方法使用，将预测和小批量转换为人类可理解的表示。\n在前面的示例中，对于tok或num，我们创建了一个名为setup的对象（如果需要为tok训练标记器并为num创建词汇表），将其应用于我们的原始文本（通过将对象作为函数调用），然后最终将结果解码回可理解的表示。大多数数据预处理任务都需要这些步骤，因此 fastai 提供了一个封装它们的类。这就是Transform类。Tokenize和Numericalize都是Transform。\n一般来说，Transform是一个行为类似于函数的对象，它具有一个可选的setup方法，用于初始化内部状态（例如num内部的词汇表），以及一个可选的decode方法，用于反转函数（正如我们在tok中看到的那样，这种反转可能不完美）。\ndecode 的一个很好的例子可以在我们在 第七章 中看到的 Normalize 转换中找到：为了能够绘制图像，它的 decode 方法会撤消归一化（即，乘以标准差并加回均值）。另一方面，数据增强转换没有 decode 方法，因为我们希望展示图像上的效果，以确保数据增强按我们的意愿进行工作。\nTransform 的一个特殊行为是它们总是应用于元组。一般来说，我们的数据总是一个元组 (input, target)（有时有多个输入或多个目标）。当对这样的项目应用转换时，例如 Resize，我们不希望整个元组被调整大小；相反，我们希望分别调整输入（如果适用）和目标（如果适用）。对于进行数据增强的批处理转换也是一样的：当输入是图像且目标是分割掩模时，需要将转换（以相同的方式）应用于输入和目标。\n如果我们将一个文本元组传递给 tok，我们可以看到这种行为：\ntok((txts[0], txts[1]))\n((#374) ['xxbos','xxmaj','well',',','\"','cube','\"','(','1997',')'...],\n (#207)\n &gt; ['xxbos','xxmaj','conrad','xxmaj','hall','went','out','with','a','bang'...])"
  },
  {
    "objectID": "Fastbook/translations/cn/11_midlevel_data.html#编写您自己的转换",
    "href": "Fastbook/translations/cn/11_midlevel_data.html#编写您自己的转换",
    "title": "第十一章：使用 fastai 的中级 API 进行数据整理",
    "section": "编写您自己的转换",
    "text": "编写您自己的转换\n如果您想编写一个自定义的转换来应用于您的数据，最简单的方法是编写一个函数。正如您在这个例子中看到的，Transform 只会应用于匹配的类型，如果提供了类型（否则，它将始终被应用）。在下面的代码中，函数签名中的 :int 表示 f 仅应用于 ints。这就是为什么 tfm(2.0) 返回 2.0，但 tfm(2) 在这里返回 3：\ndef f(x:int): return x+1\ntfm = Transform(f)\ntfm(2),tfm(2.0)\n(3, 2.0)\n在这里，f 被转换为一个没有 setup 和没有 decode 方法的 Transform。\nPython 有一种特殊的语法，用于将一个函数（如 f）传递给另一个函数（或类似函数的东西，在 Python 中称为 callable），称为 decorator。通过在可调用对象前加上 @ 并将其放在函数定义之前来使用装饰器（关于 Python 装饰器有很多很好的在线教程，如果这对您来说是一个新概念，请查看其中一个）。以下代码与前面的代码相同：\n@Transform\ndef f(x:int): return x+1\nf(2),f(2.0)\n(3, 2.0)\n如果您需要 setup 或 decode，您需要对 Transform 进行子类化，以在 encodes 中实现实际的编码行为，然后（可选）在 setups 中实现设置行为和在 decodes 中实现解码行为：\nclass NormalizeMean(Transform):\n    def setups(self, items): self.mean = sum(items)/len(items)\n    def encodes(self, x): return x-self.mean\n    def decodes(self, x): return x+self.mean\n在这里，NormalizeMean 将在设置期间初始化某个状态（传递的所有元素的平均值）；然后转换是减去该平均值。为了解码目的，我们通过添加平均值来实现该转换的反向。这里是 NormalizeMean 的一个示例：\ntfm = NormalizeMean()\ntfm.setup([1,2,3,4,5])\nstart = 2\ny = tfm(start)\nz = tfm.decode(y)\ntfm.mean,y,z\n(3.0, -1.0, 2.0)\n请注意，每个方法的调用和实现是不同的：\n\n\n\n类\n调用\n实现\n\n\n\n\nnn.Module（PyTorch）\n()（即，作为函数调用）\nforward\n\n\nTransform\n()\nencodes\n\n\nTransform\ndecode()\ndecodes\n\n\nTransform\nsetup()\nsetups\n\n\n\n因此，例如，您永远不会直接调用 setups，而是会调用 setup。原因是 setup 在为您调用 setups 之前和之后做了一些工作。要了解有关 Transform 及如何使用它们根据输入类型实现不同行为的更多信息，请务必查看 fastai 文档中的教程。"
  },
  {
    "objectID": "Fastbook/translations/cn/11_midlevel_data.html#pipeline",
    "href": "Fastbook/translations/cn/11_midlevel_data.html#pipeline",
    "title": "第十一章：使用 fastai 的中级 API 进行数据整理",
    "section": "Pipeline",
    "text": "Pipeline\n要将几个转换组合在一起，fastai 提供了 Pipeline 类。我们通过向 Pipeline 传递一个 Transform 列表来定义一个 Pipeline；然后它将组合其中的转换。当您在对象上调用 Pipeline 时，它将自动按顺序调用其中的转换：\ntfms = Pipeline([tok, num])\nt = tfms(txts[0]); t[:20]\ntensor([   2,    8,   76,   10,   23, 3112,   23,   34, 3113,   33,   10,    8,\n &gt; 4477,   22,   88,   32,   10,   27,   42,   14])\n您可以对编码结果调用 decode，以获取可以显示和分析的内容：\ntfms.decode(t)[:100]\n'xxbos xxmaj well , \" cube \" ( 1997 ) , xxmaj vincenzo \\'s first movie , was one\n &gt; of the most interesti'\nTransform 中与 Transform 不同的部分是设置。要在一些数据上正确设置 Transform 的 Pipeline，您需要使用 TfmdLists。"
  },
  {
    "objectID": "Fastbook/translations/cn/11_midlevel_data.html#tfmdlists",
    "href": "Fastbook/translations/cn/11_midlevel_data.html#tfmdlists",
    "title": "第十一章：使用 fastai 的中级 API 进行数据整理",
    "section": "TfmdLists",
    "text": "TfmdLists\n以下是在前一节中看到的转换的简短方式：\ntls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize])\n在初始化时，TfmdLists将自动调用每个Transform的setup方法，依次提供每个原始项目而不是由所有先前的Transform转换的项目。我们可以通过索引到TfmdLists中的任何原始元素来获得我们的Pipeline的结果：\nt = tls[0]; t[:20]\ntensor([    2,     8,    91,    11,    22,  5793,    22,    37,  4910,    34,\n &gt; 11,     8, 13042,    23,   107,    30,    11,    25,    44,    14])\n而TfmdLists知道如何解码以进行显示：\ntls.decode(t)[:100]\n'xxbos xxmaj well , \" cube \" ( 1997 ) , xxmaj vincenzo \\'s first movie , was one\n &gt; of the most interesti'\n实际上，它甚至有一个show方法：\ntls.show(t)\nxxbos xxmaj well , \" cube \" ( 1997 ) , xxmaj vincenzo 's first movie , was one\n &gt; of the most interesting and tricky ideas that xxmaj i 've ever seen when\n &gt; talking about movies . xxmaj they had just one scenery , a bunch of actors\n &gt; and a plot . xxmaj so , what made it so special were all the effective\n &gt; direction , great dialogs and a bizarre condition that characters had to deal\n &gt; like rats in a labyrinth . xxmaj his second movie , \" cypher \" ( 2002 ) , was\n &gt; all about its story , but it was n't so good as \" cube \" but here are the\n &gt; characters being tested like rats again .\n\n \" nothing \" is something very interesting and gets xxmaj vincenzo coming back\n &gt; to his ' cube days ' , locking the characters once again in a very different\n &gt; space with no time once more playing with the characters like playing with\n &gt; rats in an experience room . xxmaj but instead of a thriller sci - fi ( even\n &gt; some of the promotional teasers and trailers erroneous seemed like that ) , \"\n &gt; nothing \" is a loose and light comedy that for sure can be called a modern\n &gt; satire about our society and also about the intolerant world we 're living .\n &gt; xxmaj once again xxmaj xxunk amaze us with a great idea into a so small kind\n &gt; of thing . 2 actors and a blinding white scenario , that 's all you got most\n &gt; part of time and you do n't need more than that . xxmaj while \" cube \" is a\n &gt; claustrophobic experience and \" cypher \" confusing , \" nothing \" is\n &gt; completely the opposite but at the same time also desperate .\n\n xxmaj this movie proves once again that a smart idea means much more than just\n &gt; a millionaire budget . xxmaj of course that the movie fails sometimes , but\n &gt; its prime idea means a lot and offsets any flaws . xxmaj there 's nothing\n &gt; more to be said about this movie because everything is a brilliant surprise\n &gt; and a totally different experience that i had in movies since \" cube \" .\nTfmdLists以“s”命名，因为它可以使用splits参数处理训练集和验证集。您只需要传递在训练集中的元素的索引和在验证集中的元素的索引：\ncut = int(len(files)*0.8)\nsplits = [list(range(cut)), list(range(cut,len(files)))]\ntls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize],\n                splits=splits)\n然后可以通过train和valid属性访问它们：\ntls.valid[0][:20]\ntensor([    2,     8,    20,    30,    87,   510,  1570,    12,   408,   379,\n &gt; 4196,    10,     8,    20,    30,    16,    13, 12216,   202,   509])\n如果您手动编写了一个Transform，一次执行所有预处理，将原始项目转换为具有输入和目标的元组，那么TfmdLists是您需要的类。您可以使用dataloaders方法直接将其转换为DataLoaders对象。这是我们稍后在本章中将要做的事情。\n一般来说，您将有两个（或更多）并行的转换流水线：一个用于将原始项目处理为输入，另一个用于将原始项目处理为目标。例如，在这里，我们定义的流水线仅将原始文本处理为输入。如果我们要进行文本分类，还必须将标签处理为目标。\n为此，我们需要做两件事。首先，我们从父文件夹中获取标签名称。有一个名为parent_label的函数：\nlbls = files.map(parent_label)\nlbls\n(#50000) ['pos','pos','pos','pos','pos','pos','pos','pos','pos','pos'...]\n然后我们需要一个Transform，在设置期间将抓取的唯一项目构建为词汇表，然后在调用时将字符串标签转换为整数。fastai 为我们提供了这个；它被称为Categorize：\ncat = Categorize()\ncat.setup(lbls)\ncat.vocab, cat(lbls[0])\n((#2) ['neg','pos'], TensorCategory(1))\n要在我们的文件列表上自动执行整个设置，我们可以像以前一样创建一个TfmdLists：\ntls_y = TfmdLists(files, [parent_label, Categorize()])\ntls_y[0]\nTensorCategory(1)\n但是然后我们得到了两个分开的对象用于我们的输入和目标，这不是我们想要的。这就是Datasets发挥作用的地方。"
  },
  {
    "objectID": "Fastbook/translations/cn/11_midlevel_data.html#datasets",
    "href": "Fastbook/translations/cn/11_midlevel_data.html#datasets",
    "title": "第十一章：使用 fastai 的中级 API 进行数据整理",
    "section": "Datasets",
    "text": "Datasets\nDatasets将并行应用两个（或更多）流水线到相同的原始对象，并构建一个包含结果的元组。与TfmdLists一样，它将自动为我们进行设置，当我们索引到Datasets时，它将返回一个包含每个流水线结果的元组：\nx_tfms = [Tokenizer.from_folder(path), Numericalize]\ny_tfms = [parent_label, Categorize()]\ndsets = Datasets(files, [x_tfms, y_tfms])\nx,y = dsets[0]\nx[:20],y\n像TfmdLists一样，我们可以将splits传递给Datasets以在训练和验证集之间拆分我们的数据：\nx_tfms = [Tokenizer.from_folder(path), Numericalize]\ny_tfms = [parent_label, Categorize()]\ndsets = Datasets(files, [x_tfms, y_tfms], splits=splits)\nx,y = dsets.valid[0]\nx[:20],y\n(tensor([    2,     8,    20,    30,    87,   510,  1570,    12,   408,   379,\n &gt; 4196,    10,     8,    20,    30,    16,    13, 12216,   202,   509]),\n TensorCategory(0))\n它还可以解码任何处理过的元组或直接显示它：\nt = dsets.valid[0]\ndsets.decode(t)\n('xxbos xxmaj this movie had horrible lighting and terrible camera movements .\n &gt; xxmaj this movie is a jumpy horror flick with no meaning at all . xxmaj the\n &gt; slashes are totally fake looking . xxmaj it looks like some 17 year - old\n &gt; idiot wrote this movie and a 10 year old kid shot it . xxmaj with the worst\n &gt; acting you can ever find . xxmaj people are tired of knives . xxmaj at least\n &gt; move on to guns or fire . xxmaj it has almost exact lines from \" when a xxmaj\n &gt; stranger xxmaj calls \" . xxmaj with gruesome killings , only crazy people\n &gt; would enjoy this movie . xxmaj it is obvious the writer does n\\'t have kids\n &gt; or even care for them . i mean at show some mercy . xxmaj just to sum it up ,\n &gt; this movie is a \" b \" movie and it sucked . xxmaj just for your own sake , do\n &gt; n\\'t even think about wasting your time watching this crappy movie .',\n 'neg')\n最后一步是将我们的Datasets对象转换为DataLoaders，可以使用dataloaders方法完成。在这里，我们需要传递一个特殊参数来解决填充问题（正如我们在前一章中看到的）。这需要在我们批处理元素之前发生，所以我们将其传递给before_batch：\ndls = dsets.dataloaders(bs=64, before_batch=pad_input)\ndataloaders直接在我们的Datasets的每个子集上调用DataLoader。fastai 的DataLoader扩展了 PyTorch 中同名类，并负责将我们的数据集中的项目整理成批次。它有很多自定义点，但您应该知道的最重要的是：\nafter_item\n在数据集中抓取项目后应用于每个项目。这相当于DataBlock中的item_tfms。\nbefore_batch\n在整理之前应用于项目列表上。这是将项目填充到相同大小的理想位置。\nafter_batch\n在构建后对整个批次应用。这相当于DataBlock中的batch_tfms。\n最后，这是为了准备文本分类数据所需的完整代码：\ntfms = [[Tokenizer.from_folder(path), Numericalize], [parent_label, Categorize]]\nfiles = get_text_files(path, folders = ['train', 'test'])\nsplits = GrandparentSplitter(valid_name='test')(files)\ndsets = Datasets(files, tfms, splits=splits)\ndls = dsets.dataloaders(dl_type=SortedDL, before_batch=pad_input)\n与之前的代码的两个不同之处是使用GrandparentSplitter来分割我们的训练和验证数据，以及dl_type参数。这是告诉dataloaders使用DataLoader的SortedDL类，而不是通常的类。SortedDL通过将大致相同长度的样本放入批次来构建批次。\n这与我们之前的DataBlock完全相同：\npath = untar_data(URLs.IMDB)\ndls = DataBlock(\n    blocks=(TextBlock.from_folder(path),CategoryBlock),\n    get_y = parent_label,\n    get_items=partial(get_text_files, folders=['train', 'test']),\n    splitter=GrandparentSplitter(valid_name='test')\n).dataloaders(path)\n但现在你知道如何定制每一个部分了！\n让我们现在通过一个计算机视觉示例练习刚学到的关于使用这个中级 API 进行数据预处理。"
  },
  {
    "objectID": "Fastbook/translations/cn/11_midlevel_data.html#进一步研究",
    "href": "Fastbook/translations/cn/11_midlevel_data.html#进一步研究",
    "title": "第十一章：使用 fastai 的中级 API 进行数据整理",
    "section": "进一步研究",
    "text": "进一步研究\n\n使用中级 API 在自己的数据集上准备DataLoaders中的数据。尝试在 Pet 数据集和 Adult 数据集上进行此操作，这两个数据集来自第一章。\n查看fastai 文档中的 Siamese 教程，了解如何为新类型的项目自定义show_batch和show_results的行为。在您自己的项目中实现它。"
  },
  {
    "objectID": "Fastbook/translations/cn/10_nlp.html",
    "href": "Fastbook/translations/cn/10_nlp.html",
    "title": "第十章：NLP 深入探讨：RNNs",
    "section": "",
    "text": "在第一章中，我们看到深度学习可以用于处理自然语言数据集并取得出色的结果。我们的示例依赖于使用预训练的语言模型，并对其进行微调以对评论进行分类。该示例突出了 NLP 和计算机视觉中迁移学习的区别：通常情况下，在 NLP 中，预训练模型是在不同任务上训练的。\n我们所谓的语言模型是一个经过训练以猜测文本中下一个单词的模型（在读取之前的单词后）。这种任务称为自监督学习：我们不需要为我们的模型提供标签，只需向其提供大量文本。它有一个过程可以从数据中自动获取标签，这个任务并不是微不足道的：为了正确猜测句子中的下一个单词，模型将必须发展对英语（或其他语言）的理解。自监督学习也可以用于其他领域；例如，参见“自监督学习和计算机视觉”以了解视觉应用。自监督学习通常不用于直接训练的模型，而是用于预训练用于迁移学习的模型。"
  },
  {
    "objectID": "Fastbook/translations/cn/10_nlp.html#分词",
    "href": "Fastbook/translations/cn/10_nlp.html#分词",
    "title": "第十章：NLP 深入探讨：RNNs",
    "section": "分词",
    "text": "分词\n当我们说“将文本转换为单词列表”时，我们忽略了很多细节。例如，我们如何处理标点符号？我们如何处理像“don’t”这样的单词？它是一个单词还是两个？长的医学或化学术语怎么办？它们应该被分割成各自的含义部分吗？连字符词怎么处理？像德语和波兰语这样的语言如何处理，它们可以从许多部分组成一个非常长的单词？像日语和中文这样的语言如何处理，它们根本不使用基础，也没有一个明确定义的单词的概念？\n由于这些问题没有一个正确答案，所以也没有一个分词的方法。有三种主要方法：\n基于单词的\n将一个句子按空格分割，同时应用特定于语言的规则，尝试在没有空格的情况下分隔含义部分（例如将“don’t”转换为“do n’t”）。通常，标点符号也会被分割成单独的标记。\n基于子词的\n根据最常出现的子字符串将单词分割成较小的部分。例如，“occasion”可能被分词为“o c ca sion”。\n基于字符的\n将一个句子分割成其各个字符。\n我们将在这里看一下单词和子词的分词，将字符为基础的分词留给你在本章末尾的问卷中实现。"
  },
  {
    "objectID": "Fastbook/translations/cn/10_nlp.html#使用-fastai-进行单词分词",
    "href": "Fastbook/translations/cn/10_nlp.html#使用-fastai-进行单词分词",
    "title": "第十章：NLP 深入探讨：RNNs",
    "section": "使用 fastai 进行单词分词",
    "text": "使用 fastai 进行单词分词\nfastai 并没有提供自己的分词器，而是提供了一个一致的接口来使用外部库中的一系列分词器。分词是一个活跃的研究领域，新的和改进的分词器不断涌现，因此 fastai 使用的默认值也会发生变化。然而，API 和选项不应该发生太大变化，因为 fastai 试图在底层技术发生变化时保持一致的 API。\n让我们尝试一下我们在第一章中使用的 IMDb 数据集：\nfrom fastai.text.all import *\npath = untar_data(URLs.IMDB)\n我们需要获取文本文件以尝试一个分词器。就像get_image_files（我们已经使用了很多次）获取路径中的所有图像文件一样，get_text_files获取路径中的所有文本文件。我们还可以选择性地传递folders来限制搜索到特定的子文件夹列表：\nfiles = get_text_files(path, folders = ['train', 'test', 'unsup'])\n这是一个我们将要分词的评论（我们这里只打印开头部分以节省空间）：\ntxt = files[0].open().read(); txt[:75]\n'This movie, which I just discovered at the video store, has apparently sit '\n在撰写本书时，fastai 的默认英语单词分词器使用了一个名为spaCy的库。它有一个复杂的规则引擎，具有针对 URL、特殊英语单词等的特殊规则，以及更多。然而，我们不会直接使用SpacyTokenizer，而是使用WordTokenizer，因为它将始终指向 fastai 当前默认的单词分词器（取决于你阅读本书的时间，可能不一定是 spaCy）。\n让我们试一试。我们将使用 fastai 的coll_repr(*collection*,*n*)函数来显示结果。这会显示collection的前n个项目，以及完整的大小——这是L默认使用的。请注意，fastai 的分词器接受一个要分词的文档集合，因此我们必须将txt包装在一个列表中：\nspacy = WordTokenizer()\ntoks = first(spacy([txt]))\nprint(coll_repr(toks, 30))\n(#201) ['This','movie',',','which','I','just','discovered','at','the','video','s\n &gt; tore',',','has','apparently','sit','around','for','a','couple','of','years','\n &gt; without','a','distributor','.','It',\"'s\",'easy','to','see'...]\n正如你所看到的，spaCy 主要只是将单词和标点符号分开。但它在这里也做了其他事情：它将“it’s”分割成“it”和“’s”。这是直观的；这些实际上是分开的单词。分词是一个令人惊讶的微妙任务，当你考虑到所有必须处理的细节时。幸运的是，spaCy 为我们处理得相当好——例如，在这里我们看到“.”在终止句子时被分开，但在首字母缩写或数字中不会被分开：\nfirst(spacy(['The U.S. dollar $1 is $1.00.']))\n(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']\n然后 fastai 通过Tokenizer类为分词过程添加了一些额外功能：\ntkn = Tokenizer(spacy)\nprint(coll_repr(tkn(txt), 31))\n(#228) ['xxbos','xxmaj','this','movie',',','which','i','just','discovered','at',\n &gt; 'the','video','store',',','has','apparently','sit','around','for','a','couple\n &gt; ','of','years','without','a','distributor','.','xxmaj','it',\"'s\",'easy'...]\n请注意，现在有一些以“xx”开头的标记，这不是英语中常见的单词前缀。这些是特殊标记。\n例如，列表中的第一项xxbos是一个特殊标记，表示新文本的开始（“BOS”是一个标准的 NLP 缩写，意思是“流的开始”）。通过识别这个开始标记，模型将能够学习需要“忘记”先前说过的内容，专注于即将出现的单词。\n这些特殊标记并不是直接来自 spaCy。它们存在是因为 fastai 默认添加它们，通过在处理文本时应用一系列规则。这些规则旨在使模型更容易识别句子中的重要部分。在某种意义上，我们正在将原始的英语语言序列翻译成一个简化的标记化语言——这种语言被设计成易于模型学习。\n例如，规则将用一个感叹号替换四个感叹号，后面跟着一个特殊的重复字符标记，然后是数字四。通过这种方式，模型的嵌入矩阵可以编码关于重复标点等一般概念的信息，而不需要为每个标点符号的重复次数添加单独的标记。同样，一个大写的单词将被替换为一个特殊的大写标记，后面跟着单词的小写版本。这样，嵌入矩阵只需要单词的小写版本，节省了计算和内存资源，但仍然可以学习大写的概念。\n以下是一些你会看到的主要特殊标记：\nxxbos\n指示文本的开始（这里是一篇评论）\nxxmaj\n指示下一个单词以大写字母开头（因为我们将所有字母转换为小写）\nxxunk\n指示下一个单词是未知的\n要查看使用的规则，可以查看默认规则：\ndefaults.text_proc_rules\n[&lt;function fastai.text.core.fix_html(x)&gt;,\n &lt;function fastai.text.core.replace_rep(t)&gt;,\n &lt;function fastai.text.core.replace_wrep(t)&gt;,\n &lt;function fastai.text.core.spec_add_spaces(t)&gt;,\n &lt;function fastai.text.core.rm_useless_spaces(t)&gt;,\n &lt;function fastai.text.core.replace_all_caps(t)&gt;,\n &lt;function fastai.text.core.replace_maj(t)&gt;,\n &lt;function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)&gt;]\n如常，你可以通过在笔记本中键入以下内容查看每个规则的源代码：\n??replace_rep\n以下是每个标记的简要摘要：\nfix_html\n用可读版本替换特殊的 HTML 字符（IMDb 评论中有很多这样的字符）\nreplace_rep\n用一个特殊标记替换任何重复三次或更多次的字符（xxrep），重复的次数，然后是字符\nreplace_wrep\n用一个特殊标记替换任何重复三次或更多次的单词（xxwrep），重复的次数，然后是单词\nspec_add_spaces\n在/和#周围添加空格\nrm_useless_spaces\n删除所有空格的重复\nreplace_all_caps\n将所有大写字母单词转换为小写，并在其前面添加一个特殊标记（xxcap）\nreplace_maj\n将大写的单词转换为小写，并在其前面添加一个特殊标记（xxmaj）\nlowercase\n将所有文本转换为小写，并在开头（xxbos）和/或结尾（xxeos）添加一个特殊标记\n让我们看看其中一些的操作：\ncoll_repr(tkn('&copy;   Fast.ai www.fast.ai/INDEX'), 31)\n\"(#11) ['xxbos','©','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','ind\n &gt; ex'...]\"\n现在让我们看看子词标记化是如何工作的。"
  },
  {
    "objectID": "Fastbook/translations/cn/10_nlp.html#子词标记化",
    "href": "Fastbook/translations/cn/10_nlp.html#子词标记化",
    "title": "第十章：NLP 深入探讨：RNNs",
    "section": "子词标记化",
    "text": "子词标记化\n除了在前一节中看到的单词标记化方法之外，另一种流行的标记化方法是子词标记化。单词标记化依赖于一个假设，即空格在句子中提供了有意义的组件的有用分隔。然而，这个假设并不总是适用。例如，考虑这个句子：我的名字是郝杰瑞（中文中的“My name is Jeremy Howard”）。这对于单词标记器来说不会很好，因为其中没有空格！像中文和日文这样的语言不使用空格，事实上它们甚至没有一个明确定义的“单词”概念。其他语言，如土耳其语和匈牙利语，可以将许多子词组合在一起而不使用空格，创建包含许多独立信息片段的非常长的单词。\n为了处理这些情况，通常最好使用子词标记化。这个过程分为两步：\n\n分析一组文档以找到最常出现的字母组。这些将成为词汇表。\n使用这个子词单元的词汇对语料库进行标记化。\n\n让我们看一个例子。对于我们的语料库，我们将使用前 2,000 条电影评论：\ntxts = L(o.open().read() for o in files[:2000])\n我们实例化我们的标记器，传入我们想要创建的词汇表的大小，然后我们需要“训练”它。也就是说，我们需要让它阅读我们的文档并找到常见的字符序列以创建词汇表。这是通过setup完成的。正如我们将很快看到的，setup是一个特殊的 fastai 方法，在我们通常的数据处理流程中会自动调用。然而，由于目前我们正在手动执行所有操作，因此我们必须自己调用它。这是一个为给定词汇表大小执行这些步骤并显示示例输出的函数：\ndef subword(sz):\n    sp = SubwordTokenizer(vocab_sz=sz)\n    sp.setup(txts)\n    return ' '.join(first(sp([txt]))[:40])\n让我们试一试：\nsubword(1000)\n'▁This ▁movie , ▁which ▁I ▁just ▁dis c over ed ▁at ▁the ▁video ▁st or e , ▁has\n &gt; ▁a p par ent ly ▁s it ▁around ▁for ▁a ▁couple ▁of ▁years ▁without ▁a ▁dis t\n &gt; ri but or . ▁It'\n使用 fastai 的子词标记器时，特殊字符▁代表原始文本中的空格字符。\n如果我们使用较小的词汇表，每个标记将代表更少的字符，并且需要更多的标记来表示一个句子：\nsubword(200)\n'▁ T h i s ▁movie , ▁w h i ch ▁I ▁ j us t ▁ d i s c o ver ed ▁a t ▁the ▁ v id e\n &gt; o ▁ st or e , ▁h a s'\n另一方面，如果我们使用较大的词汇表，大多数常见的英语单词将最终出现在词汇表中，我们将不需要那么多来表示一个句子：\nsubword(10000)\n\"▁This ▁movie , ▁which ▁I ▁just ▁discover ed ▁at ▁the ▁video ▁store , ▁has\n &gt; ▁apparently ▁sit ▁around ▁for ▁a ▁couple ▁of ▁years ▁without ▁a ▁distributor\n &gt; . ▁It ' s ▁easy ▁to ▁see ▁why . ▁The ▁story ▁of ▁two ▁friends ▁living\"\n选择子词词汇表大小代表一种折衷：较大的词汇表意味着每个句子的标记较少，这意味着训练速度更快，内存更少，并且模型需要记住的状态更少；但是，缺点是，这意味着更大的嵌入矩阵，这需要更多的数据来学习。\n总的来说，子词标记化提供了一种在字符标记化（即使用较小的子词词汇表）和单词标记化（即使用较大的子词词汇表）之间轻松切换的方法，并且处理每种人类语言而无需开发特定于语言的算法。它甚至可以处理其他“语言”，如基因组序列或 MIDI 音乐符号！因此，过去一年中，它的流行度飙升，似乎很可能成为最常见的标记化方法（当您阅读本文时，它可能已经是了！）。\n一旦我们的文本被分割成标记，我们需要将它们转换为数字。我们将在下一步中看到这一点。"
  },
  {
    "objectID": "Fastbook/translations/cn/10_nlp.html#使用-fastai-进行数字化",
    "href": "Fastbook/translations/cn/10_nlp.html#使用-fastai-进行数字化",
    "title": "第十章：NLP 深入探讨：RNNs",
    "section": "使用 fastai 进行数字化",
    "text": "使用 fastai 进行数字化\n数字化是将标记映射到整数的过程。这些步骤基本上与创建Category变量所需的步骤相同，例如 MNIST 中数字的因变量：\n\n制作该分类变量的所有可能级别的列表（词汇表）。\n用词汇表中的索引替换每个级别。\n\n让我们看看在之前看到的单词标记化文本上的实际操作：\ntoks = tkn(txt)\nprint(coll_repr(tkn(txt), 31))\n(#228) ['xxbos','xxmaj','this','movie',',','which','i','just','discovered','at',\n &gt; 'the','video','store',',','has','apparently','sit','around','for','a','couple\n &gt; ','of','years','without','a','distributor','.','xxmaj','it',\"'s\",'easy'...]\n就像SubwordTokenizer一样，我们需要在Numericalize上调用setup；这是我们创建词汇表的方法。这意味着我们首先需要我们的标记化语料库。由于标记化需要一段时间，fastai 会并行进行；但是对于这个手动演示，我们将使用一个小的子集：\ntoks200 = txts[:200].map(tkn)\ntoks200[0]\n(#228)\n &gt; ['xxbos','xxmaj','this','movie',',','which','i','just','discovered','at'...]\n我们可以将这个传递给setup来创建我们的词汇表：\nnum = Numericalize()\nnum.setup(toks200)\ncoll_repr(num.vocab,20)\n\"(#2000) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj\n &gt; ','the','.',',','a','and','of','to','is','in','i','it'...]\"\n我们的特殊规则标记首先出现，然后每个单词按频率顺序出现一次。Numericalize的默认值为min_freq=3和max_vocab=60000。max_vocab=60000导致 fastai 用特殊的未知单词标记xxunk替换除最常见的 60,000 个单词之外的所有单词。这有助于避免过大的嵌入矩阵，因为这可能会减慢训练速度并占用太多内存，并且还可能意味着没有足够的数据来训练稀有单词的有用表示。然而，通过设置min_freq来处理最后一个问题更好；默认值min_freq=3意味着出现少于三次的任何单词都将被替换为xxunk。\nfastai 还可以使用您提供的词汇表对数据集进行数字化，方法是将单词列表作为vocab参数传递。\n一旦我们创建了我们的Numericalize对象，我们可以像使用函数一样使用它：\nnums = num(toks)[:20]; nums\ntensor([  2,   8,  21,  28,  11,  90,  18,  59,   0,  45,   9, 351, 499,  11,\n &gt; 72, 533, 584, 146,  29,  12])\n这一次，我们的标记已经转换为模型可以接收的整数张量。我们可以检查它们是否映射回原始文本：\n' '.join(num.vocab[o] for o in nums)\n'xxbos xxmaj this movie , which i just xxunk at the video store , has apparently\n &gt; sit around for a'\nxxbos | xxmaj | 在 | 这个 | 章节 | ， | 我们 | 将 | 回顾 | 一下 | 分类 | 的 | 例子 |"
  },
  {
    "objectID": "Fastbook/translations/cn/10_nlp.html#回到我们之前的例子有-6-个长度为-15-的批次如果我们选择序列长度为-5那意味着我们首先输入以下数组",
    "href": "Fastbook/translations/cn/10_nlp.html#回到我们之前的例子有-6-个长度为-15-的批次如果我们选择序列长度为-5那意味着我们首先输入以下数组",
    "title": "第十章：NLP 深入探讨：RNNs",
    "section": "回到我们之前的例子，有 6 个长度为 15 的批次，如果我们选择序列长度为 5，那意味着我们首先输入以下数组：",
    "text": "回到我们之前的例子，有 6 个长度为 15 的批次，如果我们选择序列长度为 5，那意味着我们首先输入以下数组：\n处理图像时，我们需要将它们全部调整为相同的高度和宽度，然后将它们组合在一起形成一个小批次，以便它们可以有效地堆叠在一个张量中。这里会有一点不同，因为不能简单地将文本调整为所需的长度。此外，我们希望我们的语言模型按顺序阅读文本，以便它可以有效地预测下一个单词是什么。这意味着每个新批次应该从上一个批次结束的地方开始。\n假设我们有以下文本：\n\n在这一章中，我们将回顾我们在第一章中学习的分类电影评论的例子，并深入挖掘。首先，我们将看一下将文本转换为数字所需的处理步骤以及如何自定义它。通过这样做，我们将有另一个使用数据块 API 中的预处理器的例子。\n然后我们将学习如何构建一个语言模型并训练它一段时间。\n\n标记化过程将添加特殊标记并处理标点以返回这个文本：\n\nxxbos 在这一章中，我们将回顾我们在第一章中学习的分类电影评论的例子，并深入挖掘。首先，我们将看一下将文本转换为数字所需的处理步骤以及如何自定义它。通过这样做，我们将有另一个使用数据块 API 中的预处理器的例子。\n\n现在我们有 90 个标记，用空格分隔。假设我们想要一个批次大小为 6。我们需要将这个文本分成 6 个长度为 15 的连续部分：\n\n\n\n转换\n文本\n为\n数字\n和\n\n\n\n\n电影\n评论\n我们\n研究\n在\n\n\n首先\n我们\n将\n看\n处理\n\n\n如何\n自定义\n它\n。\n通过\n\n\n预处理器\n在\n数据\n块\nxxup\n\n\n将\n学习\n我们\n如何\n构建\n\n\n\n在理想的情况下，我们可以将这一个批次提供给我们的模型。但这种方法不具有可扩展性，因为在这个玩具示例之外，一个包含所有标记的单个批次不太可能适合我们的 GPU 内存（这里有 90 个标记，但所有 IMDb 评论一起给出了数百万个）。\n因此，我们需要将这个数组更细地划分为固定序列长度的子数组。在这些子数组内部和之间保持顺序非常重要，因为我们将使用一个保持状态的模型，以便在预测接下来的内容时记住之前读到的内容。\n语言 | 模型 | 和 | 训练 |\n\n\n\nxxbos\nxxmaj\n在\n这个\n章节\n\n\n\n\n分类\n的\n例子\n在\n电影\n\n\n首先\n我们\n将\n看\n到\n\n\n如何\n自定义\n它\n。\n\n\n\n预处理器\n使用\n在\n\n\n\n\n将\n学习\n我们\n如何\n构建\n\n\n\n然后，这一个：\n\n\n\n，\n我们\n将\n回顾\n\n\n\n\n章节\n1\n和\n深入\n\n\n处理\n步骤\n必要\n将\n\n\n通过\n这样做\n，\n\n\n\n数据\n块\nxxup\napi\n\n\n现在我们有了数字，我们需要将它们分批放入模型中。\n\n\n\n\n\n\n最后：\n\n\n\n将我们的文本放入语言模型的批次中\n\n\n\n\n更深\n\n\n将文本转换为数字，并按行翻译成中文：\n\n\n我们\n\n\n。\n\n\n它\n\n\n\n回到我们的电影评论数据集，第一步是通过将各个文本串联在一起将其转换为流。与图像一样，最好随机化输入的顺序，因此在每个时期的开始，我们将对条目进行洗牌以生成新的流（我们对文档的顺序进行洗牌，而不是其中的单词顺序，否则文本将不再有意义！）。\n然后将此流切成一定数量的批次（这是我们的批量大小）。例如，如果流有 50,000 个标记，我们设置批量大小为 10，这将给我们 5,000 个标记的 10 个小流。重要的是我们保留标记的顺序（因此从 1 到 5,000 为第一个小流，然后从 5,001 到 10,000…），因为我们希望模型读取连续的文本行（如前面的示例）。在预处理期间，在每个文本的开头添加一个xxbos标记，以便模型知道当读取流时新条目何时开始。\n因此，总结一下，每个时期我们都会对文档集合进行洗牌，并将它们连接成一个标记流。然后将该流切成一批固定大小的连续小流。我们的模型将按顺序读取小流，并由于内部状态，无论我们选择的序列长度如何，它都将产生相同的激活。\n当我们创建LMDataLoader时，所有这些都是由 fastai 库在幕后完成的。我们首先将我们的Numericalize对象应用于标记化的文本\nnums200 = toks200.map(num)\n然后将其传递给LMDataLoader：\ndl = LMDataLoader(nums200)\n让我们通过获取第一批来确认这是否给出了预期的结果\nx,y = first(dl)\nx.shape,y.shape\n(torch.Size([64, 72]), torch.Size([64, 72]))\n然后查看独立变量的第一行，这应该是第一个文本的开头：\n' '.join(num.vocab[o] for o in x[0][:20])\n'xxbos xxmaj this movie , which i just xxunk at the video store , has apparently\n &gt; sit around for a'\n依赖变量是相同的，只是偏移了一个标记：\n' '.join(num.vocab[o] for o in y[0][:20])\n'xxmaj this movie , which i just xxunk at the video store , has apparently sit\n &gt; around for a couple'\n这就完成了我们需要对数据应用的所有预处理步骤。我们现在准备训练我们的文本分类器。"
  },
  {
    "objectID": "Fastbook/translations/cn/10_nlp.html#使用-datablock-的语言模型",
    "href": "Fastbook/translations/cn/10_nlp.html#使用-datablock-的语言模型",
    "title": "第十章：NLP 深入探讨：RNNs",
    "section": "使用 DataBlock 的语言模型",
    "text": "使用 DataBlock 的语言模型\n当TextBlock传递给DataBlock时，fastai 会自动处理标记化和数值化。所有可以传递给Tokenizer和Numericalize的参数也可以传递给TextBlock。在下一章中，我们将讨论分别运行每个步骤的最简单方法，以便进行调试，但您也可以通过在数据的子集上手动运行它们来进行调试，如前几节所示。不要忘记DataBlock的方便的summary方法，用于调试数据问题非常有用。\n这是我们如何使用TextBlock使用 fastai 的默认值创建语言模型的方式：\nget_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n\ndls_lm = DataBlock(\n    blocks=TextBlock.from_folder(path, is_lm=True),\n    get_items=get_imdb, splitter=RandomSplitter(0.1)\n).dataloaders(path, path=path, bs=128, seq_len=80)\n与我们在DataBlock中使用的以前类型不同的一件事是，我们不仅仅直接使用类（即TextBlock（...），而是调用类方法。类方法是 Python 方法，如其名称所示，属于类而不是对象。（如果您对类方法不熟悉，请务必在网上搜索更多信息，因为它们在许多 Python 库和应用程序中常用；我们在本书中以前使用过几次，但没有特别提到。）TextBlock之所以特殊是因为设置数值化器的词汇表可能需要很长时间（我们必须读取和标记化每个文档以获取词汇表）。\n为了尽可能高效，fastai 执行了一些优化：\n\n它将标记化的文档保存在临时文件夹中，因此不必多次对其进行标记化。\n它并行运行多个标记化过程，以利用计算机的 CPU。\n\n我们需要告诉TextBlock如何访问文本，以便它可以进行这种初始预处理——这就是from_folder的作用。\nshow_batch然后以通常的方式工作：\ndls_lm.show_batch(max_n=2)\n\n\n\n\n\n\n\n\n\ntext\ntext_\n\n\n\n\n0\nxxbos xxmaj it ’s awesome ! xxmaj in xxmaj story xxmaj mode , your going from punk to pro . xxmaj you have to complete goals that involve skating , driving , and walking . xxmaj you create your own skater and give it a name , and you can make it look stupid or realistic . xxmaj you are with your friend xxmaj eric throughout the game until he betrays you and gets you kicked off of the skateboard\nxxmaj it ’s awesome ! xxmaj in xxmaj story xxmaj mode , your going from punk to pro . xxmaj you have to complete goals that involve skating , driving , and walking . xxmaj you create your own skater and give it a name , and you can make it look stupid or realistic . xxmaj you are with your friend xxmaj eric throughout the game until he betrays you and gets you kicked off of the skateboard xxunk\n\n\n1\nwhat xxmaj i ’ve read , xxmaj death xxmaj bed is based on an actual dream , xxmaj george xxmaj barry , the director , successfully transferred dream to film , only a genius could accomplish such a task . xxmaj old mansions make for good quality horror , as do portraits , not sure what to make of the killer bed with its killer yellow liquid , quite a bizarre dream , indeed . xxmaj also , this\nxxmaj i ’ve read , xxmaj death xxmaj bed is based on an actual dream , xxmaj george xxmaj barry , the director , successfully transferred dream to film , only a genius could accomplish such a task . xxmaj old mansions make for good quality horror , as do portraits , not sure what to make of the killer bed with its killer yellow liquid , quite a bizarre dream , indeed . xxmaj also , this is\n\n\n\n现在我们的数据准备好了，我们可以对预训练语言模型进行微调。"
  },
  {
    "objectID": "Fastbook/translations/cn/10_nlp.html#微调语言模型",
    "href": "Fastbook/translations/cn/10_nlp.html#微调语言模型",
    "title": "第十章：NLP 深入探讨：RNNs",
    "section": "微调语言模型",
    "text": "微调语言模型\n将整数单词索引转换为我们可以用于神经网络的激活时，我们将使用嵌入，就像我们在协同过滤和表格建模中所做的那样。然后，我们将把这些嵌入馈送到递归神经网络（RNN）中，使用一种称为AWD-LSTM的架构（我们将在第十二章中向您展示如何从头开始编写这样一个模型）。正如我们之前讨论的，预训练模型中的嵌入与为不在预训练词汇表中的单词添加的随机嵌入合并。这在language_model_learner内部自动处理：\nlearn = language_model_learner(\n    dls_lm, AWD_LSTM, drop_mult=0.3,\n    metrics=[accuracy, Perplexity()]).to_fp16()\n默认使用的损失函数是交叉熵损失，因为我们基本上有一个分类问题（不同类别是我们词汇表中的单词）。这里使用的困惑度指标通常用于 NLP 的语言模型：它是损失的指数（即torch.exp(cross_entropy)）。我们还包括准确性指标，以查看我们的模型在尝试预测下一个单词时有多少次是正确的，因为交叉熵（正如我们所见）很难解释，并且更多地告诉我们有关模型信心而不是准确性。\n让我们回到本章开头的流程图。第一个箭头已经为我们完成，并作为 fastai 中的预训练模型提供，我们刚刚构建了第二阶段的DataLoaders和Learner。现在我们准备好对我们的语言模型进行微调！\n\n\n\nULMFiT 过程的图表\n\n\n每个时代的训练需要相当长的时间，因此我们将在训练过程中保存中间模型结果。由于fine_tune不会为我们执行此操作，因此我们将使用fit_one_cycle。就像cnn_learner一样，当使用预训练模型（这是默认设置）时，language_model_learner在使用时会自动调用freeze，因此这将仅训练嵌入（模型中唯一包含随机初始化权重的部分——即我们 IMDb 词汇表中存在但不在预训练模型词汇表中的单词的嵌入）：\nlearn.fit_one_cycle(1, 2e-2)\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n4.120048\n3.912788\n0.299565\n50.038246\n11:39\n\n\n\n这个模型训练时间较长，所以现在是谈论保存中间结果的好机会。"
  },
  {
    "objectID": "Fastbook/translations/cn/10_nlp.html#保存和加载模型",
    "href": "Fastbook/translations/cn/10_nlp.html#保存和加载模型",
    "title": "第十章：NLP 深入探讨：RNNs",
    "section": "保存和加载模型",
    "text": "保存和加载模型\n您可以轻松保存模型的状态如下：\nlearn.save('1epoch')\n这将在 learn.path/models/ 中创建一个名为 1epoch.pth 的文件。如果您想在另一台机器上加载模型，或者稍后恢复训练，可以按照以下方式加载此文件的内容：\nlearn = learn.load('1epoch')\n一旦初始训练完成，我们可以在解冻后继续微调模型：\nlearn.unfreeze()\nlearn.fit_one_cycle(10, 2e-3)\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n3.893486\n3.772820\n0.317104\n43.502548\n12:37\n\n\n1\n3.820479\n3.717197\n0.323790\n41.148880\n12:30\n\n\n2\n3.735622\n3.659760\n0.330321\n38.851997\n12:09\n\n\n3\n3.677086\n3.624794\n0.333960\n37.516987\n12:12\n\n\n4\n3.636646\n3.601300\n0.337017\n36.645859\n12:05\n\n\n5\n3.553636\n3.584241\n0.339355\n36.026001\n12:04\n\n\n6\n3.507634\n3.571892\n0.341353\n35.583862\n12:08\n\n\n7\n3.444101\n3.565988\n0.342194\n35.374371\n12:08\n\n\n8\n3.398597\n3.566283\n0.342647\n35.384815\n12:11\n\n\n9\n3.375563\n3.568166\n0.342528\n35.451500\n12:05\n\n\n\n完成后，我们保存所有模型，除了将激活转换为在我们的词汇表中选择每个标记的概率的最终层。不包括最终层的模型称为编码器。我们可以使用 save_encoder 来保存它：\nlearn.save_encoder('finetuned')"
  },
  {
    "objectID": "Fastbook/translations/cn/10_nlp.html#文本生成",
    "href": "Fastbook/translations/cn/10_nlp.html#文本生成",
    "title": "第十章：NLP 深入探讨：RNNs",
    "section": "文本生成",
    "text": "文本生成\n因为我们的模型经过训练可以猜测句子的下一个单词，所以我们可以用它来写新评论：\nTEXT = \"I liked this movie because\"\nN_WORDS = 40\nN_SENTENCES = 2\npreds = [learn.predict(TEXT, N_WORDS, temperature=0.75)\n         for _ in range(N_SENTENCES)]\nprint(\"\\n\".join(preds))\ni liked this movie because of its story and characters . The story line was very\n &gt; strong , very good for a sci - fi film . The main character , Alucard , was\n &gt; very well developed and brought the whole story\ni liked this movie because i like the idea of the premise of the movie , the (\n &gt; very ) convenient virus ( which , when you have to kill a few people , the \"\n &gt; evil \" machine has to be used to protect\n正如您所看到的，我们添加了一些随机性（我们根据模型返回的概率选择一个随机单词），这样我们就不会得到完全相同的评论两次。我们的模型没有任何关于句子结构或语法规则的编程知识，但它显然已经学会了很多关于英语句子：我们可以看到它正确地大写了（I 被转换为 i，因为我们的规则要求两个字符或更多才能认为一个单词是大写的，所以看到它小写是正常的）并且使用一致的时态。一般的评论乍一看是有意义的，只有仔细阅读时才能注意到有些地方有点不对。对于在几个小时内训练的模型来说，这还不错！\n但我们的最终目标不是训练一个生成评论的模型，而是对其进行分类…所以让我们使用这个模型来做到这一点。"
  },
  {
    "objectID": "Fastbook/translations/cn/10_nlp.html#创建分类器数据加载器",
    "href": "Fastbook/translations/cn/10_nlp.html#创建分类器数据加载器",
    "title": "第十章：NLP 深入探讨：RNNs",
    "section": "创建分类器数据加载器",
    "text": "创建分类器数据加载器\n我们现在从语言模型微调转向分类器微调。简而言之，语言模型预测文档的下一个单词，因此不需要任何外部标签。然而，分类器预测外部标签——在 IMDb 的情况下，是文档的情感。\n这意味着我们用于 NLP 分类的 DataBlock 结构看起来非常熟悉。它几乎与我们为许多图像分类数据集看到的相同：\ndls_clas = DataBlock(\n    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n    get_y = parent_label,\n    get_items=partial(get_text_files, folders=['train', 'test']),\n    splitter=GrandparentSplitter(valid_name='test')\n).dataloaders(path, path=path, bs=128, seq_len=72)\n就像图像分类一样，show_batch 显示了依赖变量（情感，在这种情况下）与每个独立变量（电影评论文本）：\ndls_clas.show_batch(max_n=3)\n\n\n\n\n\n\n\n\n\n文本\n类别\n\n\n\n\n0\nxxbos 我给这部电影打了 3 颗头骨的评分，只是因为女孩们知道如何尖叫，这部电影本可以更好，如果演员更好的话，双胞胎还行，我相信他们是邪恶的，但是最大和最小的兄弟，他们表现得真的很糟糕，看起来他们在读剧本而不是表演……。剧透：如果他们是吸血鬼，为什么他们会冻结血液？吸血鬼不能喝冻结的血液，电影中的姐姐说让我们在她活着的时候喝她……。但是当他们搬到另一栋房子时，他们带了一个冷藏盒装着他们的冻结血液。剧透结束这是浪费时间，这让我很生气，因为我读了所有关于它的评论\nneg\n\n\n1\nxxbos 我已经阅读了所有的《爱来的方式》系列书籍。我充分了解电影无法使用书中的所有方面，但通常它们至少会有书中的主要内容。我对这部电影感到非常失望。这部电影中唯一与书中相同的是，书中有 xxmaj missy 的父亲来到 xxunk （在书中父母都来了）。就是这样。故事情节扭曲且牵强，是的，悲伤，与书中完全不同，我无法享受。即使我没有读过这本书，它也太悲伤了。我知道拓荒生活很艰难，但整部电影都是一个沮丧的故事。评分\nneg\n\n\n2\nxxbos 这部电影，用一个更好的词来说，很糟糕。我从哪里开始呢……电影摄影 - 这或许是我今年看过的最糟糕的。看起来就像摄影师之间在互相抛接相机。也许他们只有一台相机。这让你感觉像是一个排球。有一堆场景，零零散散地扔进去，完全没有连贯性。当他们做 ‘分屏’ 时，那是荒谬的。一切都被压扁了，看起来荒谬。颜色调整完全错了。这些人需要学会如何平衡相机。这部 ‘电影’ 制作很差，\nneg\n\n\n\n从 DataBlock 的定义来看，每个部分都与我们构建的先前数据块相似，但有两个重要的例外：\n\nTextBlock.from_folder 不再具有 is_lm=True 参数。\n我们传递了为语言模型微调创建的 vocab。\n\n我们传递语言模型的 vocab 是为了确保我们使用相同的标记到索引的对应关系。否则，我们在微调语言模型中学到的嵌入对这个模型没有任何意义，微调步骤也没有任何用处。\n通过传递 is_lm=False（或者根本不传递 is_lm，因为它默认为 False），我们告诉 TextBlock 我们有常规标记的数据，而不是将下一个标记作为标签。然而，我们必须处理一个挑战，这与将多个文档合并成一个小批次有关。让我们通过一个示例来看，尝试创建一个包含前 10 个文档的小批次。首先我们将它们数值化：\nnums_samp = toks200[:10].map(num)\n现在让我们看看这 10 条电影评论中每条有多少个标记：\nnums_samp.map(len)\n(#10) [228,238,121,290,196,194,533,124,581,155]\n记住，PyTorch 的 DataLoader 需要将批次中的所有项目整合到一个张量中，而一个张量具有固定的形状（即，每个轴上都有特定的长度，并且所有项目必须一致）。这应该听起来很熟悉：我们在图像中也遇到了同样的问题。在那种情况下，我们使用裁剪、填充和/或压缩来使所有输入大小相同。对于文档来说，裁剪可能不是一个好主意，因为我们可能会删除一些关键信息（话虽如此，对于图像也是同样的问题，我们在那里使用裁剪；数据增强在自然语言处理领域尚未得到很好的探索，因此也许在自然语言处理中也有使用裁剪的机会！）。你不能真正“压缩”一个文档。所以只剩下填充了！\n我们将扩展最短的文本以使它们都具有相同的大小。为此，我们使用一个特殊的填充标记，该标记将被我们的模型忽略。此外，为了避免内存问题并提高性能，我们将大致相同长度的文本批量处理在一起（对于训练集进行一些洗牌）。我们通过在每个时期之前（对于训练集）按长度对文档进行排序来实现这一点。结果是，整理成单个批次的文档往往具有相似的长度。我们不会将每个批次填充到相同的大小，而是使用每个批次中最大文档的大小作为目标大小。"
  },
  {
    "objectID": "Fastbook/translations/cn/10_nlp.html#微调分类器",
    "href": "Fastbook/translations/cn/10_nlp.html#微调分类器",
    "title": "第十章：NLP 深入探讨：RNNs",
    "section": "微调分类器",
    "text": "微调分类器\n最后一步是使用有区分性的学习率和逐步解冻进行训练。在计算机视觉中，我们经常一次性解冻整个模型，但对于 NLP 分类器，我们发现逐层解冻会产生真正的差异：\nlearn.fit_one_cycle(1, 2e-2)\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.347427\n0.184480\n0.929320\n00:33\n\n\n\n仅仅一个时期，我们就获得了与第一章中的训练相同的结果——还不错！我们可以将freeze_to设置为-2，以冻结除最后两个参数组之外的所有参数组：\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.247763\n0.171683\n0.934640\n00:37\n\n\n\n然后我们可以解冻更多层并继续训练：\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.193377\n0.156696\n0.941200\n00:45\n\n\n\n最后，整个模型！\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.172888\n0.153770\n0.943120\n01:01\n\n\n1\n0.161492\n0.155567\n0.942640\n00:57\n\n\n\n我们达到了 94.3%的准确率，这在仅仅三年前是最先进的性能。通过在所有文本上训练另一个模型，并对这两个模型的预测进行平均，我们甚至可以达到 95.1%的准确率，这是由 ULMFiT 论文引入的最先进技术。仅仅几个月前，通过微调一个更大的模型并使用昂贵的数据增强技术（将句子翻译成另一种语言，然后再翻译回来，使用另一个模型进行翻译）来打破了这一记录。\n使用预训练模型让我们构建了一个非常强大的微调语言模型，可以用来生成假评论或帮助对其进行分类。这是令人兴奋的事情，但要记住这项技术也可以被用于恶意目的。"
  },
  {
    "objectID": "Fastbook/translations/cn/10_nlp.html#进一步研究",
    "href": "Fastbook/translations/cn/10_nlp.html#进一步研究",
    "title": "第十章：NLP 深入探讨：RNNs",
    "section": "进一步研究",
    "text": "进一步研究\n\n看看你能学到关于语言模型和虚假信息的什么。今天最好的语言模型是什么？看看它们的一些输出。你觉得它们令人信服吗？坏人如何最好地利用这样的模型来制造冲突和不确定性？\n考虑到模型不太可能能够一致地识别机器生成的文本，可能需要哪些其他方法来处理利用深度学习的大规模虚假信息活动？"
  },
  {
    "objectID": "Fastbook/translations/cn/15_arch_details.html",
    "href": "Fastbook/translations/cn/15_arch_details.html",
    "title": "第十五章：应用架构深入探讨",
    "section": "",
    "text": "我们现在处于一个令人兴奋的位置，我们可以完全理解我们为计算机视觉、自然语言处理和表格分析使用的最先进模型的架构。在本章中，我们将填补有关 fastai 应用模型如何工作的所有缺失细节，并向您展示如何构建它们。\n我们还将回到我们在第十一章中看到的用于 Siamese 网络的自定义数据预处理流程，并向您展示如何使用 fastai 库中的组件为新任务构建自定义预训练模型。\n我们将从计算机视觉开始。"
  },
  {
    "objectID": "Fastbook/translations/cn/15_arch_details.html#cnn_learner",
    "href": "Fastbook/translations/cn/15_arch_details.html#cnn_learner",
    "title": "第十五章：应用架构深入探讨",
    "section": "cnn_learner",
    "text": "cnn_learner\n让我们看看当我们使用cnn_learner函数时会发生什么。我们首先向这个函数传递一个用于网络主体的架构。大多数情况下，我们使用 ResNet，您已经知道如何创建，所以我们不需要深入研究。预训练权重将根据需要下载并加载到 ResNet 中。\n然后，对于迁移学习，网络需要被切割。这指的是切掉最后一层，该层仅负责 ImageNet 特定的分类。实际上，我们不仅切掉这一层，还切掉自自适应平均池化层以及之后的所有内容。这样做的原因很快就会变得清楚。由于不同的架构可能使用不同类型的池化层，甚至完全不同类型的头部，我们不仅仅搜索自适应池化层来决定在哪里切割预训练模型。相反，我们有一个信息字典，用于确定每个模型的主体在哪里结束，头部从哪里开始。我们称之为model_meta—这是resnet50的信息：\nmodel_meta[resnet50]\n{'cut': -2,\n 'split': &lt;function fastai.vision.learner._resnet_split(m)&gt;,\n 'stats': ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])}"
  },
  {
    "objectID": "Fastbook/translations/cn/15_arch_details.html#unet_learner",
    "href": "Fastbook/translations/cn/15_arch_details.html#unet_learner",
    "title": "第十五章：应用架构深入探讨",
    "section": "unet_learner",
    "text": "unet_learner\n深度学习中最有趣的架构之一是我们在第一章中用于分割的架构。分割是一项具有挑战性的任务，因为所需的输出实际上是一幅图像，或者一个像素网格，包含了每个像素的预测标签。其他任务也有类似的基本设计，比如增加图像的分辨率（超分辨率）、给黑白图像添加颜色（着色）、或将照片转换为合成画作（风格转移）——这些任务在本书的在线章节中有介绍，所以在阅读完本章后一定要查看。在每种情况下，我们都是从一幅图像开始，将其转换为另一幅具有相同尺寸或纵横比的图像，但像素以某种方式被改变。我们将这些称为生成式视觉模型。\n我们的做法是从与前一节中看到的开发 CNN 头部的确切方法开始。例如，我们从一个 ResNet 开始，然后截断自适应池化层和之后的所有层。然后我们用我们的自定义头部替换这些层，执行生成任务。\n在上一句中有很多含糊之处！我们到底如何创建一个生成图像的 CNN 头部？如果我们从一个 224 像素的输入图像开始，那么在 ResNet 主体的末尾，我们将得到一个 7×7 的卷积激活网格。我们如何将其转换为一个 224 像素的分割掩模？\n当然，我们使用神经网络来做这个！所以我们需要一种能够在 CNN 中增加网格大小的层。一个简单的方法是用一个 2×2 的方块替换 7×7 网格中的每个像素。这四个像素中的每一个将具有相同的值——这被称为最近邻插值。PyTorch 为我们提供了一个可以做到这一点的层，因此一个选项是创建一个包含步长为 1 的卷积层（以及通常的批归一化和 ReLU 层）和 2×2 最近邻插值层的头部。实际上，你现在可以尝试一下！看看你是否可以创建一个设计如此的自定义头部，并在 CamVid 分割任务上尝试一下。你应该会发现你得到了一些合理的结果，尽管它们不会像我们在第一章中的结果那样好。\n另一种方法是用转置卷积替换最近邻和卷积的组合，也被称为步长一半卷积。这与常规卷积相同，但首先在输入的所有像素之间插入零填充。这在图片上最容易看到——图 15-1 显示了一张来自我们在第十三章讨论过的优秀的卷积算术论文中的图表，展示了一个应用于 3×3 图像的 3×3 转置卷积。\n\n\n\n一个转置卷积\n\n\n\n图 15-1. 一个转置卷积（由 Vincent Dumoulin 和 Francesco Visin 提供）\n正如你所看到的，结果是增加输入的大小。你现在可以通过使用 fastai 的ConvLayer类来尝试一下；在你的自定义头部中传递参数transpose=True来创建一个转置卷积，而不是一个常规卷积。\n然而，这两种方法都不是很好。问题在于我们的 7×7 网格根本没有足够的信息来创建一个 224×224 像素的输出。要求每个网格单元的激活具有足够的信息来完全重建输出中的每个像素是非常困难的。\n解决方案是使用跳跃连接，就像 ResNet 中那样，但是从 ResNet 主体中的激活一直跳到架构对面的转置卷积的激活。这种方法在 2015 年 Olaf Ronneberger 等人的论文“U-Net:用于生物医学图像分割的卷积网络”中有所阐述。尽管该论文侧重于医学应用，但 U-Net 已经彻底改变了各种生成视觉模型。\n\n\n\nU-Net 架构\n\n\n\n\n图 15-2。U-Net 架构（由 Olaf Ronneberger、Philipp Fischer 和 Thomas Brox 提供）\n这幅图片展示了左侧的 CNN 主体（在这种情况下，它是一个常规的 CNN，而不是 ResNet，它们使用 2×2 最大池化而不是步幅为 2 的卷积，因为这篇论文是在 ResNets 出现之前写的），右侧是转置卷积（“上采样”）层。额外的跳跃连接显示为从左到右的灰色箭头（有时被称为交叉连接）。你可以看到为什么它被称为U-Net！\n有了这种架构，传递给转置卷积的输入不仅是前一层中较低分辨率的网格，还有 ResNet 头部中较高分辨率的网格。这使得 U-Net 可以根据需要使用原始图像的所有信息。U-Net 的一个挑战是确切的架构取决于图像大小。fastai 有一个独特的DynamicUnet类，根据提供的数据自动生成合适大小的架构。\n现在让我们专注于一个示例，其中我们利用 fastai 库编写一个自定义模型。"
  },
  {
    "objectID": "Fastbook/translations/cn/15_arch_details.html#孪生网络",
    "href": "Fastbook/translations/cn/15_arch_details.html#孪生网络",
    "title": "第十五章：应用架构深入探讨",
    "section": "孪生网络",
    "text": "孪生网络\n让我们回到我们在第十一章中为孪生网络设置的输入管道。你可能还记得，它由一对图像组成，标签为True或False，取决于它们是否属于同一类。\n利用我们刚刚看到的内容，让我们为这个任务构建一个自定义模型并对其进行训练。如何做？我们将使用一个预训练的架构并将我们的两个图像传递给它。然后我们可以连接结果并将它们发送到一个自定义头部，该头部将返回两个预测。在模块方面，看起来像这样：\nclass SiameseModel(Module):\n    def __init__(self, encoder, head):\n        self.encoder,self.head = encoder,head\n\n    def forward(self, x1, x2):\n        ftrs = torch.cat([self.encoder(x1), self.encoder(x2)], dim=1)\n        return self.head(ftrs)\n要创建我们的编码器，我们只需要取一个预训练模型并切割它，就像我们之前解释的那样。函数create_body为我们执行此操作；我们只需传递我们想要切割的位置。正如我们之前看到的，根据预训练模型的元数据字典，ResNet 的切割值为-2：\nencoder = create_body(resnet34, cut=-2)\n然后我们可以创建我们的头部。查看编码器告诉我们最后一层有 512 个特征，所以这个头部将需要接收512*4。为什么是 4？首先我们必须乘以 2，因为我们有两个图像。然后我们需要第二次乘以 2，因为我们的连接池技巧。因此我们创建头部如下：\nhead = create_head(512*4, 2, ps=0.5)\n有了我们的编码器和头部，我们现在可以构建我们的模型：\nmodel = SiameseModel(encoder, head)\n在使用Learner之前，我们还需要定义两件事。首先，我们必须定义要使用的损失函数。它是常规的交叉熵，但由于我们的目标是布尔值，我们需要将它们转换为整数，否则 PyTorch 会抛出错误：\ndef loss_func(out, targ):\n    return nn.CrossEntropyLoss()(out, targ.long())\n更重要的是，为了充分利用迁移学习，我们必须定义一个自定义的splitter。splitter是一个告诉 fastai 库如何将模型分成参数组的函数。这些在幕后用于在进行迁移学习时仅训练模型的头部。\n这里我们想要两个参数组：一个用于编码器，一个用于头部。因此我们可以定义以下splitter（params只是一个返回给定模块的所有参数的函数）：\ndef siamese_splitter(model):\n    return [params(model.encoder), params(model.head)]\n然后，我们可以通过传递数据、模型、损失函数、分割器和任何我们想要的指标来定义我们的Learner。由于我们没有使用 fastai 的传输学习便利函数（如cnn_learner），我们必须手动调用learn.freeze。这将确保只有最后一个参数组（在本例中是头部）被训练：\nlearn = Learner(dls, model, loss_func=loss_func,\n                splitter=siamese_splitter, metrics=accuracy)\nlearn.freeze()\n然后我们可以直接使用通常的方法训练我们的模型：\nlearn.fit_one_cycle(4, 3e-3)\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.367015\n0.281242\n0.885656\n00:26\n\n\n1\n0.307688\n0.214721\n0.915426\n00:26\n\n\n2\n0.275221\n0.170615\n0.936401\n00:26\n\n\n3\n0.223771\n0.159633\n0.943843\n00:26\n\n\n\n现在我们解冻并使用有区别的学习率微调整个模型一点（即，对于主体使用较低的学习率，对于头部使用较高的学习率）：\nlearn.unfreeze()\nlearn.fit_one_cycle(4, slice(1e-6,1e-4))\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.212744\n0.159033\n0.944520\n00:35\n\n\n1\n0.201893\n0.159615\n0.942490\n00:35\n\n\n2\n0.204606\n0.152338\n0.945196\n00:36\n\n\n3\n0.213203\n0.148346\n0.947903\n00:36\n\n\n\n94.8％是非常好的，当我们记得以相同方式训练的分类器（没有数据增强）的错误率为 7％时。\n现在我们已经看到如何创建完整的最先进的计算机视觉模型，让我们继续进行自然语言处理。"
  },
  {
    "objectID": "Fastbook/translations/cn/15_arch_details.html#进一步研究",
    "href": "Fastbook/translations/cn/15_arch_details.html#进一步研究",
    "title": "第十五章：应用架构深入探讨",
    "section": "进一步研究",
    "text": "进一步研究\n\n编写自己的自定义头，并尝试使用它训练宠物识别器。看看是否可以获得比 fastai 默认更好的结果。\n尝试在 CNN 头部之间切换AdaptiveConcatPool2d和AdaptiveAvgPool2d，看看会有什么不同。\n编写自己的自定义分割器，为每个 ResNet 块创建一个单独的参数组，以及一个单独的参数组用于干扰。尝试使用它进行训练，看看是否可以改善宠物识别器。\n阅读关于生成图像模型的在线章节，并创建自己的着色器、超分辨率模型或风格转移模型。\n使用最近邻插值创建一个自定义头，并用它在 CamVid 上进行分割。"
  },
  {
    "objectID": "Fastbook/translations/cn/13_convolutions.html",
    "href": "Fastbook/translations/cn/13_convolutions.html",
    "title": "第十三章：卷积神经网络",
    "section": "",
    "text": "在第四章中，我们学习了如何创建一个识别图像的神经网络。我们能够在区分 3 和 7 方面达到 98%以上的准确率，但我们也看到 fastai 内置的类能够接近 100%。让我们开始尝试缩小这个差距。\n在本章中，我们将首先深入研究卷积是什么，并从头开始构建一个 CNN。然后，我们将研究一系列技术来改善训练稳定性，并学习库通常为我们应用的所有调整，以获得出色的结果。"
  },
  {
    "objectID": "Fastbook/translations/cn/13_convolutions.html#映射卷积核",
    "href": "Fastbook/translations/cn/13_convolutions.html#映射卷积核",
    "title": "第十三章：卷积神经网络",
    "section": "映射卷积核",
    "text": "映射卷积核\n我们可以在坐标网格上映射apply_kernel()。也就是说，我们将取我们的 3×3 卷积核，并将其应用于图像的每个 3×3 部分。例如，图 13-2 显示了 3×3 卷积核可以应用于 5×5 图像第一行的位置。\n\n\n\n在网格上应用卷积核\n\n\n\n图 13-2. 在网格上应用卷积核\n要获得坐标网格，我们可以使用嵌套列表推导，如下所示：\n[[(i,j) for j in range(1,5)] for i in range(1,5)]\n[[(1, 1), (1, 2), (1, 3), (1, 4)],\n [(2, 1), (2, 2), (2, 3), (2, 4)],\n [(3, 1), (3, 2), (3, 3), (3, 4)],\n [(4, 1), (4, 2), (4, 3), (4, 4)]]"
  },
  {
    "objectID": "Fastbook/translations/cn/13_convolutions.html#pytorch-中的卷积",
    "href": "Fastbook/translations/cn/13_convolutions.html#pytorch-中的卷积",
    "title": "第十三章：卷积神经网络",
    "section": "PyTorch 中的卷积",
    "text": "PyTorch 中的卷积\n卷积是一个如此重要且广泛使用的操作，PyTorch 已经内置了它。它被称为F.conv2d（回想一下，F是从torch.nn.functional中导入的 fastai，正如 PyTorch 建议的）。PyTorch 文档告诉我们它包括这些参数：\ninput\n形状为(minibatch, in_channels, iH, iW)的输入张量\nweight\n形状为(out_channels, in_channels, kH, kW)的滤波器\n这里iH，iW是图像的高度和宽度（即28,28），kH，kW是我们内核的高度和宽度（3,3）。但显然 PyTorch 期望这两个参数都是秩为 4 的张量，而当前我们只有秩为 2 的张量（即矩阵，或具有两个轴的数组）。\n这些额外轴的原因是 PyTorch 有一些技巧。第一个技巧是 PyTorch 可以同时将卷积应用于多个图像。这意味着我们可以一次在批次中的每个项目上调用它！\n第二个技巧是 PyTorch 可以同时应用多个内核。因此，让我们也创建对角边缘内核，然后将我们的四个边缘内核堆叠成一个单个张量：\ndiag1_edge = tensor([[ 0,-1, 1],\n                     [-1, 1, 0],\n                     [ 1, 0, 0]]).float()\ndiag2_edge = tensor([[ 1,-1, 0],\n                     [ 0, 1,-1],\n                     [ 0, 0, 1]]).float()\n\nedge_kernels = torch.stack([left_edge, top_edge, diag1_edge, diag2_edge])\nedge_kernels.shape\ntorch.Size([4, 3, 3])\n为了测试这个，我们需要一个DataLoader和一个样本小批量。让我们使用数据块 API：\nmnist = DataBlock((ImageBlock(cls=PILImageBW), CategoryBlock),\n                  get_items=get_image_files,\n                  splitter=GrandparentSplitter(),\n                  get_y=parent_label)\n\ndls = mnist.dataloaders(path)\nxb,yb = first(dls.valid)\nxb.shape\ntorch.Size([64, 1, 28, 28])\n默认情况下，fastai 在使用数据块时会将数据放在 GPU 上。让我们将其移动到 CPU 用于我们的示例：\nxb,yb = to_cpu(xb),to_cpu(yb)\n一个批次包含 64 张图片，每张图片有 1 个通道，每个通道有 28×28 个像素。F.conv2d也可以处理多通道（彩色）图像。通道是图像中的单个基本颜色——对于常规全彩图像，有三个通道，红色、绿色和蓝色。PyTorch 将图像表示为一个秩为 3 的张量，具有以下维度：\n[*channels*, *rows*, *columns*]\n我们将在本章后面看到如何处理多个通道。传递给F.conv2d的内核需要是秩为 4 的张量：\n[*channels_in*, *features_out*, *rows*, *columns*]\nedge_kernels目前缺少其中一个：我们需要告诉 PyTorch 内核中的输入通道数是 1，我们可以通过在第一个位置插入一个大小为 1 的轴来实现（这称为单位轴），PyTorch 文档显示in_channels应该是预期的。要在张量中插入一个单位轴，我们使用unsqueeze方法：\nedge_kernels.shape,edge_kernels.unsqueeze(1).shape\n(torch.Size([4, 3, 3]), torch.Size([4, 1, 3, 3]))\n现在这是edge_kernels的正确形状。让我们将所有这些传递给conv2d：\nedge_kernels = edge_kernels.unsqueeze(1)\nbatch_features = F.conv2d(xb, edge_kernels)\nbatch_features.shape\ntorch.Size([64, 4, 26, 26])\n输出形状显示我们有 64 个图像在小批量中，4 个内核，以及 26×26 的边缘映射（我们从前面讨论中开始是 28×28 的图像，但每边丢失一个像素）。我们可以看到我们得到了与手动操作时相同的结果：\nshow_image(batch_features[0,0]);\n\nPyTorch 最重要的技巧是它可以使用 GPU 并行地完成所有这些工作-将多个核应用于多个图像，跨多个通道。并行进行大量工作对于使 GPU 高效工作至关重要；如果我们一次执行每个操作，通常会慢几百倍（如果我们使用前一节中的手动卷积循环，将慢数百万倍！）。因此，要成为一名优秀的深度学习从业者，一个需要练习的技能是让 GPU 一次处理大量工作。\n不要在每个轴上丢失这两个像素会很好。我们这样做的方法是添加填充，简单地在图像周围添加额外的像素。最常见的是添加零像素。"
  },
  {
    "objectID": "Fastbook/translations/cn/13_convolutions.html#步幅和填充",
    "href": "Fastbook/translations/cn/13_convolutions.html#步幅和填充",
    "title": "第十三章：卷积神经网络",
    "section": "步幅和填充",
    "text": "步幅和填充\n通过适当的填充，我们可以确保输出激活图与原始图像的大小相同，这在构建架构时可以使事情变得简单得多。图 13-4 显示了添加填充如何允许我们在图像角落应用核。\n\n\n\n带填充的卷积\n\n\n\n图 13-4。带填充的卷积\n使用 5×5 输入，4×4 核和 2 像素填充，我们最终得到一个 6×6 的激活图，如我们在图 13-5 中所看到的。\n\n\n\n4x4 核与 5x5 输入和 2 像素填充\n\n\n\n\n图 13-5。一个 4×4 的核与 5×5 的输入和 2 像素的填充（由 Vincent Dumoulin 和 Francesco Visin 提供）\n如果我们添加一个大小为ks乘以ks的核（其中ks是一个奇数），为了保持相同的形状，每一侧所需的填充是ks//2。对于ks的偶数，需要在上/下和左/右两侧填充不同数量，但实际上我们几乎从不使用偶数滤波器大小。\n到目前为止，当我们将核应用于网格时，我们每次将其移动一个像素。但我们可以跳得更远；例如，我们可以在每次核应用后移动两个像素，就像图 13-6 中所示。这被称为步幅-2卷积。实践中最常见的核大小是 3×3，最常见的填充是 1。正如您将看到的，步幅-2 卷积对于减小输出大小很有用，而步幅-1 卷积对于添加层而不改变输出大小也很有用。\n\n\n\n3x3 核与 5x5 输入，步幅 2 卷积和 1 像素填充\n\n\n\n\n图 13-6。一个 3×3 的核与 5×5 的输入，步幅 2 卷积和 1 像素填充（由 Vincent Dumoulin 和 Francesco Visin 提供）\n在大小为h乘以w的图像中，使用填充 1 和步幅 2 将给出大小为(h+1)//2乘以(w+1)//2的结果。每个维度的一般公式是\n(n + 2*pad - ks) // stride + 1\n其中pad是填充，ks是我们核的大小，stride是步幅。\n现在让我们看看如何计算我们卷积结果的像素值。"
  },
  {
    "objectID": "Fastbook/translations/cn/13_convolutions.html#理解卷积方程",
    "href": "Fastbook/translations/cn/13_convolutions.html#理解卷积方程",
    "title": "第十三章：卷积神经网络",
    "section": "理解卷积方程",
    "text": "理解卷积方程\n为了解释卷积背后的数学，fast.ai 学生 Matt Kleinsmith 提出了一个非常聪明的想法，展示了不同视角的 CNNs。事实上，这个想法非常聪明，非常有帮助，我们也会在这里展示！\n这是我们的 3×3 像素图像，每个像素都用字母标记：\n\n\n\n图像\n\n\n这是我们的核，每个权重都用希腊字母标记：\n\n\n\n核\n\n\n由于滤波器适合图像四次，我们有四个结果：\n\n\n\n激活\n\n\n图 13-7 显示了我们如何将核应用于图像的每个部分以产生每个结果。\n\n\n\n应用核\n\n\n\n图 13-7。应用核\n方程视图在图 13-8 中。\n\n\n\n方程\n\n\n\n\n图 13-8。方程\n请注意，偏置项b对于图像的每个部分都是相同的。您可以将偏置视为滤波器的一部分，就像权重（α、β、γ、δ）是滤波器的一部分一样。\n这里有一个有趣的见解——卷积可以被表示为一种特殊类型的矩阵乘法，如图 13-9 所示。权重矩阵就像传统神经网络中的那些一样。但是，这个权重矩阵具有两个特殊属性：\n\n灰色显示的零是不可训练的。这意味着它们在优化过程中将保持为零。\n一些权重是相等的，虽然它们是可训练的（即可更改的），但它们必须保持相等。这些被称为共享权重。\n\n零对应于滤波器无法触及的像素。权重矩阵的每一行对应于滤波器的一次应用。\n\n\n\n卷积作为矩阵乘法\n\n\n\n\n图 13-9。卷积作为矩阵乘法\n现在我们了解了卷积是什么，让我们使用它们来构建一个神经网络。"
  },
  {
    "objectID": "Fastbook/translations/cn/13_convolutions.html#创建-cnn",
    "href": "Fastbook/translations/cn/13_convolutions.html#创建-cnn",
    "title": "第十三章：卷积神经网络",
    "section": "创建 CNN",
    "text": "创建 CNN\n让我们回到第四章中的基本神经网络。它的定义如下：\nsimple_net = nn.Sequential(\n    nn.Linear(28*28,30),\n    nn.ReLU(),\n    nn.Linear(30,1)\n)\n我们可以查看模型的定义：\nsimple_net\nSequential(\n  (0): Linear(in_features=784, out_features=30, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=30, out_features=1, bias=True)\n)\n现在我们想要创建一个类似于这个线性模型的架构，但是使用卷积层而不是线性层。nn.Conv2d是F.conv2d的模块等效物。在创建架构时，它比F.conv2d更方便，因为在实例化时会自动为我们创建权重矩阵。\n这是一个可能的架构：\nbroken_cnn = sequential(\n    nn.Conv2d(1,30, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(30,1, kernel_size=3, padding=1)\n)\n这里需要注意的一点是，我们不需要指定28*28作为输入大小。这是因为线性层需要在权重矩阵中为每个像素设置一个权重，因此它需要知道有多少像素，但卷积会自动应用于每个像素。权重仅取决于输入和输出通道的数量以及核大小，正如我们在前一节中看到的。\n想一想输出形状会是什么；然后让我们尝试一下：\nbroken_cnn(xb).shape\ntorch.Size([64, 1, 28, 28])\n这不是我们可以用来进行分类的东西，因为我们需要每个图像一个单独的输出激活，而不是一个 28×28 的激活图。处理这个问题的一种方法是使用足够多的步幅为 2 的卷积，使得最终层的大小为 1。经过一次步幅为 2 的卷积后，大小将为 14×14；经过两次后，将为 7×7；然后是 4×4，2×2，最终大小为 1。\n现在让我们尝试一下。首先，我们将定义一个函数，其中包含我们在每个卷积中将使用的基本参数：\ndef conv(ni, nf, ks=3, act=True):\n    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res"
  },
  {
    "objectID": "Fastbook/translations/cn/13_convolutions.html#理解卷积算术",
    "href": "Fastbook/translations/cn/13_convolutions.html#理解卷积算术",
    "title": "第十三章：卷积神经网络",
    "section": "理解卷积算术",
    "text": "理解卷积算术\n我们可以从总结中看到，我们有一个大小为64x1x28x28的输入。轴是批次、通道、高度、宽度。这通常表示为NCHW(其中N是批次大小)。另一方面，TensorFlow 使用NHWC轴顺序。这是第一层：\nm = learn.model[0]\nm\nSequential(\n  (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (1): ReLU()\n)\n因此，我们有 1 个输入通道，4 个输出通道和一个 3×3 的内核。让我们检查第一个卷积的权重：\nm[0].weight.shape\ntorch.Size([4, 1, 3, 3])\n总结显示我们有 40 个参数，4*1*3*3是 36。其他四个参数是什么？让我们看看偏差包含什么：\nm[0].bias.shape\ntorch.Size([4])\n我们现在可以利用这些信息来澄清我们在上一节中的陈述：“当我们使用步幅为 2 的卷积时，我们经常增加特征的数量，因为我们通过 4 的因子减少了激活图中的激活数量；我们不希望一次性太多地减少层的容量。”\n每个通道都有一个偏差。(有时通道被称为特征或滤波器，当它们不是输入通道时。) 输出形状是64x4x14x14，因此这将成为下一层的输入形状。根据summary，下一层有 296 个参数。让我们忽略批次轴，保持简单。因此，对于14*14=196个位置，我们正在乘以296-8=288个权重(为简单起见忽略偏差)，因此在这一层有196*288=56,448次乘法。下一层将有7*7*(1168-16)=56,448次乘法。\n这里发生的情况是，我们的步幅为 2 的卷积将网格大小从14x14减半到7x7，并且我们将滤波器数量从 8 增加到 16，导致总体计算量没有变化。如果我们在每个步幅为 2 的层中保持通道数量不变，那么网络中所做的计算量会随着深度增加而减少。但我们知道，更深层次必须计算语义丰富的特征(如眼睛或毛发)，因此我们不会期望减少计算是有意义的。\n另一种思考这个问题的方式是基于感受野。"
  },
  {
    "objectID": "Fastbook/translations/cn/13_convolutions.html#感受野",
    "href": "Fastbook/translations/cn/13_convolutions.html#感受野",
    "title": "第十三章：卷积神经网络",
    "section": "感受野",
    "text": "感受野\n接受域是参与层计算的图像区域。在书籍网站上，您会找到一个名为conv-example.xlsx的 Excel 电子表格，展示了使用 MNIST 数字计算两个步幅为 2 的卷积层的过程。每个层都有一个单独的核。图 13-10 展示了如果我们点击conv2部分中的一个单元格，显示第二个卷积层的输出，并点击trace precedents时看到的内容。\n\n\n\nconv2 层的直接前置\n\n\n\n图 13-10. Conv2 层的直接前置\n这里，有绿色边框的单元格是我们点击的单元格，蓝色高亮显示的单元格是它的前置——用于计算其值的单元格。这些单元格是输入层（左侧）的对应 3×3 区域单元格和滤波器（右侧）的单元格。现在让我们再次点击trace precedents，看看用于计算这些输入的单元格。图 13-11 展示了发生了什么。\n\n\n\nconv2 层的次要前置\n\n\n\n\n图 13-11. Conv2 层的次要前置\n在这个例子中，我们只有两个步幅为 2 的卷积层，因此现在追溯到了输入图像。我们可以看到输入层中的一个 7×7 区域单元格用于计算 Conv2 层中的单个绿色单元格。这个 7×7 区域是 Conv2 中绿色激活的输入的接受域。我们还可以看到现在需要第二个滤波器核，因为我们有两个层。\n从这个例子中可以看出，我们在网络中越深（特别是在一个层之前有更多步幅为 2 的卷积层时），该层中激活的接受域就越大。一个大的接受域意味着输入图像的大部分被用来计算该层中每个激活。我们现在知道，在网络的深层，我们有语义丰富的特征，对应着更大的接受域。因此，我们期望我们需要更多的权重来处理这种不断增加的复杂性。这是另一种说法，与我们在前一节提到的相同：当我们在网络中引入步幅为 2 的卷积时，我们也应该增加通道数。\n在撰写这一特定章节时，我们有很多问题需要回答，以便尽可能好地向您解释 CNN。信不信由你，我们在 Twitter 上找到了大部分答案。在我们继续讨论彩色图像之前，我们将快速休息一下，与您谈谈这个问题。"
  },
  {
    "objectID": "Fastbook/translations/cn/13_convolutions.html#关于-twitter-的一点说明",
    "href": "Fastbook/translations/cn/13_convolutions.html#关于-twitter-的一点说明",
    "title": "第十三章：卷积神经网络",
    "section": "关于 Twitter 的一点说明",
    "text": "关于 Twitter 的一点说明\n总的来说，我们并不是社交网络的重度用户。但我们写这本书的目标是帮助您成为最优秀的深度学习从业者，我们不提及 Twitter 在我们自己的深度学习之旅中有多么重要是不合适的。\n您看，Twitter 还有另一部分，远离唐纳德·特朗普和卡戴珊家族，深度学习研究人员和从业者每天都在这里交流。在我们撰写这一部分时，Jeremy 想要再次确认我们关于步幅为 2 的卷积的说法是否准确，所以他在 Twitter 上提问：\n\n\n\ntwitter 1\n\n\n几分钟后，这个答案出现了：\n\n\n\ntwitter 2\n\n\nChristian Szegedy 是Inception的第一作者，这是 2014 年 ImageNet 的获奖作品，也是现代神经网络中许多关键见解的来源。两小时后，这个出现了：\n\n\n\ntwitter 3\n\n\n你认识那个名字吗？您在第二章中看到过，当时我们在谈论今天建立深度学习基础的图灵奖获得者！\nJeremy 还在 Twitter 上询问有关我们在第七章中描述的标签平滑是否准确，并再次直接从 Christian Szegedy（标签平滑最初是在 Inception 论文中引入的）那里得到了回应：\n\n\n\ntwitter 4\n\n\n今天深度学习领域的许多顶尖人物经常在 Twitter 上活跃，并且非常乐意与更广泛的社区互动。一个好的开始方法是查看 Jeremy 的最近的 Twitter 点赞，或者Sylvain 的。这样，您可以看到我们认为有趣和有用的人发表的 Twitter 用户列表。\nTwitter 是我们保持与有趣论文、软件发布和其他深度学习新闻最新的主要途径。为了与深度学习社区建立联系，我们建议在fast.ai 论坛和 Twitter 上都积极参与。\n话虽如此，让我们回到本章的重点。到目前为止，我们只展示了黑白图片的示例，每个像素只有一个值。实际上，大多数彩色图像每个像素有三个值来定义它们的颜色。接下来我们将看看如何处理彩色图像。"
  },
  {
    "objectID": "Fastbook/translations/cn/13_convolutions.html#一个简单的基线",
    "href": "Fastbook/translations/cn/13_convolutions.html#一个简单的基线",
    "title": "第十三章：卷积神经网络",
    "section": "一个简单的基线",
    "text": "一个简单的基线\n在本章的前面，我们基于类似于conv函数构建了一个模型：\ndef conv(ni, nf, ks=3, act=True):\n    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n让我们从一个基本的 CNN 作为基线开始。我们将使用与之前相同的一个，但有一个调整：我们将使用更多的激活。由于我们有更多的数字需要区分，我们可能需要学习更多的滤波器。\n正如我们讨论过的，通常我们希望每次有一个步幅为 2 的层时将滤波器数量加倍。在整个网络中增加滤波器数量的一种方法是在第一层中将激活数量加倍，然后每个之后的层也将比之前的版本大一倍。\n但这会产生一个微妙的问题。考虑应用于每个像素的卷积核。默认情况下，我们使用一个 3×3 像素的卷积核。因此，在每个位置上，卷积核被应用到了总共 3×3=9 个像素。以前，我们的第一层有四个输出滤波器。因此，在每个位置上，从九个像素计算出四个值。想想如果我们将输出加倍到八个滤波器会发生什么。然后当我们应用我们的卷积核时，我们将使用九个像素来计算八个数字。这意味着它实际上并没有学到太多：输出大小几乎与输入大小相同。只有当神经网络被迫这样做时，即从操作的输出数量明显小于输入数量时，它们才会创建有用的特征。\n为了解决这个问题，我们可以在第一层使用更大的卷积核。如果我们使用一个 5×5 像素的卷积核，每次卷积核应用时将使用 25 个像素。从中创建八个滤波器将意味着神经网络将不得不找到一些有用的特征：\ndef simple_cnn():\n    return sequential(\n        conv(1 ,8, ks=5),        #14x14\n        conv(8 ,16),             #7x7\n        conv(16,32),             #4x4\n        conv(32,64),             #2x2\n        conv(64,10, act=False),  #1x1\n        Flatten(),\n    )\n正如您将在接下来看到的，我们可以在模型训练时查看模型内部，以尝试找到使其训练更好的方法。为此，我们使用ActivationStats回调，记录每个可训练层的激活的均值、标准差和直方图（正如我们所见，回调用于向训练循环添加行为；我们将在第十六章中探讨它们的工作原理）：\nfrom fastai.callback.hook import *\n我们希望快速训练，这意味着以较高的学习率进行训练。让我们看看在 0.06 时的效果如何：\ndef fit(epochs=1):\n    learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy,\n                    metrics=accuracy, cbs=ActivationStats(with_hist=True))\n    learn.fit(epochs, 0.06)\n    return learn\nlearn = fit()\n\n\n\n轮数\n训练损失\n验证损失\n准确率\n时间\n\n\n\n\n0\n2.307071\n2.305865\n0.113500\n00:16\n\n\n\n这次训练效果不佳！让我们找出原因。\n传递给Learner的回调的一个方便功能是它们会自动提供，名称与回调类相同，除了使用驼峰命名法。因此，我们的ActivationStats回调可以通过activation_stats访问。我相信你还记得learn.recorder…你能猜到它是如何实现的吗？没错，它是一个名为Recorder的回调！\nActivationStats包含一些方便的实用程序，用于绘制训练期间的激活。plot_layer_stats(*idx*)绘制第idx层激活的均值和标准差，以及接近零的激活百分比。这是第一层的图表：\nlearn.activation_stats.plot_layer_stats(0)\n\n通常情况下，我们的模型在训练期间应该具有一致或至少平滑的层激活均值和标准差。接近零的激活值特别有问题，因为这意味着我们的模型中有一些计算根本没有做任何事情（因为乘以零得到零）。当一个层中有一些零时，它们通常会传递到下一层…然后创建更多的零。这是我们网络的倒数第二层：\nlearn.activation_stats.plot_layer_stats(-2)\n\n正如预期的那样，问题在网络末端变得更糟，因为不稳定性和零激活在层间累积。让我们看看如何使训练更稳定。"
  },
  {
    "objectID": "Fastbook/translations/cn/13_convolutions.html#增加批量大小",
    "href": "Fastbook/translations/cn/13_convolutions.html#增加批量大小",
    "title": "第十三章：卷积神经网络",
    "section": "增加批量大小",
    "text": "增加批量大小\n使训练更稳定的一种方法是增加批量大小。较大的批次具有更准确的梯度，因为它们是从更多数据计算出来的。然而，较大的批量大小意味着每个轮数的批次更少，这意味着您的模型更新权重的机会更少。让我们看看批量大小为 512 是否有帮助：\ndls = get_dls(512)\nlearn = fit()\n\n\n\n轮数\n训练损失\n验证损失\n准确率\n时间\n\n\n\n\n0\n2.309385\n2.302744\n0.113500\n00:08\n\n\n\n让我们看看倒数第二层是什么样的：\nlearn.activation_stats.plot_layer_stats(-2)\n\n再次，我们的大多数激活值接近零。让我们看看我们可以做些什么来改善训练稳定性。"
  },
  {
    "objectID": "Fastbook/translations/cn/13_convolutions.html#cycle-训练",
    "href": "Fastbook/translations/cn/13_convolutions.html#cycle-训练",
    "title": "第十三章：卷积神经网络",
    "section": "1cycle 训练",
    "text": "1cycle 训练\n我们的初始权重不适合我们要解决的任务。因此，以高学习率开始训练是危险的：我们很可能会使训练立即发散，正如我们所见。我们可能也不想以高学习率结束训练，这样我们就不会跳过一个最小值。但我们希望在训练期间保持高学习率，因为这样我们可以更快地训练。因此，我们应该在训练过程中改变学习率，从低到高，然后再次降低到低。\n莱斯利·史密斯（是的，就是发明学习率查找器的那个人！）在他的文章“超收敛：使用大学习率非常快速地训练神经网络”中发展了这个想法。他设计了一个学习率时间表，分为两个阶段：一个阶段学习率从最小值增长到最大值（预热），另一个阶段学习率再次降低到最小值（退火）。史密斯称这种方法的组合为1cycle 训练。\n1cycle 训练允许我们使用比其他类型训练更高的最大学习率，这带来了两个好处：\n\n通过使用更高的学习率进行训练，我们可以更快地训练——这种现象史密斯称之为超收敛。\n通过使用更高的学习率进行训练，我们过拟合较少，因为我们跳过了尖锐的局部最小值，最终进入了更平滑（因此更具有泛化能力）的损失部分。\n\n第二点是一个有趣而微妙的观察；它基于这样一个观察：一个泛化良好的模型，如果你稍微改变输入，它的损失不会发生很大变化。如果一个模型在较大的学习率下训练了相当长的时间，并且在这样做时能找到一个好的损失，那么它一定找到了一个泛化良好的区域，因为它在批次之间跳动很多（这基本上就是高学习率的定义）。问题在于，正如我们所讨论的，直接跳到高学习率更有可能导致损失发散，而不是看到损失改善。因此，我们不会直接跳到高学习率。相反，我们从低学习率开始，我们的损失不会发散，然后允许优化器逐渐找到参数的更平滑的区域，逐渐提高学习率。\n然后，一旦我们找到了参数的一个良好平滑区域，我们希望找到该区域的最佳部分，这意味着我们必须再次降低学习率。这就是为什么 1cycle 训练有一个渐进的学习率预热和渐进的学习率冷却。许多研究人员发现，实践中这种方法导致更准确的模型和更快的训练。这就是为什么在 fastai 中fine_tune默认使用这种方法。\n在第十六章中，我们将学习有关 SGD 中的动量。简而言之，动量是一种技术，优化器不仅朝着梯度的方向迈出一步，而且继续朝着以前的步骤的方向前进。 Leslie Smith 在“神经网络超参数的纪律方法：第 1 部分”中介绍了循环动量的概念。它建议动量与学习率的方向相反变化：当我们处于高学习率时，我们使用较少的动量，在退火阶段再次使用更多动量。\n我们可以通过调用fit_one_cycle在 fastai 中使用 1cycle 训练：\ndef fit(epochs=1, lr=0.06):\n    learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy,\n                    metrics=accuracy, cbs=ActivationStats(with_hist=True))\n    learn.fit_one_cycle(epochs, lr)\n    return learn\nlearn = fit()\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.210838\n0.084827\n0.974300\n00:08\n\n\n\n我们终于取得了一些进展！现在它给我们一个合理的准确率。\n我们可以通过在learn.recorder上调用plot_sched来查看训练过程中的学习率和动量。learn.recorder（顾名思义）记录了训练过程中发生的一切，包括损失、指标和超参数，如学习率和动量：\nlearn.recorder.plot_sched()\n\nSmith 的原始 1cycle 论文使用了线性热身和线性退火。正如您所看到的，我们通过将其与另一种流行方法——余弦退火相结合，在 fastai 中改进了这种方法。fit_one_cycle提供了以下您可以调整的参数：\nlr_max\n将使用的最高学习率（这也可以是每个层组的学习率列表，或包含第一个和最后一个层组学习率的 Python slice对象）\ndiv\n将lr_max除以多少以获得起始学习率\ndiv_final\n将lr_max除以多少以获得结束学习率\npct_start\n用于热身的批次百分比\nmoms\n一个元组(*mom1*,*mom2*,*mom3*)，其中mom1是初始动量，mom2是最小动量，mom3是最终动量\n让我们再次查看我们的层统计数据：\nlearn.activation_stats.plot_layer_stats(-2)\n\n非零权重的百分比正在得到很大的改善，尽管仍然相当高。通过使用color_dim并传递一个层索引，我们可以更多地了解我们的训练情况：\nlearn.activation_stats.color_dim(-2)\n\ncolor_dim是由 fast.ai 与学生 Stefano Giomo 共同开发的。Giomo 将这个想法称为丰富多彩维度，并提供了一个深入解释这种方法背后的历史和细节。基本思想是创建一个层的激活直方图，我们希望它会遵循一个平滑的模式，如正态分布（图 13-14）。\n\n\n\n丰富多彩维度的直方图\n\n\n\n图 13-14。丰富多彩维度的直方图（由 Stefano Giomo 提供）\n为了创建color_dim，我们将左侧显示的直方图转换为底部显示的彩色表示。然后，我们将其翻转，如右侧所示。我们发现，如果我们取直方图值的对数，分布会更清晰。然后，Giomo 描述：\n\n每个层的最终图是通过将每批次的激活直方图沿水平轴堆叠而成的。因此，可视化中的每个垂直切片代表单个批次的激活直方图。颜色强度对应直方图的高度；换句话说，每个直方图柱中的激活数量。\n\n图 13-15 展示了这一切是如何结合在一起的。\n\n\n\n丰富多彩维度的总结\n\n\n\n\n图 13-15。丰富多彩维度的总结（由 Stefano Giomo 提供）\n这说明了为什么当f遵循正态分布时，log(f)比f更丰富多彩，因为取对数会将高斯曲线变成二次曲线，这样不会那么狭窄。\n因此，让我们再次看看倒数第二层的结果：\nlearn.activation_stats.color_dim(-2)\n\n这展示了一个经典的“糟糕训练”图片。我们从几乎所有激活都为零开始——这是我们在最左边看到的，所有的深蓝色。底部的明黄色代表接近零的激活。然后，在最初的几批中，我们看到非零激活数量呈指数增长。但它走得太远并崩溃了！我们看到深蓝色回来了，底部再次变成明黄色。它几乎看起来像是训练重新从头开始。然后我们看到激活再次增加并再次崩溃。重复几次后，最终我们看到激活在整个范围内分布。\n如果训练一开始就能平稳进行会更好。指数增长然后崩溃的周期往往会导致大量接近零的激活，从而导致训练缓慢且最终结果不佳。解决这个问题的一种方法是使用批量归一化。"
  },
  {
    "objectID": "Fastbook/translations/cn/13_convolutions.html#批量归一化",
    "href": "Fastbook/translations/cn/13_convolutions.html#批量归一化",
    "title": "第十三章：卷积神经网络",
    "section": "批量归一化",
    "text": "批量归一化\n为了解决前一节中出现的训练缓慢和最终结果不佳的问题，我们需要解决初始大比例接近零的激活，并尝试在整个训练过程中保持良好的激活分布。\nSergey Ioffe 和 Christian Szegedy 在 2015 年的论文“Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift”中提出了这个问题的解决方案。在摘要中，他们描述了我们所见过的问题：\n\n训练深度神经网络的复杂性在于每一层输入的分布在训练过程中会发生变化，因为前一层的参数发生变化。这需要降低学习率和谨慎的参数初始化，从而减慢训练速度…我们将这种现象称为内部协变量转移，并通过对层输入进行归一化来解决这个问题。\n\n他们说他们的解决方案如下：\n\n将归一化作为模型架构的一部分，并对每个训练小批量进行归一化。批量归一化使我们能够使用更高的学习率，并且对初始化要求不那么严格。\n\n这篇论文一经发布就引起了极大的兴奋，因为它包含了图 13-16 中的图表，清楚地表明批量归一化可以训练出比当前最先进技术（Inception架构）更准确且速度快约 5 倍的模型。\n\n\n\n批量归一化的影响\n\n\n\n图 13-16. 批量归一化的影响（由 Sergey Ioffe 和 Christian Szegedy 提供）\n批量归一化（通常称为batchnorm）通过取层激活的均值和标准差的平均值来归一化激活。然而，这可能会导致问题，因为网络可能希望某些激活非常高才能进行准确的预测。因此，他们还添加了两个可学习参数（意味着它们将在 SGD 步骤中更新），通常称为gamma和beta。在将激活归一化以获得一些新的激活向量y之后，批量归一化层返回gamma*y + beta。\n这就是为什么我们的激活可以具有任何均值或方差，独立于前一层结果的均值和标准差。这些统计数据是分开学习的，使得我们的模型训练更容易。在训练和验证期间的行为是不同的：在训练期间，我们使用批次的均值和标准差来归一化数据，而在验证期间，我们使用训练期间计算的统计数据的运行均值。\n让我们在conv中添加一个批量归一化层：\ndef conv(ni, nf, ks=3, act=True):\n    layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)]\n    layers.append(nn.BatchNorm2d(nf))\n    if act: layers.append(nn.ReLU())\n    return nn.Sequential(*layers)\n并适应我们的模型：\nlearn = fit()\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.130036\n0.055021\n0.986400\n00:10\n\n\n\n这是一个很好的结果！让我们看看color_dim：\nlearn.activation_stats.color_dim(-4)\n\n这正是我们希望看到的：激活的平稳发展，没有“崩溃”。Batchnorm 在这里真的兑现了承诺！事实上，批量归一化非常成功，我们几乎可以在所有现代神经网络中看到它（或类似的东西）。\n关于包含批归一化层的模型的一个有趣观察是，它们往往比不包含批归一化层的模型更好地泛化。尽管我们尚未看到对这里发生的事情进行严格分析，但大多数研究人员认为原因是批归一化为训练过程添加了一些额外的随机性。每个小批次的均值和标准差都会与其他小批次有所不同。因此，激活每次都会被不同的值归一化。为了使模型能够做出准确的预测，它必须学会对这些变化变得稳健。通常，向训练过程添加额外的随机性通常有所帮助。\n由于事情进展顺利，让我们再训练几个周期，看看情况如何。实际上，让我们增加学习率，因为批归一化论文的摘要声称我们应该能够“以更高的学习率训练”：\nlearn = fit(5, lr=0.1)\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.191731\n0.121738\n0.960900\n00:11\n\n\n1\n0.083739\n0.055808\n0.981800\n00:10\n\n\n2\n0.053161\n0.044485\n0.987100\n00:10\n\n\n3\n0.034433\n0.030233\n0.990200\n00:10\n\n\n4\n0.017646\n0.025407\n0.991200\n00:10\n\n\n\nlearn = fit(5, lr=0.1)\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.183244\n0.084025\n0.975800\n00:13\n\n\n1\n0.080774\n0.067060\n0.978800\n00:12\n\n\n2\n0.050215\n0.062595\n0.981300\n00:12\n\n\n3\n0.030020\n0.030315\n0.990700\n00:12\n\n\n4\n0.015131\n0.025148\n0.992100\n00:12\n\n\n\n在这一点上，我认为我们知道如何识别数字了！是时候转向更难的东西了…"
  },
  {
    "objectID": "Fastbook/translations/cn/13_convolutions.html#进一步研究",
    "href": "Fastbook/translations/cn/13_convolutions.html#进一步研究",
    "title": "第十三章：卷积神经网络",
    "section": "进一步研究",
    "text": "进一步研究\n\n除了边缘检测器，计算机视觉中还使用了哪些特征（尤其是在深度学习变得流行之前）？\nPyTorch 中还有其他规范化层。尝试它们，看看哪种效果最好。了解其他规范化层的开发原因以及它们与批规范化的区别。\n尝试将激活函数移动到conv中的批规范化层后。这会有所不同吗？看看你能找到关于推荐顺序及原因的信息。"
  },
  {
    "objectID": "Fastbook/translations/cn/04_mnist_basics.html",
    "href": "Fastbook/translations/cn/04_mnist_basics.html",
    "title": "第四章：底层：训练数字分类器",
    "section": "",
    "text": "在第二章中看到训练各种模型的样子后，现在让我们深入了解并看看究竟发生了什么。我们将使用计算机视觉来介绍深度学习的基本工具和概念。\n确切地说，我们将讨论数组和张量的作用以及广播的作用，这是一种使用它们表达性地的强大技术。我们将解释随机梯度下降（SGD），这是通过自动更新权重学习的机制。我们将讨论基本分类任务的损失函数的选择，以及小批量的作用。我们还将描述基本神经网络正在执行的数学。最后，我们将把所有这些部分组合起来。\n在未来的章节中，我们还将深入研究其他应用，并看看这些概念和工具如何泛化。但本章是关于奠定基础的。坦率地说，这也使得这是最困难的章节之一，因为这些概念彼此相互依赖。就像一个拱门，所有的石头都需要放在正确的位置才能支撑结构。也像一个拱门，一旦发生这种情况，它就是一个强大的结构，可以支撑其他事物。但是需要一些耐心来组装。\n让我们开始吧。第一步是考虑图像在计算机中是如何表示的。"
  },
  {
    "objectID": "Fastbook/translations/cn/04_mnist_basics.html#numpy-数组和-pytorch-张量",
    "href": "Fastbook/translations/cn/04_mnist_basics.html#numpy-数组和-pytorch-张量",
    "title": "第四章：底层：训练数字分类器",
    "section": "NumPy 数组和 PyTorch 张量",
    "text": "NumPy 数组和 PyTorch 张量\nNumPy 是 Python 中用于科学和数值编程最广泛使用的库。它提供了类似的功能和类似的 API，与 PyTorch 提供的功能相似；然而，它不支持使用 GPU 或计算梯度，这两者对于深度学习都是至关重要的。因此，在本书中，我们通常会在可能的情况下使用 PyTorch 张量而不是 NumPy 数组。\n（请注意，fastai 在 NumPy 和 PyTorch 中添加了一些功能，使它们更加相似。如果本书中的任何代码在您的计算机上无法运行，可能是因为您忘记在笔记本的开头包含类似这样的一行代码：from fastai.vision.all import *。）\n但是数组和张量是什么，为什么你应该关心呢？\nPython 相对于许多语言来说速度较慢。在 Python、NumPy 或 PyTorch 中快速的任何东西，很可能是另一种语言（特别是 C）编写（并优化）的编译对象的包装器。事实上，NumPy 数组和 PyTorch 张量可以比纯 Python 快几千倍完成计算。\nNumPy 数组是一个多维数据表，所有项都是相同类型的。由于可以是任何类型，它们甚至可以是数组的数组，内部数组可能是不同大小的 - 这被称为 不规则数组。通过“多维数据表”，我们指的是，例如，一个列表（一维）、一个表或矩阵（二维）、一个表的表或立方体（三维），等等。如果所有项都是简单类型，如整数或浮点数，NumPy 将它们存储为紧凑的 C 数据结构在内存中。这就是 NumPy 的优势所在。NumPy 有各种运算符和方法，可以在这些紧凑结构上以优化的 C 速度运行计算，因为它们是用优化的 C 编写的。\nPyTorch 张量几乎与 NumPy 数组相同，但有一个额外的限制，可以解锁额外的功能。它与 NumPy 数组相同，也是一个多维数据表，所有项都是相同类型的。然而，限制是张量不能使用任何旧类型 - 它必须对所有组件使用单一基本数值类型。因此，张量不像真正的数组数组那样灵活。例如，PyTorch 张量不能是不规则的。它始终是一个形状规则的多维矩形结构。\nNumPy 在这些结构上支持的绝大多数方法和运算符在 PyTorch 上也支持，但 PyTorch 张量具有额外的功能。一个主要功能是这些结构可以存在于 GPU 上，这样它们的计算将被优化为 GPU，并且可以运行得更快（给定大量值进行处理）。此外，PyTorch 可以自动计算这些操作的导数，包括操作的组合。正如你将看到的，没有这种能力，实际上是不可能进行深度学习的。"
  },
  {
    "objectID": "Fastbook/translations/cn/04_mnist_basics.html#计算梯度",
    "href": "Fastbook/translations/cn/04_mnist_basics.html#计算梯度",
    "title": "第四章：底层：训练数字分类器",
    "section": "计算梯度",
    "text": "计算梯度\n唯一的魔法步骤是计算梯度的部分。正如我们提到的，我们使用微积分作为性能优化；它让我们更快地计算当我们调整参数时我们的损失会上升还是下降。换句话说，梯度将告诉我们我们需要改变每个权重多少才能使我们的模型更好。\n您可能还记得高中微积分课上的导数告诉您函数参数的变化会如何改变其结果。如果不记得，不用担心；我们很多人高中毕业后就忘了微积分！但在继续之前，您需要对导数有一些直观的理解，所以如果您对此一头雾水，可以前往 Khan Academy 完成基本导数课程。您不必自己计算导数；您只需要知道导数是什么。\n导数的关键点在于：对于任何函数，比如我们在前一节中看到的二次函数，我们可以计算它的导数。导数是另一个函数。它计算的是变化，而不是值。例如，在值为 3 时，二次函数的导数告诉我们函数在值为 3 时的变化速度。更具体地说，您可能还记得梯度被定义为上升/水平移动；也就是说，函数值的变化除以参数值的变化。当我们知道我们的函数将如何变化时，我们就知道我们需要做什么来使它变小。这是机器学习的关键：有一种方法来改变函数的参数使其变小。微积分为我们提供了一个计算的捷径，即导数，它让我们直接计算我们函数的梯度。\n一个重要的事情要注意的是我们的函数有很多需要调整的权重，所以当我们计算导数时，我们不会得到一个数字，而是很多个—每个权重都有一个梯度。但在这里没有数学上的技巧；您可以计算相对于一个权重的导数，将其他所有权重视为常数，然后对每个其他权重重复这个过程。这就是计算所有梯度的方法，对于每个权重。\n刚才我们提到您不必自己计算任何梯度。这怎么可能？令人惊讶的是，PyTorch 能够自动计算几乎任何函数的导数！而且，它计算得非常快。大多数情况下，它至少与您手动创建的任何导数函数一样快。让我们看一个例子。\n首先，让我们选择一个张量数值，我们想要梯度：\nxt = tensor(3.).requires_grad_()\n注意特殊方法requires_grad_？这是我们告诉 PyTorch 我们想要计算梯度的神奇咒语。这实质上是给变量打上标记，这样 PyTorch 就会记住如何计算您要求的其他直接计算的梯度。"
  },
  {
    "objectID": "Fastbook/translations/cn/04_mnist_basics.html#使用学习率进行步进",
    "href": "Fastbook/translations/cn/04_mnist_basics.html#使用学习率进行步进",
    "title": "第四章：底层：训练数字分类器",
    "section": "使用学习率进行步进",
    "text": "使用学习率进行步进\n根据梯度值来决定如何改变我们的参数是深度学习过程中的一个重要部分。几乎所有方法都从一个基本思想开始，即将梯度乘以一些小数字，称为学习率（LR）。学习率通常是 0.001 到 0.1 之间的数字，尽管它可以是任何值。通常人们通过尝试几个学习率来选择一个，并找出哪个在训练后产生最佳模型的结果（我们将在本书后面展示一个更好的方法，称为学习率查找器）。一旦选择了学习率，您可以使用这个简单函数调整参数：\nw -= w.grad * lr\n这被称为调整您的参数，使用优化步骤。\n如果您选择的学习率太低，可能意味着需要执行很多步骤。图 4-2 说明了这一点。\n\n\n\n梯度下降示例，学习率过低\n\n\n\n图 4-2。学习率过低的梯度下降\n但选择一个学习率太高的学习率更糟糕——它可能导致损失变得更糟，正如我们在图 4-3 中看到的！\n\n\n\n学习率过高的梯度下降示例\n\n\n\n\n图 4-3. 学习率过高的梯度下降\n如果学习率太高，它也可能会“弹跳”而不是发散；图 4-4 显示了这样做需要许多步骤才能成功训练。\n\n\n\n带有弹跳学习率的梯度下降示例\n\n\n\n\n图 4-4. 带有弹跳学习率的梯度下降\n现在让我们在一个端到端的示例中应用所有这些。"
  },
  {
    "objectID": "Fastbook/translations/cn/04_mnist_basics.html#一个端到端的-sgd-示例",
    "href": "Fastbook/translations/cn/04_mnist_basics.html#一个端到端的-sgd-示例",
    "title": "第四章：底层：训练数字分类器",
    "section": "一个端到端的 SGD 示例",
    "text": "一个端到端的 SGD 示例\n我们已经看到如何使用梯度来最小化我们的损失。现在是时候看一个 SGD 示例，并看看如何找到最小值来训练模型以更好地拟合数据。\n让我们从一个简单的合成示例模型开始。想象一下，您正在测量过山车通过顶峰时的速度。它会开始快速，然后随着上坡而变慢；在顶部最慢，然后在下坡时再次加速。您想建立一个关于速度随时间变化的模型。如果您每秒手动测量速度 20 秒，它可能看起来像这样：\ntime = torch.arange(0,20).float(); time\ntensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n &gt; 14., 15., 16., 17., 18., 19.])\nspeed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1\nplt.scatter(time,speed);\n\n我们添加了一些随机噪声，因为手动测量不够精确。这意味着很难回答问题：过山车的速度是多少？使用 SGD，我们可以尝试找到一个与我们的观察相匹配的函数。我们无法考虑每种可能的函数，所以让我们猜测它将是二次的；即，一个形式为a*(time**2)+(b*time)+c的函数。\n我们希望清楚地区分函数的输入（我们测量过山车速度的时间）和其参数（定义我们正在尝试的二次函数的值）。因此，让我们将参数收集在一个参数中，从而在函数的签名中分离输入t和参数params：\ndef f(t, params):\n    a,b,c = params\n    return a*(t**2) + (b*t) + c\n换句话说，我们已经将找到最佳拟合数据的最佳函数的问题限制为找到最佳二次函数。这极大地简化了问题，因为每个二次函数都由三个参数a、b和c完全定义。因此，要找到最佳二次函数，我们只需要找到最佳的a、b和c的值。\n如果我们可以解决二次函数的三个参数的问题，我们就能够对其他具有更多参数的更复杂函数应用相同的方法——比如神经网络。让我们先找到f的参数，然后我们将回来对 MNIST 数据集使用神经网络做同样的事情。\n首先，我们需要定义“最佳”是什么意思。我们通过选择一个损失函数来精确定义这一点，该函数将根据预测和目标返回一个值，其中函数的较低值对应于“更好”的预测。对于连续数据，通常使用均方误差：\ndef mse(preds, targets): return ((preds-targets)**2).mean()\n现在，让我们按照我们的七步流程进行工作。\n\n第一步：初始化参数\n首先，我们将参数初始化为随机值，并告诉 PyTorch 我们要使用requires_grad_跟踪它们的梯度：\nparams = torch.randn(3).requires_grad_()\n\n\n第二步：计算预测\n接下来，我们计算预测：\npreds = f(time, params)\n让我们创建一个小函数来查看我们的预测与目标的接近程度，并看一看：\ndef show_preds(preds, ax=None):\n    if ax is None: ax=plt.subplots()[1]\n    ax.scatter(time, speed)\n    ax.scatter(time, to_np(preds), color='red')\n    ax.set_ylim(-300,100)\nshow_preds(preds)\n\n这看起来并不接近——我们的随机参数表明过山车最终会倒退，因为我们有负速度！\n\n\n第三步：计算损失\n我们计算损失如下：\nloss = mse(preds, speed)\nloss\ntensor(25823.8086, grad_fn=&lt;MeanBackward0&gt;)\n我们的目标现在是改进这一点。为了做到这一点，我们需要知道梯度。\n\n\n第四步：计算梯度\n下一步是计算梯度，或者近似参数需要如何改变：\nloss.backward()\nparams.grad\ntensor([-53195.8594,  -3419.7146,   -253.8908])\nparams.grad * 1e-5\ntensor([-0.5320, -0.0342, -0.0025])\n我们可以利用这些梯度来改进我们的参数。我们需要选择一个学习率（我们将在下一章中讨论如何在实践中做到这一点；现在，我们将使用 1e-5 或 0.00001）：\nparams\ntensor([-0.7658, -0.7506,  1.3525], requires_grad=True)\n\n\n第 5 步：调整权重\n现在我们需要根据刚刚计算的梯度更新参数：\nlr = 1e-5\nparams.data -= lr * params.grad.data\nparams.grad = None"
  },
  {
    "objectID": "Fastbook/translations/cn/04_mnist_basics.html#总结梯度下降",
    "href": "Fastbook/translations/cn/04_mnist_basics.html#总结梯度下降",
    "title": "第四章：底层：训练数字分类器",
    "section": "总结梯度下降",
    "text": "总结梯度下降\n现在您已经看到每个步骤中发生的事情，让我们再次看一下我们的梯度下降过程的图形表示（图 4-5）并进行一个快速回顾。\n\n\n\n显示梯度下降步骤的图表\n\n\n\n图 4-5. 梯度下降过程\n在开始时，我们模型的权重可以是随机的（从头开始训练）或来自预训练模型（迁移学习）。在第一种情况下，我们从输入得到的输出与我们想要的完全无关，即使在第二种情况下，预训练模型也可能不太擅长我们所针对的特定任务。因此，模型需要学习更好的权重。\n我们首先将模型给出的输出与我们的目标进行比较（我们有标记数据，所以我们知道模型应该给出什么结果），使用一个损失函数，它返回一个数字，我们希望通过改进我们的权重使其尽可能低。为了做到这一点，我们从训练集中取出一些数据项（如图像）并将它们馈送给我们的模型。我们使用我们的损失函数比较相应的目标，我们得到的分数告诉我们我们的预测有多么错误。然后我们稍微改变权重使其稍微更好。\n为了找出如何改变权重使损失稍微变好，我们使用微积分来计算梯度。（实际上，我们让 PyTorch 为我们做这个！）让我们考虑一个类比。想象一下你在山上迷路了，你的车停在最低点。为了找到回去的路，你可能会朝着随机方向走，但那可能不会有太大帮助。由于你知道你的车在最低点，你最好是往下走。通过始终朝着最陡峭的下坡方向迈出一步，你最终应该到达目的地。我们使用梯度的大小（即坡度的陡峭程度）来告诉我们应该迈多大一步；具体来说，我们将梯度乘以我们选择的一个称为学习率的数字来决定步长。然后我们迭代直到达到最低点，那将是我们的停车场；然后我们可以停止。\n我们刚刚看到的所有内容都可以直接转换到 MNIST 数据集，除了损失函数。现在让我们看看如何定义一个好的训练目标。"
  },
  {
    "objectID": "Fastbook/translations/cn/04_mnist_basics.html#sigmoid",
    "href": "Fastbook/translations/cn/04_mnist_basics.html#sigmoid",
    "title": "第四章：底层：训练数字分类器",
    "section": "Sigmoid",
    "text": "Sigmoid\nsigmoid函数总是输出一个介于 0 和 1 之间的数字。它的定义如下：\ndef sigmoid(x): return 1/(1+torch.exp(-x))\nPyTorch 为我们定义了一个加速版本，所以我们不需要自己的。这是深度学习中一个重要的函数，因为我们经常希望确保数值在 0 和 1 之间。它看起来是这样的：\nplot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)\n\n正如您所看到的，它接受任何输入值，正数或负数，并将其压缩为 0 和 1 之间的输出值。它还是一个只上升的平滑曲线，这使得 SGD 更容易找到有意义的梯度。\n让我们更新mnist_loss，首先对输入应用sigmoid：\ndef mnist_loss(predictions, targets):\n    predictions = predictions.sigmoid()\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n现在我们可以确信我们的损失函数将起作用，即使预测不在 0 和 1 之间。唯一需要的是更高的预测对应更高的置信度。\n定义了一个损失函数，现在是一个好时机回顾为什么这样做。毕竟，我们已经有了一个度量标准，即整体准确率。那么为什么我们定义了一个损失？\n关键区别在于指标用于驱动人类理解，而损失用于驱动自动学习。为了驱动自动学习，损失必须是一个具有有意义导数的函数。它不能有大的平坦部分和大的跳跃，而必须是相当平滑的。这就是为什么我们设计了一个损失函数，可以对置信水平的小变化做出响应。这个要求意味着有时它实际上并不完全反映我们试图实现的目标，而是我们真正目标和一个可以使用其梯度进行优化的函数之间的妥协。损失函数是针对数据集中的每个项目计算的，然后在时代结束时，所有损失值都被平均，整体均值被报告为时代。\n另一方面，指标是我们关心的数字。这些是在每个时代结束时打印的值，告诉我们我们的模型表现如何。重要的是，我们学会关注这些指标，而不是损失，来评估模型的性能。"
  },
  {
    "objectID": "Fastbook/translations/cn/04_mnist_basics.html#sgd-和小批次",
    "href": "Fastbook/translations/cn/04_mnist_basics.html#sgd-和小批次",
    "title": "第四章：底层：训练数字分类器",
    "section": "SGD 和小批次",
    "text": "SGD 和小批次\n现在我们有了一个适合驱动 SGD 的损失函数，我们可以考虑学习过程的下一阶段涉及的一些细节，即根据梯度改变或更新权重。这被称为优化步骤。\n要进行优化步骤，我们需要计算一个或多个数据项的损失。我们应该使用多少？我们可以为整个数据集计算并取平均值，或者可以为单个数据项计算。但这两种方法都不理想。为整个数据集计算将需要很长时间。为单个数据项计算将不会使用太多信息，因此会导致不精确和不稳定的梯度。您将费力更新权重，但只考虑这将如何改善模型在该单个数据项上的性能。\n因此，我们做出妥协：我们一次计算几个数据项的平均损失。这被称为小批次。小批次中的数据项数量称为批次大小。较大的批次大小意味着您将从损失函数中获得更准确和稳定的数据集梯度估计，但这将需要更长时间，并且您将在每个时代处理较少的小批次。选择一个好的批次大小是您作为深度学习从业者需要做出的决定之一，以便快速准确地训练您的模型。我们将在本书中讨论如何做出这个选择。\n使用小批次而不是在单个数据项上计算梯度的另一个很好的理由是，实际上，我们几乎总是在加速器上进行训练，例如 GPU。这些加速器只有在一次有很多工作要做时才能表现良好，因此如果我们可以给它们很多数据项来处理，这将是有帮助的。使用小批次是实现这一目标的最佳方法之一。但是，如果您一次给它们太多数据来处理，它们会耗尽内存——让 GPU 保持愉快也是棘手的！\n正如您在第二章中关于数据增强的讨论中所看到的，如果我们在训练过程中可以改变一些东西，我们会获得更好的泛化能力。我们可以改变的一个简单而有效的事情是将哪些数据项放入每个小批次。我们通常不是简单地按顺序枚举我们的数据集，而是在每个时代之前随机洗牌，然后创建小批次。PyTorch 和 fastai 提供了一个类，可以为您执行洗牌和小批次整理，称为DataLoader。\nDataLoader可以将任何 Python 集合转换为一个迭代器，用于生成多个批次，就像这样：\ncoll = range(15)\ndl = DataLoader(coll, batch_size=5, shuffle=True)\nlist(dl)\n[tensor([ 3, 12,  8, 10,  2]),\n tensor([ 9,  4,  7, 14,  5]),\n tensor([ 1, 13,  0,  6, 11])]\n对于训练模型，我们不只是想要任何 Python 集合，而是一个包含独立和相关变量（模型的输入和目标）的集合。包含独立和相关变量元组的集合在 PyTorch 中被称为Dataset。这是一个极其简单的Dataset的示例：\nds = L(enumerate(string.ascii_lowercase))\nds\n(#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7,\n &gt; 'h'),(8, 'i'),(9, 'j')...]\n当我们将Dataset传递给DataLoader时，我们将得到许多批次，它们本身是表示独立和相关变量批次的张量元组：\ndl = DataLoader(ds, batch_size=6, shuffle=True)\nlist(dl)\n[(tensor([17, 18, 10, 22,  8, 14]), ('r', 's', 'k', 'w', 'i', 'o')),\n (tensor([20, 15,  9, 13, 21, 12]), ('u', 'p', 'j', 'n', 'v', 'm')),\n (tensor([ 7, 25,  6,  5, 11, 23]), ('h', 'z', 'g', 'f', 'l', 'x')),\n (tensor([ 1,  3,  0, 24, 19, 16]), ('b', 'd', 'a', 'y', 't', 'q')),\n (tensor([2, 4]), ('c', 'e'))]\n我们现在准备为使用 SGD 的模型编写我们的第一个训练循环！"
  },
  {
    "objectID": "Fastbook/translations/cn/04_mnist_basics.html#创建一个优化器",
    "href": "Fastbook/translations/cn/04_mnist_basics.html#创建一个优化器",
    "title": "第四章：底层：训练数字分类器",
    "section": "创建一个优化器",
    "text": "创建一个优化器\n因为这是一个如此通用的基础，PyTorch 提供了一些有用的类来使实现更容易。我们可以做的第一件事是用 PyTorch 的nn.Linear模块替换我们的linear函数。模块是从 PyTorch nn.Module类继承的类的对象。这个类的对象的行为与标准 Python 函数完全相同，您可以使用括号调用它们，它们将返回模型的激活。\nnn.Linear做的事情与我们的init_params和linear一样。它包含了权重和偏差在一个单独的类中。这是我们如何复制上一节中的模型：\nlinear_model = nn.Linear(28*28,1)\n每个 PyTorch 模块都知道它有哪些可以训练的参数；它们可以通过parameters方法获得：\nw,b = linear_model.parameters()\nw.shape,b.shape\n(torch.Size([1, 784]), torch.Size([1]))\n我们可以使用这些信息创建一个优化器：\nclass BasicOptim:\n    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n    def step(self, *args, **kwargs):\n        for p in self.params: p.data -= p.grad.data * self.lr\n\n    def zero_grad(self, *args, **kwargs):\n        for p in self.params: p.grad = None\n我们可以通过传入模型的参数来创建优化器：\nopt = BasicOptim(linear_model.parameters(), lr)\n我们的训练循环现在可以简化：\ndef train_epoch(model):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n我们的验证函数不需要任何更改：\nvalidate_epoch(linear_model)\n0.4157\n让我们把我们的小训练循环放在一个函数中，让事情变得更简单：\ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=' ')\n结果与上一节相同：\ntrain_model(linear_model, 20)\n0.4932 0.8618 0.8203 0.9102 0.9331 0.9468 0.9555 0.9629 0.9658 0.9673 0.9687\n &gt; 0.9707 0.9726 0.9751 0.9761 0.9761 0.9775 0.978 0.9785 0.9785\nfastai 提供了SGD类，默认情况下与我们的BasicOptim做相同的事情：\nlinear_model = nn.Linear(28*28,1)\nopt = SGD(linear_model.parameters(), lr)\ntrain_model(linear_model, 20)\n0.4932 0.852 0.8335 0.9116 0.9326 0.9473 0.9555 0.9624 0.9648 0.9668 0.9692\n &gt; 0.9712 0.9731 0.9746 0.9761 0.9765 0.9775 0.978 0.9785 0.9785\nfastai 还提供了Learner.fit，我们可以使用它来代替train_model。要创建一个Learner，我们首先需要创建一个DataLoaders，通过传入我们的训练和验证DataLoader：\ndls = DataLoaders(dl, valid_dl)\n要创建一个Learner而不使用应用程序（如cnn_learner），我们需要传入本章中创建的所有元素：DataLoaders，模型，优化函数（将传递参数），损失函数，以及可选的任何要打印的指标：\nlearn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n                loss_func=mnist_loss, metrics=batch_accuracy)\n现在我们可以调用fit：\nlearn.fit(10, lr=lr)\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.636857\n0.503549\n0.495584\n00:00\n\n\n1\n0.545725\n0.170281\n0.866045\n00:00\n\n\n2\n0.199223\n0.184893\n0.831207\n00:00\n\n\n3\n0.086580\n0.107836\n0.911187\n00:00\n\n\n4\n0.045185\n0.078481\n0.932777\n00:00\n\n\n5\n0.029108\n0.062792\n0.946516\n00:00\n\n\n6\n0.022560\n0.053017\n0.955348\n00:00\n\n\n7\n0.019687\n0.046500\n0.962218\n00:00\n\n\n8\n0.018252\n0.041929\n0.965162\n00:00\n\n\n9\n0.017402\n0.038573\n0.967615\n00:00\n\n\n\n正如您所看到的，PyTorch 和 fastai 类并没有什么神奇之处。它们只是方便的预打包部件，使您的生活变得更轻松！（它们还提供了许多我们将在未来章节中使用的额外功能。）\n有了这些类，我们现在可以用神经网络替换我们的线性模型。"
  },
  {
    "objectID": "Fastbook/translations/cn/04_mnist_basics.html#更深入地探讨",
    "href": "Fastbook/translations/cn/04_mnist_basics.html#更深入地探讨",
    "title": "第四章：底层：训练数字分类器",
    "section": "更深入地探讨",
    "text": "更深入地探讨\n我们不必止步于只有两个线性层。我们可以添加任意数量的线性层，只要在每对线性层之间添加一个非线性。然而，正如您将了解的那样，模型变得越深，实际中优化参数就越困难。在本书的后面，您将学习一些简单但非常有效的训练更深层模型的技巧。\n我们已经知道，一个带有两个线性层的单个非线性足以逼近任何函数。那么为什么要使用更深的模型呢？原因是性能。通过更深的模型（具有更多层），我们不需要使用太多参数；事实证明，我们可以使用更小的矩阵，更多的层，获得比使用更大的矩阵和少量层获得更好的结果。\n这意味着我们可以更快地训练模型，并且它将占用更少的内存。在 1990 年代，研究人员如此专注于通用逼近定理，以至于很少有人尝试超过一个非线性。这种理论但不实际的基础阻碍了该领域多年。然而，一些研究人员确实尝试了深度模型，并最终能够证明这些模型在实践中表现得更好。最终，出现了理论结果，解释了为什么会发生这种情况。今天，几乎不可能找到任何人只使用一个非线性的神经网络。\n当我们使用与我们在第一章中看到的相同方法训练一个 18 层模型时会发生什么：\ndls = ImageDataLoaders.from_folder(path)\nlearn = cnn_learner(dls, resnet18, pretrained=False,\n                    loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(1, 0.1)\n\n\n\n时代\n训练损失\n验证损失\n准确性\n时间\n\n\n\n\n0\n0.082089\n0.009578\n0.997056\n00:11\n\n\n\n近乎 100%的准确性！这与我们简单的神经网络相比有很大的差异。但是在本书的剩余部分中，您将学习到一些小技巧，可以让您自己从头开始获得如此出色的结果。您已经了解了关键的基础知识。 （当然，即使您知道所有技巧，您几乎总是希望使用 PyTorch 和 fastai 提供的预构建类，因为它们可以帮助您省去自己考虑所有细节的麻烦。）"
  },
  {
    "objectID": "Fastbook/translations/cn/04_mnist_basics.html#进一步研究",
    "href": "Fastbook/translations/cn/04_mnist_basics.html#进一步研究",
    "title": "第四章：底层：训练数字分类器",
    "section": "进一步研究",
    "text": "进一步研究\n\n从头开始创建自己的Learner实现，基于本章展示的训练循环。\n使用完整的 MNIST 数据集完成本章的所有步骤（不仅仅是 3 和 7）。这是一个重要的项目，需要花费相当多的时间来完成！您需要进行一些研究，以找出如何克服在途中遇到的障碍。"
  },
  {
    "objectID": "Fastbook/translations/cn/20_conclusion.html",
    "href": "Fastbook/translations/cn/20_conclusion.html",
    "title": "第二十章：总结思考",
    "section": "",
    "text": "第二十章：总结思考\n恭喜！你成功了！如果你已经完成了到这一点的所有笔记本，你已经加入了一个小但不断增长的人群，他们能够利用深度学习的力量解决实际问题。你可能不会觉得这样—事实上，你可能不会。我们一再看到完成 fast.ai 课程的学生明显低估了自己作为深度学习从业者的效力。我们也看到这些人经常被具有传统学术背景的其他人低估。所以如果你要超越自己的期望和他人的期望，那么在关闭这本书后，你接下来要做的事情比你到目前为止所做的更重要。\n最重要的是保持动力。事实上，从你对优化器的研究中知道，动量是可以自我增强的！所以想想你现在可以做些什么来维持和加速你的深度学习之旅。图 20-1 可以给你一些想法。\n\n\n\n接下来要做什么\n\n\n\n图 20-1。接下来要做什么\n在这本书中，我们谈了很多关于写作的价值，无论是代码还是散文。但也许你到目前为止还没有写得像你希望的那样多。没关系！现在是一个扭转局面的好机会。此时你有很多话要说。也许你已经在一个数据集上尝试了一些实验，其他人似乎没有以同样的方式看待。告诉世界！或者你可能正在考虑尝试一些在阅读时想到的想法——现在是把这些想法转化为代码的好时机。\n如果你想分享你的想法，一个相对低调的地方是fast.ai 论坛。你会发现那里的社区非常支持和乐于助人，所以请过来告诉我们你在做什么。或者看看你是否可以回答一些问题，帮助那些在学习早期的人。\n如果你在深度学习之旅中取得了一些成功，无论大小，请务必告诉我们！在论坛上发布这些信息尤其有帮助，因为了解其他学生的成功可以极大地激励人们。\n对许多人来说，保持与学习之旅的联系最重要的方法之一是围绕它建立一个社区。例如，你可以尝试在你当地社区设立一个小型深度学习聚会，或者一个学习小组，甚至可以在当地聚会上做一个关于你到目前为止学到的内容或者你感兴趣的某个特定方面的演讲。你现在还不是世界领先的专家也没关系——重要的是要记住你现在知道很多其他人不知道的东西，所以他们很可能会欣赏你的观点。\n另一个许多人发现有用的社区活动是定期的读书俱乐部或论文阅读俱乐部。你可能已经在你的社区找到了一些，如果没有，你可以尝试开始一个。即使只有另一个人和你一起做，也会帮助你获得支持和鼓励，让你开始行动起来。\n如果你不在一个容易与志同道合的人聚在一起的地方，可以去论坛，因为人们总是在组建虚拟学习小组。这些通常涉及一群人每周一次通过视频聊天讨论一个深度学习主题。\n希望到这一点，你已经有了一些小项目和实验。我们建议你的下一步是选择其中一个，并尽可能把它做得更好。真正把它打磨成你能做到的最好作品——一件让你真正自豪的作品。这将迫使你更深入地了解一个主题，测试你的理解，并让你看到当你全力以赴时你能做到什么。\n此外，您可能想看一下fast.ai 免费在线课程，它涵盖了与本书相同的内容。有时，以两种方式看同样的材料确实有助于澄清思路。事实上，人类学习研究人员发现，学习材料的最佳方式之一是从不同角度看同一件事，用不同的方式描述。\n如果您选择接受最后的任务，那就是把这本书送给您认识的某人，并帮助另一个人开始他们自己的深度学习之旅！"
  },
  {
    "objectID": "Fastbook/translations/cn/01_intro.html",
    "href": "Fastbook/translations/cn/01_intro.html",
    "title": "第一章：你的深度学习之旅",
    "section": "",
    "text": "你好，感谢你让我们加入你的深度学习之旅，无论你已经走了多远！在本章中，我们将告诉你更多关于本书的内容，介绍深度学习背后的关键概念，并在不同任务上训练我们的第一个模型。无论你是否有技术或数学背景（尽管如果你有也没关系！），我们写这本书是为了让尽可能多的人能够接触到深度学习。"
  },
  {
    "objectID": "Fastbook/translations/cn/01_intro.html#你的项目和心态",
    "href": "Fastbook/translations/cn/01_intro.html#你的项目和心态",
    "title": "第一章：你的深度学习之旅",
    "section": "你的项目和心态",
    "text": "你的项目和心态\n无论您是因为兴奋地想要从植物叶片的图片中识别植物是否患病，自动生成编织图案，从 X 射线诊断结核病，还是确定浣熊何时使用您的猫门，我们将尽快让您使用深度学习解决自己的问题（通过他人预训练的模型），然后将逐步深入更多细节。在下一章的前 30 分钟内，您将学会如何使用深度学习以最先进的准确性解决自己的问题！（如果您迫不及待地想要立即开始编码，请随时跳转到那里。）有一个错误的观念认为，要进行深度学习，您需要像谷歌那样拥有计算资源和数据集的规模，但这是不正确的。\n那么，什么样的任务适合作为良好的测试案例？您可以训练模型区分毕加索和莫奈的画作，或者挑选您女儿的照片而不是您儿子的照片。专注于您的爱好和激情有助于您设定四到五个小项目，而不是努力解决一个大问题，这在刚开始时效果更好。由于很容易陷入困境，过早野心勃勃往往会适得其反。然后，一旦掌握了基础知识，就努力完成一些让您真正自豪的事情！"
  },
  {
    "objectID": "Fastbook/translations/cn/01_intro.html#获取-gpu-深度学习服务器",
    "href": "Fastbook/translations/cn/01_intro.html#获取-gpu-深度学习服务器",
    "title": "第一章：你的深度学习之旅",
    "section": "获取 GPU 深度学习服务器",
    "text": "获取 GPU 深度学习服务器\n在本书中几乎所有的事情都需要使用一台带有 NVIDIA GPU 的计算机（不幸的是，其他品牌的 GPU 并没有得到主要深度学习库的全面支持）。然而，我们不建议你购买一台；事实上，即使你已经有一台，我们也不建议你立即使用！设置一台计算机需要时间和精力，而你现在想要把所有精力集中在深度学习上。因此，我们建议你租用一台已经预装并准备就绪的计算机。使用时的成本可能只需每小时 0.25 美元，甚至有些选项是免费的。"
  },
  {
    "objectID": "Fastbook/translations/cn/01_intro.html#运行您的第一个笔记本",
    "href": "Fastbook/translations/cn/01_intro.html#运行您的第一个笔记本",
    "title": "第一章：你的深度学习之旅",
    "section": "运行您的第一个笔记本",
    "text": "运行您的第一个笔记本\n笔记本按章节编号，与本书中呈现的顺序相同。因此，您将看到列出的第一个笔记本是您现在需要使用的笔记本。您将使用此笔记本来训练一个可以识别狗和猫照片的模型。为此，您将下载一组狗和猫照片的数据集，并使用该数据集训练模型。\n数据集只是一堆数据——可以是图像、电子邮件、财务指标、声音或其他任何东西。有许多免费提供的数据集适合用于训练模型。许多这些数据集是由学者创建的，以帮助推动研究，许多是为竞赛提供的（有一些竞赛，数据科学家可以竞争，看看谁有最准确的模型！），有些是其他过程的副产品（如财务申报）。"
  },
  {
    "objectID": "Fastbook/translations/cn/01_intro.html#什么是机器学习",
    "href": "Fastbook/translations/cn/01_intro.html#什么是机器学习",
    "title": "第一章：你的深度学习之旅",
    "section": "什么是机器学习？",
    "text": "什么是机器学习？\n你的分类器是一个深度学习模型。正如已经提到的，深度学习模型使用神经网络，这些神经网络最初可以追溯到上世纪 50 年代，并且最近由于最新的进展变得非常强大。\n另一个重要的背景是，深度学习只是更一般的机器学习领域中的一个现代领域。要理解当你训练自己的分类模型时所做的事情的本质，你不需要理解深度学习。看到你的模型和训练过程是如何成为适用于机器学习的概念的例子就足够了。\n因此，在本节中，我们将描述机器学习。我们将探讨关键概念，并看看它们如何可以追溯到最初介绍它们的原始文章。\n机器学习就像常规编程一样，是让计算机完成特定任务的一种方式。但是如果要用常规编程来完成前面部分我们刚刚做的事情：在照片中识别狗和猫，我们将不得不为计算机写下完成任务所需的确切步骤。\n通常，当我们编写程序时，很容易为我们写下完成任务的步骤。我们只需考虑如果我们必须手动完成任务时会采取的步骤，然后将它们转换为代码。例如，我们可以编写一个对列表进行排序的函数。一般来说，我们会编写一个类似于图 1-4 的函数（其中inputs可能是一个未排序的列表，results是一个排序后的列表）。\n\n\n\n管道输入、程序、结果\n\n\n\n图 1-4. 传统程序\n但是要在照片中识别物体，这有点棘手；当我们在图片中识别物体时，我们采取了什么步骤？我们真的不知道，因为这一切都发生在我们的大脑中，而我们并没有意识到！\n早在计算机诞生之初，1949 年，IBM 的一位研究员阿瑟·塞缪尔开始研究一种让计算机完成任务的不同方式，他称之为机器学习。在他经典的 1962 年文章“人工智能：自动化的前沿”中，他写道：\n\n为这样的计算编程对于我们来说是相当困难的，主要不是因为计算机本身的任何固有复杂性，而是因为需要详细说明过程的每一个细微步骤。任何程序员都会告诉你，计算机是巨大的白痴，而不是巨大的大脑。\n\n他的基本想法是这样的：不是告诉计算机解决问题所需的确切步骤，而是向其展示解决问题的示例，并让它自己找出如何解决。结果证明这非常有效：到 1961 年，他的跳棋程序学到了很多，以至于击败了康涅狄格州冠军！这是他描述自己想法的方式（与之前提到的同一篇文章）：\n\n假设我们安排一些自动手段来测试任何当前权重分配的有效性，以实际表现为准，并提供一种机制来改变权重分配以最大化性能。我们不需要详细了解这种程序的细节，就可以看到它可以完全自动化，并且可以看到一个这样编程的机器将从中学习。\n\n这个简短陈述中嵌入了一些强大的概念：\n\n“权重分配”的想法\n每个权重分配都有一些“实际表现”的事实\n要求有一种“自动手段”来测试该性能\n需要一个“机制”（即，另一个自动过程）来通过改变权重分配来提高性能\n\n让我们逐一了解这些概念，以便了解它们在实践中如何结合。首先，我们需要了解塞缪尔所说的权重分配是什么意思。\n权重只是变量，权重分配是这些变量的特定值选择。程序的输入是它处理以产生结果的值，例如，将图像像素作为输入，并返回分类“狗”作为结果。程序的权重分配是定义程序操作方式的其他值。\n因为它们会影响程序，它们在某种意义上是另一种输入。我们将更新我们的基本图片图 1-4，并用图 1-5 替换，以便考虑到这一点。\n！\n\n\n图 1-5。使用权重分配的程序\n我们已将方框的名称从程序更改为模型。这是为了遵循现代术语并反映模型是一种特殊类型的程序：它可以根据权重做许多不同的事情。它可以以许多不同的方式实现。例如，在塞缪尔的跳棋程序中，不同的权重值会导致不同的跳棋策略。\n（顺便说一句，塞缪尔所说的“权重”如今通常被称为模型参数，以防您遇到这个术语。术语权重保留给特定类型的模型参数。）\n接下来，塞缪尔说我们需要一种自动测试任何当前权重分配的有效性的方法，以实际表现为准。在他的跳棋程序中，“实际表现”模型的表现有多好。您可以通过让两个模型相互对战并看哪个通常获胜来自动测试两个模型的表现。\n最后，他说我们需要一种机制来改变权重分配，以最大化性能。例如，我们可以查看获胜模型和失败模型之间的权重差异，并将权重进一步调整到获胜方向。\n我们现在可以看到他为什么说这样的程序可以完全自动化，并且…一个这样编程的机器将从中学习。当权重的调整也是自动的时，学习将变得完全自动——当我们不再通过手动调整权重来改进模型，而是依赖于根据性能产生调整的自动化机制时。\n图 1-6 展示了塞缪尔关于训练机器学习模型的完整图景。\n！基本训练循环\n\n\n图 1-6。训练机器学习模型\n注意模型的结果（例如，在跳棋游戏中的移动）和其性能（例如，是否赢得比赛，或者赢得比赛的速度）之间的区别。\n还要注意，一旦模型训练好了，也就是说，一旦我们选择了最终的、最好的、最喜欢的权重分配，那么我们可以将权重视为模型的一部分，因为我们不再对它们进行变化。\n因此，实际上在训练后使用模型看起来像图 1-7。\n\n\n\n图 1-7。使用训练后的模型作为程序\n这看起来与我们在图 1-4 中的原始图表相同，只是将程序一词替换为模型。这是一个重要的观点：训练后的模型可以像常规计算机程序一样对待。"
  },
  {
    "objectID": "Fastbook/translations/cn/01_intro.html#什么是神经网络",
    "href": "Fastbook/translations/cn/01_intro.html#什么是神经网络",
    "title": "第一章：你的深度学习之旅",
    "section": "什么是神经网络？",
    "text": "什么是神经网络？\n不难想象跳棋程序的模型可能是什么样子。可能编码了一系列跳棋策略，以及某种搜索机制，然后权重可以变化以决定如何选择策略，在搜索期间关注棋盘的哪些部分等等。但是对于图像识别程序，或者理解文本，或者我们可能想象的许多其他有趣的问题，模型可能是什么样子却一点也不明显。\n我们希望有一种函数，它如此灵活，以至于可以通过调整其权重来解决任何给定问题。令人惊讶的是，这种函数实际上存在！这就是我们已经讨论过的神经网络。也就是说，如果您将神经网络视为数学函数，那么它将是一种极其灵活的函数，取决于其权重。一种称为通用逼近定理的数学证明表明，这种函数在理论上可以解决任何问题，达到任何精度水平。神经网络如此灵活的事实意味着，在实践中，它们通常是一种合适的模型，您可以将精力集中在训练过程上，即找到良好的权重分配。\n但是这个过程呢？人们可以想象，您可能需要为每个问题找到一种新的“机制”来自动更新权重。这将是费力的。我们在这里也希望有一种完全通用的方法来更新神经网络的权重，使其在任何给定任务上都能提高。方便的是，这也存在！\n这被称为随机梯度下降（SGD）。我们将在第四章中详细了解神经网络和 SGD 的工作原理，以及解释通用逼近定理。然而，现在，我们将使用塞缪尔自己的话来说：我们不需要深入了解这样一个过程的细节，就可以看到它可以完全自动化，并且可以看到这样一个机器编程的机器可以从中学习经验。"
  },
  {
    "objectID": "Fastbook/translations/cn/01_intro.html#一些深度学习术语",
    "href": "Fastbook/translations/cn/01_intro.html#一些深度学习术语",
    "title": "第一章：你的深度学习之旅",
    "section": "一些深度学习术语",
    "text": "一些深度学习术语\nSamuel 在 1960 年代工作，自那时术语已经发生了变化。以下是我们讨论过的所有部分的现代深度学习术语：\n\n模型的功能形式被称为架构（但要小心—有时人们将模型用作架构的同义词，这可能会让人困惑）。\n权重被称为参数。\n预测是从独立变量计算出来的，这是不包括标签的数据。\n模型的结果被称为预测。\n性能的度量被称为损失。\n损失不仅取决于预测，还取决于正确的标签（也称为目标或因变量）；例如，“狗”或“猫”。\n\n在进行这些更改后，我们在图 1-6 中的图表看起来像图 1-8。\n\n\n图 1-8. 详细训练循环"
  },
  {
    "objectID": "Fastbook/translations/cn/01_intro.html#机器学习固有的限制",
    "href": "Fastbook/translations/cn/01_intro.html#机器学习固有的限制",
    "title": "第一章：你的深度学习之旅",
    "section": "机器学习固有的限制",
    "text": "机器学习固有的限制\n从这幅图片中，我们现在可以看到关于训练深度学习模型的一些基本事情：\n\n没有数据就无法创建模型。\n模型只能学习操作训练数据中看到的模式。\n这种学习方法只创建预测，而不是推荐的行动。\n仅仅拥有输入数据的示例是不够的；我们还需要为这些数据提供标签（例如，仅有狗和猫的图片不足以训练模型；我们需要为每个图片提供一个标签，说明哪些是狗，哪些是猫）。\n\n一般来说，我们已经看到大多数组织声称他们没有足够的数据实际上意味着他们没有足够的带标签数据。如果任何组织有兴趣在实践中使用模型做一些事情，那么他们可能有一些输入数据计划运行他们的模型。并且可能他们已经以其他方式做了一段时间（例如，手动或使用一些启发式程序），因此他们有来自这些过程的数据！例如，放射学实践几乎肯定会有医学扫描的存档（因为他们需要能够检查他们的患者随时间的进展），但这些扫描可能没有包含诊断或干预措施列表的结构化标签（因为放射科医生通常创建自由文本自然语言报告，而不是结构化数据）。在本书中，我们将大量讨论标记方法，因为这在实践中是一个非常重要的问题。\n由于这类机器学习模型只能进行预测（即试图复制标签），这可能导致组织目标与模型能力之间存在显著差距。例如，在本书中，您将学习如何创建一个推荐系统，可以预测用户可能购买的产品。这通常用于电子商务，例如通过显示排名最高的商品来定制主页上显示的产品。但这样的模型通常是通过查看用户及其购买历史（输入）以及他们最终购买或查看的内容（标签）来创建的，这意味着该模型很可能会告诉您关于用户已经拥有或已经了解的产品，而不是他们最有可能对其感兴趣的新产品。这与您当地书店的专家所做的事情大不相同，他们会询问您的口味，然后告诉您您以前从未听说过的作者或系列。\n另一个关键的洞察来自于考虑模型如何与其环境互动。这可能会产生反馈循环，如此处所述：\n\n基于过去的逮捕地点创建了一个预测性执法模型。实际上，这并不是在预测犯罪，而是在预测逮捕，因此部分地只是反映了现有执法过程中的偏见。\n然后执法人员可能会使用该模型来决定在哪里集中他们的执法活动，导致这些地区的逮捕增加。\n这些额外逮捕的数据将被反馈回去重新训练未来版本的模型。\n\n这是一个正反馈循环：模型被使用得越多，数据就变得越有偏见，使模型变得更加有偏见，依此类推。\n反馈循环也可能在商业环境中造成问题。例如，视频推荐系统可能会偏向于推荐由视频最大观看者消费的内容（例如，阴谋论者和极端分子倾向于观看比平均水平更多的在线视频内容），导致这些用户增加他们的视频消费量，进而导致更多这类视频被推荐。我们将在第三章中更详细地讨论这个话题。\n既然你已经看到了理论的基础，让我们回到我们的代码示例，详细看看代码如何与我们刚刚描述的过程相对应。"
  },
  {
    "objectID": "Fastbook/translations/cn/01_intro.html#我们的图像识别器是如何工作的",
    "href": "Fastbook/translations/cn/01_intro.html#我们的图像识别器是如何工作的",
    "title": "第一章：你的深度学习之旅",
    "section": "我们的图像识别器是如何工作的",
    "text": "我们的图像识别器是如何工作的\n让我们看看我们的图像识别器代码如何映射到这些想法。我们将把每一行放入一个单独的单元格，并查看每一行正在做什么（我们暂时不会解释每个参数的每个细节，但会给出重要部分的描述；完整细节将在本书后面提供）。第一行导入了整个 fastai.vision 库：\nfrom fastai.vision.all import *\n这为我们提供了创建各种计算机视觉模型所需的所有函数和类。"
  },
  {
    "objectID": "Fastbook/translations/cn/01_intro.html#我们的图像识别器学到了什么",
    "href": "Fastbook/translations/cn/01_intro.html#我们的图像识别器学到了什么",
    "title": "第一章：你的深度学习之旅",
    "section": "我们的图像识别器学到了什么",
    "text": "我们的图像识别器学到了什么\n在这个阶段，我们有一个工作良好的图像识别器，但我们不知道它在做什么！尽管许多人抱怨深度学习导致不可理解的“黑匣子”模型（即，可以提供预测但没有人能理解的东西），但事实并非如此。有大量研究表明如何深入检查深度学习模型并从中获得丰富的见解。话虽如此，各种机器学习模型（包括深度学习和传统统计模型）都可能难以完全理解，特别是考虑到它们在遇到与用于训练它们的数据非常不同的数据时的行为。我们将在本书中讨论这个问题。\n2013 年，博士生 Matt Zeiler 和他的导师 Rob Fergus 发表了《可视化和理解卷积网络》，展示了如何可视化模型每一层学到的神经网络权重。他们仔细分析了赢得 2012 年 ImageNet 比赛的模型，并利用这一分析大大改进了模型，使他们能够赢得 2013 年的比赛！图 1-10 是他们发表的第一层权重的图片。\n\n\n\nCNN 早期层的激活\n\n\n\n图 1-10。CNN 第一层的激活（由 Matthew D. Zeiler 和 Rob Fergus 提供）\n这张图片需要一些解释。对于每一层，具有浅灰色背景的图像部分显示了重建的权重，底部较大的部分显示了与每组权重最匹配的训练图像部分。对于第一层，我们可以看到模型发现了代表对角线、水平和垂直边缘以及各种梯度的权重。（请注意，对于每一层，只显示了部分特征；实际上，在所有层中有成千上万个特征。）\n这些是模型为计算机视觉学习的基本构建块。它们已经被神经科学家和计算机视觉研究人员广泛分析，结果表明，这些学习的构建块与人眼的基本视觉机制以及在深度学习之前开发的手工计算机视觉特征非常相似。下一层在图 1-11 中表示。\n\n\n\nCNN 早期层的激活\n\n\n\n\n图 1-11。CNN 第二层的激活（由 Matthew D. Zeiler 和 Rob Fergus 提供）\n对于第 2 层，模型找到的每个特征都有九个权重重建示例。我们可以看到模型已经学会创建寻找角、重复线条、圆圈和其他简单模式的特征检测器。这些是从第一层中开发的基本构建块构建的。对于每个特征，图片右侧显示了与这些特征最匹配的实际图像的小块。例如，第 2 行第 1 列中的特定模式与日落相关的梯度和纹理相匹配。\n图 1-12 显示了一篇论文中展示第 3 层特征重建结果的图片。\n\n\n\nCNN 中间层的激活\n\n\n\n\n图 1-12。CNN 第三层的激活（由 Matthew D. Zeiler 和 Rob Fergus 提供）\n通过观察图片右侧，您可以看到特征现在能够识别和匹配更高级的语义组件，如汽车车轮、文字和花瓣。利用这些组件，第 4 层和第 5 层可以识别更高级的概念，如图 1-13 所示。\n\n\n\nCNN 末端层的激活\n\n\n\n\n图 1-13。CNN 的第四和第五层的激活（由 Matthew D. Zeiler 和 Rob Fergus 提供）\n本文研究了一个名为AlexNet的旧模型，该模型只包含五层。自那时以来开发的网络可以有数百层 - 所以你可以想象这些模型开发的特征有多丰富！\n当我们早期微调我们的预训练模型时，我们调整了最后几层关注的内容（花朵、人类、动物），以专注于猫与狗问题。更一般地，我们可以将这样的预训练模型专门用于许多不同的任务。让我们看一些例子。"
  },
  {
    "objectID": "Fastbook/translations/cn/01_intro.html#图像识别器可以处理非图像任务",
    "href": "Fastbook/translations/cn/01_intro.html#图像识别器可以处理非图像任务",
    "title": "第一章：你的深度学习之旅",
    "section": "图像识别器可以处理非图像任务",
    "text": "图像识别器可以处理非图像任务\n图像识别器只能识别图像，顾名思义。但很多事物可以被表示为图像，这意味着图像识别器可以学会完成许多任务。\n例如，声音可以转换为频谱图，这是一种图表，显示音频文件中每个时间的每个频率的数量。fast.ai 学生 Ethan Sutin 使用这种方法，轻松击败了一种最先进的环境声音检测模型的发布准确率，使用了 8732 个城市声音的数据集。fastai 的show_batch清楚地显示了每个声音具有相当独特的频谱图，如图 1-14 所示。\n\n\n\n显示具有声音频谱图的 show_batch\n\n\n\n图 1-14。显示具有声音频谱图的 show_batch\n时间序列可以很容易地通过简单地在图表上绘制时间序列来转换为图像。然而，通常最好尝试以尽可能简单的方式表示数据，以便提取出最重要的组件。在时间序列中，季节性和异常很可能是感兴趣的。\n时间序列数据有各种转换方法。例如，fast.ai 学生 Ignacio Oguiza 使用一种称为 Gramian Angular Difference Field（GADF）的技术，从一个时间序列数据集中为橄榄油分类创建图像，你可以在图 1-15 中看到结果。然后，他将这些图像输入到一个图像分类模型中，就像你在本章中看到的那样。尽管只有 30 个训练集图像，但他的结果准确率超过 90%，接近最先进水平。\n\n\n\n将时间序列转换为图像\n\n\n\n\n图 1-15。将时间序列转换为图像\n另一个有趣的 fast.ai 学生项目示例来自 Gleb Esman。他在 Splunk 上进行欺诈检测，使用了用户鼠标移动和鼠标点击的数据集。他通过绘制显示鼠标指针位置、速度和加速度的图像，使用彩色线条，并使用小彩色圆圈显示点击，将这些转换为图片，如图 1-16 所示。他将这些输入到一个图像识别模型中，就像我们在本章中使用的那样，效果非常好，导致了这种方法在欺诈分析方面的专利！\n\n\n\n将计算机鼠标行为转换为图像\n\n\n\n\n图 1-16。将计算机鼠标行为转换为图像\n另一个例子来自 Mahmoud Kalash 等人的论文“使用深度卷积神经网络进行恶意软件分类”，解释了“恶意软件二进制文件被分成 8 位序列，然后转换为等效的十进制值。这个十进制向量被重塑，生成了一个代表恶意软件样本的灰度图像”，如图 1-17 所示。\n\n\n\n恶意软件分类过程\n\n\n\n\n图 1-17。恶意软件分类过程\n作者们随后展示了通过恶意软件分类生成的“图片”，如图 1-18 所示。\n\n\n\n恶意软件示例\n\n\n\n\n图 1-18。恶意软件示例\n正如您所看到的，不同类型的恶意软件在人眼中看起来非常独特。研究人员基于这种图像表示训练的模型在恶意软件分类方面比学术文献中显示的任何先前方法都更准确。这表明将数据集转换为图像表示的一个很好的经验法则：如果人眼可以从图像中识别类别，那么深度学习模型也应该能够做到。\n总的来说，您会发现在深度学习中，少数几种通用方法可以走得很远，只要您在如何表示数据方面有点创造性！您不应该将这里描述的方法视为“巧妙的变通方法”，因为它们通常（如此处）击败了以前的最先进结果。这确实是正确思考这些问题领域的方法。"
  },
  {
    "objectID": "Fastbook/translations/cn/01_intro.html#术语回顾",
    "href": "Fastbook/translations/cn/01_intro.html#术语回顾",
    "title": "第一章：你的深度学习之旅",
    "section": "术语回顾",
    "text": "术语回顾\n我们刚刚涵盖了很多信息，让我们简要回顾一下。表 1-3 提供了一个方便的词汇表。\n表 1-3. 深度学习词汇表\n\n\n\n\n\n\n\n术语\n意义\n\n\n\n\n标签\n我们试图预测的数据，比如“狗”或“猫”\n\n\n架构\n我们试图拟合的模型的 * 模板 *；即我们将输入数据和参数传递给的实际数学函数\n\n\n模型\n架构与特定一组参数的组合\n\n\n参数\n模型中改变任务的值，通过模型训练进行更新\n\n\n拟合\n更新模型的参数，使得使用输入数据的模型预测与目标标签匹配\n\n\n训练\n* 拟合 * 的同义词\n\n\n预训练模型\n已经训练过的模型，通常使用大型数据集，并将进行微调\n\n\n微调\n为不同任务更新预训练模型\n\n\n纪元\n一次完整通过输入数据\n\n\n损失\n衡量模型好坏的指标，选择以驱动通过 SGD 进行训练\n\n\n指标\n使用验证集衡量模型好坏的测量标准，选择供人类消费\n\n\n验证集\n从训练中保留的一组数据，仅用于衡量模型好坏\n\n\n训练集\n用于拟合模型的数据；不包括验证集中的任何数据\n\n\n过拟合\n以使模型 * 记住 * 输入数据的特定特征而不是很好地泛化到训练期间未见的数据的方式训练模型\n\n\nCNN\n卷积神经网络；一种特别适用于计算机视觉任务的神经网络\n\n\n\n有了这个词汇表，我们现在可以将迄今介绍的所有关键概念汇集在一起。花点时间回顾这些定义，并阅读以下摘要。如果您能理解解释，那么您就有能力理解接下来的讨论。\n\n机器学习 是一种学科，我们通过从数据中学习来定义程序，而不是完全自己编写它。  深度学习 是机器学习中使用具有多个  层 * 的 * 神经网络 * 的专业领域。 * 图像分类 * 是一个代表性的例子（也称为 * 图像识别 ）。我们从  标记数据 * 开始 - 一组我们为每个图像分配了 * 标签 * 的图像，指示它代表什么。我们的目标是生成一个称为 * 模型 * 的程序，给定一个新图像，将对该新图像代表的内容进行准确的 * 预测 *。\n\n每个模型都从选择 * 架构 * 开始，这是该类型模型内部工作方式的一般模板。 * 训练 （或  拟合 ）模型的过程是找到一组  参数值 （或  权重 ），这些参数值将该一般架构专门化为适用于我们特定数据类型的模型。为了定义模型在单个预测上的表现如何，我们需要定义一个  损失函数 *，它确定我们如何将预测评分为好或坏。\n为了让训练过程更快，我们可以从一个预训练模型开始——一个已经在其他人的数据上训练过的模型。然后我们可以通过在我们的数据上进一步训练它来使其适应我们的数据，这个过程称为微调。\n当我们训练一个模型时，一个关键问题是确保我们的模型泛化：它从我们的数据中学到的一般性教训也适用于它将遇到的新项目，这样它就可以对这些项目做出良好的预测。风险在于，如果我们训练模型不当，它实际上会记住它已经看到的内容，而不是学习一般性教训，然后它将对新图像做出糟糕的预测。这样的失败被称为过拟合。\n为了避免这种情况，我们总是将数据分为两部分，训练集和验证集。我们通过只向模型展示训练集来训练模型，然后通过查看模型在验证集中的表现来评估模型的表现如何。通过这种方式，我们检查模型从训练集中学到的教训是否适用于验证集。为了评估模型在验证集上的整体表现，我们定义一个度量。在训练过程中，当模型看到训练集中的每个项目时，我们称之为一个周期。\n所有这些概念都适用于机器学习。它们适用于各种通过训练数据定义模型的方案。深度学习的独特之处在于一类特定的架构：基于神经网络的架构。特别是，像图像分类这样的任务在卷积神经网络上严重依赖，我们将很快讨论。"
  },
  {
    "objectID": "Fastbook/translations/cn/01_intro.html#在定义测试集时要有判断力",
    "href": "Fastbook/translations/cn/01_intro.html#在定义测试集时要有判断力",
    "title": "第一章：你的深度学习之旅",
    "section": "在定义测试集时要有判断力",
    "text": "在定义测试集时要有判断力\n要很好地定义验证集（以及可能的测试集），有时你需要做的不仅仅是随机抽取原始数据集的一部分。记住：验证和测试集的一个关键特性是它们必须代表你将来看到的新数据。这听起来可能像一个不可能的要求！根据定义，你还没有看到这些数据。但通常你仍然会知道一些事情。\n看一些例子是很有启发性的。这些例子中的许多来自于Kaggle平台上的预测建模竞赛，这是你可能在实践中看到的问题和方法的很好代表。\n一个情况可能是当你在查看时间序列数据时。对于时间序列，选择数据的随机子集既太容易（你可以查看你试图预测的日期之前和之后的数据），又不代表大多数业务用例（在这些用例中，你使用历史数据构建模型以供将来使用）。如果你的数据包含日期，并且你正在构建一个将来使用的模型，你将希望选择最新日期的连续部分作为验证集（例如，可用数据的最后两周或最后一个月）。\n假设你想将图 1-19 中的时间序列数据分成训练集和验证集。\n\n\n\n一系列数值\n\n\n\n图 1-19. 一个时间序列\n一个随机子集是一个糟糕的选择（填补缺失太容易，且不代表你在生产中所需的），正如我们在图 1-20 中所看到的。\n\n\n\n随机训练子集\n\n\n\n\n图 1-20. 一个糟糕的训练子集\n相反，使用早期数据作为训练集（以及后期数据作为验证集），如图 1-21 所示。\n\n\n\n使用直到某个时间戳的数据作为训练子集\n\n\n\n\n图 1-21. 一个好的训练子集\n例如，Kaggle 曾举办一场竞赛，要求预测厄瓜多尔杂货店连锁店的销售额。 Kaggle 的训练数据从 2013 年 1 月 1 日到 2017 年 8 月 15 日，测试数据跨越了 2017 年 8 月 16 日到 2017 年 8 月 31 日。这样，竞赛组织者确保参赛者在未来时间段进行预测，从他们模型的角度来看。这类似于量化对冲基金交易员进行回测，以检查他们的模型是否能够根据过去的数据预测未来的时间段。\n第二种常见情况是，当你可以很容易地预见到你将用来训练模型的数据与你将在生产中进行预测的数据可能在质量上有所不同时。\n在 Kaggle 分心司机比赛中，自变量是司机在车轮上的照片，因变量是文本、吃东西或安全地向前看等类别。很多照片是同一司机在不同位置的照片，正如我们在图 1-22 中所看到的。如果你是一家保险公司根据这些数据构建模型，注意你最感兴趣的是模型在未见过的司机身上的表现（因为你可能只有一小部分人的训练数据）。为此，比赛的测试数据包括那些在训练集中没有出现的人的图像。\n\n\n\n来自训练数据的两张图片，展示同一个司机\n\n\n\n\n图 1-22. 训练数据中的两张图片\n如果你将图 1-22 中的一张图片放入训练集，另一张放入验证集，你的模型将更容易预测验证集中的那张图片，因此它看起来表现得比在新人身上更好。另一个角度是，如果你在训练模型时使用了所有人，你的模型可能会过度拟合这些特定人的特点，而不仅仅是学习状态（发短信、吃东西等）。\n在Kaggle 渔业比赛中，也存在类似的动态，目的是识别渔船捕捞的鱼类物种，以减少对濒临灭绝种群的非法捕捞。测试集包括来自训练数据中没有出现的船只的图像，因此在这种情况下，你希望你的验证集也包括训练集中没有的船只。\n有时可能不清楚你的验证数据会有什么不同。例如，对于使用卫星图像的问题，你需要收集更多信息，了解训练集是否只包含某些地理位置或来自地理分散的数据。\n现在你已经尝试了如何构建模型，你可以决定接下来想深入研究什么。"
  },
  {
    "objectID": "Fastbook/translations/cn/01_intro.html#进一步研究",
    "href": "Fastbook/translations/cn/01_intro.html#进一步研究",
    "title": "第一章：你的深度学习之旅",
    "section": "进一步研究",
    "text": "进一步研究\n每章还有一个“进一步研究”部分，提出了一些在文本中没有完全回答的问题，或者给出了更高级的任务。这些问题的答案不在书的网站上；您需要自己进行研究！\n\n为什么 GPU 对深度学习有用？CPU 有什么不同之处，为什么对深度学习效果不佳？\n试着想出三个反馈循环可能影响机器学习使用的领域。看看是否能找到实践中发生这种情况的文档示例。"
  },
  {
    "objectID": "Fastbook/translations/cn/12_nlp_dive.html",
    "href": "Fastbook/translations/cn/12_nlp_dive.html",
    "title": "第十二章：从头开始的语言模型",
    "section": "",
    "text": "我们现在准备深入…深入深度学习！您已经学会了如何训练基本的神经网络，但是如何从那里创建最先进的模型呢？在本书的这一部分，我们将揭开所有的神秘，从语言模型开始。\n您在第十章中看到了如何微调预训练的语言模型以构建文本分类器。在本章中，我们将解释该模型的内部结构以及 RNN 是什么。首先，让我们收集一些数据，这些数据将允许我们快速原型化各种模型。"
  },
  {
    "objectID": "Fastbook/translations/cn/12_nlp_dive.html#我们的-pytorch-语言模型",
    "href": "Fastbook/translations/cn/12_nlp_dive.html#我们的-pytorch-语言模型",
    "title": "第十二章：从头开始的语言模型",
    "section": "我们的 PyTorch 语言模型",
    "text": "我们的 PyTorch 语言模型\n我们现在可以创建我们之前描述的语言模型模块：\nclass LMModel1(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.h_h = nn.Linear(n_hidden, n_hidden)\n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n\n    def forward(self, x):\n        h = F.relu(self.h_h(self.i_h(x[:,0])))\n        h = h + self.i_h(x[:,1])\n        h = F.relu(self.h_h(h))\n        h = h + self.i_h(x[:,2])\n        h = F.relu(self.h_h(h))\n        return self.h_o(h)\n正如您所看到的，我们已经创建了三个层：\n\n嵌入层（i_h，表示 输入 到 隐藏）\n线性层用于创建下一个单词的激活（h_h，表示 隐藏 到 隐藏）\n一个最终的线性层来预测第四个单词（h_o，表示 隐藏 到 输出）\n\n这可能更容易以图示形式表示，因此让我们定义一个基本神经网络的简单图示表示。图 12-1 显示了我们将如何用一个隐藏层表示神经网络。\n\n\n\n简单神经网络的图示表示\n\n\n\n图 12-1。简单神经网络的图示表示\n每个形状代表激活：矩形代表输入，圆圈代表隐藏（内部）层激活，三角形代表输出激活。我们将在本章中的所有图表中使用这些形状（在 图 12-2 中总结）。\n\n\n\n我们图示表示中使用的形状\n\n\n\n\n图 12-2。我们图示表示中使用的形状\n箭头代表实际的层计算——即线性层后跟激活函数。使用这种符号，图 12-3 显示了我们简单语言模型的外观。\n\n\n\n我们基本语言模型的表示\n\n\n\n\n图 12-3。我们基本语言模型的表示\n为了简化事情，我们已经从每个箭头中删除了层计算的细节。我们还对箭头进行了颜色编码，使所有具有相同颜色的箭头具有相同的权重矩阵。例如，所有输入层使用相同的嵌入矩阵，因此它们都具有相同的颜色（绿色）。\n让我们尝试训练这个模型，看看效果如何：\nlearn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy,\n                metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.824297\n1.970941\n0.467554\n00:02\n\n\n1\n1.386973\n1.823242\n0.467554\n00:02\n\n\n2\n1.417556\n1.654497\n0.494414\n00:02\n\n\n3\n1.376440\n1.650849\n0.494414\n00:02\n\n\n\n要查看这是否有效，请查看一个非常简单的模型会给我们什么结果。在这种情况下，我们总是可以预测最常见的标记，因此让我们找出在我们的验证集中最常见的目标是哪个标记：\nn,counts = 0,torch.zeros(len(vocab))\nfor x,y in dls.valid:\n    n += y.shape[0]\n    for i in range_of(vocab): counts[i] += (y==i).long().sum()\nidx = torch.argmax(counts)\nidx, vocab[idx.item()], counts[idx].item()/n\n(tensor(29), 'thousand', 0.15165200855716662)\n最常见的标记的索引是 29，对应于标记 thousand。总是预测这个标记将给我们大约 15% 的准确率，所以我们表现得更好！"
  },
  {
    "objectID": "Fastbook/translations/cn/12_nlp_dive.html#我们的第一个循环神经网络",
    "href": "Fastbook/translations/cn/12_nlp_dive.html#我们的第一个循环神经网络",
    "title": "第十二章：从头开始的语言模型",
    "section": "我们的第一个循环神经网络",
    "text": "我们的第一个循环神经网络\n查看我们模块的代码，我们可以通过用 for 循环替换调用层的重复代码来简化它。除了使我们的代码更简单外，这样做的好处是我们将能够同样适用于不同长度的标记序列——我们不会被限制在长度为三的标记列表上：\nclass LMModel2(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.h_h = nn.Linear(n_hidden, n_hidden)\n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n\n    def forward(self, x):\n        h = 0\n        for i in range(3):\n            h = h + self.i_h(x[:,i])\n            h = F.relu(self.h_h(h))\n        return self.h_o(h)\n让我们检查一下，看看我们使用这种重构是否得到相同的结果：\nlearn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy,\n                metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.816274\n1.964143\n0.460185\n00:02\n\n\n1\n1.423805\n1.739964\n0.473259\n00:02\n\n\n2\n1.430327\n1.685172\n0.485382\n00:02\n\n\n3\n1.388390\n1.657033\n0.470406\n00:02\n\n\n\n我们还可以以完全相同的方式重构我们的图示表示，如图 12-4 所示（这里我们也删除了激活大小的细节，并使用与图 12-3 相同的箭头颜色）。\n\n\n\n基本循环神经网络\n\n\n\n图 12-4. 基本循环神经网络\n您将看到一组激活在每次循环中被更新，存储在变量h中—这被称为隐藏状态。"
  },
  {
    "objectID": "Fastbook/translations/cn/12_nlp_dive.html#维护-rnn-的状态",
    "href": "Fastbook/translations/cn/12_nlp_dive.html#维护-rnn-的状态",
    "title": "第十二章：从头开始的语言模型",
    "section": "维护 RNN 的状态",
    "text": "维护 RNN 的状态\n因为我们为每个新样本将模型的隐藏状态初始化为零，这样我们就丢失了关于迄今为止看到的句子的所有信息，这意味着我们的模型实际上不知道我们在整体计数序列中的进度。这很容易修复；我们只需将隐藏状态的初始化移动到__init__中。\n但是，这种修复方法将产生自己微妙但重要的问题。它实际上使我们的神经网络变得和文档中的令牌数量一样多。例如，如果我们的数据集中有 10,000 个令牌，我们将创建一个有 10,000 层的神经网络。\n要了解为什么会出现这种情况，请考虑我们循环神经网络的原始图示表示，即在图 12-3 中，在使用for循环重构之前。您可以看到每个层对应一个令牌输入。当我们谈论使用for循环重构之前的循环神经网络的表示时，我们称之为展开表示。在尝试理解 RNN 时，考虑展开表示通常是有帮助的。\n10,000 层神经网络的问题在于，当您到达数据集的第 10,000 个单词时，您仍然需要计算直到第一层的所有导数。这将非常缓慢，且占用内存。您可能无法在 GPU 上存储一个小批量。\n解决这个问题的方法是告诉 PyTorch 我们不希望通过整个隐式神经网络反向传播导数。相反，我们将保留梯度的最后三层。为了在 PyTorch 中删除所有梯度历史，我们使用detach方法。\n这是我们 RNN 的新版本。现在它是有状态的，因为它在不同调用forward时记住了其激活，这代表了它在批处理中用于不同样本的情况：\nclass LMModel3(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.h_h = nn.Linear(n_hidden, n_hidden)\n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        self.h = 0\n\n    def forward(self, x):\n        for i in range(3):\n            self.h = self.h + self.i_h(x[:,i])\n            self.h = F.relu(self.h_h(self.h))\n        out = self.h_o(self.h)\n        self.h = self.h.detach()\n        return out\n\n    def reset(self): self.h = 0\n无论我们选择什么序列长度，这个模型将具有相同的激活，因为隐藏状态将记住上一批次的最后激活。唯一不同的是在每一步计算的梯度：它们将仅在过去的序列长度标记上计算，而不是整个流。这种方法称为时间穿梭反向传播（BPTT）。"
  },
  {
    "objectID": "Fastbook/translations/cn/12_nlp_dive.html#创建更多信号",
    "href": "Fastbook/translations/cn/12_nlp_dive.html#创建更多信号",
    "title": "第十二章：从头开始的语言模型",
    "section": "创建更多信号",
    "text": "创建更多信号\n我们当前方法的另一个问题是，我们仅为每三个输入单词预测一个输出单词。因此，我们反馈以更新权重的信号量不如可能的那么大。如果我们在每个单词后预测下一个单词，而不是每三个单词，将会更好，如图 12-5 所示。\n\n\n\nRNN 在每个标记后进行预测\n\n\n\n图 12-5。RNN 在每个标记后进行预测\n这很容易添加。我们需要首先改变我们的数据，使得因变量在每个三个输入词后的每个三个词中都有。我们使用一个属性sl（用于序列长度），并使其稍微变大：\nsl = 16\nseqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n         for i in range(0,len(nums)-sl-1,sl))\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n                             group_chunks(seqs[cut:], bs),\n                             bs=bs, drop_last=True, shuffle=False)\n查看seqs的第一个元素，我们可以看到它包含两个相同大小的列表。第二个列表与第一个相同，但偏移了一个元素：\n[L(vocab[o] for o in s) for s in seqs[0]]\n[(#16) ['one','.','two','.','three','.','four','.','five','.'...],\n (#16) ['.','two','.','three','.','four','.','five','.','six'...]]\n现在我们需要修改我们的模型，使其在每个单词之后输出一个预测，而不仅仅是在一个三个词序列的末尾：\nclass LMModel4(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.h_h = nn.Linear(n_hidden, n_hidden)\n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        self.h = 0\n\n    def forward(self, x):\n        outs = []\n        for i in range(sl):\n            self.h = self.h + self.i_h(x[:,i])\n            self.h = F.relu(self.h_h(self.h))\n            outs.append(self.h_o(self.h))\n        self.h = self.h.detach()\n        return torch.stack(outs, dim=1)\n\n    def reset(self): self.h = 0\n这个模型将返回形状为bs x sl x vocab_sz的输出（因为我们在dim=1上堆叠）。我们的目标的形状是bs x sl，所以在使用F.cross_entropy之前，我们需要将它们展平：\ndef loss_func(inp, targ):\n    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))\n我们现在可以使用这个损失函数来训练模型：\nlearn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func,\n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n3.103298\n2.874341\n0.212565\n00:01\n\n\n1\n2.231964\n1.971280\n0.462158\n00:01\n\n\n2\n1.711358\n1.813547\n0.461182\n00:01\n\n\n3\n1.448516\n1.828176\n0.483236\n00:01\n\n\n4\n1.288630\n1.659564\n0.520671\n00:01\n\n\n5\n1.161470\n1.714023\n0.554932\n00:01\n\n\n6\n1.055568\n1.660916\n0.575033\n00:01\n\n\n7\n0.960765\n1.719624\n0.591064\n00:01\n\n\n8\n0.870153\n1.839560\n0.614665\n00:01\n\n\n9\n0.808545\n1.770278\n0.624349\n00:01\n\n\n10\n0.758084\n1.842931\n0.610758\n00:01\n\n\n11\n0.719320\n1.799527\n0.646566\n00:01\n\n\n12\n0.683439\n1.917928\n0.649821\n00:01\n\n\n13\n0.660283\n1.874712\n0.628581\n00:01\n\n\n14\n0.646154\n1.877519\n0.640055\n00:01\n\n\n\n我们需要训练更长时间，因为任务有点变化，现在更加复杂。但我们最终得到了一个好结果…至少有时候是这样。如果你多次运行它，你会发现在不同的运行中可以得到非常不同的结果。这是因为实际上我们在这里有一个非常深的网络，这可能导致非常大或非常小的梯度。我们将在本章的下一部分看到如何处理这个问题。\n现在，获得更好模型的明显方法是加深：在我们基本的 RNN 中，隐藏状态和输出激活之间只有一个线性层，所以也许我们用更多的线性层会得到更好的结果。"
  },
  {
    "objectID": "Fastbook/translations/cn/12_nlp_dive.html#模型",
    "href": "Fastbook/translations/cn/12_nlp_dive.html#模型",
    "title": "第十二章：从头开始的语言模型",
    "section": "模型",
    "text": "模型\n我们可以通过使用 PyTorch 的RNN类来节省一些时间，该类实现了我们之前创建的内容，但也给了我们堆叠多个 RNN 的选项，正如我们之前讨论的那样：\nclass LMModel5(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h = torch.zeros(n_layers, bs, n_hidden)\n\n    def forward(self, x):\n        res,h = self.rnn(self.i_h(x), self.h)\n        self.h = h.detach()\n        return self.h_o(res)\n\n    def reset(self): self.h.zero_()\nlearn = Learner(dls, LMModel5(len(vocab), 64, 2),\n                loss_func=CrossEntropyLossFlat(),\n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n3.055853\n2.591640\n0.437907\n00:01\n\n\n1\n2.162359\n1.787310\n0.471598\n00:01\n\n\n2\n1.710663\n1.941807\n0.321777\n00:01\n\n\n3\n1.520783\n1.999726\n0.312012\n00:01\n\n\n4\n1.330846\n2.012902\n0.413249\n00:01\n\n\n5\n1.163297\n1.896192\n0.450684\n00:01\n\n\n6\n1.033813\n2.005209\n0.434814\n00:01\n\n\n7\n0.919090\n2.047083\n0.456706\n00:01\n\n\n8\n0.822939\n2.068031\n0.468831\n00:01\n\n\n9\n0.750180\n2.136064\n0.475098\n00:01\n\n\n10\n0.695120\n2.139140\n0.485433\n00:01\n\n\n11\n0.655752\n2.155081\n0.493652\n00:01\n\n\n12\n0.629650\n2.162583\n0.498535\n00:01\n\n\n13\n0.613583\n2.171649\n0.491048\n00:01\n\n\n14\n0.604309\n2.180355\n0.487874\n00:01\n\n\n\n现在这令人失望…我们之前的单层 RNN 表现更好。为什么？原因是我们有一个更深的模型，导致激活爆炸或消失。"
  },
  {
    "objectID": "Fastbook/translations/cn/12_nlp_dive.html#激活爆炸或消失",
    "href": "Fastbook/translations/cn/12_nlp_dive.html#激活爆炸或消失",
    "title": "第十二章：从头开始的语言模型",
    "section": "激活爆炸或消失",
    "text": "激活爆炸或消失\n在实践中，从这种类型的 RNN 创建准确的模型是困难的。如果我们调用detach的频率较少，并且有更多的层，我们将获得更好的结果 - 这使得我们的 RNN 有更长的时间跨度来学习和创建更丰富的特征。但这也意味着我们有一个更深的模型要训练。深度学习发展中的关键挑战是如何训练这种类型的模型。\n这是具有挑战性的，因为当您多次乘以一个矩阵时会发生什么。想想当您多次乘以一个数字时会发生什么。例如，如果您从 1 开始乘以 2，您会得到序列 1、2、4、8，…在 32 步之后，您已经达到 4,294,967,296。如果您乘以 0.5，类似的问题会发生：您会得到 0.5、0.25、0.125，…在 32 步之后，它是 0.00000000023。正如您所看到的，即使是比 1 稍高或稍低的数字，经过几次重复乘法后，我们的起始数字就会爆炸或消失。\n因为矩阵乘法只是将数字相乘并将它们相加，重复矩阵乘法会发生完全相同的事情。这就是深度神经网络的全部内容 - 每一层都是另一个矩阵乘法。这意味着深度神经网络很容易最终得到极大或极小的数字。\n这是一个问题，因为计算机存储数字的方式（称为浮点数）意味着随着数字远离零点，它们变得越来越不准确。来自优秀文章“关于浮点数你从未想知道但却被迫了解”的图 12-8 中的图表显示了浮点数的精度如何随着数字线变化。\n\n\n\n浮点数的精度\n\n\n\n图 12-8。浮点数的精度\n这种不准确性意味着通常为更新权重计算的梯度最终会变为零或无穷大。这通常被称为消失梯度或爆炸梯度问题。这意味着在 SGD 中，权重要么根本不更新，要么跳到无穷大。无论哪种方式，它们都不会随着训练而改善。\n研究人员已经开发出了解决这个问题的方法，我们将在本书后面讨论。一种选择是改变层的定义方式，使其不太可能出现激活爆炸。当我们讨论批量归一化时，我们将在第十三章中看到这是如何完成的，当我们讨论 ResNets 时，我们将在第十四章中看到，尽管这些细节通常在实践中并不重要（除非您是一个研究人员，正在创造解决这个问题的新方法）。另一种处理这个问题的策略是谨慎初始化，这是我们将在第十七章中调查的一个主题。\n为了避免激活爆炸，RNN 经常使用两种类型的层：门控循环单元（GRUs）和长短期记忆（LSTM）层。这两种都在 PyTorch 中可用，并且可以直接替换 RNN 层。在本书中，我们只会涵盖 LSTMs；在线上有很多好的教程解释 GRUs，它们是 LSTM 设计的一个小变体。"
  },
  {
    "objectID": "Fastbook/translations/cn/12_nlp_dive.html#从头开始构建一个-lstm",
    "href": "Fastbook/translations/cn/12_nlp_dive.html#从头开始构建一个-lstm",
    "title": "第十二章：从头开始的语言模型",
    "section": "从头开始构建一个 LSTM",
    "text": "从头开始构建一个 LSTM\n为了构建一个 LSTM，我们首先必须了解其架构。图 12-9 显示了其内部结构。\n\n\n\n显示 LSTM 内部架构的图表\n\n\n\n图 12-9. LSTM 的架构\n在这张图片中，我们的输入x t从左侧进入，带有先前的隐藏状态（h t-1）和 cell state（c t-1）。四个橙色框代表四个层（我们的神经网络），激活函数可以是 sigmoid（σ）或 tanh。tanh 只是一个重新缩放到范围-1 到 1 的 sigmoid 函数。它的数学表达式可以写成这样：\ntanh ( x ) = e x +e -x e x -e -x = 2 σ ( 2 x ) - 1\n其中σ是 sigmoid 函数。图中的绿色圆圈是逐元素操作。右侧输出的是新的隐藏状态（h t）和新的 cell state（c t），准备接受我们的下一个输入。新的隐藏状态也被用作输出，这就是为什么箭头分开向上移动。\n让我们逐一查看四个神经网络（称为门）并解释图表——但在此之前，请注意 cell state（顶部）几乎没有改变。它甚至没有直接通过神经网络！这正是为什么它将继续保持较长期的状态。\n首先，将输入和旧隐藏状态的箭头连接在一起。在本章前面编写的 RNN 中，我们将它们相加。在 LSTM 中，我们将它们堆叠在一个大张量中。这意味着我们的嵌入的维度（即x t的维度）可以与隐藏状态的维度不同。如果我们将它们称为n_in和n_hid，底部的箭头大小为n_in + n_hid；因此所有的神经网络（橙色框）都是具有n_in + n_hid输入和n_hid输出的线性层。\n第一个门（从左到右看）称为遗忘门。由于它是一个线性层后面跟着一个 sigmoid，它的输出将由 0 到 1 之间的标量组成。我们将这个结果乘以细胞状态，以确定要保留哪些信息，要丢弃哪些信息：接近 0 的值被丢弃，接近 1 的值被保留。这使得 LSTM 有能力忘记关于其长期状态的事情。例如，当穿过一个句号或一个xxbos标记时，我们期望它（已经学会）重置其细胞状态。\n第二个门称为输入门。它与第三个门（没有真正的名称，但有时被称为细胞门）一起更新细胞状态。例如，我们可能看到一个新的性别代词，这时我们需要替换遗忘门删除的关于性别的信息。与遗忘门类似，输入门决定要更新的细胞状态元素（接近 1 的值）或不更新（接近 0 的值）。第三个门确定这些更新值是什么，范围在-1 到 1 之间（由于 tanh 函数）。结果被添加到细胞状态中。\n最后一个门是输出门。它确定从细胞状态中使用哪些信息来生成输出。细胞状态经过 tanh 后与输出门的 sigmoid 输出结合，结果就是新的隐藏状态。在代码方面，我们可以这样写相同的步骤：\nclass LSTMCell(Module):\n    def __init__(self, ni, nh):\n        self.forget_gate = nn.Linear(ni + nh, nh)\n        self.input_gate  = nn.Linear(ni + nh, nh)\n        self.cell_gate   = nn.Linear(ni + nh, nh)\n        self.output_gate = nn.Linear(ni + nh, nh)\n\n    def forward(self, input, state):\n        h,c = state\n        h = torch.stack([h, input], dim=1)\n        forget = torch.sigmoid(self.forget_gate(h))\n        c = c * forget\n        inp = torch.sigmoid(self.input_gate(h))\n        cell = torch.tanh(self.cell_gate(h))\n        c = c + inp * cell\n        out = torch.sigmoid(self.output_gate(h))\n        h = outgate * torch.tanh(c)\n        return h, (h,c)\n实际上，我们可以重构代码。此外，就性能而言，做一次大矩阵乘法比做四次小矩阵乘法更好（因为我们只在 GPU 上启动一次特殊的快速内核，这样可以让 GPU 并行处理更多工作）。堆叠需要一点时间（因为我们必须在 GPU 上移动一个张量，使其全部在一个连续的数组中），所以我们为输入和隐藏状态使用两个单独的层。优化和重构后的代码如下：\nclass LSTMCell(Module):\n    def __init__(self, ni, nh):\n        self.ih = nn.Linear(ni,4*nh)\n        self.hh = nn.Linear(nh,4*nh)\n\n    def forward(self, input, state):\n        h,c = state\n        # One big multiplication for all the gates is better than 4 smaller ones\n        gates = (self.ih(input) + self.hh(h)).chunk(4, 1)\n        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n        cellgate = gates[3].tanh()\n\n        c = (forgetgate*c) + (ingate*cellgate)\n        h = outgate * c.tanh()\n        return h, (h,c)\n在这里，我们使用 PyTorch 的chunk方法将张量分成四部分。它的工作原理如下：\nt = torch.arange(0,10); t\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\nt.chunk(2)\n(tensor([0, 1, 2, 3, 4]), tensor([5, 6, 7, 8, 9]))\n现在让我们使用这个架构来训练一个语言模型！"
  },
  {
    "objectID": "Fastbook/translations/cn/12_nlp_dive.html#使用-lstms-训练语言模型",
    "href": "Fastbook/translations/cn/12_nlp_dive.html#使用-lstms-训练语言模型",
    "title": "第十二章：从头开始的语言模型",
    "section": "使用 LSTMs 训练语言模型",
    "text": "使用 LSTMs 训练语言模型\n这是与LMModel5相同的网络，使用了两层 LSTM。我们可以以更高的学习率进行训练，时间更短，获得更好的准确性：\nclass LMModel6(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n\n    def forward(self, x):\n        res,h = self.rnn(self.i_h(x), self.h)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(res)\n\n    def reset(self):\n        for h in self.h: h.zero_()\nlearn = Learner(dls, LMModel6(len(vocab), 64, 2),\n                loss_func=CrossEntropyLossFlat(),\n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 1e-2)\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n3.000821\n2.663942\n0.438314\n00:02\n\n\n1\n2.139642\n2.184780\n0.240479\n00:02\n\n\n2\n1.607275\n1.812682\n0.439779\n00:02\n\n\n3\n1.347711\n1.830982\n0.497477\n00:02\n\n\n4\n1.123113\n1.937766\n0.594401\n00:02\n\n\n5\n0.852042\n2.012127\n0.631592\n00:02\n\n\n6\n0.565494\n1.312742\n0.725749\n00:02\n\n\n7\n0.347445\n1.297934\n0.711263\n00:02\n\n\n8\n0.208191\n1.441269\n0.731201\n00:02\n\n\n9\n0.126335\n1.569952\n0.737305\n00:02\n\n\n10\n0.079761\n1.427187\n0.754150\n00:02\n\n\n11\n0.052990\n1.494990\n0.745117\n00:02\n\n\n12\n0.039008\n1.393731\n0.757894\n00:02\n\n\n13\n0.031502\n1.373210\n0.758464\n00:02\n\n\n14\n0.028068\n1.368083\n0.758464\n00:02\n\n\n\n现在这比多层 RNN 好多了！然而，我们仍然可以看到有一点过拟合，这表明一点正则化可能会有所帮助。"
  },
  {
    "objectID": "Fastbook/translations/cn/12_nlp_dive.html#dropout",
    "href": "Fastbook/translations/cn/12_nlp_dive.html#dropout",
    "title": "第十二章：从头开始的语言模型",
    "section": "Dropout",
    "text": "Dropout\nDropout是由 Geoffrey Hinton 等人在“通过防止特征探测器的共适应来改进神经网络”中引入的一种正则化技术。基本思想是在训练时随机将一些激活变为零。这确保所有神经元都积极地朝着输出工作，如图 12-10 所示（来自 Nitish Srivastava 等人的“Dropout：防止神经网络过拟合的简单方法”）。\n\n\n\n文章中显示 dropout 如何关闭神经元的图\n\n\n\n图 12-10。在神经网络中应用 dropout（由 Nitish Srivastava 等人提供）\nHinton 在一次采访中解释了 dropout 的灵感时使用了一个很好的比喻：\n\n我去了我的银行。出纳员不断变换，我问其中一个原因。他说他不知道，但他们经常被调动。我想这一定是因为需要员工之间的合作才能成功欺诈银行。这让我意识到，随机在每个示例中删除不同的神经元子集将防止阴谋，从而减少过拟合。\n\n在同一次采访中，他还解释了神经科学提供了额外的灵感：\n\n我们并不真正知道为什么神经元会突触。有一种理论是它们想要变得嘈杂以进行正则化，因为我们的参数比数据点多得多。dropout 的想法是，如果你有嘈杂的激活，你可以承担使用一个更大的模型。\n\n这解释了为什么 dropout 有助于泛化的想法：首先它帮助神经元更好地合作；然后它使激活更嘈杂，从而使模型更健壮。\n然而，我们可以看到，如果我们只是将这些激活置零而不做其他任何操作，我们的模型将会训练出问题：如果我们从五个激活的总和（由于我们应用了 ReLU，它们都是正数）变为只有两个，这不会有相同的规模。因此，如果我们以概率p应用 dropout，我们通过将所有激活除以1-p来重新缩放它们（平均p将被置零，所以剩下1-p），如图 12-11 所示。\n\n\n\n介绍 dropout 的文章中的一个图，显示神经元是开启/关闭状态\n\n\n\n\n图 12-11。应用 dropout 时为什么要缩放激活（由 Nitish Srivastava 等人提供）\n这是 PyTorch 中 dropout 层的完整实现（尽管 PyTorch 的原生层实际上是用 C 而不是 Python 编写的）：\nclass Dropout(Module):\n    def __init__(self, p): self.p = p\n    def forward(self, x):\n        if not self.training: return x\n        mask = x.new(*x.shape).bernoulli_(1-p)\n        return x * mask.div_(1-p)\nbernoulli_方法创建一个随机零（概率为p）和一（概率为1-p）的张量，然后将其乘以我们的输入，再除以1-p。注意training属性的使用，它在任何 PyTorch nn.Module中都可用，并告诉我们是否在训练或推理。"
  },
  {
    "objectID": "Fastbook/translations/cn/12_nlp_dive.html#激活正则化和时间激活正则化",
    "href": "Fastbook/translations/cn/12_nlp_dive.html#激活正则化和时间激活正则化",
    "title": "第十二章：从头开始的语言模型",
    "section": "激活正则化和时间激活正则化",
    "text": "激活正则化和时间激活正则化\n激活正则化（AR）和时间激活正则化（TAR）是两种与权重衰减非常相似的正则化方法，在第八章中讨论过。在应用权重衰减时，我们会对损失添加一个小的惩罚，旨在使权重尽可能小。对于激活正则化，我们将尝试使 LSTM 生成的最终激活尽可能小，而不是权重。\n为了对最终激活进行正则化，我们必须将它们存储在某个地方，然后将它们的平方的平均值添加到损失中（以及一个乘数alpha，就像权重衰减的wd一样）：\nloss += alpha * activations.pow(2).mean()\n时间激活正则化与我们在句子中预测标记有关。这意味着当我们按顺序阅读它们时，我们的 LSTM 的输出应该在某种程度上是有意义的。TAR 通过向损失添加惩罚来鼓励这种行为，使两个连续激活之间的差异尽可能小：我们的激活张量的形状为bs x sl x n_hid，我们在序列长度轴上（中间维度）读取连续激活。有了这个，TAR 可以表示如下：\nloss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean()\n然后，alpha和beta是要调整的两个超参数。为了使这项工作成功，我们需要让我们的带有 dropout 的模型返回三个东西：正确的输出，LSTM 在 dropout 之前的激活以及 LSTM 在 dropout 之后的激活。通常在 dropout 后的激活上应用 AR（以免惩罚我们之后转换为零的激活），而 TAR 应用在未经 dropout 的激活上（因为这些零会在两个连续时间步之间产生很大的差异）。然后，一个名为RNNRegularizer的回调将为我们应用这种正则化。"
  },
  {
    "objectID": "Fastbook/translations/cn/12_nlp_dive.html#训练带有权重绑定的正则化-lstm",
    "href": "Fastbook/translations/cn/12_nlp_dive.html#训练带有权重绑定的正则化-lstm",
    "title": "第十二章：从头开始的语言模型",
    "section": "训练带有权重绑定的正则化 LSTM",
    "text": "训练带有权重绑定的正则化 LSTM\n我们可以将 dropout（应用在我们进入输出层之前）与 AR 和 TAR 相结合，以训练我们之前的 LSTM。我们只需要返回三个东西而不是一个：我们的 LSTM 的正常输出，dropout 后的激活以及我们的 LSTM 的激活。最后两个将由回调RNNRegularization捕获，以便为其对损失的贡献做出贡献。\n我们可以从AWD-LSTM 论文中添加另一个有用的技巧是权重绑定。在语言模型中，输入嵌入表示从英语单词到激活的映射，输出隐藏层表示从激活到英语单词的映射。直觉上，我们可能会期望这些映射是相同的。我们可以通过将相同的权重矩阵分配给这些层来在 PyTorch 中表示这一点：\nself.h_o.weight = self.i_h.weight\n在LMMModel7中，我们包括了这些最终的调整：\nclass LMModel7(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.drop = nn.Dropout(p)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h_o.weight = self.i_h.weight\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n\n    def forward(self, x):\n        raw,h = self.rnn(self.i_h(x), self.h)\n        out = self.drop(raw)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(out),raw,out\n\n    def reset(self):\n        for h in self.h: h.zero_()\n我们可以使用RNNRegularizer回调函数创建一个正则化的Learner：\nlearn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5),\n                loss_func=CrossEntropyLossFlat(), metrics=accuracy,\n                cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)])\nTextLearner会自动为我们添加这两个回调函数（使用alpha和beta的默认值），因此我们可以简化前面的行：\nlearn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),\n                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n然后我们可以训练模型，并通过增加权重衰减到0.1来添加额外的正则化：\nlearn.fit_one_cycle(15, 1e-2, wd=0.1)\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.693885\n2.013484\n0.466634\n00:02\n\n\n1\n1.685549\n1.187310\n0.629313\n00:02\n\n\n2\n0.973307\n0.791398\n0.745605\n00:02\n\n\n3\n0.555823\n0.640412\n0.794108\n00:02\n\n\n4\n0.351802\n0.557247\n0.836100\n00:02\n\n\n5\n0.244986\n0.594977\n0.807292\n00:02\n\n\n6\n0.192231\n0.511690\n0.846761\n00:02\n\n\n7\n0.162456\n0.520370\n0.858073\n00:02\n\n\n8\n0.142664\n0.525918\n0.842285\n00:02\n\n\n9\n0.128493\n0.495029\n0.858073\n00:02\n\n\n10\n0.117589\n0.464236\n0.867188\n00:02\n\n\n11\n0.109808\n0.466550\n0.869303\n00:02\n\n\n12\n0.104216\n0.455151\n0.871826\n00:02\n\n\n13\n0.100271\n0.452659\n0.873617\n00:02\n\n\n14\n0.098121\n0.458372\n0.869385\n00:02\n\n\n\n现在这比我们之前的模型好多了！"
  },
  {
    "objectID": "Fastbook/translations/cn/12_nlp_dive.html#进一步研究",
    "href": "Fastbook/translations/cn/12_nlp_dive.html#进一步研究",
    "title": "第十二章：从头开始的语言模型",
    "section": "进一步研究",
    "text": "进一步研究\n\n在LMModel2中，为什么forward可以从h=0开始？为什么我们不需要写h=torch.zeros(...)？\n从头开始编写一个 LSTM 的代码（你可以参考图 12-9）。\n搜索互联网了解 GRU 架构并从头开始实现它，尝试训练一个模型。看看能否获得类似于本章中看到的结果。将你的结果与 PyTorch 内置的GRU模块的结果进行比较。\n查看 fastai 中 AWD-LSTM 的源代码，并尝试将每行代码映射到本章中展示的概念。"
  },
  {
    "objectID": "Fastbook/10_nlp.html",
    "href": "Fastbook/10_nlp.html",
    "title": "NLP Deep Dive: RNNs",
    "section": "",
    "text": "#hide\n!pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n#hide\nfrom fastbook import *\nfrom IPython.display import display,HTML\n[[chapter_nlp]]\nIn &lt;&gt; we saw that deep learning can be used to get great results with natural language datasets. Our example relied on using a pretrained language model and fine-tuning it to classify reviews. That example highlighted a difference between transfer learning in NLP and computer vision: in general in NLP the pretrained model is trained on a different task.\nWhat we call a language model is a model that has been trained to guess what the next word in a text is (having read the ones before). This kind of task is called self-supervised learning: we do not need to give labels to our model, just feed it lots and lots of texts. It has a process to automatically get labels from the data, and this task isn’t trivial: to properly guess the next word in a sentence, the model will have to develop an understanding of the English (or other) language. Self-supervised learning can also be used in other domains; for instance, see “Self-Supervised Learning and Computer Vision” for an introduction to vision applications. Self-supervised learning is not usually used for the model that is trained directly, but instead is used for pretraining a model used for transfer learning.\nThe language model we used in &lt;&gt; to classify IMDb reviews was pretrained on Wikipedia. We got great results by directly fine-tuning this language model to a movie review classifier, but with one extra step, we can do even better. The Wikipedia English is slightly different from the IMDb English, so instead of jumping directly to the classifier, we could fine-tune our pretrained language model to the IMDb corpus and then use that as the base for our classifier.\nEven if our language model knows the basics of the language we are using in the task (e.g., our pretrained model is in English), it helps to get used to the style of the corpus we are targeting. It may be more informal language, or more technical, with new words to learn or different ways of composing sentences. In the case of the IMDb dataset, there will be lots of names of movie directors and actors, and often a less formal style of language than that seen in Wikipedia.\nWe already saw that with fastai, we can download a pretrained English language model and use it to get state-of-the-art results for NLP classification. (We expect pretrained models in many more languages to be available soon—they might well be available by the time you are reading this book, in fact.) So, why are we learning how to train a language model in detail?\nOne reason, of course, is that it is helpful to understand the foundations of the models that you are using. But there is another very practical reason, which is that you get even better results if you fine-tune the (sequence-based) language model prior to fine-tuning the classification model. For instance, for the IMDb sentiment analysis task, the dataset includes 50,000 additional movie reviews that do not have any positive or negative labels attached. Since there are 25,000 labeled reviews in the training set and 25,000 in the validation set, that makes 100,000 movie reviews altogether. We can use all of these reviews to fine-tune the pretrained language model, which was trained only on Wikipedia articles; this will result in a language model that is particularly good at predicting the next word of a movie review.\nThis is known as the Universal Language Model Fine-tuning (ULMFit) approach. The paper showed that this extra stage of fine-tuning of the language model, prior to transfer learning to a classification task, resulted in significantly better predictions. Using this approach, we have three stages for transfer learning in NLP, as summarized in &lt;&gt;.\nWe’ll now explore how to apply a neural network to this language modeling problem, using the concepts introduced in the last two chapters. But before reading further, pause and think about how you would approach this."
  },
  {
    "objectID": "Fastbook/10_nlp.html#text-preprocessing",
    "href": "Fastbook/10_nlp.html#text-preprocessing",
    "title": "NLP Deep Dive: RNNs",
    "section": "Text Preprocessing",
    "text": "Text Preprocessing\nIt’s not at all obvious how we’re going to use what we’ve learned so far to build a language model. Sentences can be different lengths, and documents can be very long. So, how can we predict the next word of a sentence using a neural network? Let’s find out!\nWe’ve already seen how categorical variables can be used as independent variables for a neural network. The approach we took for a single categorical variable was to:\n\nMake a list of all possible levels of that categorical variable (we’ll call this list the vocab).\nReplace each level with its index in the vocab.\nCreate an embedding matrix for this containing a row for each level (i.e., for each item of the vocab).\nUse this embedding matrix as the first layer of a neural network. (A dedicated embedding matrix can take as inputs the raw vocab indexes created in step 2; this is equivalent to but faster and more efficient than a matrix that takes as input one-hot-encoded vectors representing the indexes.)\n\nWe can do nearly the same thing with text! What is new is the idea of a sequence. First we concatenate all of the documents in our dataset into one big long string and split it into words, giving us a very long list of words (or “tokens”). Our independent variable will be the sequence of words starting with the first word in our very long list and ending with the second to last, and our dependent variable will be the sequence of words starting with the second word and ending with the last word.\nOur vocab will consist of a mix of common words that are already in the vocabulary of our pretrained model and new words specific to our corpus (cinematographic terms or actors names, for instance). Our embedding matrix will be built accordingly: for words that are in the vocabulary of our pretrained model, we will take the corresponding row in the embedding matrix of the pretrained model; but for new words we won’t have anything, so we will just initialize the corresponding row with a random vector.\nEach of the steps necessary to create a language model has jargon associated with it from the world of natural language processing, and fastai and PyTorch classes available to help. The steps are:\n\nTokenization:: Convert the text into a list of words (or characters, or substrings, depending on the granularity of your model)\nNumericalization:: Make a list of all of the unique words that appear (the vocab), and convert each word into a number, by looking up its index in the vocab\nLanguage model data loader creation:: fastai provides an LMDataLoader class which automatically handles creating a dependent variable that is offset from the independent variable by one token. It also handles some important details, such as how to shuffle the training data in such a way that the dependent and independent variables maintain their structure as required\nLanguage model creation:: We need a special kind of model that does something we haven’t seen before: handles input lists which could be arbitrarily big or small. There are a number of ways to do this; in this chapter we will be using a recurrent neural network (RNN). We will get to the details of these RNNs in the &lt;&gt;, but for now, you can think of it as just another deep neural network.\n\nLet’s take a look at how each step works in detail.\n\nTokenization\nWhen we said “convert the text into a list of words,” we left out a lot of details. For instance, what do we do with punctuation? How do we deal with a word like “don’t”? Is it one word, or two? What about long medical or chemical words? Should they be split into their separate pieces of meaning? How about hyphenated words? What about languages like German and Polish where we can create really long words from many, many pieces? What about languages like Japanese and Chinese that don’t use bases at all, and don’t really have a well-defined idea of word?\nBecause there is no one correct answer to these questions, there is no one approach to tokenization. There are three main approaches:\n\nWord-based:: Split a sentence on spaces, as well as applying language-specific rules to try to separate parts of meaning even when there are no spaces (such as turning “don’t” into “do n’t”). Generally, punctuation marks are also split into separate tokens.\nSubword based:: Split words into smaller parts, based on the most commonly occurring substrings. For instance, “occasion” might be tokenized as “o c ca sion.”\nCharacter-based:: Split a sentence into its individual characters.\n\nWe’ll be looking at word and subword tokenization here, and we’ll leave character-based tokenization for you to implement in the questionnaire at the end of this chapter.\n\njargon: token: One element of a list created by the tokenization process. It could be a word, part of a word (a subword), or a single character.\n\n\n\nWord Tokenization with fastai\nRather than providing its own tokenizers, fastai instead provides a consistent interface to a range of tokenizers in external libraries. Tokenization is an active field of research, and new and improved tokenizers are coming out all the time, so the defaults that fastai uses change too. However, the API and options shouldn’t change too much, since fastai tries to maintain a consistent API even as the underlying technology changes.\nLet’s try it out with the IMDb dataset that we used in &lt;&gt;:\n\nfrom fastai.text.all import *\npath = untar_data(URLs.IMDB)\n\nWe’ll need to grab the text files in order to try out a tokenizer. Just like get_image_files, which we’ve used many times already, gets all the image files in a path, get_text_files gets all the text files in a path. We can also optionally pass folders to restrict the search to a particular list of subfolders:\n\nfiles = get_text_files(path, folders = ['train', 'test', 'unsup'])\n\nHere’s a review that we’ll tokenize (we’ll just print the start of it here to save space):\n\ntxt = files[0].open().read(); txt[:75]\n\n'This movie, which I just discovered at the video store, has apparently sit '\n\n\nAs we write this book, the default English word tokenizer for fastai uses a library called spaCy. It has a sophisticated rules engine with special rules for URLs, individual special English words, and much more. Rather than directly using SpacyTokenizer, however, we’ll use WordTokenizer, since that will always point to fastai’s current default word tokenizer (which may not necessarily be spaCy, depending when you’re reading this).\nLet’s try it out. We’ll use fastai’s coll_repr(collection, n) function to display the results. This displays the first n items of collection, along with the full size—it’s what L uses by default. Note that fastai’s tokenizers take a collection of documents to tokenize, so we have to wrap txt in a list:\n\nspacy = WordTokenizer()\ntoks = first(spacy([txt]))\nprint(coll_repr(toks, 30))\n\n(#201) ['This','movie',',','which','I','just','discovered','at','the','video','store',',','has','apparently','sit','around','for','a','couple','of','years','without','a','distributor','.','It',\"'s\",'easy','to','see'...]\n\n\nAs you see, spaCy has mainly just separated out the words and punctuation. But it does something else here too: it has split “it’s” into “it” and “’s”. That makes intuitive sense; these are separate words, really. Tokenization is a surprisingly subtle task, when you think about all the little details that have to be handled. Fortunately, spaCy handles these pretty well for us—for instance, here we see that “.” is separated when it terminates a sentence, but not in an acronym or number:\n\nfirst(spacy(['The U.S. dollar $1 is $1.00.']))\n\n(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']\n\n\nfastai then adds some additional functionality to the tokenization process with the Tokenizer class:\n\ntkn = Tokenizer(spacy)\nprint(coll_repr(tkn(txt), 31))\n\n(#228) ['xxbos','xxmaj','this','movie',',','which','i','just','discovered','at','the','video','store',',','has','apparently','sit','around','for','a','couple','of','years','without','a','distributor','.','xxmaj','it',\"'s\",'easy'...]\n\n\nNotice that there are now some tokens that start with the characters “xx”, which is not a common word prefix in English. These are special tokens.\nFor example, the first item in the list, xxbos, is a special token that indicates the start of a new text (“BOS” is a standard NLP acronym that means “beginning of stream”). By recognizing this start token, the model will be able to learn it needs to “forget” what was said previously and focus on upcoming words.\nThese special tokens don’t come from spaCy directly. They are there because fastai adds them by default, by applying a number of rules when processing text. These rules are designed to make it easier for a model to recognize the important parts of a sentence. In a sense, we are translating the original English language sequence into a simplified tokenized language—a language that is designed to be easy for a model to learn.\nFor instance, the rules will replace a sequence of four exclamation points with a special repeated character token, followed by the number four, and then a single exclamation point. In this way, the model’s embedding matrix can encode information about general concepts such as repeated punctuation rather than requiring a separate token for every number of repetitions of every punctuation mark. Similarly, a capitalized word will be replaced with a special capitalization token, followed by the lowercase version of the word. This way, the embedding matrix only needs the lowercase versions of the words, saving compute and memory resources, but can still learn the concept of capitalization.\nHere are some of the main special tokens you’ll see:\n\nxxbos:: Indicates the beginning of a text (here, a review)\nxxmaj:: Indicates the next word begins with a capital (since we lowercased everything)\nxxunk:: Indicates the word is unknown\n\nTo see the rules that were used, you can check the default rules:\n\ndefaults.text_proc_rules\n\n[&lt;function fastai.text.core.fix_html(x)&gt;,\n &lt;function fastai.text.core.replace_rep(t)&gt;,\n &lt;function fastai.text.core.replace_wrep(t)&gt;,\n &lt;function fastai.text.core.spec_add_spaces(t)&gt;,\n &lt;function fastai.text.core.rm_useless_spaces(t)&gt;,\n &lt;function fastai.text.core.replace_all_caps(t)&gt;,\n &lt;function fastai.text.core.replace_maj(t)&gt;,\n &lt;function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)&gt;]\n\n\nAs always, you can look at the source code of each of them in a notebook by typing:\n??replace_rep\nHere is a brief summary of what each does:\n\nfix_html:: Replaces special HTML characters with a readable version (IMDb reviews have quite a few of these)\nreplace_rep:: Replaces any character repeated three times or more with a special token for repetition (xxrep), the number of times it’s repeated, then the character\nreplace_wrep:: Replaces any word repeated three times or more with a special token for word repetition (xxwrep), the number of times it’s repeated, then the word\nspec_add_spaces:: Adds spaces around / and #\nrm_useless_spaces:: Removes all repetitions of the space character\nreplace_all_caps:: Lowercases a word written in all caps and adds a special token for all caps (xxup) in front of it\nreplace_maj:: Lowercases a capitalized word and adds a special token for capitalized (xxmaj) in front of it\nlowercase:: Lowercases all text and adds a special token at the beginning (xxbos) and/or the end (xxeos)\n\nLet’s take a look at a few of them in action:\n\ncoll_repr(tkn('&copy;   Fast.ai www.fast.ai/INDEX'), 31)\n\n\"(#11) ['xxbos','©','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','index'...]\"\n\n\nNow let’s take a look at how subword tokenization would work.\n\n\nSubword Tokenization\nIn addition to the word tokenization approach seen in the last section, another popular tokenization method is subword tokenization. Word tokenization relies on an assumption that spaces provide a useful separation of components of meaning in a sentence. However, this assumption is not always appropriate. For instance, consider this sentence: 我的名字是郝杰瑞 (“My name is Jeremy Howard” in Chinese). That’s not going to work very well with a word tokenizer, because there are no spaces in it! Languages like Chinese and Japanese don’t use spaces, and in fact they don’t even have a well-defined concept of a “word.” There are also languages, like Turkish and Hungarian, that can add many subwords together without spaces, creating very long words that include a lot of separate pieces of information.\nTo handle these cases, it’s generally best to use subword tokenization. This proceeds in two steps:\n\nAnalyze a corpus of documents to find the most commonly occurring groups of letters. These become the vocab.\nTokenize the corpus using this vocab of subword units.\n\nLet’s look at an example. For our corpus, we’ll use the first 2,000 movie reviews:\n\ntxts = L(o.open().read() for o in files[:2000])\n\nWe instantiate our tokenizer, passing in the size of the vocab we want to create, and then we need to “train” it. That is, we need to have it read our documents and find the common sequences of characters to create the vocab. This is done with setup. As we’ll see shortly, setup is a special fastai method that is called automatically in our usual data processing pipelines. Since we’re doing everything manually at the moment, however, we have to call it ourselves. Here’s a function that does these steps for a given vocab size, and shows an example output:\n\ndef subword(sz):\n    sp = SubwordTokenizer(vocab_sz=sz)\n    sp.setup(txts)\n    return ' '.join(first(sp([txt]))[:40])\n\nLet’s try it out:\n\nsubword(1000)\n\n\n\n\n'▁This ▁movie , ▁which ▁I ▁just ▁dis c over ed ▁at ▁the ▁video ▁st or e , ▁has ▁a p par ent ly ▁s it ▁around ▁for ▁a ▁couple ▁of ▁years ▁without ▁a ▁dis t ri but or . ▁It'\n\n\nWhen using fastai’s subword tokenizer, the special character ▁ represents a space character in the original text.\nIf we use a smaller vocab, then each token will represent fewer characters, and it will take more tokens to represent a sentence:\n\nsubword(200)\n\n\n\n\n'▁ T h i s ▁movie , ▁w h i ch ▁I ▁ j us t ▁ d i s c o ver ed ▁a t ▁the ▁ v id e o ▁ st or e , ▁h a s'\n\n\nOn the other hand, if we use a larger vocab, then most common English words will end up in the vocab themselves, and we will not need as many to represent a sentence:\n\nsubword(10000)\n\n\n\n\n\"▁This ▁movie , ▁which ▁I ▁just ▁discover ed ▁at ▁the ▁video ▁store , ▁has ▁apparently ▁sit ▁around ▁for ▁a ▁couple ▁of ▁years ▁without ▁a ▁distributor . ▁It ' s ▁easy ▁to ▁see ▁why . ▁The ▁story ▁of ▁two ▁friends ▁living\"\n\n\nPicking a subword vocab size represents a compromise: a larger vocab means fewer tokens per sentence, which means faster training, less memory, and less state for the model to remember; but on the downside, it means larger embedding matrices, which require more data to learn.\nOverall, subword tokenization provides a way to easily scale between character tokenization (i.e., using a small subword vocab) and word tokenization (i.e., using a large subword vocab), and handles every human language without needing language-specific algorithms to be developed. It can even handle other “languages” such as genomic sequences or MIDI music notation! For this reason, in the last year its popularity has soared, and it seems likely to become the most common tokenization approach (it may well already be, by the time you read this!).\nOnce our texts have been split into tokens, we need to convert them to numbers. We’ll look at that next.\n\n\nNumericalization with fastai\nNumericalization is the process of mapping tokens to integers. The steps are basically identical to those necessary to create a Category variable, such as the dependent variable of digits in MNIST:\n\nMake a list of all possible levels of that categorical variable (the vocab).\nReplace each level with its index in the vocab.\n\nLet’s take a look at this in action on the word-tokenized text we saw earlier:\n\ntoks = tkn(txt)\nprint(coll_repr(tkn(txt), 31))\n\n(#228) ['xxbos','xxmaj','this','movie',',','which','i','just','discovered','at','the','video','store',',','has','apparently','sit','around','for','a','couple','of','years','without','a','distributor','.','xxmaj','it',\"'s\",'easy'...]\n\n\nJust like with SubwordTokenizer, we need to call setup on Numericalize; this is how we create the vocab. That means we’ll need our tokenized corpus first. Since tokenization takes a while, it’s done in parallel by fastai; but for this manual walkthrough, we’ll use a small subset:\n\ntoks200 = txts[:200].map(tkn)\ntoks200[0]\n\n(#228) ['xxbos','xxmaj','this','movie',',','which','i','just','discovered','at'...]\n\n\nWe can pass this to setup to create our vocab:\n\nnum = Numericalize()\nnum.setup(toks200)\ncoll_repr(num.vocab,20)\n\n\"(#2000) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','a','and','of','to','is','in','i','it'...]\"\n\n\nOur special rules tokens appear first, and then every word appears once, in frequency order. The defaults to Numericalize are min_freq=3,max_vocab=60000. max_vocab=60000 results in fastai replacing all words other than the most common 60,000 with a special unknown word token, xxunk. This is useful to avoid having an overly large embedding matrix, since that can slow down training and use up too much memory, and can also mean that there isn’t enough data to train useful representations for rare words. However, this last issue is better handled by setting min_freq; the default min_freq=3 means that any word appearing less than three times is replaced with xxunk.\nfastai can also numericalize your dataset using a vocab that you provide, by passing a list of words as the vocab parameter.\nOnce we’ve created our Numericalize object, we can use it as if it were a function:\n\nnums = num(toks)[:20]; nums\n\ntensor([  2,   8,  21,  28,  11,  90,  18,  59,   0,  45,   9, 351, 499,  11,  72, 533, 584, 146,  29,  12])\n\n\nThis time, our tokens have been converted to a tensor of integers that our model can receive. We can check that they map back to the original text:\n\n' '.join(num.vocab[o] for o in nums)\n\n'xxbos xxmaj this movie , which i just xxunk at the video store , has apparently sit around for a'\n\n\nNow that we have numbers, we need to put them in batches for our model.\n\n\nPutting Our Texts into Batches for a Language Model\nWhen dealing with images, we needed to resize them all to the same height and width before grouping them together in a mini-batch so they could stack together efficiently in a single tensor. Here it’s going to be a little different, because one cannot simply resize text to a desired length. Also, we want our language model to read text in order, so that it can efficiently predict what the next word is. This means that each new batch should begin precisely where the previous one left off.\nSuppose we have the following text:\n\n: In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we’ll have another example of the PreProcessor used in the data block API.we will study how we build a language model and train it for a while.\n\nThe tokenization process will add special tokens and deal with punctuation to return this text:\n\n: xxbos xxmaj in this chapter , we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface . xxmaj first we will look at the processing steps necessary to convert text into numbers and how to customize it . xxmaj by doing this , we ’ll have another example of the preprocessor used in the data block xxup api . xxmaj then we will study how we build a language model and train it for a while .\n\nWe now have 90 tokens, separated by spaces. Let’s say we want a batch size of 6. We need to break this text into 6 contiguous parts of length 15:\n\n#hide_input\nstream = \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\ntokens = tkn(stream)\nbs,seq_len = 6,15\nd_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])\ndf = pd.DataFrame(d_tokens)\ndisplay(HTML(df.to_html(index=False,header=None)))\n\n\n\n\nxxbos\nxxmaj\nin\nthis\nchapter\n,\nwe\nwill\ngo\nback\nover\nthe\nexample\nof\nclassifying\n\n\nmovie\nreviews\nwe\nstudied\nin\nchapter\n1\nand\ndig\ndeeper\nunder\nthe\nsurface\n.\nxxmaj\n\n\nfirst\nwe\nwill\nlook\nat\nthe\nprocessing\nsteps\nnecessary\nto\nconvert\ntext\ninto\nnumbers\nand\n\n\nhow\nto\ncustomize\nit\n.\nxxmaj\nby\ndoing\nthis\n,\nwe\n'll\nhave\nanother\nexample\n\n\nof\nthe\npreprocessor\nused\nin\nthe\ndata\nblock\nxxup\napi\n.\n\\n\nxxmaj\nthen\nwe\n\n\nwill\nstudy\nhow\nwe\nbuild\na\nlanguage\nmodel\nand\ntrain\nit\nfor\na\nwhile\n.\n\n\n\n\n\nIn a perfect world, we could then give this one batch to our model. But that approach doesn’t scale, because outside of this toy example it’s unlikely that a single batch containing all the texts would fit in our GPU memory (here we have 90 tokens, but all the IMDb reviews together give several million).\nSo, we need to divide this array more finely into subarrays of a fixed sequence length. It is important to maintain order within and across these subarrays, because we will use a model that maintains a state so that it remembers what it read previously when predicting what comes next.\nGoing back to our previous example with 6 batches of length 15, if we chose a sequence length of 5, that would mean we first feed the following array:\n\n#hide_input\nbs,seq_len = 6,5\nd_tokens = np.array([tokens[i*15:i*15+seq_len] for i in range(bs)])\ndf = pd.DataFrame(d_tokens)\ndisplay(HTML(df.to_html(index=False,header=None)))\n\n\n\n\nxxbos\nxxmaj\nin\nthis\nchapter\n\n\nmovie\nreviews\nwe\nstudied\nin\n\n\nfirst\nwe\nwill\nlook\nat\n\n\nhow\nto\ncustomize\nit\n.\n\n\nof\nthe\npreprocessor\nused\nin\n\n\nwill\nstudy\nhow\nwe\nbuild\n\n\n\n\n\nThen this one:\n\n#hide_input\nbs,seq_len = 6,5\nd_tokens = np.array([tokens[i*15+seq_len:i*15+2*seq_len] for i in range(bs)])\ndf = pd.DataFrame(d_tokens)\ndisplay(HTML(df.to_html(index=False,header=None)))\n\n\n\n\n,\nwe\nwill\ngo\nback\n\n\nchapter\n1\nand\ndig\ndeeper\n\n\nthe\nprocessing\nsteps\nnecessary\nto\n\n\nxxmaj\nby\ndoing\nthis\n,\n\n\nthe\ndata\nblock\nxxup\napi\n\n\na\nlanguage\nmodel\nand\ntrain\n\n\n\n\n\nAnd finally:\n\n#hide_input\nbs,seq_len = 6,5\nd_tokens = np.array([tokens[i*15+10:i*15+15] for i in range(bs)])\ndf = pd.DataFrame(d_tokens)\ndisplay(HTML(df.to_html(index=False,header=None)))\n\n\n\n\nover\nthe\nexample\nof\nclassifying\n\n\nunder\nthe\nsurface\n.\nxxmaj\n\n\nconvert\ntext\ninto\nnumbers\nand\n\n\nwe\n'll\nhave\nanother\nexample\n\n\n.\n\\n\nxxmaj\nthen\nwe\n\n\nit\nfor\na\nwhile\n.\n\n\n\n\n\nGoing back to our movie reviews dataset, the first step is to transform the individual texts into a stream by concatenating them together. As with images, it’s best to randomize the order of the inputs, so at the beginning of each epoch we will shuffle the entries to make a new stream (we shuffle the order of the documents, not the order of the words inside them, or the texts would not make sense anymore!).\nWe then cut this stream into a certain number of batches (which is our batch size). For instance, if the stream has 50,000 tokens and we set a batch size of 10, this will give us 10 mini-streams of 5,000 tokens. What is important is that we preserve the order of the tokens (so from 1 to 5,000 for the first mini-stream, then from 5,001 to 10,000…), because we want the model to read continuous rows of text (as in the preceding example). An xxbos token is added at the start of each during preprocessing, so that the model knows when it reads the stream when a new entry is beginning.\nSo to recap, at every epoch we shuffle our collection of documents and concatenate them into a stream of tokens. We then cut that stream into a batch of fixed-size consecutive mini-streams. Our model will then read the mini-streams in order, and thanks to an inner state, it will produce the same activation whatever sequence length we picked.\nThis is all done behind the scenes by the fastai library when we create an LMDataLoader. We do this by first applying our Numericalize object to the tokenized texts:\n\nnums200 = toks200.map(num)\n\nand then passing that to LMDataLoader:\n\ndl = LMDataLoader(nums200)\n\nLet’s confirm that this gives the expected results, by grabbing the first batch:\n\nx,y = first(dl)\nx.shape,y.shape\n\n(torch.Size([64, 72]), torch.Size([64, 72]))\n\n\nand then looking at the first row of the independent variable, which should be the start of the first text:\n\n' '.join(num.vocab[o] for o in x[0][:20])\n\n'xxbos xxmaj this movie , which i just xxunk at the video store , has apparently sit around for a'\n\n\nThe dependent variable is the same thing offset by one token:\n\n' '.join(num.vocab[o] for o in y[0][:20])\n\n'xxmaj this movie , which i just xxunk at the video store , has apparently sit around for a couple'\n\n\nThis concludes all the preprocessing steps we need to apply to our data. We are now ready to train our text classifier."
  },
  {
    "objectID": "Fastbook/10_nlp.html#training-a-text-classifier",
    "href": "Fastbook/10_nlp.html#training-a-text-classifier",
    "title": "NLP Deep Dive: RNNs",
    "section": "Training a Text Classifier",
    "text": "Training a Text Classifier\nAs we saw at the beginning of this chapter, there are two steps to training a state-of-the-art text classifier using transfer learning: first we need to fine-tune our language model pretrained on Wikipedia to the corpus of IMDb reviews, and then we can use that model to train a classifier.\nAs usual, let’s start with assembling our data.\n\nLanguage Model Using DataBlock\nfastai handles tokenization and numericalization automatically when TextBlock is passed to DataBlock. All of the arguments that can be passed to Tokenize and Numericalize can also be passed to TextBlock. In the next chapter we’ll discuss the easiest ways to run each of these steps separately, to ease debugging—but you can always just debug by running them manually on a subset of your data as shown in the previous sections. And don’t forget about DataBlock’s handy summary method, which is very useful for debugging data issues.\nHere’s how we use TextBlock to create a language model, using fastai’s defaults:\n\nget_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n\ndls_lm = DataBlock(\n    blocks=TextBlock.from_folder(path, is_lm=True),\n    get_items=get_imdb, splitter=RandomSplitter(0.1)\n).dataloaders(path, path=path, bs=128, seq_len=80)\n\nOne thing that’s different to previous types we’ve used in DataBlock is that we’re not just using the class directly (i.e., TextBlock(...), but instead are calling a class method. A class method is a Python method that, as the name suggests, belongs to a class rather than an object. (Be sure to search online for more information about class methods if you’re not familiar with them, since they’re commonly used in many Python libraries and applications; we’ve used them a few times previously in the book, but haven’t called attention to them.) The reason that TextBlock is special is that setting up the numericalizer’s vocab can take a long time (we have to read and tokenize every document to get the vocab). To be as efficient as possible it performs a few optimizations:\n\nIt saves the tokenized documents in a temporary folder, so it doesn’t have to tokenize them more than once\nIt runs multiple tokenization processes in parallel, to take advantage of your computer’s CPUs\n\nWe need to tell TextBlock how to access the texts, so that it can do this initial preprocessing—that’s what from_folder does.\nshow_batch then works in the usual way:\n\ndls_lm.show_batch(max_n=2)\n\n\n\n\n\ntext\ntext_\n\n\n\n\n0\nxxbos xxmaj it 's awesome ! xxmaj in xxmaj story xxmaj mode , your going from punk to pro . xxmaj you have to complete goals that involve skating , driving , and walking . xxmaj you create your own skater and give it a name , and you can make it look stupid or realistic . xxmaj you are with your friend xxmaj eric throughout the game until he betrays you and gets you kicked off of the skateboard\nxxmaj it 's awesome ! xxmaj in xxmaj story xxmaj mode , your going from punk to pro . xxmaj you have to complete goals that involve skating , driving , and walking . xxmaj you create your own skater and give it a name , and you can make it look stupid or realistic . xxmaj you are with your friend xxmaj eric throughout the game until he betrays you and gets you kicked off of the skateboard xxunk\n\n\n1\nwhat xxmaj i 've read , xxmaj death xxmaj bed is based on an actual dream , xxmaj george xxmaj barry , the director , successfully transferred dream to film , only a genius could accomplish such a task . \\n\\n xxmaj old mansions make for good quality horror , as do portraits , not sure what to make of the killer bed with its killer yellow liquid , quite a bizarre dream , indeed . xxmaj also , this\nxxmaj i 've read , xxmaj death xxmaj bed is based on an actual dream , xxmaj george xxmaj barry , the director , successfully transferred dream to film , only a genius could accomplish such a task . \\n\\n xxmaj old mansions make for good quality horror , as do portraits , not sure what to make of the killer bed with its killer yellow liquid , quite a bizarre dream , indeed . xxmaj also , this is\n\n\n\n\n\nNow that our data is ready, we can fine-tune the pretrained language model.\n\n\nFine-Tuning the Language Model\nTo convert the integer word indices into activations that we can use for our neural network, we will use embeddings, just like we did for collaborative filtering and tabular modeling. Then we’ll feed those embeddings into a recurrent neural network (RNN), using an architecture called AWD-LSTM (we will show you how to write such a model from scratch in &lt;&gt;). As we discussed earlier, the embeddings in the pretrained model are merged with random embeddings added for words that weren’t in the pretraining vocabulary. This is handled automatically inside language_model_learner:\n\nlearn = language_model_learner(\n    dls_lm, AWD_LSTM, drop_mult=0.3, \n    metrics=[accuracy, Perplexity()]).to_fp16()\n\nThe loss function used by default is cross-entropy loss, since we essentially have a classification problem (the different categories being the words in our vocab). The perplexity metric used here is often used in NLP for language models: it is the exponential of the loss (i.e., torch.exp(cross_entropy)). We also include the accuracy metric, to see how many times our model is right when trying to predict the next word, since cross-entropy (as we’ve seen) is both hard to interpret, and tells us more about the model’s confidence than its accuracy.\nLet’s go back to the process diagram from the beginning of this chapter. The first arrow has been completed for us and made available as a pretrained model in fastai, and we’ve just built the DataLoaders and Learner for the second stage. Now we’re ready to fine-tune our language model!\n\nIt takes quite a while to train each epoch, so we’ll be saving the intermediate model results during the training process. Since fine_tune doesn’t do that for us, we’ll use fit_one_cycle. Just like vision_learner, language_model_learner automatically calls freeze when using a pretrained model (which is the default), so this will only train the embeddings (the only part of the model that contains randomly initialized weights—i.e., embeddings for words that are in our IMDb vocab, but aren’t in the pretrained model vocab):\n\nlearn.fit_one_cycle(1, 2e-2)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n4.120048\n3.912788\n0.299565\n50.038246\n11:39\n\n\n\n\n\nThis model takes a while to train, so it’s a good opportunity to talk about saving intermediary results.\n\n\nSaving and Loading Models\nYou can easily save the state of your model like so:\n\nlearn.save('1epoch')\n\nThis will create a file in learn.path/models/ named 1epoch.pth. If you want to load your model in another machine after creating your Learner the same way, or resume training later, you can load the content of this file with:\n\nlearn = learn.load('1epoch')\n\nOnce the initial training has completed, we can continue fine-tuning the model after unfreezing:\n\nlearn.unfreeze()\nlearn.fit_one_cycle(10, 2e-3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n3.893486\n3.772820\n0.317104\n43.502548\n12:37\n\n\n1\n3.820479\n3.717197\n0.323790\n41.148880\n12:30\n\n\n2\n3.735622\n3.659760\n0.330321\n38.851997\n12:09\n\n\n3\n3.677086\n3.624794\n0.333960\n37.516987\n12:12\n\n\n4\n3.636646\n3.601300\n0.337017\n36.645859\n12:05\n\n\n5\n3.553636\n3.584241\n0.339355\n36.026001\n12:04\n\n\n6\n3.507634\n3.571892\n0.341353\n35.583862\n12:08\n\n\n7\n3.444101\n3.565988\n0.342194\n35.374371\n12:08\n\n\n8\n3.398597\n3.566283\n0.342647\n35.384815\n12:11\n\n\n9\n3.375563\n3.568166\n0.342528\n35.451500\n12:05\n\n\n\n\n\nOnce this is done, we save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the encoder. We can save it with save_encoder:\n\nlearn.save_encoder('finetuned')\n\n\njargon: Encoder: The model not including the task-specific final layer(s). This term means much the same thing as body when applied to vision CNNs, but “encoder” tends to be more used for NLP and generative models.\n\nThis completes the second stage of the text classification process: fine-tuning the language model. We can now use it to fine-tune a classifier using the IMDb sentiment labels.\n\n\nText Generation\nBefore we move on to fine-tuning the classifier, let’s quickly try something different: using our model to generate random reviews. Since it’s trained to guess what the next word of the sentence is, we can use the model to write new reviews:\n\nTEXT = \"I liked this movie because\"\nN_WORDS = 40\nN_SENTENCES = 2\npreds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n         for _ in range(N_SENTENCES)]\n\n\n\n\n\n\n\n\nprint(\"\\n\".join(preds))\n\ni liked this movie because of its story and characters . The story line was very strong , very good for a sci - fi film . The main character , Alucard , was very well developed and brought the whole story\ni liked this movie because i like the idea of the premise of the movie , the ( very ) convenient virus ( which , when you have to kill a few people , the \" evil \" machine has to be used to protect\n\n\nAs you can see, we add some randomness (we pick a random word based on the probabilities returned by the model) so we don’t get exactly the same review twice. Our model doesn’t have any programmed knowledge of the structure of a sentence or grammar rules, yet it has clearly learned a lot about English sentences: we can see it capitalizes properly (I is just transformed to i because our rules require two characters or more to consider a word as capitalized, so it’s normal to see it lowercased) and is using consistent tense. The general review makes sense at first glance, and it’s only if you read carefully that you can notice something is a bit off. Not bad for a model trained in a couple of hours!\nBut our end goal wasn’t to train a model to generate reviews, but to classify them… so let’s use this model to do just that.\n\n\nCreating the Classifier DataLoaders\nWe’re now moving from language model fine-tuning to classifier fine-tuning. To recap, a language model predicts the next word of a document, so it doesn’t need any external labels. A classifier, however, predicts some external label—in the case of IMDb, it’s the sentiment of a document.\nThis means that the structure of our DataBlock for NLP classification will look very familiar. It’s actually nearly the same as we’ve seen for the many image classification datasets we’ve worked with:\n\ndls_clas = DataBlock(\n    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n    get_y = parent_label,\n    get_items=partial(get_text_files, folders=['train', 'test']),\n    splitter=GrandparentSplitter(valid_name='test')\n).dataloaders(path, path=path, bs=128, seq_len=72)\n\nJust like with image classification, show_batch shows the dependent variable (sentiment, in this case) with each independent variable (movie review text):\n\ndls_clas.show_batch(max_n=3)\n\n\n\n\n\ntext\ncategory\n\n\n\n\n0\nxxbos i rate this movie with 3 skulls , only coz the girls knew how to scream , this could 've been a better movie , if actors were better , the twins were xxup ok , i believed they were evil , but the eldest and youngest brother , they sucked really bad , it seemed like they were reading the scripts instead of acting them … . spoiler : if they 're vampire 's why do they freeze the blood ? vampires ca n't drink frozen blood , the sister in the movie says let 's drink her while she is alive … .but then when they 're moving to another house , they take on a cooler they 're frozen blood . end of spoiler \\n\\n it was a huge waste of time , and that made me mad coz i read all the reviews of how\nneg\n\n\n1\nxxbos i have read all of the xxmaj love xxmaj come xxmaj softly books . xxmaj knowing full well that movies can not use all aspects of the book , but generally they at least have the main point of the book . i was highly disappointed in this movie . xxmaj the only thing that they have in this movie that is in the book is that xxmaj missy 's father comes to xxunk in the book both parents come ) . xxmaj that is all . xxmaj the story line was so twisted and far fetch and yes , sad , from the book , that i just could n't enjoy it . xxmaj even if i did n't read the book it was too sad . i do know that xxmaj pioneer life was rough , but the whole movie was a downer . xxmaj the rating\nneg\n\n\n2\nxxbos xxmaj this , for lack of a better term , movie is lousy . xxmaj where do i start … … \\n\\n xxmaj cinemaphotography - xxmaj this was , perhaps , the worst xxmaj i 've seen this year . xxmaj it looked like the camera was being tossed from camera man to camera man . xxmaj maybe they only had one camera . xxmaj it gives you the sensation of being a volleyball . \\n\\n xxmaj there are a bunch of scenes , haphazardly , thrown in with no continuity at all . xxmaj when they did the ' split screen ' , it was absurd . xxmaj everything was squished flat , it looked ridiculous . \\n\\n xxmaj the color tones were way off . xxmaj these people need to learn how to balance a camera . xxmaj this ' movie ' is poorly made , and\nneg\n\n\n\n\n\nLooking at the DataBlock definition, every piece is familiar from previous data blocks we’ve built, with two important exceptions:\n\nTextBlock.from_folder no longer has the is_lm=True parameter.\nWe pass the vocab we created for the language model fine-tuning.\n\nThe reason that we pass the vocab of the language model is to make sure we use the same correspondence of token to index. Otherwise the embeddings we learned in our fine-tuned language model won’t make any sense to this model, and the fine-tuning step won’t be of any use.\nBy passing is_lm=False (or not passing is_lm at all, since it defaults to False) we tell TextBlock that we have regular labeled data, rather than using the next tokens as labels. There is one challenge we have to deal with, however, which is to do with collating multiple documents into a mini-batch. Let’s see with an example, by trying to create a mini-batch containing the first 10 documents. First we’ll numericalize them:\n\nnums_samp = toks200[:10].map(num)\n\nLet’s now look at how many tokens each of these 10 movie reviews have:\n\nnums_samp.map(len)\n\n(#10) [228,238,121,290,196,194,533,124,581,155]\n\n\nRemember, PyTorch DataLoaders need to collate all the items in a batch into a single tensor, and a single tensor has a fixed shape (i.e., it has some particular length on every axis, and all items must be consistent). This should sound familiar: we had the same issue with images. In that case, we used cropping, padding, and/or squishing to make all the inputs the same size. Cropping might not be a good idea for documents, because it seems likely we’d remove some key information (having said that, the same issue is true for images, and we use cropping there; data augmentation hasn’t been well explored for NLP yet, so perhaps there are actually opportunities to use cropping in NLP too!). You can’t really “squish” a document. So that leaves padding!\nWe will expand the shortest texts to make them all the same size. To do this, we use a special padding token that will be ignored by our model. Additionally, to avoid memory issues and improve performance, we will batch together texts that are roughly the same lengths (with some shuffling for the training set). We do this by (approximately, for the training set) sorting the documents by length prior to each epoch. The result of this is that the documents collated into a single batch will tend to be of similar lengths. We won’t pad every batch to the same size, but will instead use the size of the largest document in each batch as the target size. (It is possible to do something similar with images, which is especially useful for irregularly sized rectangular images, but at the time of writing no library provides good support for this yet, and there aren’t any papers covering it. It’s something we’re planning to add to fastai soon, however, so keep an eye on the book’s website; we’ll add information about this as soon as we have it working well.)\nThe sorting and padding are automatically done by the data block API for us when using a TextBlock, with is_lm=False. (We don’t have this same issue for language model data, since we concatenate all the documents together first, and then split them into equally sized sections.)\nWe can now create a model to classify our texts:\n\nlearn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n                                metrics=accuracy).to_fp16()\n\nThe final step prior to training the classifier is to load the encoder from our fine-tuned language model. We use load_encoder instead of load because we only have pretrained weights available for the encoder; load by default raises an exception if an incomplete model is loaded:\n\nlearn = learn.load_encoder('finetuned')\n\n\n\nFine-Tuning the Classifier\nThe last step is to train with discriminative learning rates and gradual unfreezing. In computer vision we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference:\n\nlearn.fit_one_cycle(1, 2e-2)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.347427\n0.184480\n0.929320\n00:33\n\n\n\n\n\nIn just one epoch we get the same result as our training in &lt;&gt;: not too bad! We can pass -2 to freeze_to to freeze all except the last two parameter groups:\n\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.247763\n0.171683\n0.934640\n00:37\n\n\n\n\n\nThen we can unfreeze a bit more, and continue training:\n\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.193377\n0.156696\n0.941200\n00:45\n\n\n\n\n\nAnd finally, the whole model!\n\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.172888\n0.153770\n0.943120\n01:01\n\n\n1\n0.161492\n0.155567\n0.942640\n00:57\n\n\n\n\n\nWe reached 94.3% accuracy, which was state-of-the-art performance just three years ago. By training another model on all the texts read backwards and averaging the predictions of those two models, we can even get to 95.1% accuracy, which was the state of the art introduced by the ULMFiT paper. It was only beaten a few months ago, by fine-tuning a much bigger model and using expensive data augmentation techniques (translating sentences in another language and back, using another model for translation).\nUsing a pretrained model let us build a fine-tuned language model that was pretty powerful, to either generate fake reviews or help classify them. This is exciting stuff, but it’s good to remember that this technology can also be used for malign purposes."
  },
  {
    "objectID": "Fastbook/10_nlp.html#disinformation-and-language-models",
    "href": "Fastbook/10_nlp.html#disinformation-and-language-models",
    "title": "NLP Deep Dive: RNNs",
    "section": "Disinformation and Language Models",
    "text": "Disinformation and Language Models\nEven simple algorithms based on rules, before the days of widely available deep learning language models, could be used to create fraudulent accounts and try to influence policymakers. Jeff Kao, now a computational journalist at ProPublica, analyzed the comments that were sent to the US Federal Communications Commission (FCC) regarding a 2017 proposal to repeal net neutrality. In his article “More than a Million Pro-Repeal Net Neutrality Comments Were Likely Faked”, he reports how he discovered a large cluster of comments opposing net neutrality that seemed to have been generated by some sort of Mad Libs-style mail merge. In &lt;&gt;, the fake comments have been helpfully color-coded by Kao to highlight their formulaic nature.\n\nKao estimated that “less than 800,000 of the 22M+ comments… could be considered truly unique” and that “more than 99% of the truly unique comments were in favor of keeping net neutrality.”\nGiven advances in language modeling that have occurred since 2017, such fraudulent campaigns could be nearly impossible to catch now. You now have all the necessary tools at your disposal to create a compelling language model—that is, something that can generate context-appropriate, believable text. It won’t necessarily be perfectly accurate or correct, but it will be plausible. Think about what this technology would mean when put together with the kinds of disinformation campaigns we have learned about in recent years. Take a look at the Reddit dialogue shown in &lt;&gt;, where a language model based on OpenAI’s GPT-2 algorithm is having a conversation with itself about whether the US government should cut defense spending.\n\nIn this case, it was explicitly said that an algorithm was used, but imagine what would happen if a bad actor decided to release such an algorithm across social networks. They could do it slowly and carefully, allowing the algorithm to gradually develop followers and trust over time. It would not take many resources to have literally millions of accounts doing this. In such a situation we could easily imagine getting to a point where the vast majority of discourse online was from bots, and nobody would have any idea that it was happening.\nWe are already starting to see examples of machine learning being used to generate identities. For example, &lt;&gt; shows a LinkedIn profile for Katie Jones.\n\nKatie Jones was connected on LinkedIn to several members of mainstream Washington think tanks. But she didn’t exist. That image you see was auto-generated by a generative adversarial network, and somebody named Katie Jones has not, in fact, graduated from the Center for Strategic and International Studies.\nMany people assume or hope that algorithms will come to our defense here—that we will develop classification algorithms that can automatically recognise autogenerated content. The problem, however, is that this will always be an arms race, in which better classification (or discriminator) algorithms can be used to create better generation algorithms."
  },
  {
    "objectID": "Fastbook/10_nlp.html#conclusion",
    "href": "Fastbook/10_nlp.html#conclusion",
    "title": "NLP Deep Dive: RNNs",
    "section": "Conclusion",
    "text": "Conclusion\nIn this chapter we explored the last application covered out of the box by the fastai library: text. We saw two types of models: language models that can generate texts, and a classifier that determines if a review is positive or negative. To build a state-of-the art classifier, we used a pretrained language model, fine-tuned it to the corpus of our task, then used its body (the encoder) with a new head to do the classification.\nBefore we end this section, we’ll take a look at how the fastai library can help you assemble your data for your specific problems."
  },
  {
    "objectID": "Fastbook/10_nlp.html#questionnaire",
    "href": "Fastbook/10_nlp.html#questionnaire",
    "title": "NLP Deep Dive: RNNs",
    "section": "Questionnaire",
    "text": "Questionnaire\n\nWhat is “self-supervised learning”?\nWhat is a “language model”?\nWhy is a language model considered self-supervised?\nWhat are self-supervised models usually used for?\nWhy do we fine-tune language models?\nWhat are the three steps to create a state-of-the-art text classifier?\nHow do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset?\nWhat are the three steps to prepare your data for a language model?\nWhat is “tokenization”? Why do we need it?\nName three different approaches to tokenization.\nWhat is xxbos?\nList four rules that fastai applies to text during tokenization.\nWhy are repeated characters replaced with a token showing the number of repetitions and the character that’s repeated?\nWhat is “numericalization”?\nWhy might there be words that are replaced with the “unknown word” token?\nWith a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful—students often get this one wrong! Be sure to check your answer on the book’s website.)\nWhy do we need padding for text classification? Why don’t we need it for language modeling?\nWhat does an embedding matrix for NLP contain? What is its shape?\nWhat is “perplexity”?\nWhy do we have to pass the vocabulary of the language model to the classifier data block?\nWhat is “gradual unfreezing”?\nWhy is text generation always likely to be ahead of automatic identification of machine-generated texts?\n\n\nFurther Research\n\nSee what you can learn about language models and disinformation. What are the best language models today? Take a look at some of their outputs. Do you find them convincing? How could a bad actor best use such a model to create conflict and uncertainty?\nGiven the limitation that models are unlikely to be able to consistently recognize machine-generated texts, what other approaches may be needed to handle large-scale disinformation campaigns that leverage deep learning?"
  },
  {
    "objectID": "Fastbook/README_pt.html",
    "href": "Fastbook/README_pt.html",
    "title": "O Livro fastai",
    "section": "",
    "text": "English / Spanish / Korean / Chinese / Bengali / Indonesian / Italian / Portuguese / Vietnamese / Japanese"
  },
  {
    "objectID": "Fastbook/README_pt.html#citações",
    "href": "Fastbook/README_pt.html#citações",
    "title": "O Livro fastai",
    "section": "Citações",
    "text": "Citações\nSe deseja citar o livre, pode fazê-lo da seguinte forma:\n@book{howard2020deep,\ntitle={Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD},\nauthor={Howard, J. and Gugger, S.},\nisbn={9781492045526},\nurl={https://books.google.no/books?id=xd6LxgEACAAJ},\nyear={2020},\npublisher={O'Reilly Media, Incorporated}\n}"
  }
]