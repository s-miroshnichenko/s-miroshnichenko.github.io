<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>The Training Process – Сергей Мирошниченко</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-c00b42e811b0fd9a18ac62455ba22490.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Сергей Мирошниченко</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../posts.html"> 
<span class="menu-text">Блог</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">The Training Process</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div id="cell-1" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#hide</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span> [ <span class="op">-</span>e <span class="op">/</span>content ] <span class="op">&amp;&amp;</span> pip install <span class="op">-</span>Uqq fastbook</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fastbook</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>fastbook.setup_book()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-2" class="cell" data-hide_input="false">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">#hide</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastbook <span class="im">import</span> <span class="op">*</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>[[chapter_accel_sgd]]</p>
<p>You now know how to create state-of-the-art architectures for computer vision, natural language processing, tabular analysis, and collaborative filtering, and you know how to train them quickly. So we’re done, right? Not quite yet. We still have to explore a little bit more the training process.</p>
<p>We explained in &lt;<chapter_mnist_basics>&gt; the basis of stochastic gradient descent: pass a mini-batch to the model, compare it to our target with the loss function, then compute the gradients of this loss function with regard to each weight before updating the weights with the formula:</chapter_mnist_basics></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>new_weight <span class="op">=</span> weight <span class="op">-</span> lr <span class="op">*</span> weight.grad</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>We implemented this from scratch in a training loop, and also saw that PyTorch provides a simple <code>nn.SGD</code> class that does this calculation for each parameter for us. In this chapter we will build some faster optimizers, using a flexible foundation. But that’s not all we might want to change in the training process. For any tweak of the training loop, we will need a way to add some code to the basis of SGD. The fastai library has a system of callbacks to do this, and we will teach you all about it.</p>
<p>Let’s start with standard SGD to get a baseline, then we will introduce the most commonly used optimizers.</p>
<section id="establishing-a-baseline" class="level2">
<h2 class="anchored" data-anchor-id="establishing-a-baseline">Establishing a Baseline</h2>
<p>First, we’ll create a baseline, using plain SGD, and compare it to fastai’s default optimizer. We’ll start by grabbing Imagenette with the same <code>get_data</code> we used in &lt;<chapter_resnet>&gt;:</chapter_resnet></p>
<div id="cell-8" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">#hide_input</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_data(url, presize, resize):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    path <span class="op">=</span> untar_data(url)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> DataBlock(</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        blocks<span class="op">=</span>(ImageBlock, CategoryBlock), get_items<span class="op">=</span>get_image_files, </span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        splitter<span class="op">=</span>GrandparentSplitter(valid_name<span class="op">=</span><span class="st">'val'</span>),</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        get_y<span class="op">=</span>parent_label, item_tfms<span class="op">=</span>Resize(presize),</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        batch_tfms<span class="op">=</span>[<span class="op">*</span>aug_transforms(min_scale<span class="op">=</span><span class="fl">0.5</span>, size<span class="op">=</span>resize),</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>                    Normalize.from_stats(<span class="op">*</span>imagenet_stats)],</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    ).dataloaders(path, bs<span class="op">=</span><span class="dv">128</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-9" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> get_data(URLs.IMAGENETTE_160, <span class="dv">160</span>, <span class="dv">128</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We’ll create a ResNet-34 without pretraining, and pass along any arguments received:</p>
<div id="cell-11" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_learner(<span class="op">**</span>kwargs):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> vision_learner(dls, resnet34, pretrained<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>                    metrics<span class="op">=</span>accuracy, <span class="op">**</span>kwargs).to_fp16()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Here’s the default fastai optimizer, with the usual 3e-3 learning rate:</p>
<div id="cell-13" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> get_learner()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">3</span>, <span class="fl">0.003</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>2.571932</td>
<td>2.685040</td>
<td>0.322548</td>
<td>00:11</td>
</tr>
<tr class="even">
<td>1</td>
<td>1.904674</td>
<td>1.852589</td>
<td>0.437452</td>
<td>00:11</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1.586909</td>
<td>1.374908</td>
<td>0.594904</td>
<td>00:11</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Now let’s try plain SGD. We can pass <code>opt_func</code> (optimization function) to <code>vision_learner</code> to get fastai to use any optimizer:</p>
<div id="cell-15" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> get_learner(opt_func<span class="op">=</span>SGD)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The first thing to look at is <code>lr_find</code>:</p>
<div id="cell-17" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>learn.lr_find()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="16_accel_sgd_files/figure-html/cell-9-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It looks like we’ll need to use a higher learning rate than we normally use:</p>
<div id="cell-19" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">3</span>, <span class="fl">0.03</span>, moms<span class="op">=</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>2.969412</td>
<td>2.214596</td>
<td>0.242038</td>
<td>00:09</td>
</tr>
<tr class="even">
<td>1</td>
<td>2.442730</td>
<td>1.845950</td>
<td>0.362548</td>
<td>00:09</td>
</tr>
<tr class="odd">
<td>2</td>
<td>2.157159</td>
<td>1.741143</td>
<td>0.408917</td>
<td>00:09</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Because accelerating SGD with momentum is such a good idea, fastai does this by default in <code>fit_one_cycle</code>, so we turn it off with <code>moms=(0,0,0)</code>. We’ll be discussing momentum shortly.)</p>
<p>Clearly, plain SGD isn’t training as fast as we’d like. So let’s learn some tricks to get accelerated training!</p>
</section>
<section id="a-generic-optimizer" class="level2">
<h2 class="anchored" data-anchor-id="a-generic-optimizer">A Generic Optimizer</h2>
<p>To build up our accelerated SGD tricks, we’ll need to start with a nice flexible optimizer foundation. No library prior to fastai provided such a foundation, but during fastai’s development we realized that all the optimizer improvements we’d seen in the academic literature could be handled using <em>optimizer callbacks</em>. These are small pieces of code that we can compose, mix and match in an optimizer to build the optimizer <code>step</code>. They are called by fastai’s lightweight <code>Optimizer</code> class. These are the definitions in <code>Optimizer</code> of the two key methods that we’ve been using in this book:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> zero_grad(<span class="va">self</span>):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p,<span class="op">*</span>_ <span class="kw">in</span> <span class="va">self</span>.all_params():</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>        p.grad.detach_()</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>        p.grad.zero_()</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p,pg,state,hyper <span class="kw">in</span> <span class="va">self</span>.all_params():</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> cb <span class="kw">in</span> <span class="va">self</span>.cbs:</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>            state <span class="op">=</span> _update(state, cb(p, <span class="op">**</span>{<span class="op">**</span>state, <span class="op">**</span>hyper}))</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.state[p] <span class="op">=</span> state</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>As we saw when training an MNIST model from scratch, <code>zero_grad</code> just loops through the parameters of the model and sets the gradients to zero. It also calls <code>detach_</code>, which removes any history of gradient computation, since it won’t be needed after <code>zero_grad</code>.</p>
<p>The more interesting method is <code>step</code>, which loops through the callbacks (<code>cbs</code>) and calls them to update the parameters (the <code>_update</code> function just calls <code>state.update</code> if there’s anything returned by <code>cb</code>). As you can see, <code>Optimizer</code> doesn’t actually do any SGD steps itself. Let’s see how we can add SGD to <code>Optimizer</code>.</p>
<p>Here’s an optimizer callback that does a single SGD step, by multiplying <code>-lr</code> by the gradients and adding that to the parameter (when <code>Tensor.add_</code> in PyTorch is passed two parameters, they are multiplied together before the addition):</p>
<div id="cell-24" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sgd_cb(p, lr, <span class="op">**</span>kwargs): p.data.add_(<span class="op">-</span>lr, p.grad.data)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We can pass this to <code>Optimizer</code> using the <code>cbs</code> parameter; we’ll need to use <code>partial</code> since <code>Learner</code> will call this function to create our optimizer later:</p>
<div id="cell-26" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>opt_func <span class="op">=</span> partial(Optimizer, cbs<span class="op">=</span>[sgd_cb])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Let’s see if this trains:</p>
<div id="cell-28" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> get_learner(opt_func<span class="op">=</span>opt_func)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>learn.fit(<span class="dv">3</span>, <span class="fl">0.03</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>2.730918</td>
<td>2.009971</td>
<td>0.332739</td>
<td>00:09</td>
</tr>
<tr class="even">
<td>1</td>
<td>2.204893</td>
<td>1.747202</td>
<td>0.441529</td>
<td>00:09</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1.875621</td>
<td>1.684515</td>
<td>0.445350</td>
<td>00:09</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>It’s working! So that’s how we create SGD from scratch in fastai. Now let’s see what “momentum” is.</p>
</section>
<section id="momentum" class="level2">
<h2 class="anchored" data-anchor-id="momentum">Momentum</h2>
<p>As described in &lt;<chapter_mnist_basics>&gt;, SGD can be thought of as standing at the top of a mountain and working your way down by taking a step in the direction of the steepest slope at each point in time. But what if we have a ball rolling down the mountain? It won’t, at each given point, exactly follow the direction of the gradient, as it will have <em>momentum</em>. A ball with more momentum (for instance, a heavier ball) will skip over little bumps and holes, and be more likely to get to the bottom of a bumpy mountain. A ping pong ball, on the other hand, will get stuck in every little crevice.</chapter_mnist_basics></p>
<p>So how can we bring this idea over to SGD? We can use a moving average, instead of only the current gradient, to make our step:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>weight.avg <span class="op">=</span> beta <span class="op">*</span> weight.avg <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>beta) <span class="op">*</span> weight.grad</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>new_weight <span class="op">=</span> weight <span class="op">-</span> lr <span class="op">*</span> weight.avg</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Here <code>beta</code> is some number we choose which defines how much momentum to use. If <code>beta</code> is 0, then the first equation becomes <code>weight.avg = weight.grad</code>, so we end up with plain SGD. But if it’s a number close to 1, then the main direction chosen is an average of the previous steps. (If you have done a bit of statistics, you may recognize in the first equation an <em>exponentially weighted moving average</em>, which is very often used to denoise data and get the underlying tendency.)</p>
<p>Note that we are writing <code>weight.avg</code> to highlight the fact that we need to store the moving averages for each parameter of the model (they all have their own independent moving averages).</p>
<p>&lt;<img_momentum>&gt; shows an example of noisy data for a single parameter, with the momentum curve plotted in red, and the gradients of the parameter plotted in blue. The gradients increase, then decrease, and the momentum does a good job of following the general trend without getting too influenced by noise.</img_momentum></p>
<div id="cell-32" class="cell" data-hide_input="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">#hide_input</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="co">#id img_momentum</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co">#caption An example of momentum</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co">#alt Graph showing an example of momentum</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">100</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> (x<span class="op">/</span><span class="dv">3</span>) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>x1 <span class="op">=</span> x <span class="op">+</span> np.random.randn(<span class="dv">100</span>) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>y1 <span class="op">=</span> y <span class="op">+</span> np.random.randn(<span class="dv">100</span>) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>plt.scatter(x1,y1)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> x1.argsort()</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>beta,avg,res <span class="op">=</span> <span class="fl">0.7</span>,<span class="dv">0</span>,[]</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> idx:</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    avg <span class="op">=</span> beta <span class="op">*</span> avg <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>beta) <span class="op">*</span> y1[i]</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    res.append(avg<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>beta<span class="op">**</span>(i<span class="op">+</span><span class="dv">1</span>)))</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>plt.plot(x1[idx],np.array(res), color<span class="op">=</span><span class="st">'red'</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="16_accel_sgd_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It works particularly well if the loss function has narrow canyons we need to navigate: vanilla SGD would send us bouncing from one side to the other, while SGD with momentum will average those to roll smoothly down the side. The parameter <code>beta</code> determines the strength of the momentum we are using: with a small <code>beta</code> we stay closer to the actual gradient values, whereas with a high <code>beta</code> we will mostly go in the direction of the average of the gradients and it will take a while before any change in the gradients makes that trend move.</p>
<p>With a large <code>beta</code>, we might miss that the gradients have changed directions and roll over a small local minima. This is a desired side effect: intuitively, when we show a new input to our model, it will look like something in the training set but won’t be <em>exactly</em> like it. That means it will correspond to a point in the loss function that is close to the minimum we ended up with at the end of training, but not exactly <em>at</em> that minimum. So, we would rather end up training in a wide minimum, where nearby points have approximately the same loss (or if you prefer, a point where the loss is as flat as possible). &lt;<img_betas>&gt; shows how the chart in &lt;<img_momentum>&gt; varies as we change <code>beta</code>.</img_momentum></img_betas></p>
<div id="cell-34" class="cell" data-hide_input="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">#hide_input</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="co">#id img_betas</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co">#caption Momentum with different beta values</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co">#alt Graph showing how the beta value influences momentum</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">100</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> (x<span class="op">/</span><span class="dv">3</span>) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>x1 <span class="op">=</span> x <span class="op">+</span> np.random.randn(<span class="dv">100</span>) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>y1 <span class="op">=</span> y <span class="op">+</span> np.random.randn(<span class="dv">100</span>) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>_,axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>,<span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">8</span>))</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>betas <span class="op">=</span> [<span class="fl">0.5</span>,<span class="fl">0.7</span>,<span class="fl">0.9</span>,<span class="fl">0.99</span>]</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> x1.argsort()</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> beta,ax <span class="kw">in</span> <span class="bu">zip</span>(betas, axs.flatten()):</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    ax.scatter(x1,y1)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    avg,res <span class="op">=</span> <span class="dv">0</span>,[]</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> idx:</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>        avg <span class="op">=</span> beta <span class="op">*</span> avg <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>beta) <span class="op">*</span> y1[i]</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>        res.append(avg)<span class="co">#/(1-beta**(i+1)))</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    ax.plot(x1[idx],np.array(res), color<span class="op">=</span><span class="st">'red'</span>)<span class="op">;</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f'beta=</span><span class="sc">{</span>beta<span class="sc">}</span><span class="ss">'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="16_accel_sgd_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can see in these examples that a <code>beta</code> that’s too high results in the overall changes in gradient getting ignored. In SGD with momentum, a value of <code>beta</code> that is often used is 0.9.</p>
<p><code>fit_one_cycle</code> by default starts with a <code>beta</code> of 0.95, gradually adjusts it to 0.85, and then gradually moves it back to 0.95 at the end of training. Let’s see how our training goes with momentum added to plain SGD.</p>
<p>In order to add momentum to our optimizer, we’ll first need to keep track of the moving average gradient, which we can do with another callback. When an optimizer callback returns a <code>dict</code>, it is used to update the state of the optimizer and is passed back to the optimizer on the next step. So this callback will keep track of the gradient averages in a parameter called <code>grad_avg</code>:</p>
<div id="cell-37" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> average_grad(p, mom, grad_avg<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> grad_avg <span class="kw">is</span> <span class="va">None</span>: grad_avg <span class="op">=</span> torch.zeros_like(p.grad.data)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">'grad_avg'</span>: grad_avg<span class="op">*</span>mom <span class="op">+</span> p.grad.data}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>To use it, we just have to replace <code>p.grad.data</code> with <code>grad_avg</code> in our step function:</p>
<div id="cell-39" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> momentum_step(p, lr, grad_avg, <span class="op">**</span>kwargs): p.data.add_(<span class="op">-</span>lr, grad_avg)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-40" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>opt_func <span class="op">=</span> partial(Optimizer, cbs<span class="op">=</span>[average_grad,momentum_step], mom<span class="op">=</span><span class="fl">0.9</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><code>Learner</code> will automatically schedule <code>mom</code> and <code>lr</code>, so <code>fit_one_cycle</code> will even work with our custom <code>Optimizer</code>:</p>
<div id="cell-42" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> get_learner(opt_func<span class="op">=</span>opt_func)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">3</span>, <span class="fl">0.03</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>2.856000</td>
<td>2.493429</td>
<td>0.246115</td>
<td>00:10</td>
</tr>
<tr class="even">
<td>1</td>
<td>2.504205</td>
<td>2.463813</td>
<td>0.348280</td>
<td>00:10</td>
</tr>
<tr class="odd">
<td>2</td>
<td>2.187387</td>
<td>1.755670</td>
<td>0.418853</td>
<td>00:10</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="cell-43" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>learn.recorder.plot_sched()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="16_accel_sgd_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We’re still not getting great results, so let’s see what else we can do.</p>
</section>
<section id="rmsprop" class="level2">
<h2 class="anchored" data-anchor-id="rmsprop">RMSProp</h2>
<p>RMSProp is another variant of SGD introduced by Geoffrey Hinton in Lecture 6e of his Coursera class <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">“Neural Networks for Machine Learning”</a>. The main difference from SGD is that it uses an adaptive learning rate: instead of using the same learning rate for every parameter, each parameter gets its own specific learning rate controlled by a global learning rate. That way we can speed up training by giving a higher learning rate to the weights that need to change a lot while the ones that are good enough get a lower learning rate.</p>
<p>How do we decide which parameters should have a high learning rate and which should not? We can look at the gradients to get an idea. If a parameter’s gradients have been close to zero for a while, that parameter will need a higher learning rate because the loss is flat. On the other hand, if the gradients are all over the place, we should probably be careful and pick a low learning rate to avoid divergence. We can’t just average the gradients to see if they’re changing a lot, because the average of a large positive and a large negative number is close to zero. Instead, we can use the usual trick of either taking the absolute value or the squared values (and then taking the square root after the mean).</p>
<p>Once again, to determine the general tendency behind the noise, we will use a moving average—specifically the moving average of the gradients squared. Then we will update the corresponding weight by using the current gradient (for the direction) divided by the square root of this moving average (that way if it’s low, the effective learning rate will be higher, and if it’s high, the effective learning rate will be lower):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>w.square_avg <span class="op">=</span> alpha <span class="op">*</span> w.square_avg <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>alpha) <span class="op">*</span> (w.grad <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>new_w <span class="op">=</span> w <span class="op">-</span> lr <span class="op">*</span> w.grad <span class="op">/</span> math.sqrt(w.square_avg <span class="op">+</span> eps)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The <code>eps</code> (<em>epsilon</em>) is added for numerical stability (usually set at 1e-8), and the default value for <code>alpha</code> is usually 0.99.</p>
<p>We can add this to <code>Optimizer</code> by doing much the same thing we did for <code>avg_grad</code>, but with an extra <code>**2</code>:</p>
<div id="cell-48" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> average_sqr_grad(p, sqr_mom, sqr_avg<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> sqr_avg <span class="kw">is</span> <span class="va">None</span>: sqr_avg <span class="op">=</span> torch.zeros_like(p.grad.data)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">'sqr_avg'</span>: sqr_mom<span class="op">*</span>sqr_avg <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>sqr_mom)<span class="op">*</span>p.grad.data<span class="op">**</span><span class="dv">2</span>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>And we can define our step function and optimizer as before:</p>
<div id="cell-50" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rms_prop_step(p, lr, sqr_avg, eps, grad_avg<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    denom <span class="op">=</span> sqr_avg.sqrt().add_(eps)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    p.data.addcdiv_(<span class="op">-</span>lr, p.grad, denom)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>opt_func <span class="op">=</span> partial(Optimizer, cbs<span class="op">=</span>[average_sqr_grad,rms_prop_step],</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>                   sqr_mom<span class="op">=</span><span class="fl">0.99</span>, eps<span class="op">=</span><span class="fl">1e-7</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Let’s try it out:</p>
<div id="cell-52" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> get_learner(opt_func<span class="op">=</span>opt_func)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">3</span>, <span class="fl">0.003</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>2.766912</td>
<td>1.845900</td>
<td>0.402548</td>
<td>00:11</td>
</tr>
<tr class="even">
<td>1</td>
<td>2.194586</td>
<td>1.510269</td>
<td>0.504459</td>
<td>00:11</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1.869099</td>
<td>1.447939</td>
<td>0.544968</td>
<td>00:11</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Much better! Now we just have to bring these ideas together, and we have Adam, fastai’s default optimizer.</p>
</section>
<section id="adam" class="level2">
<h2 class="anchored" data-anchor-id="adam">Adam</h2>
<p>Adam mixes the ideas of SGD with momentum and RMSProp together: it uses the moving average of the gradients as a direction and divides by the square root of the moving average of the gradients squared to give an adaptive learning rate to each parameter.</p>
<p>There is one other difference in how Adam calculates moving averages. It takes the <em>unbiased</em> moving average, which is:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>w.avg <span class="op">=</span> beta <span class="op">*</span> w.avg <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>beta) <span class="op">*</span> w.grad</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>unbias_avg <span class="op">=</span> w.avg <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> (beta<span class="op">**</span>(i<span class="op">+</span><span class="dv">1</span>)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>if we are the <code>i</code>-th iteration (starting at 0 like Python does). This divisor of <code>1 - (beta**(i+1))</code> makes sure the unbiased average looks more like the gradients at the beginning (since <code>beta &lt; 1</code>, the denominator is very quickly close to 1).</p>
<p>Putting everything together, our update step looks like:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>w.avg <span class="op">=</span> beta1 <span class="op">*</span> w.avg <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>beta1) <span class="op">*</span> w.grad</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>unbias_avg <span class="op">=</span> w.avg <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> (beta1<span class="op">**</span>(i<span class="op">+</span><span class="dv">1</span>)))</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>w.sqr_avg <span class="op">=</span> beta2 <span class="op">*</span> w.sqr_avg <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>beta2) <span class="op">*</span> (w.grad <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>new_w <span class="op">=</span> w <span class="op">-</span> lr <span class="op">*</span> unbias_avg <span class="op">/</span> sqrt(w.sqr_avg <span class="op">+</span> eps)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Like for RMSProp, <code>eps</code> is usually set to 1e-8, and the default for <code>(beta1,beta2)</code> suggested by the literature is <code>(0.9,0.999)</code>.</p>
<p>In fastai, Adam is the default optimizer we use since it allows faster training, but we’ve found that <code>beta2=0.99</code> is better suited to the type of schedule we are using. <code>beta1</code> is the momentum parameter, which we specify with the argument <code>moms</code> in our call to <code>fit_one_cycle</code>. As for <code>eps</code>, fastai uses a default of 1e-5. <code>eps</code> is not just useful for numerical stability. A higher <code>eps</code> limits the maximum value of the adjusted learning rate. To take an extreme example, if <code>eps</code> is 1, then the adjusted learning will never be higher than the base learning rate.</p>
<p>Rather than show all the code for this in the book, we’ll let you look at the optimizer notebook in <a href="https://github.com/fastai/fastai">fastai’s GitHub repository</a> (browse the <em>nbs</em> folder and search for the notebook called optimizer). You’ll see all the code we’ve shown so far, along with Adam and other optimizers, and lots of examples and tests.</p>
<p>One thing that changes when we go from SGD to Adam is the way we apply weight decay, and it can have important consequences.</p>
</section>
<section id="decoupled-weight-decay" class="level2">
<h2 class="anchored" data-anchor-id="decoupled-weight-decay">Decoupled Weight Decay</h2>
<p>Weight decay, which we discussed in &lt;<chapter_collab>&gt;, is equivalent to (in the case of vanilla SGD) updating the parameters with:</chapter_collab></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>new_weight <span class="op">=</span> weight <span class="op">-</span> lr<span class="op">*</span>weight.grad <span class="op">-</span> lr<span class="op">*</span>wd<span class="op">*</span>weight</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The last part of this formula explains the name of this technique: each weight is decayed by a factor <code>lr * wd</code>.</p>
<p>The other name of weight decay is L2 regularization, which consists in adding the sum of all squared weights to the loss (multiplied by the weight decay). As we have seen in &lt;<chapter_collab>&gt;, this can be directly expressed on the gradients with:</chapter_collab></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>weight.grad <span class="op">+=</span> wd<span class="op">*</span>weight</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>For SGD, those two formulas are equivalent. However, this equivalence only holds for standard SGD, because we have seen that with momentum, RMSProp or in Adam, the update has some additional formulas around the gradient.</p>
<p>Most libraries use the second formulation, but it was pointed out in <a href="https://arxiv.org/pdf/1711.05101.pdf">“Decoupled Weight Decay Regularization”</a> by Ilya Loshchilov and Frank Hutter, that the first one is the only correct approach with the Adam optimizer or momentum, which is why fastai makes it its default.</p>
<p>Now you know everything that is hidden behind the line <code>learn.fit_one_cycle</code>!</p>
<p>Optimizers are only one part of the training process, however when you need to change the training loop with fastai, you can’t directly change the code inside the library. Instead, we have designed a system of callbacks to let you write any tweaks you like in independent blocks that you can then mix and match.</p>
</section>
<section id="callbacks" class="level2">
<h2 class="anchored" data-anchor-id="callbacks">Callbacks</h2>
<p>Sometimes you need to change how things work a little bit. In fact, we have already seen examples of this: Mixup, fp16 training, resetting the model after each epoch for training RNNs, and so forth. How do we go about making these kinds of tweaks to the training process?</p>
<p>We’ve seen the basic training loop, which, with the help of the <code>Optimizer</code> class, looks like this for a single epoch:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> xb,yb <span class="kw">in</span> dl:</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_func(model(xb), yb)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    opt.step()</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    opt.zero_grad()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>&lt;<basic_loop>&gt; shows how to picture that.</basic_loop></p>
<p><img alt="Basic training loop" width="300" caption="Basic training loop" id="basic_loop" src="images/att_00048.png"></p>
<p>The usual way for deep learning practitioners to customize the training loop is to make a copy of an existing training loop, and then insert the code necessary for their particular changes into it. This is how nearly all code that you find online will look. But it has some very serious problems.</p>
<p>It’s not very likely that some particular tweaked training loop is going to meet your particular needs. There are hundreds of changes that can be made to a training loop, which means there are billions and billions of possible permutations. You can’t just copy one tweak from a training loop here, another from a training loop there, and expect them all to work together. Each will be based on different assumptions about the environment that it’s working in, use different naming conventions, and expect the data to be in different formats.</p>
<p>We need a way to allow users to insert their own code at any part of the training loop, but in a consistent and well-defined way. Computer scientists have already come up with an elegant solution: the callback. A callback is a piece of code that you write, and inject into another piece of code at some predefined point. In fact, callbacks have been used with deep learning training loops for years. The problem is that in previous libraries it was only possible to inject code in a small subset of places where this may have been required, and, more importantly, callbacks were not able to do all the things they needed to do.</p>
<p>In order to be just as flexible as manually copying and pasting a training loop and directly inserting code into it, a callback must be able to read every possible piece of information available in the training loop, modify all of it as needed, and fully control when a batch, epoch, or even the whole training loop should be terminated. fastai is the first library to provide all of this functionality. It modifies the training loop so it looks like &lt;<cb_loop>&gt;.</cb_loop></p>
<p><img alt="Training loop with callbacks" width="550" caption="Training loop with callbacks" id="cb_loop" src="images/att_00049.png"></p>
<p>The real effectiveness of this approach has been borne out over the last couple of years—it has turned out that, by using the fastai callback system, we were able to implement every single new paper we tried and fulfilled every user request for modifying the training loop. The training loop itself has not required modifications. &lt;<some_cbs>&gt; shows just a few of the callbacks that have been added.</some_cbs></p>
<p><img alt="Some fastai callbacks" width="500" caption="Some fastai callbacks" id="some_cbs" src="images/att_00050.png"></p>
<p>The reason that this is important is because it means that whatever idea we have in our head, we can implement it. We need never dig into the source code of PyTorch or fastai and hack together some one-off system to try out our ideas. And when we do implement our own callbacks to develop our own ideas, we know that they will work together with all of the other functionality provided by fastai–so we will get progress bars, mixed-precision training, hyperparameter annealing, and so forth.</p>
<p>Another advantage is that it makes it easy to gradually remove or add functionality and perform ablation studies. You just need to adjust the list of callbacks you pass along to your fit function.</p>
<p>As an example, here is the fastai source code that is run for each batch of the training loop:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>._split(b)<span class="op">;</span>                                  <span class="va">self</span>(<span class="st">'before_batch'</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.pred <span class="op">=</span> <span class="va">self</span>.model(<span class="op">*</span><span class="va">self</span>.xb)<span class="op">;</span>                <span class="va">self</span>(<span class="st">'after_pred'</span>)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.loss <span class="op">=</span> <span class="va">self</span>.loss_func(<span class="va">self</span>.pred, <span class="op">*</span><span class="va">self</span>.yb)<span class="op">;</span> <span class="va">self</span>(<span class="st">'after_loss'</span>)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.training: <span class="cf">return</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.loss.backward()<span class="op">;</span>                            <span class="va">self</span>(<span class="st">'after_backward'</span>)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.opt.step()<span class="op">;</span>                                 <span class="va">self</span>(<span class="st">'after_step'</span>)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.opt.zero_grad()</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> CancelBatchException:                         <span class="va">self</span>(<span class="st">'after_cancel_batch'</span>)</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="cf">finally</span>:                                             <span class="va">self</span>(<span class="st">'after_batch'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The calls of the form <code>self('...')</code> are where the callbacks are called. As you see, this happens after every step. The callback will receive the entire state of training, and can also modify it. For instance, the input data and target labels are in <code>self.xb</code> and <code>self.yb</code>, respectively; a callback can modify these to alter the data the training loop sees. It can also modify <code>self.loss</code>, or even the gradients.</p>
<p>Let’s see how this works in practice by writing a callback.</p>
<section id="creating-a-callback" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-callback">Creating a Callback</h3>
<p>When you want to write your own callback, the full list of available events is:</p>
<ul>
<li><code>before_fit</code>:: called before doing anything; ideal for initial setup.</li>
<li><code>before_epoch</code>:: called at the beginning of each epoch; useful for any behavior you need to reset at each epoch.</li>
<li><code>before_train</code>:: called at the beginning of the training part of an epoch.</li>
<li><code>before_batch</code>:: called at the beginning of each batch, just after drawing said batch. It can be used to do any setup necessary for the batch (like hyperparameter scheduling) or to change the input/target before it goes into the model (for instance, apply Mixup).</li>
<li><code>after_pred</code>:: called after computing the output of the model on the batch. It can be used to change that output before it’s fed to the loss function.</li>
<li><code>after_loss</code>:: called after the loss has been computed, but before the backward pass. It can be used to add penalty to the loss (AR or TAR in RNN training, for instance).</li>
<li><code>after_backward</code>:: called after the backward pass, but before the update of the parameters. It can be used to make changes to the gradients before said update (via gradient clipping, for instance).</li>
<li><code>after_step</code>:: called after the step and before the gradients are zeroed.</li>
<li><code>after_batch</code>:: called at the end of a batch, to perform any required cleanup before the next one.</li>
<li><code>after_train</code>:: called at the end of the training phase of an epoch.</li>
<li><code>before_validate</code>:: called at the beginning of the validation phase of an epoch; useful for any setup needed specifically for validation.</li>
<li><code>after_validate</code>:: called at the end of the validation part of an epoch.</li>
<li><code>after_epoch</code>:: called at the end of an epoch, for any cleanup before the next one.</li>
<li><code>after_fit</code>:: called at the end of training, for final cleanup.</li>
</ul>
<p>The elements of this list are available as attributes of the special variable <code>event</code>, so you can just type <code>event.</code> and hit Tab in your notebook to see a list of all the options.</p>
<p>Let’s take a look at an example. Do you recall how in &lt;<chapter_nlp_dive>&gt; we needed to ensure that our special <code>reset</code> method was called at the start of training and validation for each epoch? We used the <code>ModelResetter</code> callback provided by fastai to do this for us. But how does it work? Here’s the full source code for that class:</chapter_nlp_dive></p>
<div id="cell-70" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ModelResetter(Callback):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> before_train(<span class="va">self</span>):    <span class="va">self</span>.model.reset()</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> before_validate(<span class="va">self</span>): <span class="va">self</span>.model.reset()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Yes, that’s actually it! It just does what we said in the preceding paragraph: after completing training or validation for an epoch, call a method named <code>reset</code>.</p>
<p>Callbacks are often “short and sweet” like this one. In fact, let’s look at one more. Here’s the fastai source for the callback that adds RNN regularization (AR and TAR):</p>
<div id="cell-72" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RNNRegularizer(Callback):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, alpha<span class="op">=</span><span class="fl">0.</span>, beta<span class="op">=</span><span class="fl">0.</span>): <span class="va">self</span>.alpha,<span class="va">self</span>.beta <span class="op">=</span> alpha,beta</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> after_pred(<span class="va">self</span>):</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.raw_out,<span class="va">self</span>.out <span class="op">=</span> <span class="va">self</span>.pred[<span class="dv">1</span>],<span class="va">self</span>.pred[<span class="dv">2</span>]</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learn.pred <span class="op">=</span> <span class="va">self</span>.pred[<span class="dv">0</span>]</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> after_loss(<span class="va">self</span>):</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.training: <span class="cf">return</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.alpha <span class="op">!=</span> <span class="fl">0.</span>:</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.learn.loss <span class="op">+=</span> <span class="va">self</span>.alpha <span class="op">*</span> <span class="va">self</span>.out[<span class="op">-</span><span class="dv">1</span>].<span class="bu">float</span>().<span class="bu">pow</span>(<span class="dv">2</span>).mean()</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.beta <span class="op">!=</span> <span class="fl">0.</span>:</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>            h <span class="op">=</span> <span class="va">self</span>.raw_out[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(h)<span class="op">&gt;</span><span class="dv">1</span>:</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.learn.loss <span class="op">+=</span> <span class="va">self</span>.beta <span class="op">*</span> (h[:,<span class="dv">1</span>:] <span class="op">-</span> h[:,:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>                                               ).<span class="bu">float</span>().<span class="bu">pow</span>(<span class="dv">2</span>).mean()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<blockquote class="blockquote">
<p>note: Code It Yourself: Go back and reread “Activation Regularization and Temporal Activation Regularization” in &lt;<chapter_nlp_dive>&gt; then take another look at the code here. Make sure you understand what it’s doing, and why.</chapter_nlp_dive></p>
</blockquote>
<p>In both of these examples, notice how we can access attributes of the training loop by directly checking <code>self.model</code> or <code>self.pred</code>. That’s because a <code>Callback</code> will always try to get an attribute it doesn’t have inside the <code>Learner</code> associated with it. These are shortcuts for <code>self.learn.model</code> or <code>self.learn.pred</code>. Note that they work for reading attributes, but not for writing them, which is why when <code>RNNRegularizer</code> changes the loss or the predictions you see <code>self.learn.loss =</code> or <code>self.learn.pred =</code>.</p>
<p>When writing a callback, the following attributes of <code>Learner</code> are available:</p>
<ul>
<li><code>model</code>:: The model used for training/validation.</li>
<li><code>data</code>:: The underlying <code>DataLoaders</code>.</li>
<li><code>loss_func</code>:: The loss function used.</li>
<li><code>opt</code>:: The optimizer used to update the model parameters.</li>
<li><code>opt_func</code>:: The function used to create the optimizer.</li>
<li><code>cbs</code>:: The list containing all the <code>Callback</code>s.</li>
<li><code>dl</code>:: The current <code>DataLoader</code> used for iteration.</li>
<li><code>x</code>/<code>xb</code>:: The last input drawn from <code>self.dl</code> (potentially modified by callbacks). <code>xb</code> is always a tuple (potentially with one element) and <code>x</code> is detuplified. You can only assign to <code>xb</code>.</li>
<li><code>y</code>/<code>yb</code>:: The last target drawn from <code>self.dl</code> (potentially modified by callbacks). <code>yb</code> is always a tuple (potentially with one element) and <code>y</code> is detuplified. You can only assign to <code>yb</code>.</li>
<li><code>pred</code>:: The last predictions from <code>self.model</code> (potentially modified by callbacks).</li>
<li><code>loss</code>:: The last computed loss (potentially modified by callbacks).</li>
<li><code>n_epoch</code>:: The number of epochs in this training.</li>
<li><code>n_iter</code>:: The number of iterations in the current <code>self.dl</code>.</li>
<li><code>epoch</code>:: The current epoch index (from 0 to <code>n_epoch-1</code>).</li>
<li><code>iter</code>:: The current iteration index in <code>self.dl</code> (from 0 to <code>n_iter-1</code>).</li>
</ul>
<p>The following attributes are added by <code>TrainEvalCallback</code> and should be available unless you went out of your way to remove that callback:</p>
<ul>
<li><code>train_iter</code>:: The number of training iterations done since the beginning of this training</li>
<li><code>pct_train</code>:: The percentage of training iterations completed (from 0. to 1.)</li>
<li><code>training</code>:: A flag to indicate whether or not we’re in training mode</li>
</ul>
<p>The following attribute is added by <code>Recorder</code> and should be available unless you went out of your way to remove that callback:</p>
<ul>
<li><code>smooth_loss</code>:: An exponentially averaged version of the training loss</li>
</ul>
<p>Callbacks can also interrupt any part of the training loop by using a system of exceptions.</p>
</section>
<section id="callback-ordering-and-exceptions" class="level3">
<h3 class="anchored" data-anchor-id="callback-ordering-and-exceptions">Callback Ordering and Exceptions</h3>
<p>Sometimes, callbacks need to be able to tell fastai to skip over a batch, or an epoch, or stop training altogether. For instance, consider <code>TerminateOnNaNCallback</code>. This handy callback will automatically stop training any time the loss becomes infinite or <code>NaN</code> (<em>not a number</em>). Here’s the fastai source for this callback:</p>
<div id="cell-79" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TerminateOnNaNCallback(Callback):</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    run_before<span class="op">=</span>Recorder</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> after_batch(<span class="va">self</span>):</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> torch.isinf(<span class="va">self</span>.loss) <span class="kw">or</span> torch.isnan(<span class="va">self</span>.loss):</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> CancelFitException</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The line <code>raise CancelFitException</code> tells the training loop to interrupt training at this point. The training loop catches this exception and does not run any further training or validation. The callback control flow exceptions available are:</p>
<ul>
<li><code>CancelBatchException</code>:: Skip the rest of this batch and go to <code>after_batch</code>.</li>
<li><code>CancelTrainException</code>:: Skip the rest of the training part of the epoch and go to <code>after_train</code>.</li>
<li><code>CancelValidException</code>:: Skip the rest of the validation part of the epoch and go to <code>after_validate</code>.</li>
<li><code>CancelEpochException</code>:: Skip the rest of this epoch and go to <code>after_epoch</code>.</li>
<li><code>CancelFitException</code>:: Interrupt training and go to <code>after_fit</code>.</li>
</ul>
<p>You can detect if one of those exceptions has occurred and add code that executes right after with the following events:</p>
<ul>
<li><code>after_cancel_batch</code>:: Reached immediately after a <code>CancelBatchException</code> before proceeding to <code>after_batch</code></li>
<li><code>after_cancel_train</code>:: Reached immediately after a <code>CancelTrainException</code> before proceeding to <code>after_train</code></li>
<li><code>after_cancel_valid</code>:: Reached immediately after a <code>CancelValidException</code> before proceeding to <code>after_valid</code></li>
<li><code>after_cancel_epoch</code>:: Reached immediately after a <code>CancelEpochException</code> before proceeding to <code>after_epoch</code></li>
<li><code>after_cancel_fit</code>:: Reached immediately after a <code>CancelFitException</code> before proceeding to <code>after_fit</code></li>
</ul>
<p>Sometimes, callbacks need to be called in a particular order. For example, in the case of <code>TerminateOnNaNCallback</code>, it’s important that <code>Recorder</code> runs its <code>after_batch</code> after this callback, to avoid registering an <code>NaN</code> loss. You can specify <code>run_before</code> (this callback must run before …) or <code>run_after</code> (this callback must run after …) in your callback to ensure the ordering that you need.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In this chapter we took a close look at the training loop, exploring different variants of SGD and why they can be more powerful. At the time of writing, developing new optimizers is a very active area of research, so by the time you read this chapter there may be an addendum on the book’s website that presents new variants. Be sure to check out how our general optimizer framework can help you implement new optimizers very quickly.</p>
<p>We also examined the powerful callback system that allows you to customize every bit of the training loop by enabling you to inspect and modify any parameter you like between each step.</p>
</section>
<section id="questionnaire" class="level2">
<h2 class="anchored" data-anchor-id="questionnaire">Questionnaire</h2>
<ol type="1">
<li>What is the equation for a step of SGD, in math or code (as you prefer)?</li>
<li>What do we pass to <code>vision_learner</code> to use a non-default optimizer?</li>
<li>What are optimizer callbacks?</li>
<li>What does <code>zero_grad</code> do in an optimizer?</li>
<li>What does <code>step</code> do in an optimizer? How is it implemented in the general optimizer?</li>
<li>Rewrite <code>sgd_cb</code> to use the <code>+=</code> operator, instead of <code>add_</code>.</li>
<li>What is “momentum”? Write out the equation.</li>
<li>What’s a physical analogy for momentum? How does it apply in our model training settings?</li>
<li>What does a bigger value for momentum do to the gradients?</li>
<li>What are the default values of momentum for 1cycle training?</li>
<li>What is RMSProp? Write out the equation.</li>
<li>What do the squared values of the gradients indicate?</li>
<li>How does Adam differ from momentum and RMSProp?</li>
<li>Write out the equation for Adam.</li>
<li>Calculate the values of <code>unbias_avg</code> and <code>w.avg</code> for a few batches of dummy values.</li>
<li>What’s the impact of having a high <code>eps</code> in Adam?</li>
<li>Read through the optimizer notebook in fastai’s repo, and execute it.</li>
<li>In what situations do dynamic learning rate methods like Adam change the behavior of weight decay?</li>
<li>What are the four steps of a training loop?</li>
<li>Why is using callbacks better than writing a new training loop for each tweak you want to add?</li>
<li>What aspects of the design of fastai’s callback system make it as flexible as copying and pasting bits of code?</li>
<li>How can you get the list of events available to you when writing a callback?</li>
<li>Write the <code>ModelResetter</code> callback (without peeking).</li>
<li>How can you access the necessary attributes of the training loop inside a callback? When can you use or not use the shortcuts that go with them?</li>
<li>How can a callback influence the control flow of the training loop?</li>
<li>Write the <code>TerminateOnNaN</code> callback (without peeking, if possible).</li>
<li>How do you make sure your callback runs after or before another callback?</li>
</ol>
<section id="further-research" class="level3">
<h3 class="anchored" data-anchor-id="further-research">Further Research</h3>
<ol type="1">
<li>Look up the “Rectified Adam” paper, implement it using the general optimizer framework, and try it out. Search for other recent optimizers that work well in practice, and pick one to implement.</li>
<li>Look at the mixed-precision callback with the documentation. Try to understand what each event and line of code does.</li>
<li>Implement your own version of the learning rate finder from scratch. Compare it with fastai’s version.</li>
<li>Look at the source code of the callbacks that ship with fastai. See if you can find one that’s similar to what you’re looking to do, to get some inspiration.</li>
</ol>
</section>
</section>
<section id="foundations-of-deep-learning-wrap-up" class="level2">
<h2 class="anchored" data-anchor-id="foundations-of-deep-learning-wrap-up">Foundations of Deep Learning: Wrap up</h2>
<p>Congratulations, you have made it to the end of the “foundations of deep learning” section of the book! You now understand how all of fastai’s applications and most important architectures are built, and the recommended ways to train them—and you have all the information you need to build these from scratch. While you probably won’t need to create your own training loop, or batchnorm layer, for instance, knowing what is going on behind the scenes is very helpful for debugging, profiling, and deploying your solutions.</p>
<p>Since you understand the foundations of fastai’s applications now, be sure to spend some time digging through the source notebooks and running and experimenting with parts of them. This will give you a better idea of how everything in fastai is developed.</p>
<p>In the next section, we will be looking even further under the covers: we’ll explore how the actual forward and backward passes of a neural network are done, and we will see what tools are at our disposal to get better performance. We will then continue with a project that brings together all the material in the book, which we will use to build a tool for interpreting convolutional neural networks. Last but not least, we’ll finish by building fastai’s <code>Learner</code> class from scratch.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/ssmiro\.ru");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>