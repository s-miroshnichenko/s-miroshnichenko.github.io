<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>nlp_dive – Сергей Мирошниченко</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-c00b42e811b0fd9a18ac62455ba22490.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Сергей Мирошниченко</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../posts.html"> 
<span class="menu-text">Блог</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="第十二章从头开始的语言模型" class="level1">
<h1>第十二章：从头开始的语言模型</h1>
<p>我们现在准备深入…深入深度学习！您已经学会了如何训练基本的神经网络，但是如何从那里创建最先进的模型呢？在本书的这一部分，我们将揭开所有的神秘，从语言模型开始。</p>
<p>您在第十章中看到了如何微调预训练的语言模型以构建文本分类器。在本章中，我们将解释该模型的内部结构以及 RNN 是什么。首先，让我们收集一些数据，这些数据将允许我们快速原型化各种模型。</p>
</section>
<section id="数据" class="level1">
<h1>数据</h1>
<p>每当我们开始处理一个新问题时，我们总是首先尝试想出一个最简单的数据集，这样可以让我们快速轻松地尝试方法并解释结果。几年前我们开始进行语言建模时，我们没有找到任何可以快速原型的数据集，所以我们自己制作了一个。我们称之为<em>Human Numbers</em>，它简单地包含了用英语写出的前 10000 个数字。</p>
</section>
<section id="jeremy-说" class="level1">
<h1>Jeremy 说</h1>
<p>我在高度经验丰富的从业者中经常看到的一个常见实际错误是在分析过程中未能在适当的时间使用适当的数据集。特别是，大多数人倾向于从太大、太复杂的数据集开始。</p>
<p>我们可以按照通常的方式下载、提取并查看我们的数据集：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.text.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> untar_data(URLs.HUMAN_NUMBERS)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>path.ls()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>(<span class="co">#2) [Path('train.txt'),Path('valid.txt')]</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>让我们打开这两个文件，看看里面有什么。首先，我们将把所有文本连接在一起，忽略数据集给出的训练/验证拆分（我们稍后会回到这一点）：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>lines <span class="op">=</span> L()</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(path<span class="op">/</span><span class="st">'train.txt'</span>) <span class="im">as</span> f: lines <span class="op">+=</span> L(<span class="op">*</span>f.readlines())</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(path<span class="op">/</span><span class="st">'valid.txt'</span>) <span class="im">as</span> f: lines <span class="op">+=</span> L(<span class="op">*</span>f.readlines())</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>lines</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>(<span class="co">#9998) ['one \n','two \n','three \n','four \n','five \n','six \n','seven</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a> <span class="op">&gt;</span> \n<span class="st">','</span>eight \n<span class="st">','</span>nine \n<span class="st">','</span>ten \n<span class="st">'...]</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>我们将所有这些行连接在一个大流中。为了标记我们从一个数字到下一个数字的转变，我们使用<code>.</code>作为分隔符：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">' . '</span>.join([l.strip() <span class="cf">for</span> l <span class="kw">in</span> lines])</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>text[:<span class="dv">100</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">'one . two . three . four . five . six . seven . eight . nine . ten . eleven .</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="er"> &gt; twelve . thirteen . fo'</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>我们可以通过在空格上拆分来对这个数据集进行标记化：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> text.split(<span class="st">' '</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>tokens[:<span class="dv">10</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>[<span class="st">'one'</span>, <span class="st">'.'</span>, <span class="st">'two'</span>, <span class="st">'.'</span>, <span class="st">'three'</span>, <span class="st">'.'</span>, <span class="st">'four'</span>, <span class="st">'.'</span>, <span class="st">'five'</span>, <span class="st">'.'</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>为了数值化，我们必须创建一个包含所有唯一标记（我们的<em>词汇表</em>）的列表：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> L(<span class="op">*</span>tokens).unique()</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>vocab</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>(<span class="co">#30) ['one','.','two','three','four','five','six','seven','eight','nine'...]</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>然后，我们可以通过查找每个词在词汇表中的索引，将我们的标记转换为数字：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>word2idx <span class="op">=</span> {w:i <span class="cf">for</span> i,w <span class="kw">in</span> <span class="bu">enumerate</span>(vocab)}</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>nums <span class="op">=</span> L(word2idx[i] <span class="cf">for</span> i <span class="kw">in</span> tokens)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>nums</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>(<span class="co">#63095) [0,1,2,1,3,1,4,1,5,1...]</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>现在我们有了一个小数据集，语言建模应该是一个简单的任务，我们可以构建我们的第一个模型。</p>
</section>
<section id="我们的第一个从头开始的语言模型" class="level1">
<h1>我们的第一个从头开始的语言模型</h1>
<p>将这转换为神经网络的一个简单方法是指定我们将基于前三个单词预测每个单词。我们可以创建一个包含每个三个单词序列的列表作为我们的自变量，以及每个序列后面的下一个单词作为因变量。</p>
<p>我们可以用普通的 Python 来做到这一点。首先让我们用标记来确认它是什么样子的：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>L((tokens[i:i<span class="op">+</span><span class="dv">3</span>], tokens[i<span class="op">+</span><span class="dv">3</span>]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="bu">len</span>(tokens)<span class="op">-</span><span class="dv">4</span>,<span class="dv">3</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>(<span class="co">#21031) [(['one', '.', 'two'], '.'),(['.', 'three', '.'], 'four'),(['four',</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a> <span class="op">&gt;</span> <span class="st">'.'</span>, <span class="st">'five'</span>], <span class="st">'.'</span>),([<span class="st">'.'</span>, <span class="st">'six'</span>, <span class="st">'.'</span>], <span class="st">'seven'</span>),([<span class="st">'seven'</span>, <span class="st">'.'</span>, <span class="st">'eight'</span>],</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a> <span class="op">&gt;</span> <span class="st">'.'</span>),([<span class="st">'.'</span>, <span class="st">'nine'</span>, <span class="st">'.'</span>], <span class="st">'ten'</span>),([<span class="st">'ten'</span>, <span class="st">'.'</span>, <span class="st">'eleven'</span>], <span class="st">'.'</span>),([<span class="st">'.'</span>,</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a> <span class="op">&gt;</span> <span class="st">'twelve'</span>, <span class="st">'.'</span>], <span class="st">'thirteen'</span>),([<span class="st">'thirteen'</span>, <span class="st">'.'</span>, <span class="st">'fourteen'</span>], <span class="st">'.'</span>),([<span class="st">'.'</span>,</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a> <span class="op">&gt;</span> <span class="st">'fifteen'</span>, <span class="st">'.'</span>], <span class="st">'sixteen'</span>)...]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>现在我们将使用数值化值的张量来做到这一点，这正是模型实际使用的：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>seqs <span class="op">=</span> L((tensor(nums[i:i<span class="op">+</span><span class="dv">3</span>]), nums[i<span class="op">+</span><span class="dv">3</span>]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="bu">len</span>(nums)<span class="op">-</span><span class="dv">4</span>,<span class="dv">3</span>))</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>seqs</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>(<span class="co">#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]),</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a> <span class="op">&gt;</span> <span class="dv">1</span>),(tensor([<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">1</span>]), <span class="dv">7</span>),(tensor([<span class="dv">7</span>, <span class="dv">1</span>, <span class="dv">8</span>]), <span class="dv">1</span>),(tensor([<span class="dv">1</span>, <span class="dv">9</span>, <span class="dv">1</span>]),</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a> <span class="op">&gt;</span> <span class="dv">10</span>),(tensor([<span class="dv">10</span>,  <span class="dv">1</span>, <span class="dv">11</span>]), <span class="dv">1</span>),(tensor([ <span class="dv">1</span>, <span class="dv">12</span>,  <span class="dv">1</span>]), <span class="dv">13</span>),(tensor([<span class="dv">13</span>,  <span class="dv">1</span>,</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a> <span class="op">&gt;</span> <span class="dv">14</span>]), <span class="dv">1</span>),(tensor([ <span class="dv">1</span>, <span class="dv">15</span>,  <span class="dv">1</span>]), <span class="dv">16</span>)...]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>我们可以使用<code>DataLoader</code>类轻松地对这些进行批处理。现在，我们将随机拆分序列：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>bs <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>cut <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(seqs) <span class="op">*</span> <span class="fl">0.8</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>现在我们可以创建一个神经网络架构，它以三个单词作为输入，并返回词汇表中每个可能的下一个单词的概率预测。我们将使用三个标准线性层，但有两个调整。</p>
<p>第一个调整是，第一个线性层将仅使用第一个词的嵌入作为激活，第二层将使用第二个词的嵌入加上第一层的输出激活，第三层将使用第三个词的嵌入加上第二层的输出激活。关键效果是每个词都在其前面的任何单词的信息上下文中被解释。</p>
<p>第二个调整是，这三个层中的每一个将使用相同的权重矩阵。一个词对来自前面单词的激活的影响方式不应该取决于单词的位置。换句话说，激活值会随着数据通过层移动而改变，但是层权重本身不会从一层到另一层改变。因此，一个层不会学习一个序列位置；它必须学会处理所有位置。</p>
<p>由于层权重不会改变，您可能会认为顺序层是“重复的相同层”。事实上，PyTorch 使这一点具体化；我们可以创建一个层并多次使用它。</p>
<section id="我们的-pytorch-语言模型" class="level2">
<h2 class="anchored" data-anchor-id="我们的-pytorch-语言模型">我们的 PyTorch 语言模型</h2>
<p>我们现在可以创建我们之前描述的语言模型模块：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LMModel1(Module):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_sz, n_hidden):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.i_h <span class="op">=</span> nn.Embedding(vocab_sz, n_hidden)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_h <span class="op">=</span> nn.Linear(n_hidden, n_hidden)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_o <span class="op">=</span> nn.Linear(n_hidden,vocab_sz)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> F.relu(<span class="va">self</span>.h_h(<span class="va">self</span>.i_h(x[:,<span class="dv">0</span>])))</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> h <span class="op">+</span> <span class="va">self</span>.i_h(x[:,<span class="dv">1</span>])</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> F.relu(<span class="va">self</span>.h_h(h))</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> h <span class="op">+</span> <span class="va">self</span>.i_h(x[:,<span class="dv">2</span>])</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> F.relu(<span class="va">self</span>.h_h(h))</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.h_o(h)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>正如您所看到的，我们已经创建了三个层：</p>
<ul>
<li><p>嵌入层（<code>i_h</code>，表示 <em>输入</em> 到 <em>隐藏</em>）</p></li>
<li><p>线性层用于创建下一个单词的激活（<code>h_h</code>，表示 <em>隐藏</em> 到 <em>隐藏</em>）</p></li>
<li><p>一个最终的线性层来预测第四个单词（<code>h_o</code>，表示 <em>隐藏</em> 到 <em>输出</em>）</p></li>
</ul>
<p>这可能更容易以图示形式表示，因此让我们定义一个基本神经网络的简单图示表示。图&nbsp;12-1 显示了我们将如何用一个隐藏层表示神经网络。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/dlcf_1201.png" class="img-fluid figure-img"></p>
<figcaption>简单神经网络的图示表示</figcaption>
</figure>
</div>
<section id="图-12-1简单神经网络的图示表示" class="level6">
<h6 class="anchored" data-anchor-id="图-12-1简单神经网络的图示表示">图 12-1。简单神经网络的图示表示</h6>
<p>每个形状代表激活：矩形代表输入，圆圈代表隐藏（内部）层激活，三角形代表输出激活。我们将在本章中的所有图表中使用这些形状（在 图&nbsp;12-2 中总结）。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/dlcf_1202.png" class="img-fluid figure-img"></p>
<figcaption>我们图示表示中使用的形状</figcaption>
</figure>
</div>
</section>
<section id="图-12-2我们图示表示中使用的形状" class="level6">
<h6 class="anchored" data-anchor-id="图-12-2我们图示表示中使用的形状">图 12-2。我们图示表示中使用的形状</h6>
<p>箭头代表实际的层计算——即线性层后跟激活函数。使用这种符号，图&nbsp;12-3 显示了我们简单语言模型的外观。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/dlcf_1203.png" class="img-fluid figure-img"></p>
<figcaption>我们基本语言模型的表示</figcaption>
</figure>
</div>
</section>
<section id="图-12-3我们基本语言模型的表示" class="level6">
<h6 class="anchored" data-anchor-id="图-12-3我们基本语言模型的表示">图 12-3。我们基本语言模型的表示</h6>
<p>为了简化事情，我们已经从每个箭头中删除了层计算的细节。我们还对箭头进行了颜色编码，使所有具有相同颜色的箭头具有相同的权重矩阵。例如，所有输入层使用相同的嵌入矩阵，因此它们都具有相同的颜色（绿色）。</p>
<p>让我们尝试训练这个模型，看看效果如何：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, LMModel1(<span class="bu">len</span>(vocab), <span class="dv">64</span>), loss_func<span class="op">=</span>F.cross_entropy,</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>                metrics<span class="op">=</span>accuracy)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">4</span>, <span class="fl">1e-3</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<table class="caption-top table">
<thead>
<tr class="header">
<th>epoch</th>
<th>train_loss</th>
<th>valid_loss</th>
<th>accuracy</th>
<th>time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.824297</td>
<td>1.970941</td>
<td>0.467554</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>1</td>
<td>1.386973</td>
<td>1.823242</td>
<td>0.467554</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1.417556</td>
<td>1.654497</td>
<td>0.494414</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>3</td>
<td>1.376440</td>
<td>1.650849</td>
<td>0.494414</td>
<td>00:02</td>
</tr>
</tbody>
</table>
<p>要查看这是否有效，请查看一个非常简单的模型会给我们什么结果。在这种情况下，我们总是可以预测最常见的标记，因此让我们找出在我们的验证集中最常见的目标是哪个标记：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb21"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>n,counts <span class="op">=</span> <span class="dv">0</span>,torch.zeros(<span class="bu">len</span>(vocab))</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> x,y <span class="kw">in</span> dls.valid:</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    n <span class="op">+=</span> y.shape[<span class="dv">0</span>]</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> range_of(vocab): counts[i] <span class="op">+=</span> (y<span class="op">==</span>i).<span class="bu">long</span>().<span class="bu">sum</span>()</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> torch.argmax(counts)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>idx, vocab[idx.item()], counts[idx].item()<span class="op">/</span>n</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb22"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>(tensor(<span class="dv">29</span>), <span class="st">'thousand'</span>, <span class="fl">0.15165200855716662</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>最常见的标记的索引是 29，对应于标记 <code>thousand</code>。总是预测这个标记将给我们大约 15% 的准确率，所以我们表现得更好！</p>
</section>
</section>
</section>
<section id="alexis-说" class="level1">
<h1>Alexis 说</h1>
<p>我的第一个猜测是分隔符会是最常见的标记，因为每个数字都有一个分隔符。但查看 <code>tokens</code> 提醒我，大数字用许多单词写成，所以在通往 10,000 的路上，你会经常写“thousand”：five thousand, five thousand and one, five thousand and two 等等。糟糕！查看数据对于注意到微妙特征以及尴尬明显的特征都很有帮助。</p>
<p>这是一个不错的第一个基线。让我们看看如何用循环重构它。</p>
<section id="我们的第一个循环神经网络" class="level2">
<h2 class="anchored" data-anchor-id="我们的第一个循环神经网络">我们的第一个循环神经网络</h2>
<p>查看我们模块的代码，我们可以通过用 <code>for</code> 循环替换调用层的重复代码来简化它。除了使我们的代码更简单外，这样做的好处是我们将能够同样适用于不同长度的标记序列——我们不会被限制在长度为三的标记列表上：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb23"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LMModel2(Module):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_sz, n_hidden):</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.i_h <span class="op">=</span> nn.Embedding(vocab_sz, n_hidden)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_h <span class="op">=</span> nn.Linear(n_hidden, n_hidden)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_o <span class="op">=</span> nn.Linear(n_hidden,vocab_sz)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>            h <span class="op">=</span> h <span class="op">+</span> <span class="va">self</span>.i_h(x[:,i])</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>            h <span class="op">=</span> F.relu(<span class="va">self</span>.h_h(h))</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.h_o(h)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>让我们检查一下，看看我们使用这种重构是否得到相同的结果：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb24"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, LMModel2(<span class="bu">len</span>(vocab), <span class="dv">64</span>), loss_func<span class="op">=</span>F.cross_entropy,</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>                metrics<span class="op">=</span>accuracy)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">4</span>, <span class="fl">1e-3</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<table class="caption-top table">
<thead>
<tr class="header">
<th>epoch</th>
<th>train_loss</th>
<th>valid_loss</th>
<th>accuracy</th>
<th>time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.816274</td>
<td>1.964143</td>
<td>0.460185</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>1</td>
<td>1.423805</td>
<td>1.739964</td>
<td>0.473259</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1.430327</td>
<td>1.685172</td>
<td>0.485382</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>3</td>
<td>1.388390</td>
<td>1.657033</td>
<td>0.470406</td>
<td>00:02</td>
</tr>
</tbody>
</table>
<p>我们还可以以完全相同的方式重构我们的图示表示，如图 12-4 所示（这里我们也删除了激活大小的细节，并使用与图 12-3 相同的箭头颜色）。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/dlcf_1204.png" class="img-fluid figure-img"></p>
<figcaption>基本循环神经网络</figcaption>
</figure>
</div>
<section id="图-12-4.-基本循环神经网络" class="level6">
<h6 class="anchored" data-anchor-id="图-12-4.-基本循环神经网络">图 12-4. 基本循环神经网络</h6>
<p>您将看到一组激活在每次循环中被更新，存储在变量<code>h</code>中—这被称为<em>隐藏状态</em>。</p>
</section>
</section>
</section>
<section id="术语隐藏状态" class="level1">
<h1>术语：隐藏状态</h1>
<p>在循环神经网络的每一步中更新的激活。</p>
<p>使用这样的循环定义的神经网络称为<em>循环神经网络</em>（RNN）。重要的是要意识到 RNN 并不是一个复杂的新架构，而只是使用<code>for</code>循环对多层神经网络进行重构。</p>
</section>
<section id="alexis-说-1" class="level1">
<h1>Alexis 说</h1>
<p>我的真实看法：如果它们被称为“循环神经网络”或 LNNs，它们看起来会少恐怖 50%！</p>
<p>现在我们知道了什么是 RNN，让我们试着让它变得更好一点。</p>
</section>
<section id="改进-rnn" class="level1">
<h1>改进 RNN</h1>
<p>观察我们的 RNN 代码，有一个看起来有问题的地方是，我们为每个新的输入序列将隐藏状态初始化为零。为什么这是个问题呢？我们将样本序列设置得很短，以便它们可以轻松地适应批处理。但是，如果我们正确地对这些样本进行排序，模型将按顺序读取样本序列，使模型暴露于原始序列的长时间段。</p>
<p>我们还可以考虑增加更多信号：为什么只预测第四个单词，而不使用中间预测来预测第二和第三个单词呢？让我们看看如何实现这些变化，首先从添加一些状态开始。</p>
<section id="维护-rnn-的状态" class="level2">
<h2 class="anchored" data-anchor-id="维护-rnn-的状态">维护 RNN 的状态</h2>
<p>因为我们为每个新样本将模型的隐藏状态初始化为零，这样我们就丢失了关于迄今为止看到的句子的所有信息，这意味着我们的模型实际上不知道我们在整体计数序列中的进度。这很容易修复；我们只需将隐藏状态的初始化移动到<code>__init__</code>中。</p>
<p>但是，这种修复方法将产生自己微妙但重要的问题。它实际上使我们的神经网络变得和文档中的令牌数量一样多。例如，如果我们的数据集中有 10,000 个令牌，我们将创建一个有 10,000 层的神经网络。</p>
<p>要了解为什么会出现这种情况，请考虑我们循环神经网络的原始图示表示，即在图 12-3 中，在使用<code>for</code>循环重构之前。您可以看到每个层对应一个令牌输入。当我们谈论使用<code>for</code>循环重构之前的循环神经网络的表示时，我们称之为<em>展开表示</em>。在尝试理解 RNN 时，考虑展开表示通常是有帮助的。</p>
<p>10,000 层神经网络的问题在于，当您到达数据集的第 10,000 个单词时，您仍然需要计算直到第一层的所有导数。这将非常缓慢，且占用内存。您可能无法在 GPU 上存储一个小批量。</p>
<p>解决这个问题的方法是告诉 PyTorch 我们不希望通过整个隐式神经网络反向传播导数。相反，我们将保留梯度的最后三层。为了在 PyTorch 中删除所有梯度历史，我们使用<code>detach</code>方法。</p>
<p>这是我们 RNN 的新版本。现在它是有状态的，因为它在不同调用<code>forward</code>时记住了其激活，这代表了它在批处理中用于不同样本的情况：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb25"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LMModel3(Module):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_sz, n_hidden):</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.i_h <span class="op">=</span> nn.Embedding(vocab_sz, n_hidden)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_h <span class="op">=</span> nn.Linear(n_hidden, n_hidden)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_o <span class="op">=</span> nn.Linear(n_hidden,vocab_sz)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.h <span class="op">=</span> <span class="va">self</span>.h <span class="op">+</span> <span class="va">self</span>.i_h(x[:,i])</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.h <span class="op">=</span> F.relu(<span class="va">self</span>.h_h(<span class="va">self</span>.h))</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.h_o(<span class="va">self</span>.h)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> <span class="va">self</span>.h.detach()</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>): <span class="va">self</span>.h <span class="op">=</span> <span class="dv">0</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>无论我们选择什么序列长度，这个模型将具有相同的激活，因为隐藏状态将记住上一批次的最后激活。唯一不同的是在每一步计算的梯度：它们将仅在过去的序列长度标记上计算，而不是整个流。这种方法称为<em>时间穿梭反向传播</em>（BPTT）。</p>
</section>
</section>
<section id="术语时间穿梭反向传播" class="level1">
<h1>术语：时间穿梭反向传播</h1>
<p>将一个神经网络有效地视为每个时间步长一个层（通常使用循环重构），并以通常的方式在其上计算梯度。为了避免内存和时间不足，我们通常使用<em>截断</em> BPTT，每隔几个时间步“分离”隐藏状态的计算历史。</p>
<p>要使用<code>LMModel3</code>，我们需要确保样本按照一定顺序进行查看。正如我们在第十章中看到的，如果第一批的第一行是我们的<code>dset[0]</code>，那么第二批应该将<code>dset[1]</code>作为第一行，以便模型看到文本流动。</p>
<p><code>LMDataLoader</code>在第十章中为我们做到了这一点。这次我们要自己做。</p>
<p>为此，我们将重新排列我们的数据集。首先，我们将样本分成<code>m = len(dset) // bs</code>组（这相当于将整个连接数据集分成，例如，64 个大小相等的部分，因为我们在这里使用<code>bs=64</code>）。<code>m</code>是每个这些部分的长度。例如，如果我们使用整个数据集（尽管我们实际上将在一会儿将其分成训练和验证），我们有：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb26"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="bu">len</span>(seqs)<span class="op">//</span>bs</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>m,bs,<span class="bu">len</span>(seqs)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb27"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>(<span class="dv">328</span>, <span class="dv">64</span>, <span class="dv">21031</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>第一批将由样本组成</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb28"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span>, m, <span class="dv">2</span><span class="op">*</span>m, ..., (bs<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>m)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>样本的第二批</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb29"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span>, m<span class="op">+</span><span class="dv">1</span>, <span class="dv">2</span><span class="op">*</span>m<span class="op">+</span><span class="dv">1</span>, ..., (bs<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>m<span class="op">+</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>等等。这样，每个时期，模型将在每批次的每行上看到大小为<code>3*m</code>的连续文本块（因为每个文本的大小为 3）。</p>
<p>以下函数执行重新索引：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb30"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> group_chunks(ds, bs):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> <span class="bu">len</span>(ds) <span class="op">//</span> bs</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    new_ds <span class="op">=</span> L()</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(m): new_ds <span class="op">+=</span> L(ds[i <span class="op">+</span> m<span class="op">*</span>j] <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(bs))</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> new_ds</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>然后，我们在构建<code>DataLoaders</code>时只需传递<code>drop_last=True</code>来删除最后一个形状不为<code>bs</code>的批次。我们还传递<code>shuffle=False</code>以确保文本按顺序阅读：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb31"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>cut <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(seqs) <span class="op">*</span> <span class="fl">0.8</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders.from_dsets(</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    group_chunks(seqs[:cut], bs),</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    group_chunks(seqs[cut:], bs),</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    bs<span class="op">=</span>bs, drop_last<span class="op">=</span><span class="va">True</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>我们添加的最后一件事是通过<code>Callback</code>对训练循环进行微调。我们将在第十六章中更多地讨论回调；这个回调将在每个时期的开始和每个验证阶段之前调用我们模型的<code>reset</code>方法。由于我们实现了该方法来将模型的隐藏状态设置为零，这将确保我们在阅读这些连续文本块之前以干净的状态开始。我们也可以开始训练更长一点：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb32"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, LMModel3(<span class="bu">len</span>(vocab), <span class="dv">64</span>), loss_func<span class="op">=</span>F.cross_entropy,</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>                metrics<span class="op">=</span>accuracy, cbs<span class="op">=</span>ModelResetter)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">10</span>, <span class="fl">3e-3</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<table class="caption-top table">
<thead>
<tr class="header">
<th>epoch</th>
<th>train_loss</th>
<th>valid_loss</th>
<th>accuracy</th>
<th>time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.677074</td>
<td>1.827367</td>
<td>0.467548</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>1</td>
<td>1.282722</td>
<td>1.870913</td>
<td>0.388942</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1.090705</td>
<td>1.651793</td>
<td>0.462500</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>3</td>
<td>1.005092</td>
<td>1.613794</td>
<td>0.516587</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.965975</td>
<td>1.560775</td>
<td>0.551202</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>5</td>
<td>0.916182</td>
<td>1.595857</td>
<td>0.560577</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>6</td>
<td>0.897657</td>
<td>1.539733</td>
<td>0.574279</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>7</td>
<td>0.836274</td>
<td>1.585141</td>
<td>0.583173</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>8</td>
<td>0.805877</td>
<td>1.629808</td>
<td>0.586779</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>9</td>
<td>0.795096</td>
<td>1.651267</td>
<td>0.588942</td>
<td>00:02</td>
</tr>
</tbody>
</table>
<p>这已经更好了！下一步是使用更多目标并将它们与中间预测进行比较。</p>
<section id="创建更多信号" class="level2">
<h2 class="anchored" data-anchor-id="创建更多信号">创建更多信号</h2>
<p>我们当前方法的另一个问题是，我们仅为每三个输入单词预测一个输出单词。因此，我们反馈以更新权重的信号量不如可能的那么大。如果我们在每个单词后预测下一个单词，而不是每三个单词，将会更好，如图 12-5 所示。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/dlcf_1205.png" class="img-fluid figure-img"></p>
<figcaption>RNN 在每个标记后进行预测</figcaption>
</figure>
</div>
<section id="图-12-5rnn-在每个标记后进行预测" class="level6">
<h6 class="anchored" data-anchor-id="图-12-5rnn-在每个标记后进行预测">图 12-5。RNN 在每个标记后进行预测</h6>
<p>这很容易添加。我们需要首先改变我们的数据，使得因变量在每个三个输入词后的每个三个词中都有。我们使用一个属性<code>sl</code>（用于序列长度），并使其稍微变大：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb33"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>sl <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>seqs <span class="op">=</span> L((tensor(nums[i:i<span class="op">+</span>sl]), tensor(nums[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span>sl<span class="op">+</span><span class="dv">1</span>]))</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>         <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="bu">len</span>(nums)<span class="op">-</span>sl<span class="op">-</span><span class="dv">1</span>,sl))</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>cut <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(seqs) <span class="op">*</span> <span class="fl">0.8</span>)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>                             group_chunks(seqs[cut:], bs),</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>                             bs<span class="op">=</span>bs, drop_last<span class="op">=</span><span class="va">True</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>查看<code>seqs</code>的第一个元素，我们可以看到它包含两个相同大小的列表。第二个列表与第一个相同，但偏移了一个元素：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb34"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>[L(vocab[o] <span class="cf">for</span> o <span class="kw">in</span> s) <span class="cf">for</span> s <span class="kw">in</span> seqs[<span class="dv">0</span>]]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb35"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>[(<span class="co">#16) ['one','.','two','.','three','.','four','.','five','.'...],</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a> (<span class="co">#16) ['.','two','.','three','.','four','.','five','.','six'...]]</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>现在我们需要修改我们的模型，使其在每个单词之后输出一个预测，而不仅仅是在一个三个词序列的末尾：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb36"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LMModel4(Module):</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_sz, n_hidden):</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.i_h <span class="op">=</span> nn.Embedding(vocab_sz, n_hidden)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_h <span class="op">=</span> nn.Linear(n_hidden, n_hidden)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_o <span class="op">=</span> nn.Linear(n_hidden,vocab_sz)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>        outs <span class="op">=</span> []</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(sl):</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.h <span class="op">=</span> <span class="va">self</span>.h <span class="op">+</span> <span class="va">self</span>.i_h(x[:,i])</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.h <span class="op">=</span> F.relu(<span class="va">self</span>.h_h(<span class="va">self</span>.h))</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>            outs.append(<span class="va">self</span>.h_o(<span class="va">self</span>.h))</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> <span class="va">self</span>.h.detach()</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.stack(outs, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>): <span class="va">self</span>.h <span class="op">=</span> <span class="dv">0</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>这个模型将返回形状为<code>bs x sl x vocab_sz</code>的输出（因为我们在<code>dim=1</code>上堆叠）。我们的目标的形状是<code>bs x sl</code>，所以在使用<code>F.cross_entropy</code>之前，我们需要将它们展平：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb37"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loss_func(inp, targ):</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> F.cross_entropy(inp.view(<span class="op">-</span><span class="dv">1</span>, <span class="bu">len</span>(vocab)), targ.view(<span class="op">-</span><span class="dv">1</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>我们现在可以使用这个损失函数来训练模型：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb38"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, LMModel4(<span class="bu">len</span>(vocab), <span class="dv">64</span>), loss_func<span class="op">=</span>loss_func,</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>                metrics<span class="op">=</span>accuracy, cbs<span class="op">=</span>ModelResetter)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">15</span>, <span class="fl">3e-3</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<table class="caption-top table">
<thead>
<tr class="header">
<th>epoch</th>
<th>train_loss</th>
<th>valid_loss</th>
<th>accuracy</th>
<th>time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>3.103298</td>
<td>2.874341</td>
<td>0.212565</td>
<td>00:01</td>
</tr>
<tr class="even">
<td>1</td>
<td>2.231964</td>
<td>1.971280</td>
<td>0.462158</td>
<td>00:01</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1.711358</td>
<td>1.813547</td>
<td>0.461182</td>
<td>00:01</td>
</tr>
<tr class="even">
<td>3</td>
<td>1.448516</td>
<td>1.828176</td>
<td>0.483236</td>
<td>00:01</td>
</tr>
<tr class="odd">
<td>4</td>
<td>1.288630</td>
<td>1.659564</td>
<td>0.520671</td>
<td>00:01</td>
</tr>
<tr class="even">
<td>5</td>
<td>1.161470</td>
<td>1.714023</td>
<td>0.554932</td>
<td>00:01</td>
</tr>
<tr class="odd">
<td>6</td>
<td>1.055568</td>
<td>1.660916</td>
<td>0.575033</td>
<td>00:01</td>
</tr>
<tr class="even">
<td>7</td>
<td>0.960765</td>
<td>1.719624</td>
<td>0.591064</td>
<td>00:01</td>
</tr>
<tr class="odd">
<td>8</td>
<td>0.870153</td>
<td>1.839560</td>
<td>0.614665</td>
<td>00:01</td>
</tr>
<tr class="even">
<td>9</td>
<td>0.808545</td>
<td>1.770278</td>
<td>0.624349</td>
<td>00:01</td>
</tr>
<tr class="odd">
<td>10</td>
<td>0.758084</td>
<td>1.842931</td>
<td>0.610758</td>
<td>00:01</td>
</tr>
<tr class="even">
<td>11</td>
<td>0.719320</td>
<td>1.799527</td>
<td>0.646566</td>
<td>00:01</td>
</tr>
<tr class="odd">
<td>12</td>
<td>0.683439</td>
<td>1.917928</td>
<td>0.649821</td>
<td>00:01</td>
</tr>
<tr class="even">
<td>13</td>
<td>0.660283</td>
<td>1.874712</td>
<td>0.628581</td>
<td>00:01</td>
</tr>
<tr class="odd">
<td>14</td>
<td>0.646154</td>
<td>1.877519</td>
<td>0.640055</td>
<td>00:01</td>
</tr>
</tbody>
</table>
<p>我们需要训练更长时间，因为任务有点变化，现在更加复杂。但我们最终得到了一个好结果…至少有时候是这样。如果你多次运行它，你会发现在不同的运行中可以得到非常不同的结果。这是因为实际上我们在这里有一个非常深的网络，这可能导致非常大或非常小的梯度。我们将在本章的下一部分看到如何处理这个问题。</p>
<p>现在，获得更好模型的明显方法是加深：在我们基本的 RNN 中，隐藏状态和输出激活之间只有一个线性层，所以也许我们用更多的线性层会得到更好的结果。</p>
</section>
</section>
</section>
<section id="多层-rnns" class="level1">
<h1>多层 RNNs</h1>
<p>在多层 RNN 中，我们将来自我们递归神经网络的激活传递到第二个递归神经网络中，就像图 12-6 中所示。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/dlcf_1206.png" class="img-fluid figure-img"></p>
<figcaption>2 层 RNN</figcaption>
</figure>
</div>
<section id="图-12-6.-2-层-rnn" class="level6">
<h6 class="anchored" data-anchor-id="图-12-6.-2-层-rnn">图 12-6. 2 层 RNN</h6>
<p>展开的表示在图 12-7 中显示（类似于图 12-3）。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/dlcf_1207.png" class="img-fluid figure-img"></p>
<figcaption>2 层展开的 RNN</figcaption>
</figure>
</div>
</section>
<section id="图-12-7.-2-层展开的-rnn" class="level6">
<h6 class="anchored" data-anchor-id="图-12-7.-2-层展开的-rnn">图 12-7. 2 层展开的 RNN</h6>
<p>让我们看看如何在实践中实现这一点。</p>
</section>
<section id="模型" class="level2">
<h2 class="anchored" data-anchor-id="模型">模型</h2>
<p>我们可以通过使用 PyTorch 的<code>RNN</code>类来节省一些时间，该类实现了我们之前创建的内容，但也给了我们堆叠多个 RNN 的选项，正如我们之前讨论的那样：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb39"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LMModel5(Module):</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_sz, n_hidden, n_layers):</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.i_h <span class="op">=</span> nn.Embedding(vocab_sz, n_hidden)</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.RNN(n_hidden, n_hidden, n_layers, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_o <span class="op">=</span> nn.Linear(n_hidden, vocab_sz)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> torch.zeros(n_layers, bs, n_hidden)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>        res,h <span class="op">=</span> <span class="va">self</span>.rnn(<span class="va">self</span>.i_h(x), <span class="va">self</span>.h)</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> h.detach()</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.h_o(res)</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>): <span class="va">self</span>.h.zero_()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb40"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, LMModel5(<span class="bu">len</span>(vocab), <span class="dv">64</span>, <span class="dv">2</span>),</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>                loss_func<span class="op">=</span>CrossEntropyLossFlat(),</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>                metrics<span class="op">=</span>accuracy, cbs<span class="op">=</span>ModelResetter)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">15</span>, <span class="fl">3e-3</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<table class="caption-top table">
<thead>
<tr class="header">
<th>epoch</th>
<th>train_loss</th>
<th>valid_loss</th>
<th>accuracy</th>
<th>time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>3.055853</td>
<td>2.591640</td>
<td>0.437907</td>
<td>00:01</td>
</tr>
<tr class="even">
<td>1</td>
<td>2.162359</td>
<td>1.787310</td>
<td>0.471598</td>
<td>00:01</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1.710663</td>
<td>1.941807</td>
<td>0.321777</td>
<td>00:01</td>
</tr>
<tr class="even">
<td>3</td>
<td>1.520783</td>
<td>1.999726</td>
<td>0.312012</td>
<td>00:01</td>
</tr>
<tr class="odd">
<td>4</td>
<td>1.330846</td>
<td>2.012902</td>
<td>0.413249</td>
<td>00:01</td>
</tr>
<tr class="even">
<td>5</td>
<td>1.163297</td>
<td>1.896192</td>
<td>0.450684</td>
<td>00:01</td>
</tr>
<tr class="odd">
<td>6</td>
<td>1.033813</td>
<td>2.005209</td>
<td>0.434814</td>
<td>00:01</td>
</tr>
<tr class="even">
<td>7</td>
<td>0.919090</td>
<td>2.047083</td>
<td>0.456706</td>
<td>00:01</td>
</tr>
<tr class="odd">
<td>8</td>
<td>0.822939</td>
<td>2.068031</td>
<td>0.468831</td>
<td>00:01</td>
</tr>
<tr class="even">
<td>9</td>
<td>0.750180</td>
<td>2.136064</td>
<td>0.475098</td>
<td>00:01</td>
</tr>
<tr class="odd">
<td>10</td>
<td>0.695120</td>
<td>2.139140</td>
<td>0.485433</td>
<td>00:01</td>
</tr>
<tr class="even">
<td>11</td>
<td>0.655752</td>
<td>2.155081</td>
<td>0.493652</td>
<td>00:01</td>
</tr>
<tr class="odd">
<td>12</td>
<td>0.629650</td>
<td>2.162583</td>
<td>0.498535</td>
<td>00:01</td>
</tr>
<tr class="even">
<td>13</td>
<td>0.613583</td>
<td>2.171649</td>
<td>0.491048</td>
<td>00:01</td>
</tr>
<tr class="odd">
<td>14</td>
<td>0.604309</td>
<td>2.180355</td>
<td>0.487874</td>
<td>00:01</td>
</tr>
</tbody>
</table>
<p>现在这令人失望…我们之前的单层 RNN 表现更好。为什么？原因是我们有一个更深的模型，导致激活爆炸或消失。</p>
</section>
<section id="激活爆炸或消失" class="level2">
<h2 class="anchored" data-anchor-id="激活爆炸或消失">激活爆炸或消失</h2>
<p>在实践中，从这种类型的 RNN 创建准确的模型是困难的。如果我们调用<code>detach</code>的频率较少，并且有更多的层，我们将获得更好的结果 - 这使得我们的 RNN 有更长的时间跨度来学习和创建更丰富的特征。但这也意味着我们有一个更深的模型要训练。深度学习发展中的关键挑战是如何训练这种类型的模型。</p>
<p>这是具有挑战性的，因为当您多次乘以一个矩阵时会发生什么。想想当您多次乘以一个数字时会发生什么。例如，如果您从 1 开始乘以 2，您会得到序列 1、2、4、8，…在 32 步之后，您已经达到 4,294,967,296。如果您乘以 0.5，类似的问题会发生：您会得到 0.5、0.25、0.125，…在 32 步之后，它是 0.00000000023。正如您所看到的，即使是比 1 稍高或稍低的数字，经过几次重复乘法后，我们的起始数字就会爆炸或消失。</p>
<p>因为矩阵乘法只是将数字相乘并将它们相加，重复矩阵乘法会发生完全相同的事情。这就是深度神经网络的全部内容 - 每一层都是另一个矩阵乘法。这意味着深度神经网络很容易最终得到极大或极小的数字。</p>
<p>这是一个问题，因为计算机存储数字的方式（称为<em>浮点数</em>）意味着随着数字远离零点，它们变得越来越不准确。来自优秀文章“关于浮点数你从未想知道但却被迫了解”的图 12-8 中的图表显示了浮点数的精度如何随着数字线变化。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/dlcf_1208.png" class="img-fluid figure-img"></p>
<figcaption>浮点数的精度</figcaption>
</figure>
</div>
<section id="图-12-8浮点数的精度" class="level6">
<h6 class="anchored" data-anchor-id="图-12-8浮点数的精度">图 12-8。浮点数的精度</h6>
<p>这种不准确性意味着通常为更新权重计算的梯度最终会变为零或无穷大。这通常被称为<em>消失梯度</em>或<em>爆炸梯度</em>问题。这意味着在 SGD 中，权重要么根本不更新，要么跳到无穷大。无论哪种方式，它们都不会随着训练而改善。</p>
<p>研究人员已经开发出了解决这个问题的方法，我们将在本书后面讨论。一种选择是改变层的定义方式，使其不太可能出现激活爆炸。当我们讨论批量归一化时，我们将在第十三章中看到这是如何完成的，当我们讨论 ResNets 时，我们将在第十四章中看到，尽管这些细节通常在实践中并不重要（除非您是一个研究人员，正在创造解决这个问题的新方法）。另一种处理这个问题的策略是谨慎初始化，这是我们将在第十七章中调查的一个主题。</p>
<p>为了避免激活爆炸，RNN 经常使用两种类型的层：<em>门控循环单元</em>（GRUs）和<em>长短期记忆</em>（LSTM）层。这两种都在 PyTorch 中可用，并且可以直接替换 RNN 层。在本书中，我们只会涵盖 LSTMs；在线上有很多好的教程解释 GRUs，它们是 LSTM 设计的一个小变体。</p>
</section>
</section>
</section>
<section id="lstm" class="level1">
<h1>LSTM</h1>
<p>LSTM 是由 Jürgen Schmidhuber 和 Sepp Hochreiter 于 1997 年引入的一种架构。在这种架构中，不是一个，而是两个隐藏状态。在我们的基本 RNN 中，隐藏状态是 RNN 在上一个时间步的输出。那个隐藏状态负责两件事：</p>
<ul>
<li><p>拥有正确的信息来预测正确的下一个标记的输出层</p></li>
<li><p>保留句子中发生的一切记忆</p></li>
</ul>
<p>例如，考虑句子“Henry has a dog and he likes his dog very much”和“Sophie has a dog and she likes her dog very much。”很明显，RNN 需要记住句子开头的名字才能预测<em>he/she</em>或<em>his/her</em>。</p>
<p>在实践中，RNN 在保留句子中较早发生的记忆方面表现非常糟糕，这就是在 LSTM 中有另一个隐藏状态（称为<em>cell state</em>）的动机。cell state 将负责保持<em>长期短期记忆</em>，而隐藏状态将专注于预测下一个标记。让我们更仔细地看看如何实现这一点，并从头开始构建一个 LSTM。</p>
<section id="从头开始构建一个-lstm" class="level2">
<h2 class="anchored" data-anchor-id="从头开始构建一个-lstm">从头开始构建一个 LSTM</h2>
<p>为了构建一个 LSTM，我们首先必须了解其架构。图 12-9 显示了其内部结构。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/dlcf_1209.png" class="img-fluid figure-img"></p>
<figcaption>显示 LSTM 内部架构的图表</figcaption>
</figure>
</div>
<section id="图-12-9.-lstm-的架构" class="level6">
<h6 class="anchored" data-anchor-id="图-12-9.-lstm-的架构">图 12-9. LSTM 的架构</h6>
<p>在这张图片中，我们的输入<math alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>从左侧进入，带有先前的隐藏状态（<math alttext="h Subscript t minus 1"><msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>）和 cell state（<math alttext="c Subscript t minus 1"><msub><mi>c</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>）。四个橙色框代表四个层（我们的神经网络），激活函数可以是 sigmoid（<math alttext="sigma"><mi>σ</mi></math>）或 tanh。tanh 只是一个重新缩放到范围-1 到 1 的 sigmoid 函数。它的数学表达式可以写成这样：</p>
<p><math alttext="双曲正切左括号 x 右括号等于开始分数 e 上标 x 基线加 e 上标 负 x 基线 除以 e 上标 x 基线减 e 上标 负 x 基线 等于 2 sigma 左括号 2 x 右括号 减 1" display="block"><mrow><mo form="prefix">tanh</mo> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><msup><mi>e</mi> <mi>x</mi></msup> <mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>x</mi></mrow></msup></mrow> <mrow><msup><mi>e</mi> <mi>x</mi></msup> <mo>-</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>x</mi></mrow></msup></mrow></mfrac> <mo>=</mo> <mn>2</mn> <mi>σ</mi> <mrow><mo>(</mo> <mn>2</mn> <mi>x</mi> <mo>)</mo></mrow> <mo>-</mo> <mn>1</mn></mrow></math></p>
<p>其中<math alttext="sigma"><mi>σ</mi></math>是 sigmoid 函数。图中的绿色圆圈是逐元素操作。右侧输出的是新的隐藏状态（<math alttext="h Subscript t"><msub><mi>h</mi> <mi>t</mi></msub></math>）和新的 cell state（<math alttext="c Subscript t"><msub><mi>c</mi> <mi>t</mi></msub></math>），准备接受我们的下一个输入。新的隐藏状态也被用作输出，这就是为什么箭头分开向上移动。</p>
<p>让我们逐一查看四个神经网络（称为<em>门</em>）并解释图表——但在此之前，请注意 cell state（顶部）几乎没有改变。它甚至没有直接通过神经网络！这正是为什么它将继续保持较长期的状态。</p>
<p>首先，将输入和旧隐藏状态的箭头连接在一起。在本章前面编写的 RNN 中，我们将它们相加。在 LSTM 中，我们将它们堆叠在一个大张量中。这意味着我们的嵌入的维度（即<math alttext="x Subscript t"><msub><mi>x</mi> <mi>t</mi></msub></math>的维度）可以与隐藏状态的维度不同。如果我们将它们称为<code>n_in</code>和<code>n_hid</code>，底部的箭头大小为<code>n_in + n_hid</code>；因此所有的神经网络（橙色框）都是具有<code>n_in + n_hid</code>输入和<code>n_hid</code>输出的线性层。</p>
<p>第一个门（从左到右看）称为<em>遗忘门</em>。由于它是一个线性层后面跟着一个 sigmoid，它的输出将由 0 到 1 之间的标量组成。我们将这个结果乘以细胞状态，以确定要保留哪些信息，要丢弃哪些信息：接近 0 的值被丢弃，接近 1 的值被保留。这使得 LSTM 有能力忘记关于其长期状态的事情。例如，当穿过一个句号或一个<code>xxbos</code>标记时，我们期望它（已经学会）重置其细胞状态。</p>
<p>第二个门称为<em>输入门</em>。它与第三个门（没有真正的名称，但有时被称为<em>细胞门</em>）一起更新细胞状态。例如，我们可能看到一个新的性别代词，这时我们需要替换遗忘门删除的关于性别的信息。与遗忘门类似，输入门决定要更新的细胞状态元素（接近 1 的值）或不更新（接近 0 的值）。第三个门确定这些更新值是什么，范围在-1 到 1 之间（由于 tanh 函数）。结果被添加到细胞状态中。</p>
<p>最后一个门是<em>输出门</em>。它确定从细胞状态中使用哪些信息来生成输出。细胞状态经过 tanh 后与输出门的 sigmoid 输出结合，结果就是新的隐藏状态。在代码方面，我们可以这样写相同的步骤：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb41"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LSTMCell(Module):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ni, nh):</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.forget_gate <span class="op">=</span> nn.Linear(ni <span class="op">+</span> nh, nh)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_gate  <span class="op">=</span> nn.Linear(ni <span class="op">+</span> nh, nh)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cell_gate   <span class="op">=</span> nn.Linear(ni <span class="op">+</span> nh, nh)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_gate <span class="op">=</span> nn.Linear(ni <span class="op">+</span> nh, nh)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>, state):</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>        h,c <span class="op">=</span> state</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> torch.stack([h, <span class="bu">input</span>], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>        forget <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.forget_gate(h))</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>        c <span class="op">=</span> c <span class="op">*</span> forget</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>        inp <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.input_gate(h))</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>        cell <span class="op">=</span> torch.tanh(<span class="va">self</span>.cell_gate(h))</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>        c <span class="op">=</span> c <span class="op">+</span> inp <span class="op">*</span> cell</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.output_gate(h))</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> outgate <span class="op">*</span> torch.tanh(c)</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> h, (h,c)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>实际上，我们可以重构代码。此外，就性能而言，做一次大矩阵乘法比做四次小矩阵乘法更好（因为我们只在 GPU 上启动一次特殊的快速内核，这样可以让 GPU 并行处理更多工作）。堆叠需要一点时间（因为我们必须在 GPU 上移动一个张量，使其全部在一个连续的数组中），所以我们为输入和隐藏状态使用两个单独的层。优化和重构后的代码如下：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb42"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LSTMCell(Module):</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ni, nh):</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ih <span class="op">=</span> nn.Linear(ni,<span class="dv">4</span><span class="op">*</span>nh)</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hh <span class="op">=</span> nn.Linear(nh,<span class="dv">4</span><span class="op">*</span>nh)</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>, state):</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>        h,c <span class="op">=</span> state</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># One big multiplication for all the gates is better than 4 smaller ones</span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>        gates <span class="op">=</span> (<span class="va">self</span>.ih(<span class="bu">input</span>) <span class="op">+</span> <span class="va">self</span>.hh(h)).chunk(<span class="dv">4</span>, <span class="dv">1</span>)</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>        ingate,forgetgate,outgate <span class="op">=</span> <span class="bu">map</span>(torch.sigmoid, gates[:<span class="dv">3</span>])</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>        cellgate <span class="op">=</span> gates[<span class="dv">3</span>].tanh()</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>        c <span class="op">=</span> (forgetgate<span class="op">*</span>c) <span class="op">+</span> (ingate<span class="op">*</span>cellgate)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> outgate <span class="op">*</span> c.tanh()</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> h, (h,c)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>在这里，我们使用 PyTorch 的<code>chunk</code>方法将张量分成四部分。它的工作原理如下：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb43"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.arange(<span class="dv">0</span>,<span class="dv">10</span>)<span class="op">;</span> t</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb44"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>tensor([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb45"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>t.chunk(<span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb46"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>(tensor([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>]), tensor([<span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>]))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>现在让我们使用这个架构来训练一个语言模型！</p>
</section>
</section>
<section id="使用-lstms-训练语言模型" class="level2">
<h2 class="anchored" data-anchor-id="使用-lstms-训练语言模型">使用 LSTMs 训练语言模型</h2>
<p>这是与<code>LMModel5</code>相同的网络，使用了两层 LSTM。我们可以以更高的学习率进行训练，时间更短，获得更好的准确性：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb47"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LMModel6(Module):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_sz, n_hidden, n_layers):</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.i_h <span class="op">=</span> nn.Embedding(vocab_sz, n_hidden)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.LSTM(n_hidden, n_hidden, n_layers, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_o <span class="op">=</span> nn.Linear(n_hidden, vocab_sz)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> [torch.zeros(n_layers, bs, n_hidden) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)]</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>        res,h <span class="op">=</span> <span class="va">self</span>.rnn(<span class="va">self</span>.i_h(x), <span class="va">self</span>.h)</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> [h_.detach() <span class="cf">for</span> h_ <span class="kw">in</span> h]</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.h_o(res)</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>):</span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> h <span class="kw">in</span> <span class="va">self</span>.h: h.zero_()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb48"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, LMModel6(<span class="bu">len</span>(vocab), <span class="dv">64</span>, <span class="dv">2</span>),</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>                loss_func<span class="op">=</span>CrossEntropyLossFlat(),</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>                metrics<span class="op">=</span>accuracy, cbs<span class="op">=</span>ModelResetter)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">15</span>, <span class="fl">1e-2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<table class="caption-top table">
<thead>
<tr class="header">
<th>epoch</th>
<th>train_loss</th>
<th>valid_loss</th>
<th>accuracy</th>
<th>time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>3.000821</td>
<td>2.663942</td>
<td>0.438314</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>1</td>
<td>2.139642</td>
<td>2.184780</td>
<td>0.240479</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1.607275</td>
<td>1.812682</td>
<td>0.439779</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>3</td>
<td>1.347711</td>
<td>1.830982</td>
<td>0.497477</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>4</td>
<td>1.123113</td>
<td>1.937766</td>
<td>0.594401</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>5</td>
<td>0.852042</td>
<td>2.012127</td>
<td>0.631592</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>6</td>
<td>0.565494</td>
<td>1.312742</td>
<td>0.725749</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>7</td>
<td>0.347445</td>
<td>1.297934</td>
<td>0.711263</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>8</td>
<td>0.208191</td>
<td>1.441269</td>
<td>0.731201</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>9</td>
<td>0.126335</td>
<td>1.569952</td>
<td>0.737305</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>10</td>
<td>0.079761</td>
<td>1.427187</td>
<td>0.754150</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>11</td>
<td>0.052990</td>
<td>1.494990</td>
<td>0.745117</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>12</td>
<td>0.039008</td>
<td>1.393731</td>
<td>0.757894</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>13</td>
<td>0.031502</td>
<td>1.373210</td>
<td>0.758464</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>14</td>
<td>0.028068</td>
<td>1.368083</td>
<td>0.758464</td>
<td>00:02</td>
</tr>
</tbody>
</table>
<p>现在这比多层 RNN 好多了！然而，我们仍然可以看到有一点过拟合，这表明一点正则化可能会有所帮助。</p>
</section>
</section>
<section id="正则化-lstm" class="level1">
<h1>正则化 LSTM</h1>
<p>循环神经网络总体上很难训练，因为我们之前看到的激活和梯度消失问题。使用 LSTM（或 GRU）单元比使用普通 RNN 更容易训练，但它们仍然很容易过拟合。数据增强虽然是一种可能性，但在文本数据中使用得比图像数据少，因为在大多数情况下，它需要另一个模型来生成随机增强（例如，将文本翻译成另一种语言，然后再翻译回原始语言）。总的来说，目前文本数据的数据增强并不是一个被充分探索的领域。</p>
<p>然而，我们可以使用其他正则化技术来减少过拟合，这些技术在与 LSTMs 一起使用时进行了深入研究，如 Stephen Merity 等人的论文<a href="https://oreil.ly/Rf-OG">“正则化和优化 LSTM 语言模型”</a>。这篇论文展示了如何有效地使用 dropout、激活正则化和时间激活正则化可以使一个 LSTM 击败以前需要更复杂模型的最新结果。作者将使用这些技术的 LSTM 称为<em>AWD-LSTM</em>。我们将依次看看这些技术。</p>
<section id="dropout" class="level2">
<h2 class="anchored" data-anchor-id="dropout">Dropout</h2>
<p><em>Dropout</em>是由 Geoffrey Hinton 等人在<a href="https://oreil.ly/-_xie">“通过防止特征探测器的共适应来改进神经网络”</a>中引入的一种正则化技术。基本思想是在训练时随机将一些激活变为零。这确保所有神经元都积极地朝着输出工作，如图 12-10 所示（来自 Nitish Srivastava 等人的<a href="https://oreil.ly/pYNxF">“Dropout：防止神经网络过拟合的简单方法”</a>）。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/dlcf_1210.png" class="img-fluid figure-img"></p>
<figcaption>文章中显示 dropout 如何关闭神经元的图</figcaption>
</figure>
</div>
<section id="图-12-10在神经网络中应用-dropout由-nitish-srivastava-等人提供" class="level6">
<h6 class="anchored" data-anchor-id="图-12-10在神经网络中应用-dropout由-nitish-srivastava-等人提供">图 12-10。在神经网络中应用 dropout（由 Nitish Srivastava 等人提供）</h6>
<p>Hinton 在一次采访中解释了 dropout 的灵感时使用了一个很好的比喻：</p>
<blockquote class="blockquote">
<p>我去了我的银行。出纳员不断变换，我问其中一个原因。他说他不知道，但他们经常被调动。我想这一定是因为需要员工之间的合作才能成功欺诈银行。这让我意识到，随机在每个示例中删除不同的神经元子集将防止阴谋，从而减少过拟合。</p>
</blockquote>
<p>在同一次采访中，他还解释了神经科学提供了额外的灵感：</p>
<blockquote class="blockquote">
<p>我们并不真正知道为什么神经元会突触。有一种理论是它们想要变得嘈杂以进行正则化，因为我们的参数比数据点多得多。dropout 的想法是，如果你有嘈杂的激活，你可以承担使用一个更大的模型。</p>
</blockquote>
<p>这解释了为什么 dropout 有助于泛化的想法：首先它帮助神经元更好地合作；然后它使激活更嘈杂，从而使模型更健壮。</p>
<p>然而，我们可以看到，如果我们只是将这些激活置零而不做其他任何操作，我们的模型将会训练出问题：如果我们从五个激活的总和（由于我们应用了 ReLU，它们都是正数）变为只有两个，这不会有相同的规模。因此，如果我们以概率<code>p</code>应用 dropout，我们通过将所有激活除以<code>1-p</code>来重新缩放它们（平均<code>p</code>将被置零，所以剩下<code>1-p</code>），如图 12-11 所示。</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/dlcf_1211.png" class="img-fluid figure-img"></p>
<figcaption>介绍 dropout 的文章中的一个图，显示神经元是开启/关闭状态</figcaption>
</figure>
</div>
</section>
<section id="图-12-11应用-dropout-时为什么要缩放激活由-nitish-srivastava-等人提供" class="level6">
<h6 class="anchored" data-anchor-id="图-12-11应用-dropout-时为什么要缩放激活由-nitish-srivastava-等人提供">图 12-11。应用 dropout 时为什么要缩放激活（由 Nitish Srivastava 等人提供）</h6>
<p>这是 PyTorch 中 dropout 层的完整实现（尽管 PyTorch 的原生层实际上是用 C 而不是 Python 编写的）：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb49"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Dropout(Module):</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, p): <span class="va">self</span>.p <span class="op">=</span> p</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.training: <span class="cf">return</span> x</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> x.new(<span class="op">*</span>x.shape).bernoulli_(<span class="dv">1</span><span class="op">-</span>p)</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">*</span> mask.div_(<span class="dv">1</span><span class="op">-</span>p)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><code>bernoulli_</code>方法创建一个随机零（概率为<code>p</code>）和一（概率为<code>1-p</code>）的张量，然后将其乘以我们的输入，再除以<code>1-p</code>。注意<code>training</code>属性的使用，它在任何 PyTorch <code>nn.Module</code>中都可用，并告诉我们是否在训练或推理。</p>
</section>
</section>
</section>
<section id="做你自己的实验" class="level1">
<h1>做你自己的实验</h1>
<p>在本书的前几章中，我们会在这里添加一个<code>bernoulli_</code>的代码示例，这样您就可以看到它的确切工作原理。但是现在您已经了解足够多，可以自己做这个，我们将为您提供越来越少的示例，而是期望您自己进行实验以了解事物是如何工作的。在这种情况下，您将在章节末尾的问卷中看到，我们要求您尝试使用<code>bernoulli_</code>，但不要等到我们要求您进行实验才开发您对我们正在研究的代码的理解；无论如何都可以开始做。</p>
<p>在将我们的 LSTM 的输出传递到最终层之前使用 dropout 将有助于减少过拟合。在许多其他模型中也使用了 dropout，包括<code>fastai.vision</code>中使用的默认 CNN 头部，并且通过传递<code>ps</code>参数（其中每个“p”都传递给每个添加的<code>Dropout</code>层）在<code>fastai.tabular</code>中也可用，正如我们将在第十五章中看到的。</p>
<p>在训练和验证模式下，dropout 的行为不同，我们使用<code>Dropout</code>中的<code>training</code>属性进行指定。在<code>Module</code>上调用<code>train</code>方法会将<code>training</code>设置为<code>True</code>（对于您调用该方法的模块以及递归包含的每个模块），而<code>eval</code>将其设置为<code>False</code>。在调用<code>Learner</code>的方法时会自动执行此操作，但如果您没有使用该类，请记住根据需要在两者之间切换。</p>
<section id="激活正则化和时间激活正则化" class="level2">
<h2 class="anchored" data-anchor-id="激活正则化和时间激活正则化">激活正则化和时间激活正则化</h2>
<p>激活正则化（AR）和时间激活正则化（TAR）是两种与权重衰减非常相似的正则化方法，在第八章中讨论过。在应用权重衰减时，我们会对损失添加一个小的惩罚，旨在使权重尽可能小。对于激活正则化，我们将尝试使 LSTM 生成的最终激活尽可能小，而不是权重。</p>
<p>为了对最终激活进行正则化，我们必须将它们存储在某个地方，然后将它们的平方的平均值添加到损失中（以及一个乘数<code>alpha</code>，就像权重衰减的<code>wd</code>一样）：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb50"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>loss <span class="op">+=</span> alpha <span class="op">*</span> activations.<span class="bu">pow</span>(<span class="dv">2</span>).mean()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>时间激活正则化与我们在句子中预测标记有关。这意味着当我们按顺序阅读它们时，我们的 LSTM 的输出应该在某种程度上是有意义的。TAR 通过向损失添加惩罚来鼓励这种行为，使两个连续激活之间的差异尽可能小：我们的激活张量的形状为<code>bs x sl x n_hid</code>，我们在序列长度轴上（中间维度）读取连续激活。有了这个，TAR 可以表示如下：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb51"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>loss <span class="op">+=</span> beta <span class="op">*</span> (activations[:,<span class="dv">1</span>:] <span class="op">-</span> activations[:,:<span class="op">-</span><span class="dv">1</span>]).<span class="bu">pow</span>(<span class="dv">2</span>).mean()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>然后，<code>alpha</code>和<code>beta</code>是要调整的两个超参数。为了使这项工作成功，我们需要让我们的带有 dropout 的模型返回三个东西：正确的输出，LSTM 在 dropout 之前的激活以及 LSTM 在 dropout 之后的激活。通常在 dropout 后的激活上应用 AR（以免惩罚我们之后转换为零的激活），而 TAR 应用在未经 dropout 的激活上（因为这些零会在两个连续时间步之间产生很大的差异）。然后，一个名为<code>RNNRegularizer</code>的回调将为我们应用这种正则化。</p>
</section>
<section id="训练带有权重绑定的正则化-lstm" class="level2">
<h2 class="anchored" data-anchor-id="训练带有权重绑定的正则化-lstm">训练带有权重绑定的正则化 LSTM</h2>
<p>我们可以将 dropout（应用在我们进入输出层之前）与 AR 和 TAR 相结合，以训练我们之前的 LSTM。我们只需要返回三个东西而不是一个：我们的 LSTM 的正常输出，dropout 后的激活以及我们的 LSTM 的激活。最后两个将由回调<code>RNNRegularization</code>捕获，以便为其对损失的贡献做出贡献。</p>
<p>我们可以从<a href="https://oreil.ly/ETQ5X">AWD-LSTM 论文</a>中添加另一个有用的技巧是<em>权重绑定</em>。在语言模型中，输入嵌入表示从英语单词到激活的映射，输出隐藏层表示从激活到英语单词的映射。直觉上，我们可能会期望这些映射是相同的。我们可以通过将相同的权重矩阵分配给这些层来在 PyTorch 中表示这一点：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb52"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.h_o.weight <span class="op">=</span> <span class="va">self</span>.i_h.weight</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>在<code>LMMModel7</code>中，我们包括了这些最终的调整：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb53"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LMModel7(Module):</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_sz, n_hidden, n_layers, p):</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.i_h <span class="op">=</span> nn.Embedding(vocab_sz, n_hidden)</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.LSTM(n_hidden, n_hidden, n_layers, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.drop <span class="op">=</span> nn.Dropout(p)</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_o <span class="op">=</span> nn.Linear(n_hidden, vocab_sz)</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h_o.weight <span class="op">=</span> <span class="va">self</span>.i_h.weight</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> [torch.zeros(n_layers, bs, n_hidden) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)]</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>        raw,h <span class="op">=</span> <span class="va">self</span>.rnn(<span class="va">self</span>.i_h(x), <span class="va">self</span>.h)</span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.drop(raw)</span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> [h_.detach() <span class="cf">for</span> h_ <span class="kw">in</span> h]</span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.h_o(out),raw,out</span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>):</span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> h <span class="kw">in</span> <span class="va">self</span>.h: h.zero_()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>我们可以使用<code>RNNRegularizer</code>回调函数创建一个正则化的<code>Learner</code>：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb54"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, LMModel7(<span class="bu">len</span>(vocab), <span class="dv">64</span>, <span class="dv">2</span>, <span class="fl">0.5</span>),</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>                loss_func<span class="op">=</span>CrossEntropyLossFlat(), metrics<span class="op">=</span>accuracy,</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>                cbs<span class="op">=</span>[ModelResetter, RNNRegularizer(alpha<span class="op">=</span><span class="dv">2</span>, beta<span class="op">=</span><span class="dv">1</span>)])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><code>TextLearner</code>会自动为我们添加这两个回调函数（使用<code>alpha</code>和<code>beta</code>的默认值），因此我们可以简化前面的行：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb55"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> TextLearner(dls, LMModel7(<span class="bu">len</span>(vocab), <span class="dv">64</span>, <span class="dv">2</span>, <span class="fl">0.4</span>),</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>                    loss_func<span class="op">=</span>CrossEntropyLossFlat(), metrics<span class="op">=</span>accuracy)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>然后我们可以训练模型，并通过增加权重衰减到<code>0.1</code>来添加额外的正则化：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb56"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">15</span>, <span class="fl">1e-2</span>, wd<span class="op">=</span><span class="fl">0.1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<table class="caption-top table">
<thead>
<tr class="header">
<th>epoch</th>
<th>train_loss</th>
<th>valid_loss</th>
<th>accuracy</th>
<th>time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>2.693885</td>
<td>2.013484</td>
<td>0.466634</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>1</td>
<td>1.685549</td>
<td>1.187310</td>
<td>0.629313</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.973307</td>
<td>0.791398</td>
<td>0.745605</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.555823</td>
<td>0.640412</td>
<td>0.794108</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.351802</td>
<td>0.557247</td>
<td>0.836100</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>5</td>
<td>0.244986</td>
<td>0.594977</td>
<td>0.807292</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>6</td>
<td>0.192231</td>
<td>0.511690</td>
<td>0.846761</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>7</td>
<td>0.162456</td>
<td>0.520370</td>
<td>0.858073</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>8</td>
<td>0.142664</td>
<td>0.525918</td>
<td>0.842285</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>9</td>
<td>0.128493</td>
<td>0.495029</td>
<td>0.858073</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>10</td>
<td>0.117589</td>
<td>0.464236</td>
<td>0.867188</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>11</td>
<td>0.109808</td>
<td>0.466550</td>
<td>0.869303</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>12</td>
<td>0.104216</td>
<td>0.455151</td>
<td>0.871826</td>
<td>00:02</td>
</tr>
<tr class="even">
<td>13</td>
<td>0.100271</td>
<td>0.452659</td>
<td>0.873617</td>
<td>00:02</td>
</tr>
<tr class="odd">
<td>14</td>
<td>0.098121</td>
<td>0.458372</td>
<td>0.869385</td>
<td>00:02</td>
</tr>
</tbody>
</table>
<p>现在这比我们之前的模型好多了！</p>
</section>
</section>
<section id="结论" class="level1">
<h1>结论</h1>
<p>您现在已经看到了我们在第十章中用于文本分类的 AWD-LSTM 架构内部的所有内容。它在更多地方使用了丢失：</p>
<ul>
<li><p>嵌入丢失（就在嵌入层之后）</p></li>
<li><p>输入丢失（在嵌入层之后）</p></li>
<li><p>权重丢失（应用于每个训练步骤中 LSTM 的权重）</p></li>
<li><p>隐藏丢失（应用于两个层之间的隐藏状态）</p></li>
</ul>
<p>这使得它更加规范化。由于微调这五个丢失值（包括输出层之前的丢失）很复杂，我们已经确定了良好的默认值，并允许通过您在该章节中看到的<code>drop_mult</code>参数来整体调整丢失的大小。</p>
<p>另一个非常强大的架构，特别适用于“序列到序列”问题（依赖变量本身是一个变长序列的问题，例如语言翻译），是 Transformer 架构。您可以在<a href="https://book.fast.ai">书籍网站</a>的额外章节中找到它。</p>
</section>
<section id="问卷" class="level1">
<h1>问卷</h1>
<ol type="1">
<li><p>如果您的项目数据集非常庞大且复杂，处理它需要大量时间，您应该怎么做？</p></li>
<li><p>为什么在创建语言模型之前我们要将数据集中的文档连接起来？</p></li>
<li><p>要使用标准的全连接网络来预测前三个单词给出的第四个单词，我们需要对模型进行哪两个调整？</p></li>
<li><p>我们如何在 PyTorch 中跨多个层共享权重矩阵？</p></li>
<li><p>编写一个模块，预测句子前两个单词给出的第三个单词，而不偷看。</p></li>
<li><p>什么是循环神经网络？</p></li>
<li><p>隐藏状态是什么？</p></li>
<li><p><code>LMModel1</code>中隐藏状态的等价物是什么？</p></li>
<li><p>为了在 RNN 中保持状态，为什么按顺序将文本传递给模型很重要？</p></li>
<li><p>什么是 RNN 的“展开”表示？</p></li>
<li><p>为什么在 RNN 中保持隐藏状态会导致内存和性能问题？我们如何解决这个问题？</p></li>
<li><p>什么是 BPTT？</p></li>
<li><p>编写代码打印出验证集的前几个批次，包括将标记 ID 转换回英文字符串，就像我们在第十章中展示的 IMDb 数据批次一样。</p></li>
<li><p><code>ModelResetter</code>回调函数的作用是什么？我们为什么需要它？</p></li>
<li><p>为每三个输入词预测一个输出词的缺点是什么？</p></li>
<li><p>为什么我们需要为<code>LMModel4</code>设计一个自定义损失函数？</p></li>
<li><p>为什么<code>LMModel4</code>的训练不稳定？</p></li>
<li><p>在展开表示中，我们可以看到递归神经网络有许多层。那么为什么我们需要堆叠 RNN 以获得更好的结果？</p></li>
<li><p>绘制一个堆叠（多层）RNN 的表示。</p></li>
<li><p>如果我们不经常调用<code>detach</code>，为什么在 RNN 中应该获得更好的结果？为什么在实践中可能不会发生这种情况？</p></li>
<li><p>为什么深度网络可能导致非常大或非常小的激活？这为什么重要？</p></li>
<li><p>在计算机的浮点数表示中，哪些数字是最精确的？</p></li>
<li><p>为什么消失的梯度会阻止训练？</p></li>
<li><p>在 LSTM 架构中有两个隐藏状态为什么有帮助？每个的目的是什么？</p></li>
<li><p>在 LSTM 中这两个状态被称为什么？</p></li>
<li><p>tanh 是什么，它与 sigmoid 有什么关系？</p></li>
<li><p><code>LSTMCell</code>中这段代码的目的是什么：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb57"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> torch.stack([h, <span class="bu">input</span>], dim<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>在 PyTorch 中<code>chunk</code>是做什么的？</p></li>
<li><p>仔细研究<code>LSTMCell</code>的重构版本，确保你理解它如何以及为什么与未重构版本执行相同的操作。</p></li>
<li><p>为什么我们可以为<code>LMModel6</code>使用更高的学习率？</p></li>
<li><p>AWD-LSTM 模型中使用的三种正则化技术是什么？</p></li>
<li><p>什么是 dropout？</p></li>
<li><p>为什么我们要用 dropout 来缩放权重？这是在训练期间、推理期间还是两者都应用？</p></li>
<li><p><code>Dropout</code>中这行代码的目的是什么：</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb58"><pre class="sourceCode py code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.training: <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>尝试使用<code>bernoulli_</code>来了解它的工作原理。</p></li>
<li><p>如何在 PyTorch 中将模型设置为训练模式？在评估模式下呢？</p></li>
<li><p>写出激活正则化的方程（数学或代码，任你选择）。它与权重衰减有什么不同？</p></li>
<li><p>写出时间激活正则化的方程（数学或代码，任你选择）。为什么我们不会在计算机视觉问题中使用这个？</p></li>
<li><p>语言模型中的权重绑定是什么？</p></li>
</ol>
<section id="进一步研究" class="level2">
<h2 class="anchored" data-anchor-id="进一步研究">进一步研究</h2>
<ol type="1">
<li><p>在<code>LMModel2</code>中，为什么<code>forward</code>可以从<code>h=0</code>开始？为什么我们不需要写<code>h=torch.zeros(...)</code>？</p></li>
<li><p>从头开始编写一个 LSTM 的代码（你可以参考图 12-9）。</p></li>
<li><p>搜索互联网了解 GRU 架构并从头开始实现它，尝试训练一个模型。看看能否获得类似于本章中看到的结果。将你的结果与 PyTorch 内置的<code>GRU</code>模块的结果进行比较。</p></li>
<li><p>查看 fastai 中 AWD-LSTM 的源代码，并尝试将每行代码映射到本章中展示的概念。</p></li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/ssmiro\.ru");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>